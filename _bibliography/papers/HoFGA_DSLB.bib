@article{10.1137/1.9781611973099.48,
  title = {Using Hashing to Solve the Dictionary Problem (in External Memory)},
  year = {Unknown},
  journal = {Unknown},
  doi = {10.1137/1.9781611973099.48}
}

@article{10.3403/30197441,
  title = {Cellprobesurvey.Pdf},
  year = {Unknown},
  journal = {Unknown},
  doi = {10.3403/30197441},
  keywords = {cell probe}
}

@article{a.apostolicoSetUnionProblem1994,
  title = {The {{Set Union Problem With Unlimited Backtracking}}},
  author = {A. Apostolico and G. Italiano and G. Gambosi and M. Talamo},
  year = {1994},
  journal = {SIAM journal on computing (Print)},
  doi = {10.1137/S0097539789173597},
  abstract = {An extension of the disjoint set union problem is considered, where the extra primitive backtrack(\$i\$) can undo the last \$i\$ unions not yet undone. Let \$n\$ be the total number of elements in all the sets. A data structure is presented that supports each union and find in \$O({\textbackslash}log n/{\textbackslash}log {\textbackslash}log n)\$ worst-case time and each backtrack(\$i\$) in \$O(1)\$ worst-case time, irrespective of \$i\$. The total space required by the data structure is \$O(n)\$. A byproduct of this construction is a partially persistent data structure for the standard set union problem, capable of supporting union, find, and find-in-the-past operations, each in \$O({\textbackslash}log n/{\textbackslash}log {\textbackslash}log n)\$ worst-case time. All these upper bounds are tight for the class of separable-pointer algorithms as well as in the cell probe model of computation.},
  keywords = {cell probe,data structure},
  annotation = {Citation Count: 11}
}

@article{a.blassAlgorithmsQuestAbsolute2003,
  title = {Algorithms: {{A Quest}} for {{Absolute Definitions}}},
  author = {A. Blass and Y. Gurevich},
  year = {2003},
  journal = {Bull. EATCS},
  doi = {10.1515/9783110325461.24},
  abstract = {What is an algorithm? The interest in this foundational problem is not only theoretical; applications include specification, validation and verification of software and hardware systems. We describe the quest to understand and define the notion of algorithm. We start with the Church-Turing thesis and contrast Church's and Turing's approaches, and we finish with some recent investigations.},
  annotation = {Citation Count: 101}
}

@article{a.buchsbaumLinearTimeAlgorithmsDominators2008,
  title = {Linear-{{Time Algorithms}} for {{Dominators}} and {{Other Path-Evaluation Problems}}},
  author = {A. Buchsbaum and L. Georgiadis and Haim Kaplan and Anne Rogers and R. Tarjan and J. Westbrook},
  year = {2008},
  journal = {SIAM journal on computing (Print)},
  doi = {10.1137/070693217},
  abstract = {We present linear-time algorithms for the classic problem of finding dominators in a flowgraph, and for several other problems whose solutions require evaluating a function defined on paths in a tree. Although all these problems had linear-time solutions previously, our algorithms are simpler, in some cases substantially. Our improvements come from three new ideas: a refined analysis of path compression that gives a linear bound if the compressions favor certain nodes; replacement of random-access table look-up by a radix sort; and a more careful partitioning of a tree into easily managed parts. In addition to finding dominators, our algorithms find nearest common ancestors off-line, verify and construct minimum spanning trees, do interval analysis of a flowgraph, and build the component tree of a weighted tree. Our algorithms do not require the power of a random-access machine; they run in linear time on a pointer machine. The genesis of our work was the discovery of a subtle error in the analysis of a previous allegedly linear-time algorithm for finding dominators. That algorithm was an attempt to simplify a more complicated algorithm, which itself was intended to correct errors in a yet earlier algorithm. Our work provides a systematic study of the subtleties in the dominators problem, the techniques needed to solve it in linear time, and the range of application of the resulting methods. We have tried to make our techniques as simple and as general as possible and to understand exactly how earlier approaches to the dominators problem were either incorrect or overly complicated.},
  annotation = {Citation Count: 80}
}

@article{a.buchsbaumLineartimePointermachineAlgorithms1998,
  title = {Linear-Time Pointer-Machine Algorithms for Least Common Ancestors, {{MST}} Verification, and Dominators},
  author = {A. Buchsbaum and Haim Kaplan and Anne Rogers and J. Westbrook},
  year = {1998},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/276698.276764},
  abstract = {We present two new data structure tools---disjoint set union with bottom-up linking, and pointer-based radix sort---and combine them with bottom-level microtrees to devise the first linear-time pointer-machine algorithms for off-line least common ancestors, minimum spanning tree (MST) verification, randomized MST construction, and computing dominators in a flowgraph.},
  keywords = {data structure},
  annotation = {Citation Count: 53}
}

@article{a.buchsbaumNewSimplerLineartime1998,
  title = {A New, Simpler Linear-Time Dominators Algorithm},
  author = {A. Buchsbaum and Haim Kaplan and Anne Rogers and J. Westbrook},
  year = {1998},
  journal = {TOPL},
  doi = {10.1145/295656.295663},
  abstract = {We present a new linear-time algorithm to find the immediate dominators of all vertices in a flowgraph. Our algorithm is simpler than previous linear-time algorithms: rather than employ complicated data structures, we combine the use of microtrees and memoization with new observations on a restricted class of path compressions. We have implemented our algorithm, and we report experimental results that show that the constant factors are low. Compared to the standard, slightly superlinear algorithm of Lengauer and Tarjan, which has much less overhead, our algorithm runs 10-20\% slower on real flowgraphs of reasonable size and only a few percent slower on very large flowgraphs.},
  keywords = {data structure},
  annotation = {Citation Count: 70}
}

@article{a.c.kaporisDynamicInterpolationSearch2006,
  title = {Dynamic {{Interpolation Search Revisited}}},
  author = {A. C. Kaporis and C. Makris and S. Sioutas and A. Tsakalidis and K. Tsichlas and C. Zaroliagis},
  year = {2006},
  journal = {International Colloquium on Automata, Languages and Programming},
  doi = {10.1007/11786986_34},
  keywords = {dynamic},
  annotation = {Citation Count: 36}
}

@article{a.c.kaporisImprovedBoundsFinger2003,
  title = {Improved {{Bounds}} for {{Finger Search}} on a {{RAM}}},
  author = {A. C. Kaporis and C. Makris and S. Sioutas and A. Tsakalidis and K. Tsichlas and C. Zaroliagis},
  year = {2003},
  journal = {Algorithmica},
  doi = {10.1007/s00453-012-9636-4},
  annotation = {Citation Count: 28}
}

@article{a.d.paluEfficientParallelPointer2002,
  title = {An {{Efficient Parallel Pointer Machine Algorithm}} for the {{NCA Problem}}},
  author = {A. D. Pal{\`u} and Enrico Pontelli and D. Ranjan},
  year = {2002},
  journal = {IFIP TCS},
  doi = {10.1007/978-0-387-35608-2_14},
  annotation = {Citation Count: 1}
}

@article{a.d.paluSequentialParallelAlgorithms2006,
  title = {Sequential and Parallel Algorithms for the {{NCA}} Problem on Pure Pointer Machines},
  author = {A. D. Pal{\`u} and Enrico Pontelli and D. Ranjan},
  year = {2006},
  journal = {Theoretical Computer Science},
  doi = {10.1016/j.tcs.2005.10.040},
  annotation = {Citation Count: 1}
}

@article{a.deshpandeSubspaceApproximationOutliers2020,
  title = {Subspace Approximation with Outliers},
  author = {A. Deshpande and Rameshwar Pratap},
  year = {2020},
  journal = {International Computing and Combinatorics Conference},
  doi = {10.1007/978-3-030-58150-3_1},
  annotation = {Citation Count: 3}
}

@article{a.harrowTightSoSDegreeBounds2016,
  title = {Tight {{SoS-Degree Bounds}} for {{Approximate Nash Equilibria}}},
  author = {A. Harrow and Anand Natarajan and Xiaodi Wu},
  year = {2016},
  journal = {Cybersecurity and Cyberforensics Conference},
  doi = {10.4230/LIPIcs.CCC.2016.22},
  abstract = {Nash equilibria always exist, but are widely conjectured to require time to find that is exponential in the number of strategies, even for two-player games. By contrast, a simple quasi-polynomial time algorithm, due to Lipton, Markakis and Mehta (LMM), can find approximate Nash equilibria, in which no player can improve their utility by more than e by changing their strategy. The LMM algorithm can also be used to find an approximate Nash equilibrium with near-maximal total welfare. Matching hardness results for this optimization problem were found assuming the hardness of the planted-clique problem (by Hazan and Krauthgamer) and assuming the Exponential Time Hypothesis (by Braverman, Ko and Weinstein).    In this paper we consider the application of the sum-squares (SoS) algorithm from convex optimization to the problem of optimizing over Nash equilibria. We show the first unconditional lower bounds on the number of levels of SoS needed to achieve a constant factor approximation to this problem. While it may seem that Nash equilibria do not naturally lend themselves to convex optimization, we also describe a simple LP (linear programming) hierarchy that can find an approximate Nash equilibrium in time comparable to that of the LMM algorithm, although neither algorithm is obviously a generalization of the other. This LP can be viewed as arising from the SoS algorithm at log n levels -- matching our lower bounds. The lower bounds involve a modification of the Braverman-Ko-Weinstein embedding of CSPs into strategic games and techniques from sum-of-squares proof systems. The upper bound (i.e. analysis of the LP) uses information-theory techniques that have been recently applied to other linear- and semidefinite-programming hierarchies.},
  keywords = {information,lower bound},
  annotation = {Citation Count: 5}
}

@article{a.prokopenkoFastTreebasedAlgorithms2021,
  title = {Fast Tree-Based Algorithms for {{DBSCAN}} for Low-Dimensional Data on {{GPUs}}},
  author = {A. Prokopenko and {D. Lebrun-Grandi{\'e}} and Daniel Arndt},
  year = {2021},
  journal = {International Conference on Parallel Processing},
  doi = {10.1145/3605573.3605594},
  abstract = {DBSCAN is a well-known density-based clustering algorithm to discover arbitrary shape clusters. While conceptually simple in serial, the algorithm is challenging to efficiently parallelize on manycore GPU architectures. Common pitfalls, such as asynchronous range query calls, result in high thread execution divergence in many implementations. In this paper, we propose a new framework for GPU-accelerated DBSCAN, and describe two tree-based algorithms within that framework. Both algorithms fuse the search for neighbors with updating cluster information, but differ in their treatment of dense regions of the data. We show that the time taken to compute clusters is at most twice that of determination of the neighbors. We compare the proposed algorithms with existing CPU and GPU implementations, and demonstrate their competitiveness and performance using a fast traversal structure (bounding volume hierarchy) for low dimensional data. We also show that the memory usage can be reduced by processing object neighbors dynamically without storing them.},
  keywords = {dynamic,information,query},
  annotation = {Citation Count: 3}
}

@article{a.rubinsteinDetectingCommunitiesHard2016,
  title = {Detecting Communities Is {{Hard}} ({{And Counting Them}} Is {{Even Harder}})},
  author = {A. Rubinstein},
  year = {2016},
  journal = {Information Technology Convergence and Services},
  doi = {10.4230/LIPIcs.ITCS.2017.42},
  abstract = {We consider the algorithmic problem of community detection in networks. Given an undirected friendship graph \$G={\textbackslash}left(V,E{\textbackslash}right)\$, a subset \$S{\textbackslash}subseteq V\$ is an \${\textbackslash}left({\textbackslash}alpha,{\textbackslash}beta{\textbackslash}right)\$-community if:  * Every member of the community is friends with an \${\textbackslash}alpha\$-fraction of the community;  * Every non-member is friends with at most a \${\textbackslash}beta\$-fraction of the community.  Arora et al [AGSS12] gave a quasi-polynomial time algorithm for enumerating all the \${\textbackslash}left({\textbackslash}alpha,{\textbackslash}beta{\textbackslash}right)\$-communities for any constants \${\textbackslash}alpha{$>\backslash$}beta\$.  Here, we prove that, assuming the Exponential Time Hypothesis (ETH), quasi-polynomial time is in fact necessary - and even for a much weaker approximation desideratum. Namely, distinguishing between:  * \$G\$ contains an \${\textbackslash}left(1,o{\textbackslash}left(1{\textbackslash}right){\textbackslash}right)\$-community; and  * \$G\$ does not contain an \${\textbackslash}left({\textbackslash}beta+o{\textbackslash}left(1{\textbackslash}right),{\textbackslash}beta{\textbackslash}right)\$-community for any \${\textbackslash}beta{\textbackslash}in{\textbackslash}left[0,1{\textbackslash}right]\$.  We also prove that counting the number of \${\textbackslash}left(1,o{\textbackslash}left(1{\textbackslash}right){\textbackslash}right)\$-communities requires quasi-polynomial time assuming the weaker \#ETH.},
  annotation = {Citation Count: 10}
}

@article{a.rubinsteinHardnessApproximationNP2019,
  title = {Hardness of {{Approximation Between P}} and {{NP}}},
  author = {A. Rubinstein},
  year = {2019},
  journal = {Hardness of Approximation Between P and NP},
  doi = {10.1145/3241304},
  abstract = {Nash equilibrium is the central solution concept in Game Theory. Since Nash's original paper in 1951, it has found countless applications in modeling strategic behavior of traders in markets, (human) drivers and (electronic) routers in congested networks, nations in nuclear disarmament negotiations, and more. A decade ago, the relevance of this solution concept was called into question by computer scientists, who proved (under appropriate complexity assumptions) that computing a Nash equilibrium is an intractable problem. And if centralized, specially designed algorithms cannot find Nash equilibria, why should we expect distributed, selfish agents to converge to one? The remaining hope was that at least approximate Nash equilibria can be efficiently computed.    Understanding whether there is an efficient algorithm for approximate Nash equilibrium has been the central open problem in this field for the past decade. In this book, we provide strong evidence that even finding an approximate Nash equilibrium is intractable. We prove several intractability theorems for different settings (two-player games and many-player games) and models (computational complexity, query complexity, and communication complexity). In particular, our main result is that under a plausible and natural complexity assumption ("Exponential Time Hypothesis for PPAD"), there is no polynomial-time algorithm for finding an approximate Nash equilibrium in two-player games.    The problem of approximate Nash equilibrium in a two-player game poses a unique technical challenge: it is a member of the class PPAD, which captures the complexity of several fundamental total problems, i.e., problems that always have a solution; and it also admits a quasipolynomial time algorithm. Either property alone is believed to place this problem far below NP-hard problems in the complexity hierarchy; having both simultaneously places it just above P, at what can be called the frontier of intractability. Indeed, the tools we develop in this book to advance on this frontier are useful for proving hardness of approximation of several other important problems whose complexity lies between P and NP: Brouwer's fixed point, market equilibrium, CourseMatch (A-CEEI), densest k-subgraph, community detection, VC dimension and Littlestone dimension, and signaling in zero-sum games.},
  keywords = {communication,communication complexity,query,query complexity},
  annotation = {Citation Count: 9}
}

@article{a.rubinsteinSettlingComplexityComputing2016,
  title = {Settling the {{Complexity}} of {{Computing Approximate Two-Player Nash Equilibria}}},
  author = {A. Rubinstein},
  year = {2016},
  journal = {IEEE Annual Symposium on Foundations of Computer Science},
  doi = {10.1145/3055589.3055596},
  abstract = {We prove that there exists a constant {$\varepsilon$} {$>$} 0 such that, assuming the Exponential Time Hypothesis for PPAD, computing an {$\varepsilon$}-approximate Nash equilibrium in a two-player (n {\texttimes} n) game requires quasi-polynomial time, nlog1-o(1) n. This matches (up to the o(1) term) the algorithm of Lipton, Markakis, and Mehta [54]. Our proof relies on a variety of techniques from the study of probabilistically checkable proofs (PCP), this is the first time that such ideas are used for a reduction between problems inside PPAD. En route, we also prove new hardness results for computing Nash equilibria in games with many players. In particular, we show that computing an {$\varepsilon$}-approximate Nash equilibrium in a game with n players requires 2{\textohm}(n) oracle queries to the payoff tensors. This resolves an open problem posed by Hart and Nisan [43], Babichenko [13], and Chen et al. [28]. In fact, our results for n-player games are stronger: they hold with respect to the ({$\varepsilon$},{$\delta$})-WeakNash relaxation recently introduced by Babichenko et al. [15].},
  keywords = {query,reduction},
  annotation = {Citation Count: 138}
}

@article{a.tsakalidisNearestCommonAncestor1987,
  title = {The Nearest Common Ancestor in a Dynamic Tree},
  author = {A. Tsakalidis},
  year = {1987},
  journal = {Acta Informatica},
  doi = {10.1007/BF00268844},
  keywords = {dynamic},
  annotation = {Citation Count: 13}
}

@article{a.yaoShouldTablesBe1981,
  title = {Should {{Tables Be Sorted}}?},
  author = {A. Yao},
  year = {1981},
  journal = {JACM},
  doi = {10.1145/322261.322274},
  abstract = {We examine optimality questions in the following information retrieval problem: Given a set S of n keys, store them so that queries of the form "Is x \${\textbackslash}in\$ S?" can be answered quickly. It is shown that, in a rather general model including al1 the commonly-used schemes, \${\textbackslash}lceil\$ lg(n+l) \${\textbackslash}rceil\$ probes to the table are needed in the worst case, provided the key space is sufficiently large. The effects of smaller key space and arbitrary encoding are also explored.},
  keywords = {query},
  annotation = {Citation Count: 326}
}

@article{aamandOptimalDecrementalConnectivity2021,
  title = {Optimal Decremental Connectivity in Non-Sparse Graphs},
  author = {Aamand, Anders and Karczmarz, Adam and Lacki, Jakub and Parotsidis, Nikos and Rasmussen, P. M. R. and Thorup, M.},
  year = {2021},
  doi = {10.4230/LIPIcs.ICALP.2023.6},
  abstract = {We present a dynamic algorithm for maintaining the connected and 2-edge-connected components in an undirected graph subject to edge deletions. The algorithm is Monte-Carlo randomized and processes any sequence of edge deletions in O(m+n\{polylog\}n) total time. Interspersed with the deletions, it can answer queries to whether any two given vertices currently belong to the same (2-edge-)connected component in constant time. Our result is based on a general Monte-Carlo randomized reduction from decremental c-edge-connectivity to a variant of fully-dynamic c-edge-connectivity on a sparse graph. While being Monte-Carlo, our reduction supports a certain final self-check that can be used in Las Vegas algorithms for static problems such as Unique Perfect Matching. For non-sparse graphs with {\textohm}(n\{polylog\}n) edges, our connectivity and 2-edge-connectivity algorithms handle all deletions in optimal linear total time, using existing algorithms for the respective fully-dynamic problems. This improves upon an O(m(n{$^2$}/m)+n\{polylog\}n)-time algorithm of Thorup [J.Alg. 1999], which runs in linear time only for graphs with {\textohm}(n{$^2$}) edges. Our constant amortized cost for edge deletions in decremental connectivity in non-sparse graphs should be contrasted with an {\textohm}(n/n) worst-case time lower bound in the decremental setting [Alstrup, Thore Husfeldt, FOCS'98] as well as an {\textohm}(n) amortized time lower-bound in the fully-dynamic setting [Patrascu and Demaine STOC'04].},
  citationcount = {9},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {dynamic,lower bound,query,reduction,static}
}

@article{aaronsonAlgebrizationANew2008,
  title = {Algebrization: A New Barrier in Complexity Theory},
  author = {Aaronson, S. and Wigderson, A.},
  year = {2008},
  doi = {10.1145/1374376.1374481},
  abstract = {Any proof of P!=NP will have to overcome two barriers: relativization and natural proofs. Yet over the last decade, we have seen circuit lower bounds (for example, that PP does not have linear-size circuits) that overcome both barriers simultaneously. So the question arises of whether there is a third barrier to progress on the central questions in complexity theory. In this paper we present such a barrier, which we call algebraic relativization or algebrization. The idea is that, when we relativize some complexity class inclusion, we should give the simulating machine access not only to an oracle A, but also to a low-degree extension of A over a finite field or ring. We systematically go through basic results and open problems in complexity theory to delineate the power of the new algebrization barrier. First, we show that all known non-relativizing results based on arithmetization -- both inclusions such as IP=PSPACE and MIP=NEXP, and separations such as MAEXP not in P/poly -- do indeed algebrize. Second, we show that almost all of the major open problems -- including P versus NP, P versus RP, and NEXP versus P/poly -- will require non-algebrizing techniques. In some cases algebrization seems to explain exactly why progress stopped where it did: for example, why we have superlinear circuit lower bounds for PromiseMA but not for NP. Our second set of results follows from lower bounds in a new model of algebraic query complexity, which we introduce in this paper and which is interesting in its own right. Some of our lower bounds use direct combinatorial and algebraic arguments, while others stem from a surprising connection between our model and communication complexity. Using this connection, we are also able to give an MA-protocol for the Inner Product function with O(sqrt(n) log n) communication (essentially matching a lower bound of Klauck).},
  citationcount = {263},
  venue = {Symposium on the Theory of Computing}
}

@article{abadiSecurityAnalysisOf2005,
  title = {Security Analysis of Cryptographically Controlled Access to {{XML}} Documents},
  author = {Abadi, M. and Warinschi, B.},
  year = {2005},
  doi = {10.1145/1065167.1065182},
  abstract = {Some promising recent schemes for XML access control employ encryption for implementing security policies on published data, avoiding data duplication. In this paper we study one such scheme, due to Miklau and Suciu. That scheme was introduced with some intuitive explanations and goals, but without precise definitions and guarantees for the use of cryptography (specifically, symmetric encryption and secret sharing). We bridge this gap in the present work. We analyze the scheme in the context of the rigorous models of modern cryptography. We obtain formal results in simple, symbolic terms close to the vocabulary of Miklau and Suciu. We also obtain more detailed computational results that establish security against probabilistic polynomial-time adversaries. Our approach, which relates these two layers of the analysis, continues a recent thrust in security research and may be applicable to a broad class of systems that rely on cryptographic data protection.},
  citationcount = {129},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems}
}

@article{abbottIntersectionTheoremsFor1972,
  title = {Intersection Theorems for Systems of Sets},
  author = {Abbott, H. L. and Hanson, D. and Sauer, N.},
  year = {1972},
  doi = {10.1016/0097-3165(72)90103-3},
  abstract = {No abstract available},
  citationcount = {366},
  venue = {Journal of Combinatorial Theory}
}

@article{abboudAllPairsMax2023,
  title = {All-Pairs Max-Flow Is No Harder than Single-Pair Max-Flow: {{Gomory-hu}} Trees in Almost-Linear Time},
  author = {Abboud, Amir and Li, Jason and Panigrahi, Debmalya and Saranurak, Thatchaphol},
  year = {2023},
  doi = {10.1109/FOCS57990.2023.00137},
  abstract = {A Gomory-Hu tree (also called a cut tree) succinctly represents (s,t) min-cuts (and therefore, (s,t) max-flow values) of all pairs of vertices s,t in an undirected graph. In this paper, we give an m\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}-time algorithm for constructing a Gomory-Hu tree for a graph with m edges. This shows that the all-pairs max-flows problem has the same running time as the single-pair max-flow problem, up to a subpolynomial factor. Prior to our work, the best known Gomory-Hu tree algorithm was obtained in recent work by Abboud et al. (FOCS 2022) and requires O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2\vphantom\{\}) time for a graph with n vertices. Our result marks a natural culmination of over 60 years of research into the all-pairs maxflows problem that started with Gomory and Hu's pathbreaking result introducing the Gomory-Hu tree in 1961.},
  citationcount = {10},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{abboudConsequencesOfFaster2014,
  title = {Consequences of Faster Alignment of Sequences},
  author = {Abboud, Amir and Williams, V. V. and Weimann, Oren},
  year = {2014},
  doi = {10.1007/978-3-662-43948-7_4},
  abstract = {No abstract available},
  citationcount = {155},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{abboudDistributedPcpTheorems2017,
  title = {Distributed {{PCP}} Theorems for Hardness of Approximation in {{P}}},
  author = {Abboud, Amir and Rubinstein, A. and Williams, Richard Ryan},
  year = {2017},
  doi = {10.1109/FOCS.2017.12},
  abstract = {We present a new distributed\vphantom\{\} model of probabilistically checkable proofs (PCP). A satisfying assignment x \&\#x220A;  0,1 {\textasciicircum}n to a CNF formula {$\phi$}is shared between two parties, where Alice knows x\_1, {\dots}, x\_\{n/2, Bob knows x\_\{n/2+1\},{\dots},x\_n, and both parties know {$\phi$}. The goal is to have Alice and Bob jointly write a PCP that x satisfies {$\phi$}, while exchanging little or no information. Unfortunately, this model as-is does not allow for nontrivial query complexity. Instead, we focus on a non-deterministic\} variant, where the players are helped by Merlin, a third party who knows all of x.Using our framework, we obtain, for the first time, PCP-like reductions from the Strong Exponential Time Hypothesis (SETH) to approximation problems in {\P}. In particular, under SETH we show that},
  citationcount = {91},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {query,query complexity,reduction}
}

@article{abboudDynamicSetCover2019,
  title = {Dynamic Set Cover: Improved Algorithms and Lower Bounds},
  author = {Abboud, Amir and Addanki, Raghavendra and Grandoni, F. and Panigrahi, Debmalya and Saha, B.},
  year = {2019},
  doi = {10.1145/3313276.3316376},
  abstract = {We give new upper and lower bounds for the dynamic set cover problem. First, we give a (1+{\cyrchar\cyrie}) f-approximation for fully dynamic set cover in O(f2logn/{\cyrchar\cyrie}5) (amortized) update time, for any {\cyrchar\cyrie} {\textquestiondown} 0, where f is the maximum number of sets that an element belongs to. In the decremental setting, the update time can be improved to O(f2/{\cyrchar\cyrie}5), while still obtaining an (1+{\cyrchar\cyrie}) f-approximation. These are the first algorithms that obtain an approximation factor linear in f for dynamic set cover, thereby almost matching the best bounds known in the offline setting and improving upon the previous best approximation of O(f2) in the dynamic setting. To complement our upper bounds, we also show that a linear dependence of the update time on f is necessary unless we can tolerate much worse approximation factors. Using the recent distributed PCP-framework, we show that any dynamic set cover algorithm that has an amortized update time of O(f1-{\cyrchar\cyrie}) must have an approximation factor that is {\textohm}(n{$\delta$}) for some constant {$\delta$}{\textquestiondown}0 under the Strong Exponential Time Hypothesis.},
  citationcount = {35},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,lower bound,update,update time}
}

@article{abboudExactWeightSubgraphs2013,
  title = {Exact Weight Subgraphs and the K-{{Sum}} Conjecture},
  author = {Abboud, Amir and Lewi, Kevin},
  year = {2013},
  doi = {10.1007/978-3-642-39206-1_1},
  abstract = {No abstract available},
  citationcount = {58},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{abboudFasterCombinatorialK2024,
  title = {Faster Combinatorial K-{{Clique}} Algorithms},
  author = {Abboud, A. and Fischer, Nick and Shechter, Yarin},
  year = {2024},
  doi = {10.48550/arXiv.2401.13502},
  abstract = {Detecting if a graph contains a k-Clique is one of the most fundamental problems in computer science. The asymptotically fastest algorithm runs in time O(n\textsuperscript{\{\vphantom\}}{$\omega$}k/3\vphantom\{\}), where {$\omega$} is the exponent of Boolean matrix multiplication. To date, this is the only technique capable of beating the trivial O(n\textsuperscript{k}) bound by a polynomial factor. Due to this technique's various limitations, much effort has gone into designing"combinatorial"algorithms that improve over exhaustive search via other techniques. The first contribution of this work is a faster combinatorial algorithm for k-Clique, improving Vassilevska's bound of O(n\textsuperscript{\{\vphantom\}}k\vphantom\{\}/\textsuperscript{\{\vphantom\}}k-1\vphantom\{\}\{n\}) by two log factors. Technically, our main result is a new reduction from k-Clique to Triangle detection that exploits the same divide-and-conquer at the core of recent combinatorial algorithms by Chan (SODA'15) and Yu (ICALP'15). Our second contribution is exploiting combinatorial techniques to improve the state-of-the-art (even of non-combinatorial algorithms) for generalizations of the k-Clique problem. In particular, we give the first o(n\textsuperscript{k}) algorithm for k-clique in hypergraphs and an O(n{$^3$}/\textsuperscript{\{\vphantom\}}2.25\vphantom\{\}\{n\}+t) algorithm for listing t triangles in a graph.},
  citationcount = {5},
  venue = {Latin American Symposium on Theoretical Informatics},
  keywords = {reduction}
}

@article{abboudFineGrainedComplexity2017,
  title = {Fine-Grained Complexity of Analyzing Compressed Data: {{Quantifying}} Improvements over Decompress-and-Solve},
  author = {Abboud, Amir and Backurs, A. and Bringmann, K. and K{\"u}nnemann, Marvin},
  year = {2017},
  doi = {10.1109/FOCS.2017.26},
  abstract = {Can we analyze data without decompressing it? As our data keeps growing, understanding the time complexity of problems on compressed inputs, rather than in convenient uncompressed forms, becomes more and more relevant. Suppose we are given a compression of size n of data that originally has size N, and we want to solve a problem with time complexity T(\&\#x22C5;). The na\&\#x00EF;ve strategy of decompress-and-solve gives time T(N), whereas the gold standard is time T(n): to analyze the compression as efficiently as if the original data was small.We restrict our attention to data in the form of a string (text, files, genomes, etc.) and study the most ubiquitous tasks. While the challenge might seem to depend heavily on the specific compression scheme, most methods of practical relevance (Lempel-Ziv-family, dictionary methods, and others) can be unified under the elegant notion of Grammar-Compressions. A vast literature, across many disciplines, established this as an influential notion for Algorithm design.We introduce a direly needed framework for proving (conditional) lower bounds in this field, allowing us to assess whether decompress-and-solve can be improved, and by how much. Our main results are:\&\#x2022; The O(nN\&\#x221A;log(N/n)) bound for LCS and the O(min(N log N, nM)) bound for Pattern Matching with Wildcards are optimal up to N{\textasciicircum}\{o(1)\} factors, under the Strong Exponential Time Hypothesis. (Here, M denotes the uncompressed length of the compressed pattern.)\&\#x2022; Decompress-and-solve is essentially optimal for Context-Free Grammar Parsing and RNA Folding, under the k-Clique conjecture.\&\#x2022; We give an algorithm showing that decompress-and-solve is not optimal for Disjointness.},
  citationcount = {38},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {lower bound}
}

@article{abboudFineGrainedHardness2021,
  title = {Fine-Grained Hardness for Edit Distance to a Fixed Sequence},
  author = {Abboud, Amir and Williams, V. V.},
  year = {2021},
  doi = {10.4230/LIPIcs.ICALP.2021.7},
  abstract = {Nearly all quadratic lower bounds conditioned on the Strong Exponential Time Hypothesis (SETH) start by reducing k-SAT to the Orthogonal Vectors (OV) problem: Given two sets A,B of n binary vectors, decide if there is an orthogonal pair a {$\in$} A, b {$\in$} B. In this paper, we give an alternative reduction in which the set A does not depend on the input to k-SAT; thus, the quadratic lower bound for OV holds even if one of the sets is fixed in advance. Using the reductions in the literature from OV to other problems such as computing similarity measures on strings, we get hardness results of a stronger kind: there is a family of sequences \{Sn\}n=1, {\textbar}Sn{\textbar} = n such that computing the Edit Distance between an input sequence X of length n and the (fixed) sequence Sn requires n2-o(1) time under SETH. 2012 ACM Subject Classification Theory of computation {$\rightarrow$} Design and analysis of algorithms},
  citationcount = {3},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {lower bound,reduction}
}

@article{abboudFoolingViewsA2017,
  title = {Fooling Views: A New Lower Bound Technique for Distributed Computations under Congestion},
  author = {Abboud, Amir and {Censor-Hillel}, K. and Khoury, Seri and Lenzen, C.},
  year = {2017},
  doi = {10.1007/s00446-020-00373-4},
  abstract = {No abstract available},
  citationcount = {29},
  venue = {Distributed computing},
  keywords = {lower bound}
}

@article{abboudFriendlyCutSparsifiers2021,
  title = {Friendly Cut Sparsifiers and Faster Gomory-Hu Trees},
  author = {Abboud, Amir and Krauthgamer, Robert and Trabelsi, Ohad},
  year = {2021},
  doi = {10.1137/1.9781611977073.143},
  abstract = {We devise new cut sparsifiers that are related to the classical sparsification of Nagamochi and Ibaraki [Algorithmica, 1992], which is an algorithm that, given an unweighted graph G on n nodes and a parameter k, computes a subgraph with O(nk) edges that preserves all cuts of value up to k. We put forward the notion of a friendly cut sparsifier, which is a minor of G that preserves all friendly cuts of value up to k, where a cut in G is called friendly if every node has more edges connecting it to its own side of the cut than to the other side. We present an algorithm that, given a simple graph G, computes in almost-linear time a friendly cut sparsifier with O\vphantom\{\}(n{\textsurd}\{k\}) edges. Using similar techniques, we also show how, given in addition a terminal set T, one can compute in almost-linear time a terminal sparsifier, which preserves the minimum st-cut between every pair of terminals, with O\vphantom\{\}(n{\textsurd}\{k\}+{\textbar}T{\textbar}k) edges. Plugging these sparsifiers into the recent n\textsuperscript{\{\vphantom\}}2+o(1)\vphantom\{\}-time algorithms for constructing a Gomory-Hu tree of simple graphs, along with a relatively simple procedure for handling the unfriendly minimum cuts, we improve the running time for moderately dense graphs (e.g., with m=n\textsuperscript{\{\vphantom\}}1.75\vphantom\{\} edges). In particular, assuming a linear-time Max-Flow algorithm, the new state-of-the-art for Gomory-Hu tree is the minimum between our (m+n\textsuperscript{\{\vphantom\}}1.75\vphantom\{\})\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\} and the known mn\textsuperscript{\{\vphantom\}}1/2+o(1)\vphantom\{\}. We further investigate the limits of this approach and the possibility of better sparsification. Under the hypothesis that an O\vphantom\{\}(n)-edge sparsifier that preserves all friendly minimum st-cuts can be computed efficiently, our upper bound improves to O\vphantom\{\}(m+n\textsuperscript{\{\vphantom\}}1.5\vphantom\{\}) which is the best possible without breaking the cubic barrier for constructing Gomory-Hu trees in non-simple graphs.},
  citationcount = {12},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{abboudHardnessOfApproximation2022,
  title = {Hardness of Approximation in p via Short Cycle Removal: Cycle Detection, Distance Oracles, and Beyond},
  author = {Abboud, Amir and Bringmann, K. and Khoury, Seri and Zamir, Or},
  year = {2022},
  doi = {10.1145/3519935.3520066},
  abstract = {We present a new technique for efficiently removing almost all short cycles in a graph without unintentionally removing its triangles. Consequently, triangle finding problems do not become easy even in almost k-cycle free graphs, for any constant k{$\geq$} 4. Triangle finding is at the base of many conditional lower bounds in P, mainly for distance computation problems, and the existence of many 4- or 5-cycles in a worst-case instance had been the obstacle towards resolving major open questions. Hardness of approximation: Are there distance oracles with m1+o(1) preprocessing time and mo(1) query time that achieve a constant approximation? Existing algorithms with such desirable time bounds only achieve super-constant approximation factors, while only 3- factors were conditionally ruled out (P{\u a}tra{\c s}cu, Roditty, and Thorup; FOCS 2012). We prove that no O(1) approximations are possible, assuming the 3-SUM or APSP conjectures. In particular, we prove that k-approximations require {\textohm}(m1+1/ck) time, which is tight up to the constant c. The lower bound holds even for the offline version where we are given the queries in advance, and extends to other problems such as dynamic shortest paths. The 4-Cycle problem: An infamous open question in fine-grained complexity is to establish any surprising consequences from a subquadratic or even linear-time algorithm for detecting a 4-cycle in a graph. This is arguably one of the simplest problems without a near-linear time algorithm nor a conditional lower bound. We prove that {\textohm}(m1.1194) time is needed for k-cycle detection for all k{$\geq$} 4, unless we can detect a triangle in {\textsurd}n-degree graphs in O(n2-{$\delta$}) time; a breakthrough that is not known to follow even from optimal matrix multiplication algorithms.},
  citationcount = {26},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,lower bound,query,query time}
}

@article{abboudListing4Cycles2022,
  title = {Listing 4-Cycles},
  author = {Abboud, Amir and Khoury, Seri and Leibowitz, Oree and Safier, Ron},
  year = {2022},
  doi = {10.48550/arXiv.2211.10022},
  abstract = {In this note we present an algorithm that lists all 4-cycles in a graph in time O\vphantom\{\}((n{$^2$},m\textsuperscript{\{\vphantom\}}4/3\vphantom\{\})+t) where t is their number. Notably, this separates 4-cycle listing from triangle-listing, since the latter has a ((n{$^3$},m\textsuperscript{\{\vphantom\}}3/2\vphantom\{\})+t)\textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\} lower bound under the 3-SUM Conjecture. Our upper bound is conditionally tight because (1) O(n{$^2$},m\textsuperscript{\{\vphantom\}}4/3\vphantom\{\}) is the best known bound for detecting if the graph has any 4-cycle, and (2) it matches a recent ((n{$^3$},m\textsuperscript{\{\vphantom\}}3/2\vphantom\{\})+t)\textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\} 3-SUM lower bound for enumeration algorithms. The latter lower bound was proved very recently by Abboud, Bringmann, and Fischer [arXiv, 2022] and independently by Jin and Xu [arXiv, 2022]. In an independent work, Jin and Xu [arXiv, 2022] also present an algorithm with the same time bound.},
  citationcount = {8},
  venue = {Foundations of Software Technology and Theoretical Computer Science},
  keywords = {lower bound}
}

@article{abboudLosingWeightBy2013,
  title = {Losing Weight by Gaining Edges},
  author = {Abboud, Amir and Lewi, Kevin and Williams, Ryan},
  year = {2013},
  doi = {10.1007/978-3-662-44777-2_1},
  abstract = {No abstract available},
  citationcount = {50},
  venue = {Embedded Systems and Applications}
}

@article{abboudMatchingTrianglesAnd2015,
  title = {Matching Triangles and Basing Hardness on an Extremely Popular Conjecture},
  author = {Abboud, Amir and Williams, V. V. and Yu, Huacheng},
  year = {2015},
  doi = {10.1145/2746539.2746594},
  abstract = {Due to the lack of unconditional polynomial lower bounds, it is now in fashion to prove conditional lower bounds in order to advance our understanding of the class P. The vast majority of these lower bounds are based on one of three famous hypotheses: the 3-SUM conjecture, the APSP conjecture, and the Strong Exponential Time Hypothesis. Only circumstantial evidence is known in support of these hypotheses, and no formal relationship between them is known. In hopes of obtaining "less conditional" and therefore more reliable lower bounds, we consider the conjecture that at least one of the above three hypotheses is true. We design novel reductions from 3-SUM, APSP, and CNF-SAT, and derive interesting consequences of this very plausible conjecture, including: Tight n3-o(1) lower bounds for purely-combinatorial problems about the triangles in unweighted graphs. New n1-o(1) lower bounds for the amortized update and query times of dynamic algorithms for single-source reachability, strongly connected components, and Max-Flow. New n1.5-o(1) lower bound for computing a set of n st-maximum-flow values in a directed graph with n nodes and  O(n) edges. There is a hierarchy of natural graph problems on n nodes with complexity nc for c {$\in$} (2,3). Only slightly non-trivial consequences of this conjecture were known prior to our work. Along the way we also obtain new conditional lower bounds for the Single-Source-Max-Flow problem.},
  citationcount = {120},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,lower bound,query,query time,reduction,update}
}

@article{abboudMoreApplicationsOf2015,
  title = {More Applications of the Polynomial Method to Algorithm Design},
  author = {Abboud, Amir and Williams, Richard Ryan and Yu, Huacheng},
  year = {2015},
  doi = {10.1137/1.9781611973730.17},
  abstract = {In low-depth circuit complexity, the polynomial method is a way to prove lower bounds by translating weak circuits into low-degree polynomials, then analyzing properties of these polynomials. Recently, this method found an application to algorithm design: Williams (STOC 2014) used it to compute all-pairs shortest paths in n3/2{\textohm}([EQUATION]log n) time on dense n-node graphs. In this paper, we extend this methodology to solve a number of problems in combinatorial pattern matching and Boolean algebra, considerably faster than previously known methods. First, we give an algorithm for Boolean Orthogonal Detection, which is to detect among two sets A, B {$\subseteq$} \{0, 1\}d of size n if there is an x {$\in$} A and y {$\in$} B such that {\textlangle}x, y{\textrangle} = 0. For vectors of dimension d = c(n) log n, we solve Boolean Orthogonal Detection in n2-1/O(log c(n)) time by a Monte Carlo randomized algorithm. We apply this as a subroutine in several other new algorithms: {$\bullet$} In Batch Partial Match, we are given n query strings from from \{0, 1, *\}c(n)log n (* is a "don't care"), n strings from \{0, 1\}c(n)log n, and wish to determine for each query whether or not there is a string matching the query. We solve this problem in n2-1/O(log c(n)) time by a Monte Carlo randomized algorithm. {$\bullet$} Let t {$\leq$} v be integers. Given a DNF F on c log t variables with t terms, and v arbitrary assignments on the variables, F can be evaluated on all v assignments in v {$\cdot$} t1-1/O(log c) time, with high probability. {$\bullet$} There is a randomized algorithm that solves the Longest Common Substring with don't cares problem on two strings of length n in n2/2{\textohm}([EQUATION]log n) time. {$\bullet$} Given two strings S, T of length n, there is a randomized algorithm that computes the length of the longest substring of S that has Edit-Distance less than k to a substring of T in k1.5n2/2{\textohm}([EQUATION]) time. {$\bullet$} Symmetric Boolean Constraint Satisfaction Problems (CSPs) with n variables and m constraints are solvable in poly(m) {$\cdot$} 2n(1-1/O(log mn)) time.},
  citationcount = {145},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {lower bound,query}
}

@article{abboudMoreConsequencesOf2018,
  title = {More Consequences of Falsifying {{SETH}} and the Orthogonal Vectors Conjecture},
  author = {Abboud, Amir and Bringmann, K. and Dell, Holger and Nederlof, Jesper},
  year = {2018},
  doi = {10.1145/3188745.3188938},
  abstract = {The Strong Exponential Time Hypothesis and the OV-conjecture are two popular hardness assumptions used to prove a plethora of lower bounds, especially in the realm of polynomial-time algorithms. The OV-conjecture in moderate dimension states there is no {$\varepsilon$}{\textquestiondown}0 for which an O(N2-{$\varepsilon$}) poly(D) time algorithm can decide whether there is a pair of orthogonal vectors in a given set of size N that contains D-dimensional binary vectors. We strengthen the evidence for these hardness assumptions. In particular, we show that if the OV-conjecture fails, then two problems for which we are far from obtaining even tiny improvements over exhaustive search would have surprisingly fast algorithms. If the OV conjecture is false, then there is a fixed {$\varepsilon$}{\textquestiondown}0 such that: - For all d and all large enough k, there is a randomized algorithm that takes O(n(1-{$\varepsilon$})k) time to solve the Zero-Weight-k-Clique and Min-Weight-k-Clique problems on d-hypergraphs with n vertices. As a consequence, the OV-conjecture is implied by the Weighted Clique conjecture. - For all c, the satisfiability of sparse TC1 circuits on n inputs (that is, circuits with cn wires, depth clogn, and negation, AND, OR, and threshold gates) can be computed in time O((2-{$\varepsilon$})n).},
  citationcount = {40},
  venue = {Symposium on the Theory of Computing},
  keywords = {lower bound}
}

@article{abboudNewGraphDecompositions2023,
  title = {New Graph Decompositions and Combinatorial Boolean Matrix Multiplication Algorithms},
  author = {Abboud, Amir and Fischer, Nick and Kelley, Zander and Lovett, Shachar and Meka, Raghu},
  year = {2023},
  doi = {10.1145/3618260.3649696},
  abstract = {We revisit the fundamental Boolean Matrix Multiplication (BMM) problem. With the invention of algebraic fast matrix multiplication over 50 years ago, it also became known that BMM can be solved in truly subcubic O(n{$\omega$}) time, where {$\omega$}{\textexclamdown}3; much work has gone into bringing {$\omega$} closer to 2. Since then, a parallel line of work has sought comparably fast combinatorial algorithms but with limited success. The na'ive O(n3)-time algorithm was initially improved by a log2n factor [Arlazarov et al.; RAS'70], then by log2.25n [Bansal and Williams; FOCS'09], then by log3n [Chan; SODA'15], and finally by log4n [Yu; ICALP'15]. We design a combinatorial algorithm for BMM running in time n3 / 2{\textohm}((logn)1/7) -- a speed-up over cubic time that is stronger than any poly-log factor. This comes tantalizingly close to refuting the conjecture from the 90s that truly subcubic combinatorial algorithms for BMM are impossible. This popular conjecture is the basis for dozens of fine-grained hardness results. Our main technical contribution is a new regularity decomposition theorem for Boolean matrices (or equivalently, bipartite graphs) under a notion of regularity that was recently introduced and analyzed analytically in the context of communication complexity [Kelley, Lovett, Meka; STOC'24], and is related to a similar notion from the recent work on 3-term arithmetic progression free sets [Kelley, Meka; FOCS'23].},
  citationcount = {8},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {communication,communication complexity}
}

@article{abboudNewHardnessResults2020,
  title = {New Hardness Results for Planar Graph Problems in p and an Algorithm for Sparsest Cut},
  author = {Abboud, Amir and {Cohen-Addad}, Vincent and Klein, P.},
  year = {2020},
  doi = {10.1145/3357713.3384310},
  abstract = {The Sparsest Cut is a fundamental optimization problem that have been extensively studied. For planar inputs the problem is in P and can be solved in {\~O}(n 3 ) time if all vertex weights are 1. Despite a significant amount of effort, the best algorithms date back to the early 90's and can only achieve O(log n)-approximation in {\~O}(n) time or 3.5-approximation in {\~O}(n 2 ) time [Rao, STOC92]. Our main result is an {\textohm}(n 2-{$\varepsilon$} ) lower bound for Sparsest Cut even in planar graphs with unit vertex weights, under the (min, +)-Convolution conjecture, showing that approxima- tions are inevitable in the near-linear time regime. To complement the lower bound, we provide a 3.3-approximation in near-linear time, improving upon the 25-year old result of Rao in both time and accuracy. We also show that our lower bound is not far from optimal by observing an exact algorithm with running time {\~O}(n 5/2 ) improving upon the {\~O}(n 3 ) algorithm of Park and Phillips [STOC93]. Our lower bound accomplishes a repeatedly raised challenge by being the first fine-grained lower bound for a natural planar graph problem in P. Building on our construction we prove near-quadratic lower bounds under SETH for variants of the closest pair problem in planar graphs, and use them to show that the popular Average-Linkage procedure for Hierarchical Clustering cannot be simulated in truly subquadratic time. At the core of our constructions is a diamond-like gadget that also settles the complexity of Diameter in distributed planar networks. We prove an {\textohm}(n/ log n) lower bound on the number of communication rounds required to compute the weighted diameter of a network in the CONGET model, even when the underlying graph is planar and all nodes are D = 4 hops away from each other. This is the first poly(n) lower bound in the planar-distributed setting, and it complements the recent poly(D, log n) upper bounds of Li and Parter [STOC 2019] for (exact) unweighted diameter and for (1 + {$\varepsilon$}) approximate weighted diameter.},
  citationcount = {12},
  venue = {Symposium on the Theory of Computing},
  keywords = {communication,lower bound}
}

@article{abboudOnDiameterApproximation2023,
  title = {On Diameter Approximation in Directed Graphs},
  author = {Abboud, Amir and Dalirrooyfard, Mina and Li, Ray and Williams, V. V.},
  year = {2023},
  doi = {10.48550/arXiv.2307.07583},
  abstract = {Computing the diameter of a graph, i.e. the largest distance, is a fundamental problem that is central in fine-grained complexity. In undirected graphs, the Strong Exponential Time Hypothesis (SETH) yields a lower bound on the time vs. approximation trade-off that is quite close to the upper bounds. In \{directed\} graphs, however, where only some of the upper bounds apply, much larger gaps remain. Since d(u,v) may not be the same as d(v,u), there are multiple ways to define the problem, the two most natural being the \{(one-way) diameter\} (\textsubscript{\{\vphantom\}}(u,v)\vphantom\{\}d(u,v)) and the \{roundtrip diameter\} (\textsubscript{\{\vphantom\}}u,v\vphantom\{\}d(u,v)+d(v,u)). In this paper we make progress on the outstanding open question for each of them. -- We design the first algorithm for diameter in sparse directed graphs to achieve n\textsuperscript{\{\vphantom\}}1.5-{$\varepsilon$}\vphantom\{\} time with an approximation factor better than 2. The new upper bound trade-off makes the directed case appear more similar to the undirected case. Notably, this is the first algorithm for diameter in sparse graphs that benefits from fast matrix multiplication. -- We design new hardness reductions separating roundtrip diameter from directed and undirected diameter. In particular, a 1.5-approximation in subquadratic time would refute the All-Nodes k-Cycle hypothesis, and any (2-{$\varepsilon$})-approximation would imply a breakthrough algorithm for approximate {$\ell$}\textsubscript{\{\vphantom\}}{$\infty$}\vphantom\{\}-Closest-Pair. Notably, these are the first conditional lower bounds for diameter that are not based on SETH.},
  citationcount = {1},
  venue = {Embedded Systems and Applications},
  keywords = {lower bound,reduction}
}

@article{abboudOnTheFine2020,
  title = {On the Fine-Grained Complexity of Parity Problems},
  author = {Abboud, Amir and Feller, S. and Weimann, Oren},
  year = {2020},
  doi = {10.4230/LIPIcs.ICALP.2020.5},
  abstract = {We consider the parity variants of basic problems studied in fine-grained complexity. We show that finding the exact solution is just as hard as finding its parity (i.e. if the solution is even or odd) for a large number of classical problems, including All-Pairs Shortest Paths (APSP), Diameter, Radius, Median, Second Shortest Path, Maximum Consecutive Subsums, Min-Plus Convolution, and 0/1-Knapsack. A direct reduction from a problem to its parity version is often difficult to design. Instead, we revisit the existing hardness reductions and tailor them in a problem-specific way to the parity version. Nearly all reductions from APSP in the literature proceed via the (subcubic-equivalent but simpler) Negative Weight Triangle (NWT) problem. Our new modified reductions also start from NWT or a non-standard parity variant of it. We are not able to establish a subcubic-equivalence with the more natural parity counting variant of NWT, where we ask if the number of negative triangles is even or odd. Perhaps surprisingly, we justify this by designing a reduction from the seemingly-harder Zero Weight Triangle problem, showing that parity is (conditionally) strictly harder than decision for NWT.},
  citationcount = {6},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {reduction}
}

@article{abboudPopularConjecturesAs2016,
  title = {Popular Conjectures as a Barrier for Dynamic Planar Graph Algorithms},
  author = {Abboud, Amir and Dahlgaard, S{\o}ren},
  year = {2016},
  doi = {10.1109/FOCS.2016.58},
  abstract = {The dynamic shortest paths problem on planar graphs asks us to preprocess a planar graph G such that we may support insertions and deletions of edges in G as well as distance queries between any two nodes u, v subject to the constraint that the graph remains planar at all times. This problem has been extensively studied in both the theory and experimental communities over the past decades. The best known algorithm performs queries and updates in {\~O}(n2/3) time, based on ideas of a seminal paper by Fakcharoenphol and Rao [FOCS'01]. A (1+{$\varepsilon$})-approximation algorithm of Abraham et al. [STOC'12] performs updates and queries in {\~O}({\textsurd}n) time. An algorithm with a more practical O(polylog(n)) runtime would be a major breakthrough. However, such runtimes are only known for a (1+{$\varepsilon$})-approximation in a model where only restricted weight updates are allowed due to Abraham et al. [SODA'16], or for easier problems like connectivity. In this paper, we follow a recent and very active line of work on showing lower bounds for polynomial time problems based on popular conjectures, obtaining the first such results for natural problems in planar graphs. Such results were previously out of reach due to the highly non-planar nature of known reductions and the impossibility of "planarizing gadgets". We introduce a new framework which is inspired by techniques from the literatures on distance labelling schemes and on parameterized complexity. Using our framework, we show that no algorithm for dynamic shortest paths or maximum weight bipartite matching in planar graphs can support both updates and queries in amortized O(n1/2-{$\varepsilon$}) time, for any {$\varepsilon$}{\textquestiondown}0, unless the classical all-pairs-shortest-paths problem can be solved in truly subcubic time, which is widely believed to be impossible. We extend these results to obtain strong lower bounds for other related problems as well as for possible trade-offs between query and update time. Interestingly, our lower bounds hold even in very restrictive models where only weight updates are allowed.},
  citationcount = {72},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,lower bound,query,reduction,update,update time}
}

@article{abboudPopularConjecturesImply2014,
  title = {Popular Conjectures Imply Strong Lower Bounds for Dynamic Problems},
  author = {Abboud, Amir and Williams, V. V.},
  year = {2014},
  doi = {10.1109/FOCS.2014.53},
  abstract = {We consider several well-studied problems in dynamic algorithms and prove that sufficient progress on any of them would imply a breakthrough on one of five major open problems in the theory of algorithms: 1) Is the 3SUM problem on n numbers in O(n2-{$\varepsilon$}) time for some {$\varepsilon$} {\textquestiondown} 0? 2) Can one determine the satisfiability of a CNF formula on n variables and poly n clauses in O((2 - {$\varepsilon$})npoly n) time for some {$\varepsilon$} {\textquestiondown} 0? 3) Is the All Pairs Shortest Paths problem for graphs on n vertices in O(n3-{$\varepsilon$}) time for some {$\varepsilon$} {\textquestiondown} 0? 4) Is there a linear time algorithm that detects whether a given graph contains a triangle? 5) Is there an O(n3-{$\varepsilon$}) time combinatorial algorithm for n{\texttimes}n Boolean matrix multiplication? The problems we consider include dynamic versions of bipartite perfect matching, bipartite maximum weight matching, single source reachability, single source shortest paths, strong connectivity, subgraph connectivity, diameter approximation and some nongraph problems such as Pagh's problem defined in a recent paper by Patrascu[STOC 2010].},
  citationcount = {382},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,lower bound}
}

@article{abboudSethBasedLower2017,
  title = {{{SETH-based}} Lower Bounds for Subset Sum and Bicriteria Path},
  author = {Abboud, Amir and Bringmann, K. and Hermelin, D. and Shabtay, D.},
  year = {2017},
  doi = {10.1145/3450524},
  abstract = {Subset Sumand k-SAT are two of the most extensively studied problems in computer science, and conjectures about their hardness are among the cornerstones of fine-grained complexity. An important open problem in this area is to base the hardness of one of these problems on the other. Our main result is a tight reduction from k-SAT to Subset Sum on dense instances, proving that Bellman's 1962 pseudo-polynomial O*(T)-time algorithm for Subset Sum on n numbers and target T cannot be improved to time T1-{$\varepsilon$} {$\cdot$} 2o(n) for any {$\varepsilon$} {\textquestiondown} 0, unless the Strong Exponential Time Hypothesis (SETH) fails. As a corollary, we prove a ``Direct-OR'' theorem for Subset Sum under SETH, offering a new tool for proving conditional lower bounds: It is now possible to assume that deciding whether one out of N given instances of Subset Sum is a YES instance requires time (N T)1-o(1). As an application of this corollary, we prove a tight SETH-based lower bound for the classical Bicriteria s,t-Path problem, which is extensively studied in Operations Research. We separate its complexity from that of Subset Sum: On graphs with m edges and edge lengths bounded by L, we show that the O(Lm) pseudo-polynomial time algorithm by Joksch from 1966 cannot be improved to {\~O}(L + m), in contrast to a recent improvement for Subset Sum (Bringmann, SODA 2017).},
  citationcount = {84},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {lower bound,reduction}
}

@article{abboudStronger3Sum2022,
  title = {Stronger 3-{{SUM}} Lower Bounds for Approximate Distance Oracles via Additive Combinatorics},
  author = {Abboud, Amir and Bringmann, K. and Fischer, N.},
  year = {2022},
  doi = {10.1145/3564246.3585240},
  abstract = {The ``short cycle removal'' technique was recently introduced by Abboud, Bringmann, Khoury and Zamir (STOC '22) to prove fine-grained hardness of approximation. Its main technical result is that listing all triangles in an n1/2-regular graph is n2-o(1)-hard even when the number of short cycles is small; namely, when the number of k-cycles is O(nk/2+{$\gamma$}) for {$\gamma$}{\textexclamdown}1/2. Its corollaries are based on the 3-SUM conjecture and their strength depends on {$\gamma$}, i.e. on how effectively the short cycles are removed. Abboud et al. achieve {$\gamma\geq$} 1/4 by applying structure versus randomness arguments on graphs. In this paper, we take a step back and apply conceptually similar arguments on the numbers of the 3-SUM problem, from which the hardness of triangle listing is derived. Consequently, we achieve the best possible\;{$\gamma$}=0 and the following lower bound corollaries under the 3-SUM conjecture: * Approximate distance oracles: The seminal Thorup-Zwick distance oracles achieve stretch 2k{\textpm} O(1) after preprocessing a graph in O(m n1/k) time. For the same stretch, and assuming the query time is\;no(1) Abboud et al. proved an {\textohm}(m1+1/12.7552 {$\cdot$} k) lower bound on the preprocessing time; we improve it to {\textohm}(m1+1/2k) which is only a factor\;2 away from the upper bound. Additionally, we obtain tight bounds for stretch 2+o(1) and 3-{\cyrchar\cyrie} and higher lower bounds for dynamic shortest paths. * Listing 4-cycles: Abboud et al. proved the first super-linear lower bound for listing all 4-cycles in a graph, ruling out (m1.1927+t)1+o(1) time algorithms where t is the number of 4-cycles. We settle the complexity of this basic problem by showing that the O(min(m4/3,n2) +t) upper bound is tight up to no(1) factors. Our results exploit a rich tool set from additive combinatorics, most notably the Balog-Szemer{\'e}di-Gowers theorem and Rusza's covering lemma. A key ingredient that may be of independent interest is a truly subquadratic algorithm for 3-SUM if one of the sets has small doubling.},
  citationcount = {27},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,lower bound,query,query time}
}

@article{abboudSubcubicEquivalencesGraph2015,
  title = {Subcubic Equivalences between Graph Centrality Problems, {{APSP}}, and Diameter},
  author = {Abboud, Amir and Grandoni, F. and Williams, Virginia Vassilevska},
  year = {2015},
  doi = {10.1145/3563393},
  abstract = {Measuring the importance of a node in a network is a major goal in the analysis of social networks, biological systems, transportation networks, and so forth. Different centrality measures have been proposed to capture the notion of node importance. For example, the center of a graph is a node that minimizes the maximum distance to any other node (the latter distance is the radius of the graph). The median of a graph is a node that minimizes the sum of the distances to all other nodes. Informally, the betweenness centrality of a node w measures the fraction of shortest paths that have w as an intermediate node. Finally, the reach centrality of a node w is the smallest distance r such that any s-t shortest path passing through w has either s or t in the ball of radius r around w. The fastest known algorithms to compute the center and the median of a graph and to compute the betweenness or reach centrality even of a single node take roughly cubic time in the number n of nodes in the input graph. It is open whether these problems admit truly subcubic algorithms, i.e., algorithms with running time {\~O}(n3-{$\delta$}) for some constant {$\delta$} {\textquestiondown} 0.1 We relate the complexity of the mentioned centrality problems to two classical problems for which no truly subcubic algorithm is known, namely All Pairs Shortest Paths (APSP) and Diameter. We show that Radius, Median, and Betweenness Centrality are equivalent under subcubic reductions to APSP, i.e., that a truly subcubic algorithm for any of these problems implies a truly subcubic algorithm for all of them. We then show that Reach Centrality is equivalent to Diameter under subcubic reductions. The same holds for the problem of approximating Betweenness Centrality within any finite factor. Thus, the latter two centrality problems could potentially be solved in truly subcubic time, even if APSP required essentially cubic time. On the positive side, our reductions for Reach Centrality imply an improved {\~O}(Mn{$\omega$})-time algorithm for this problem in case of non-negative integer weights upper bounded by M, where {$\omega$} is a fast matrix multiplication exponent.},
  citationcount = {138},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {reduction}
}

@article{abboudTheTimeComplexity2023,
  title = {The Time Complexity of Fully Sparse Matrix Multiplication},
  author = {Abboud, Amir and Bringmann, K. and Fischer, N. and K{\"u}nnemann, Marvin},
  year = {2023},
  doi = {10.48550/arXiv.2309.06317},
  abstract = {What is the time complexity of matrix multiplication of sparse integer matrices with m\textsubscript{\{\vphantom\}}in\vphantom\{\} nonzeros in the input and m\textsubscript{\{\vphantom\}}out\vphantom\{\} nonzeros in the output? This paper provides improved upper bounds for this question for almost any choice of m\textsubscript{\{\vphantom\}}in\vphantom\{\} vs. m\textsubscript{\{\vphantom\}}out\vphantom\{\}, and provides evidence that these new bounds might be optimal up to further progress on fast matrix multiplication. Our main contribution is a new algorithm that reduces sparse matrix multiplication to dense (but smaller) rectangular matrix multiplication. Our running time thus depends on the optimal exponent {$\omega$}(a,b,c) of multiplying dense n\textsuperscript{a}{\texttimes}n\textsuperscript{b} by n\textsuperscript{b}{\texttimes}n\textsuperscript{c} matrices. We discover that when m\textsubscript{\{\vphantom\}}out\vphantom\{\}={$\Theta$}(m\textsubscript{\{\vphantom\}}in\vphantom\{\}\textsuperscript{r}) the time complexity of sparse matrix multiplication is O(m\textsubscript{\{\vphantom\}}in\vphantom\{\}\textsuperscript{\{\vphantom\}}{$\sigma$}+{$\epsilon$}\vphantom\{\}), for all {$\epsilon>$}0, where {$\sigma$} is the solution to the equation {$\omega$}({$\sigma$}-1,2-{$\sigma$},1+r-{$\sigma$})={$\sigma$}. No matter what {$\omega$}({$\cdot$},{$\cdot$},{$\cdot$}) turns out to be, and for all r{$\in$}(0,2), the new bound beats the state of the art, and we provide evidence that it is optimal based on the complexity of the all-edge triangle problem. In particular, in terms of the input plus output size m=m\textsubscript{\{\vphantom\}}in\vphantom\{\}+m\textsubscript{\{\vphantom\}}out\vphantom\{\} our algorithm runs in time O(m\textsuperscript{\{\vphantom\}}1.3459\vphantom\{\}). Even for Boolean matrices, this improves over the previous m\textsuperscript{\{\{\vphantom{\}\}}}{\textfractionsolidus}{$_{2}\omega$}\vphantom\{\}\{{$\omega$}+1\}+{$\epsilon$}\vphantom\{\}=O(m\textsuperscript{\{\vphantom\}}1.4071\vphantom\{\}) bound [Amossen, Pagh; 2009], which was a natural barrier since it coincides with the longstanding bound of all-edge triangle in sparse graphs [Alon, Yuster, Zwick; 1994]. We find it interesting that matrix multiplication can be solved faster than triangle detection in this natural setting. In fact, we establish an equivalence to a special case of the all-edge triangle problem.},
  citationcount = {5},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{abboudTightHardnessResults2015,
  title = {Tight Hardness Results for {{LCS}} and Other Sequence Similarity Measures},
  author = {Abboud, Amir and Backurs, A. and Williams, V. V.},
  year = {2015},
  doi = {10.1109/FOCS.2015.14},
  abstract = {Two important similarity measures between sequences are the longest common subsequence (LCS) and the dynamic time warping distance (DTWD). The computations of these measures for two given sequences are central tasks in a variety of applications. Simple dynamic programming algorithms solve these tasks in O(n2) time, and despite an extensive amount of research, no algorithms with significantly better worst case upper bounds are known. In this paper, we show that for any constant {$\varepsilon$} {\textquestiondown}0, an O(n2-{$\varepsilon$}) time algorithm for computing the LCS or the DTWD of two sequences of length n over a constant size alphabet, refutes the popular Strong Exponential Time Hypothesis (SETH).},
  citationcount = {235},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic}
}

@article{abboudTowardsHardnessOf2017,
  title = {Towards Hardness of Approximation for Polynomial Time Problems},
  author = {Abboud, Amir and Backurs, A.},
  year = {2017},
  doi = {10.4230/LIPIcs.ITCS.2017.11},
  abstract = {Proving hardness of approximation is a major challenge in the field of fine-grained complexity and conditional lower bounds in P. How well can the Longest Common Subsequence (LCS) or the Edit Distance be approximated by an algorithm that runs in near-linear time? In this paper, we make progress towards answering these questions. We introduce a framework that exhibits barriers for truly subquadratic and deterministic algorithms with good approximation guarantees. Our framework highlights a novel connection between deterministic approximation algorithms for natural problems in P and circuit lower bounds. In particular, we discover a curious connection of the following form: if there exists a {$\delta$}{\textquestiondown}0 such that for all {\textquestiondown}0 there is a deterministic (1+)-approximation algorithm for LCS on two sequences of length n over an alphabet of size n{\textasciicircum}\{o(1)\} that runs in O(n{\textasciicircum}\{2-{$\delta$}\}) time, then a certain plausible hypothesis is refuted, and the class E{\textasciicircum}NP does not have non-uniform linear size Valiant Series-Parallel circuits. Thus, designing a "truly subquadratic PTAS" for LCS is as hard as resolving an old open question in complexity theory.},
  citationcount = {36},
  venue = {Information Technology Convergence and Services},
  keywords = {lower bound}
}

@article{abboudWorstCaseTo2024,
  title = {Worst-Case to Expander-Case Reductions: {{Derandomized}} and Generalized},
  author = {Abboud, Amir and Wallheimer, Nathan},
  year = {2024},
  doi = {10.48550/arXiv.2403.08394},
  abstract = {A recent paper by Abboud and Wallheimer [ITCS 2023] presents self-reductions for various fundamental graph problems, which transform worst-case instances to expanders, thus proving that the complexity remains unchanged if the input is assumed to be an expander. An interesting corollary of their self-reductions is that if some problem admits such reduction, then the popular algorithmic paradigm based on expander-decompositions is useless against it. In this paper, we improve their core gadget, which augments a graph to make it an expander while retaining its important structure. Our new core construction has the benefit of being simple to analyze and generalize while obtaining the following results: 1. A derandomization of the self-reductions, showing that the equivalence between worst-case and expander-case holds even for deterministic algorithms, and ruling out the use of expander-decompositions as a derandomization tool. 2. An extension of the results to other models of computation, such as the Fully Dynamic model and the Congested Clique model. In the former, we either improve or provide an alternative approach to some recent hardness results for dynamic expander graphs by Henzinger, Paz, and Sricharan [ESA 2022]. In addition, we continue this line of research by designing new self-reductions for more problems, such as Max-Cut and dynamic Densest Subgraph, and demonstrating that the core gadget can be utilized to lift lower bounds based on the OMv Conjecture to expanders.},
  citationcount = {Unknown},
  venue = {Embedded Systems and Applications},
  keywords = {dynamic,lower bound,reduction}
}

@article{abdel-ghaffarTheOptimalityOf2006,
  title = {The Optimality of Allocation Methods for Bounded Disagreement Search Queries: {{The}} Possible and the Impossible},
  author = {{Abdel-Ghaffar}, K. and Abbadi, A. El},
  year = {2006},
  doi = {10.1109/TKDE.2006.149},
  abstract = {Data allocation on multiple I/O devices manifests itself in many computing systems, both centralized and distributed. Data is partitioned on multiple I/O devices and clients issue various types of queries to retrieve relevant information. In this paper, we derive necessary and sufficient conditions for a data allocation method to be optimal for two important types of queries: partial match and bounded disagreement search queries. We formally define these query types and derive the optimality conditions based on coding-theoretic arguments. Although these conditions are fairly strict, we show how to construct good allocation methods for practical realistic situations. Not only are the response times bounded by a small value, but also the identification of the relevant answer set is efficient},
  citationcount = {Unknown},
  venue = {IEEE Transactions on Knowledge and Data Engineering},
  keywords = {query}
}

@article{abdullahADirectedIsoperimetric2014,
  title = {A Directed Isoperimetric Inequality with Application to Bregman near Neighbor Lower Bounds},
  author = {Abdullah, A. and Venkatasubramanian, Suresh},
  year = {2014},
  doi = {10.1145/2746539.2746595},
  abstract = {Bregman divergences are important distance measures that are used in applications such as computer vision, text mining, and speech processing, and are a focus of interest in machine learning due to their information-theoretic properties. There has been extensive study of algorithms for clustering and near neighbor search with respect to these divergences. In all cases, the guarantees depend not just on the data size n and dimensionality d, but also on a structure constant {$\mu$} {$\geq$} 1 that depends solely on a generating convex function {$\varphi$} and can grow without bound independently. In general, this {$\mu$} parametrizes the degree to which a given divergence is "asymmetric". In this paper, we provide the first evidence that this dependence on {$\mu$} might be intrinsic. We focus on the problem of ac\{ann\} search for Bregman divergences. We show that under the cell probe model, any non-adaptive data structure (like locality-sensitive hashing) for c-approximate near-neighbor search that admits r probes must use space {\textohm}(dn1 + {$\mu$}/c r). In contrast for LSH under l1 the best bound is {\textohm}(dn1+ 1/cr). Our results interpolate between known lower bounds both for LSH-based ANN under l1 as well as the generally harder Partial Match problem (in non-adaptive settings). The bounds match the former when {$\mu$} is small and the latter when {$\mu$} is {\textohm}(d/log n). This further strengthens the intuition that Partial Match corresponds to an "asymmetric" version of ANN, as well as opening up the possibility of a new line of attack for lower bounds on Partial Match. Our new tool is a directed variant of the standard boolean noise operator. We prove a generalization of the Bonami-Beckner hypercontractivity inequality (restricted to certain subsets of the Hamming cube), and use this to prove the desired directed isoperimetric inequality that we use in our data structure lower bound.},
  citationcount = {16},
  venue = {Symposium on the Theory of Computing},
  keywords = {cell probe,data structure,information theoretic,lower bound,non-adaptive,sorted}
}

@article{abdullahSpectralApproachesTo2014,
  title = {Spectral Approaches to Nearest Neighbor Search},
  author = {Abdullah, A. and Andoni, Alexandr and Kannan, R. and Krauthgamer, Robert},
  year = {2014},
  doi = {10.1109/FOCS.2014.68},
  abstract = {We study spectral algorithms for the high-dimensional Nearest Neighbor Search problem (NNS). In particular, we consider a semi-random setting where a dataset is chosen arbitrarily from an unknown subspace of low dimension, and then perturbed by full-dimensional Gaussian noise. We design spectral NNS algorithms whose query time depends polynomially on the dimension and logarithmically on the size of the point set. These spectral algorithms use a repeated computation of the top PCA vector/subspace, and are effective even when the random-noise magnitude is much larger than the interpoint distances. Our motivation is that in practice, a number of spectral NNS algorithms outperform the random-projection methods that seem otherwise theoretically optimal on worst-case datasets. In this paper we aim to provide theoretical justification for this disparity. The full version of this extended abstract is available on arXiv.},
  citationcount = {21},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {query,query time}
}

@article{abeliukPracticalCompressedSuffix2013,
  title = {Practical Compressed Suffix Trees},
  author = {Abeliuk, Andr{\'e}s and C{\'a}novas, Rodrigo and Navarro, G.},
  year = {2013},
  doi = {10.3390/a6020319},
  abstract = {The suffix tree is an extremely important data structure in bioinformatics. Classical implementations require much space, which renders them useless to handle large sequence collections. Recent research has obtained various compressed representations for suffix trees, with widely different space-time tradeoffs. In this paper we show how the use of range min-max trees yields novel representations achieving practical space/time tradeoffs. In addition, we show how those trees can be modified to index highly repetitive collections, obtaining the first compressed suffix tree representation that effectively adapts to that scenario.},
  citationcount = {68},
  venue = {Algorithms}
}

@article{abrahamOnFullyDynamic2016,
  title = {On Fully Dynamic Graph Sparsifiers},
  author = {Abraham, Ittai and Durfee, D. and Koutis, I. and Krinninger, Sebastian and Peng, Richard},
  year = {2016},
  doi = {10.1109/FOCS.2016.44},
  abstract = {We initiate the study of fast dynamic algorithms for graph sparsification problems and obtain fully dynamic algorithms, allowing both edge insertions and edge deletions, that take polylogarithmic time after each update in the graph. Our three main results are as follows. First, we give a fully dynamic algorithm for maintaining a (1 {\textpm} {$\epsilon$})-spectral sparsifier with amortized update time poly(log n, {$\epsilon$}{\textexclamdown}sup{\textquestiondown}-1{\textexclamdown}/sup{\textquestiondown}). Second, we give a fully dynamic algorithm for maintaining a (1 {\textpm} {$\epsilon$})-cut sparsifier with worst-case update time poly(log n, {$\epsilon$}{\textexclamdown}sup{\textquestiondown}-1{\textexclamdown}/sup{\textquestiondown}). Both sparsifiers have size n {$\cdot$} poly(log n, {$\epsilon$}{\textexclamdown}sup{\textquestiondown}-1{\textexclamdown}/sup{\textquestiondown}). Third, we apply our dynamic sparsifier algorithm to obtain a fully dynamic algorithm for maintaining a (1 - {$\epsilon$})-approximation to the value of the maximum flow in an unweighted, undirected, bipartite graph with amortized update time poly(log n, {$\epsilon$}{\textexclamdown}sup{\textquestiondown}-1{\textexclamdown}/sup{\textquestiondown}).},
  citationcount = {66},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,update,update time}
}

@article{abramowitzHandbookOfMathematical1966,
  title = {Handbook of Mathematical Functions.},
  author = {Abramowitz, M. and Stegun, I. and Mcquarrie, D. A.},
  year = {1966},
  doi = {10.2307/2314682},
  abstract = {No abstract available},
  citationcount = {36669},
  venue = {No venue available}
}

@article{adityabhaskaraNonNegativeSparseRegression2018,
  title = {Non-{{Negative Sparse Regression}} and {{Column Subset Selection}} with {{L1 Error}}},
  author = {Aditya Bhaskara and Silvio Lattanzi},
  year = {2018},
  journal = {Information Technology Convergence and Services},
  doi = {10.4230/LIPIcs.ITCS.2018.7},
  abstract = {We consider the problems of sparse regression and column subset selection under L1 error. For both problems, we show that in the non-negative setting it is possible to obtain tight and efficient approximations, without any additional structural assumptions (such as restricted isometry, incoherence, expansion, etc.). For sparse regression, given a matrix A and a vector b with non-negative entries, we give an efficient algorithm to output a vector x of sparsity O(k), for which {\textbar}Ax - b{\textbar}\_1 is comparable to the smallest error possible using non-negative k-sparse x. We then use this technique to obtain our main result: an efficient algorithm for column subset selection under L1 error for non-negative matrices.},
  annotation = {Citation Count: 3}
}

@article{adlemanTwoTheoremsOn1978,
  title = {Two Theorems on Random Polynomial Time},
  author = {Adleman, L.},
  year = {1978},
  doi = {10.1109/SFCS.1978.37},
  abstract = {The use of randomness in computation was first studied in abstraction by Gill [4]. In recent years its use in both practical and theoretical areas has become apparent. Strassen and Solovay [10]; Miller [7]; and Rabin [8] have used it to transform primality testing into a (for many purposes) tractible problem. We can see in retrospect that it was implicit in algorithms by Ber1ekamp [2], Lehmer [6], and Cippola [3] (1903!). Where the traditional method of polynomial reduction has been inapplicable, randomness has been used in demonstrating intractibility by Adleman and Manders [1], and in showing problems equivalent by Rabin [9]. In light of these developments and the insights they provide, a new examination of randomness is in order.},
  citationcount = {355},
  venue = {19th Annual Symposium on Foundations of Computer Science (sfcs 1978)}
}

@article{adlerAsynchronousSharedMemory1996,
  title = {Asynchronous Shared Memory Search Structures},
  author = {Adler, M.},
  year = {1996},
  doi = {10.1007/s002240000094},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {ACM Symposium on Parallelism in Algorithms and Architectures}
}

@article{afshaniACellProbe2025,
  title = {A Cell Probe Lower Bound for the Predecessor Search Problem in {{PRAM}}},
  author = {Afshani, Peyman and Sitchinava, Nodari},
  year = {2025},
  doi = {10.1137/1.9781611978322.136},
  abstract = {We study the predecessor search problem in the classical PRAM model of computation. In this problem, the input is a set of n (cid:96) -bit integers and the goal is to store the input in a data structure of size S ( n ) such that given a query value q , the predecessor of q can be found efficiently. This is a very classical problem with an extensive history. We prove a lower bound for this problem in the strongest CRCW PRAM model. A simplified version of the lower bound states that in a K -processor PRAM model with O (log n )-bit registers, the query requires {\textohm}(log K log n ) worst-case time under the realistic setting where the space is near-linear.},
  citationcount = {Unknown},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,data structure,lower bound,query,sorted}
}

@article{afshaniConcurrentRangeReporting2014,
  title = {Concurrent Range Reporting in Two-Dimensional Space},
  author = {Afshani, Peyman and Sheng, Cheng and Tao, Yufei and Wilkinson, Bryan T.},
  year = {2014},
  doi = {10.1137/1.9781611973402.73},
  abstract = {In the concurrent range reporting (CRR) problem, the input is L disjoint sets S1, ..., SL of points in Rd with a total of N points. The goal is to preprocess the sets into a structure such that, given a query range r and an arbitrary set Q {$\subseteq$} \{1, ..., L\}, we can efficiently report all the points in Si {$\cap$} r for each i e Q. The problem was studied as early as 1986 by Chazelle and Guibas [9] and has recently re-emerged when studying higher-dimensional complexity of orthogonal range reporting [2, 3]. We focus on the one- and two-dimensional cases of the problem. We prove that in the pointer-machine model (as well as comparison models such as the real RAM model), answering queries requires {\textohm}({\textbar}Q{\textbar} log(L/{\textbar}Q{\textbar}) + log N + K) time in the worst case, where K is the number of output points. In one dimension, we achieve this query time with a linear-space dynamic data structure that requires optimal O(log N) time to update. We also achieve this query time in the static case for dominance and halfspace queries in the plane. For three-sided ranges, we get close to within an inverse Ackermann ({$\alpha$}({$\cdot$})) factor: we answer queries in O({\textbar}Q{\textbar} log(L/{\textbar}Q{\textbar}){$\alpha$}(L) + log N + K) time, improving the best previously known query times of O({\textbar}Q{\textbar}log(N/{\textbar}Q{\textbar}) + K) and O(2LL + log N + K). Finally, we give an optimal data structure for three-sided ranges for the case L = O(log N).},
  citationcount = {6},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,query,query time,static,update}
}

@article{afshaniDominanceReporting3D2008,
  title = {On Dominance Reporting in {{3D}}},
  author = {Afshani, Peyman},
  year = {2008},
  doi = {10.1007/978-3-540-87744-8_4},
  abstract = {No abstract available},
  citationcount = {55},
  venue = {Embedded Systems and Applications}
}

@article{afshaniDynamicConnectivityFor2006,
  title = {Dynamic Connectivity for Axis-Parallel Rectangles},
  author = {Afshani, Peyman and Chan, Timothy M.},
  year = {2006},
  doi = {10.1007/s00453-008-9234-7},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {Algorithmica},
  keywords = {dynamic}
}

@article{afshaniImprovedPointerMachine2013,
  title = {Improved Pointer Machine and i/o Lower Bounds for Simplex Range Reporting and Related Problems},
  author = {Afshani, Peyman},
  year = {2013},
  month = aug,
  journal = {Int. J. Comput. Geom. Appl.},
  volume = {23},
  number = {04n05},
  pages = {233--251},
  publisher = {World Scientific Publishing Co.},
  issn = {0218-1959},
  doi = {10.1142/S0218195913600054},
  url = {https://www.worldscientific.com/doi/abs/10.1142/S0218195913600054},
  urldate = {2024-09-20},
  abstract = {We investigate one of the fundamental areas in computational geometry: lower bounds for range reporting problems in the pointer machine and the external memory models. We develop new techniques that lead to new and improved lower bounds for simplex range reporting as well as some other geometric problems. Simplex range reporting is the problem of storing n points in the d-dimensional space in a data structure such that the k points that lie inside a query simplex can be found efficiently. This is one of the fundamental and extensively studied problems in computational geometry. Currently, the best data structures for the problem achieve Q(n) + O(k) query time using  space in which the  notation either hides a polylogarithmic or an n{$\varepsilon$} factor for any constant {$\varepsilon$} {$>$} 0, (depending on the data structure and Q(n)). The best lower bound on this problem is due to Chazelle and Rosenberg who showed any pointer machine data structure that can answer queries in O(n{$\gamma$} + k) time must use {\textohm}(nd-{$\varepsilon$}-d{$\gamma$}) space. Observe that this bound is a polynomial factor away from the best known data structures. In this article, we improve the space lower bound to . Not only this bridges the gap from polynomial to sub-polynomial, it also offers a smooth trade-off curve. For instance, for polylogarithmic values of Q(n), our space lower bound almost equals {\textohm}((n/Q(n))d); the latter is generally believed to be the ``right'' bound. By a simple geometric transformation, we also improve the best lower bounds for the halfspace range reporting problem. Furthermore, we study the external memory model and offer a new simple framework for proving lower bounds in this model. We show that answering simplex range reporting queries with Q(n)+O(k/B) I/Os requires ) space or  blocks, in which B is the block size.},
  keywords = {data structure,I/O model,lower bound,pointer machine,query,query time,Range searching}
}

@article{afshaniIndependentRangeSampling2017,
  title = {Independent Range Sampling, Revisited},
  author = {Afshani, Peyman and Wei, Zhewei},
  year = {2017},
  doi = {10.4230/LIPIcs.ESA.2017.3},
  abstract = {In the independent range sampling (IRS) problem, given an input set P of n points in R, the task is to build a data structure, such that given a range R and an integer t {$\geq$} 1, it returns t points that are uniformly and independently drawn from P {$\cap$} R. The samples must satisfy inter-query independence, that is, the samples returned by every query must be independent of the samples returned by all the previous queries. This problem was first tackled by Hu et al. [15], who proposed optimal structures for one-dimensional dynamic IRS problem in internal memory and one-dimensional static IRS problem in external memory. In this paper, we study two natural extensions of the independent range sampling problem. In the first extension, we consider the static IRS problem in two and three dimensions in internal memory. We obtain data structures with optimal space-query tradeoffs for 3D halfspace, 3D dominance, and 2D three-sided queries. The second extension considers weighted IRS problem. Each point is associated with a real-valued weight, and given a query range R, a sample is drawn independently such that each point in P {$\cap$}R is selected with probability proportional to its weight. Walker's alias method is a classic solution to this problem when no query range is specified. We obtain optimal data structure for one dimensional weighted range sampling problem, thereby extending the alias method to allow range queries. Digital Object Identifier 10.4230/LIPIcs...},
  citationcount = {17},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,dynamic,query,static}
}

@article{afshaniOptimalDeterministicShallow2014,
  title = {Optimal Deterministic Shallow Cuttings for 3-d Dominance Ranges},
  author = {Afshani, Peyman and Tsakalidis, Konstantinos},
  year = {2014},
  doi = {10.1007/s00453-017-0376-3},
  abstract = {No abstract available},
  citationcount = {13},
  venue = {Algorithmica}
}

@article{agarwalFineGrainedComplexity2018,
  title = {Fine-Grained Complexity for Sparse Graphs},
  author = {Agarwal, U. and Ramachandran, V.},
  year = {2018},
  doi = {10.1145/3188745.3188888},
  abstract = {We consider the fine-grained complexity of sparse graph problems that currently have {\~O}(mn) time algorithms, where m is the number of edges and n is the number of vertices in the input graph. This class includes several important path problems on both directed and undirected graphs, including APSP, MWC (Minimum Weight Cycle), Radius, Eccentricities, BC (Betweenness Centrality), etc. We introduce the notion of a sparse reduction which preserves the sparsity of graphs, and we present near linear-time sparse reductions between various pairs of graph problems in the {\~O}(mn) class. There are many sub-cubic reductions between graph problems in the {\~O}(mn) class, but surprisingly few of these preserve sparsity. In the directed case, our results give a partial order on a large collection of problems in the {\~O}(mn) class (along with some equivalences), and many of our reductions are very nontrivial. In the undirected case we give two nontrivial sparse reductions: from MWC to APSP, and from unweighted ANSC (all nodes shortest cycles) to unweighted APSP. We develop a new `bit-sampling' method for these sparse reductions on undirected graphs, which also gives rise to improved or simpler algorithms for cycle finding problems in undirected graphs. We formulate the the notion of MWC hardness, which is based on the assumption that a minimum weight cycle in a directed graph cannot be computed in time polynomially smaller than mn. Our sparse reductions for directed path problems in the {\~O}(mn) class establish that several problems in this class, including 2-SiSP (second simple shortest path), s-t Replacement Paths, Radius, Eccentricities and BC are MWC hard. Our sparse reductions give MWC hardness a status for the {\~O}(mn) class similar to 3SUM hardness for the quadratic class, since they show sub-mn hardness for a large collection of fundamental and well-studied graph problems that have maintained an {\~O}(mn) time bound for over half a century. We also identify Eccentricities and BC as key problems in the {\~O}(mn) class which are simultaneously MWC-hard, SETH-hard and k-DSH-hard, where SETH is the Strong Exponential Time Hypothesis, and k-DSH is the hypothesis that a dominating set of size k cannot be computed in time polynomially smaller than nk. Our framework using sparse reductions is very relevant to real-world graphs, which tend to be sparse and for which the {\~O}(mn) time algorithms are the ones typically used in practice, and not the {\~O}(n3) time algorithms.},
  citationcount = {26},
  venue = {Symposium on the Theory of Computing},
  keywords = {reduction}
}

@article{agarwalGeometricPartitioningAnd1990,
  title = {Geometric Partitioning and Its Applications},
  author = {Agarwal, P.},
  year = {1990},
  doi = {10.1090/dimacs/006/01},
  abstract = {In this survey paper we review some results related to \{\vphantom\}\emph{geometric partitioning}\vphantom\{\}\emph{, i.e. given a set of objects in \{R\}\textsuperscript{\{\vphantom\}}d\vphantom\{\}, partition the space into few regions so that each region intersects a small number of objects. We first describe the known bounds on the size of such a partitioning and present some of the algorithms for computing a geometric partitioning. We then discuss several applications of geometric partitioning.}},
  citationcount = {39},
  venue = {Discrete \& Computational Geometry}
}

@article{agarwalGeometricRangeSearching2007,
  title = {Geometric Range Searching and Its Relatives},
  author = {Agarwal, P. and Erickson, J.},
  year = {1997},
  doi = {10.1090/conm/223/03131},
  abstract = {About ten years ago, the eld of range searching, especially simplex range searching, was wide open. At that time, neither e cient algorithms nor nontrivial lower bounds were known for most range-searching problems. A series of papers by Haussler and Welzl [161], Clarkson [88, 89], and Clarkson and Shor [92] not only marked the beginning of a new chapter in geometric searching, but also revitalized computational geometry as a whole. Led by these and a number of subsequent papers, tremendous progress has been made in geometric range searching, both in terms of developing e cient data structures and proving nontrivial lower bounds. From a theoretical point of view, range searching is now almost completely solved. The impact of general techniques developed for geometric range searching {\textbar} "-nets, 1=rcuttings, partition trees, multi-level data structures, to name a few {\textbar} is evident throughout computational geometry. This volume provides an excellent opportunity to recapitulate the current status of geometric range searching and to summarize the recent progress in this area. Range searching arises in a wide range of applications, including geographic information systems, computer graphics, spatial databases, and time-series databases. Furthermore, a variety of geometric problems can be formulated as a range-searching problem. A typical range-searching problem has the following form. Let S be a set of n points in R , and let},
  citationcount = {358},
  venue = {No venue available},
  file = {/Users/tulasi/Zotero/storage/ZM93B27Z/Agarwal and Erickson - 1999 - Geometric range searching and its relatives.pdf}
}

@article{agarwalMNHashing2019,
  title = {M-n Hashing: {{Search}} Time Optimization with Collision Resolution Using Balanced Tree},
  author = {Agarwal, Arushi and Pathak, Sashakt and Agarwal, Sakshi},
  year = {2019},
  doi = {10.1007/978-981-15-4451-4_16},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {No venue available}
}

@article{agarwalOnRangeSearching2012,
  title = {On Range Searching with Semialgebraic Sets {{II}}},
  author = {Agarwal, P. and Matou{\v s}ek, J. and Sharir, M.},
  year = {2012},
  doi = {10.1109/FOCS.2012.32},
  abstract = {Let P be a set of n points in Rd. We present a linear-size data structure for answering range queries on P with constant-complexity semialgebraic sets as ranges, in time close to O(n1-1/d). It essentially matches the performance of similar structures for simplex range searching, and, for d {$\geq$} 5, significantly improves earlier solutions by the first two authors obtained in 1994. This almost settles a long-standing open problem in range searching. The data structure is based on the polynomial-partitioning technique of Guth and Katz [arXiv:1011.4105], which shows that for a parameter r, 1 {\textexclamdown}; r {$\leq$} n, there exists a d-variate polynomial f of degree O(r1/d) such that each connected component of Rd Z(f) contains at most n/r points of P, where Z(f) is the zero set of f. We present an efficient randomized algorithm for computing such a polynomial partition, which is of independent interest and is likely to have additional applications.},
  citationcount = {66},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,query}
}

@article{agarwalOnReportingDurable2024,
  title = {On Reporting Durable Patterns in Temporal Proximity Graphs},
  author = {Agarwal, Pankaj K. and Hu, Xiao and Sintos, Stavros and Yang, Jun},
  year = {2024},
  doi = {10.1145/3651144},
  abstract = {Finding patterns in graphs is a fundamental problem in databases and data mining. In many applications, graphs are temporal and evolve over time, so we are interested in finding durable patterns, such as triangles and paths, which persist over a long time. While there has been work on finding durable simple patterns, existing algorithms do not have provable guarantees and run in strictly super-linear time. The paper leverages the observation that many graphs arising in practice are naturally proximity graphs or can be approximated as such, where nodes are embedded as points in some high-dimensional space, and two nodes are connected by an edge if they are close to each other. We work with an implicit representation of the proximity graph, where nodes are additionally annotated by time intervals, and design near-linear-time algorithms for finding (approximately) durable patterns above a given durability threshold. We also consider an interactive setting where a client experiments with different durability thresholds in a sequence of queries; we show how to compute incremental changes to result patterns efficiently in time near-linear to the size of the changes.},
  citationcount = {1},
  venue = {Proc. ACM Manag. Data},
  keywords = {query}
}

@article{agarwalRangeSearching1996,
  title = {Range Searching},
  author = {Agarwal, P.},
  year = {1996},
  doi = {10.1201/9781420035315.ch36},
  abstract = {This paper surveys the known results and techniques on range searching, and describes some of their applications.},
  citationcount = {42},
  venue = {Handbook of Discrete and Computational Geometry, 2nd Ed.}
}

@article{agarwalRangeSearchingIn2002,
  title = {Range Searching in Categorical Data: {{Colored}} Range Searching on Grid},
  author = {Agarwal, P. and Govindarajan, Sathish and Muthukrishnan, S.},
  year = {2002},
  doi = {10.1007/3-540-45749-6_6},
  abstract = {No abstract available},
  citationcount = {42},
  venue = {Embedded Systems and Applications}
}

@article{agarwalRayShootingAnd1992,
  title = {Ray Shooting and Parametric Search},
  author = {Agarwal, P. and Matou{\v s}ek, J.},
  year = {1992},
  doi = {10.1145/129712.129763},
  abstract = {Efficient algorithms for the ray shooting problem are presented: Given a collection {$\Gamma$} of objects in \{R\}\textsuperscript{d}, build a data structure so that, for a query ray, the first object of {$\Gamma$} hit by the ray can be quickly determined. Using the parametric search technique, this problem is reduced to the segment emptiness problem. For various ray shooting problems, space/query-time trade-offs of the following type are achieved: For some integer b and a parameter m(n{$\leq$}m{$\leq$}n\textsuperscript{b}) the queries are answered in time O((\{n/\{m\vphantom{\}\}}\textsuperscript{\{\vphantom\}}\{1/b\}\vphantom\{\}\vphantom\{\}\vphantom\{\})\textsuperscript{\{\vphantom\}}O(1)\vphantom\{\}n), with O(m\textsuperscript{\{\vphantom\}}1+{$\varepsilon$}\vphantom\{\}) space and preprocessing time ({$\varepsilon>$}0 is arbitrarily small but fixed constant). b={$\lfloor$}\{\{d/2\}\}{$\rfloor$} is obtained for ray shooting in a convex d-polytope defined as an intersection of n half spaces, b=d for an arrangement of n hyperplanes in \{R\}\textsuperscript{d}, and b=3 for an arrangement of n half planes in \{R\}{$^3$}. This approach also yields fast procedures for fin...},
  citationcount = {222},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,query,query time}
}

@article{agarwalStreamingAlgorithmsFor2010,
  title = {Streaming Algorithms for Extent Problems in High Dimensions},
  author = {Agarwal, P. and Sharathkumar, R.},
  year = {2010},
  doi = {10.1007/s00453-013-9846-4},
  abstract = {No abstract available},
  citationcount = {57},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{aggarwalAlgorithmsAndComputation2000,
  title = {Algorithms and Computation},
  author = {Aggarwal, A. and Rangan, C.},
  year = {2000},
  doi = {10.1007/3-540-46632-0},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Lecture Notes in Computer Science}
}

@article{aggarwalInputOutputComplexity1988,
  title = {The Input/Output Complexity of Sorting and Related Problems},
  author = {Aggarwal, A. and Vitter, J. S.},
  year = {1988},
  journal = {Communications of the ACM},
  volume = {31},
  pages = {1116--1127},
  doi = {10.1145/48529.48535},
  annotation = {Beigel , R. , and Gill , J . Personal Communication . 1986 . Beigel, R., and Gill, J. Personal Communication. 1986.\\
\\
Floyd , R.W. Permuting information in idealized two-l, evel storage . In Complexity of Computer Calculations , R. Miller and J. Thatcher, Eds. Plenum, New York , 1972 , pp. 105 - 109 . Floyd, R.W. Permuting information in idealized two-l, evel storage. In Complexity of Computer Calculations, R. Miller and J. Thatcher, Eds. Plenum, New York, 1972, pp. 105-109.\\
\\
Knuth , D.E. The Art of Computer Programming, Volume III: Sorting and Searching . Addison-Wesley , Reading, Mass ., 1973 . Knuth, D.E. The Art of Computer Programming, Volume III: Sorting and Searching. Addison-Wesley, Reading, Mass., 1973.\\
\\
\\
\\
Savage , J.E. , and Vitter , \vphantom\{\}. S. Parallelism in space-time tradeoffs , in Advances in Computing Research, Volume 4: Special Issue on Parallel and Distributed Computing . JAI Press , Greenwich, Conn ., 1987 , pp. 117 - 146 . Savage, J.E., and Vitter, \vphantom\{\}.S. Parallelism in space-time tradeoffs, in Advances in Computing Research, Volume 4: Special Issue on Parallel and Distributed Computing. JAI Press, Greenwich, Conn., 1987, pp. 117-146.\\
Wu , C.L. , and Feng , T.Y . The universality of the shuffle-exchange network . IEEE Trans. Comput. C-30 , 5 ( May 1981 ), 324 - 332 . Wu, C.L., and Feng, T.Y. The universality of the shuffle-exchange network. IEEE Trans. Comput. C-30, 5 (May 1981), 324-332.},
  file = {/Users/tulasi/Zotero/storage/N3EZB2FK/Aggarwal and Vitter - 1988 - The inputoutput complexity of sorting and related problems.pdf}
}

@article{aggarwalTheIO1987,
  title = {The {{I}}/{{O}} Complexity of Sorting and Related Problems (Extended Abstract)},
  author = {Aggarwal, A. and Vitter, J.},
  year = {1987},
  doi = {10.1007/3-540-18088-5_40},
  abstract = {No abstract available},
  citationcount = {31},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{ahleSubsetsAndSupermajorities2019,
  title = {Subsets and Supermajorities: {{Optimal}} Hashing-Based Set Similarity Search},
  author = {Ahle, Thomas Dybdahl and Knudsen, Jakob B{\ae}k Tejs},
  year = {2019},
  doi = {10.1109/FOCS46700.2020.00073},
  abstract = {We formulate and optimally solve a new generalized Set Similarity Search problem, which assumes the size of the database and query sets are known in advance. By creating polylog copies of our data-structure, we optimally solve any symmetric Approximate Set Similarity Search problem, including approximate versions of Subset Search, Maximum Inner Product Search (MIPS), Jaccard Similarity Search, and Partial Match. Our algorithm can be seen as a natural generalization of previous work on Set as well as Euclidean Similarity Search, but conceptually it differs by optimally exploiting the information present in the sets as well as their complements, and doing so asymmetrically between queries and stored sets. Doing so we improve upon the best previous work: MinHash [J. Discrete Algorithms 1998], SimHash [STOC 2002], Spherical LSF [SODA 2016, 2017], and Chosen Path [STOC 2017] by as much as a factor n\textsuperscript{\{\vphantom\}}0.14\vphantom\{\} in both time and space; or in the near-constant time regime, in space, by an arbitrarily large polynomial factor. Turning the geometric concept, based on Boolean supermajority functions, into a practical algorithm requires ideas from branching random walks on \{Z\}\textsuperscript{\{\vphantom\}}2\vphantom\{\}, for which we give the first non-asymptotic near tight analysis. Our lower bounds follow from new hypercontractive arguments, which can be seen as characterizing the exact family of similarity search problems for which supermajorities are optimal. The optimality holds for among all hashing based data structures in the random setting, and by reductions, for 1 cell and 2 cell probe data structures.},
  citationcount = {Unknown},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {cell probe,data structure,lower bound,query,reduction,sorted}
}

@article{ahoOnFindingLowest1973,
  title = {On Finding Lowest Common Ancestors in Trees},
  author = {Aho, A. and Hopcroft, J. and Ullman, J.},
  year = {1973},
  doi = {10.1145/800125.804056},
  abstract = {Trees in an n node forest are to be merged according to instructions in a given sequence, while other instructions in the sequence ask for the lowest common ancestor of pairs of nodes. We show that any sequence of O(n) instructions can be processed ``on line'' in O(n log n) steps on a random access computer. If we can accept our answer ``off-line'', that is, no answers need to be produced until the entire sequence of instructions has been seen seen, then we may perform the task in O(n G(n)) steps, where G(n) is the number of times we must apply log2 to n to obtain a number less than or equal to zero. A third algorithm solves a problem of intermediate complexity. We require the answers on line, but we suppose that all tree merging instructions precede the information requests. This algorithm requires O(n log log n) time. We apply the first on line algorithm to a problem in code optimization, that of computing immediate dominators in a reducible flow graph. We show how this computation can be performed in O(n log n) steps.},
  citationcount = {192},
  venue = {SIAM journal on computing (Print)}
}

@article{aidanhoganScalableDistributedMethods2012,
  title = {Scalable and Distributed Methods for Entity Matching, Consolidation and Disambiguation over Linked Data Corpora},
  author = {Aidan Hogan and Antoine Zimmermann and J{\"u}rgen Umbrich and A. Polleres and S. Decker},
  year = {2012},
  journal = {Journal of Web Semantics},
  doi = {10.1016/j.websem.2011.11.002},
  annotation = {Citation Count: 106}
}

@article{ailonApproximateNearestNeighbors2006,
  title = {Approximate Nearest Neighbors and the Fast {{Johnson-Lindenstrauss}} Transform},
  author = {Ailon, Nir and Chazelle, B.},
  year = {2006},
  doi = {10.1145/1132516.1132597},
  abstract = {We introduce a new low-distortion embedding of l{\textexclamdown}sub{\textquestiondown}2{\textexclamdown}/sub{\textquestiondown}{\textexclamdown}sup{\textquestiondown}d{\textexclamdown}/sup{\textquestiondown} into l{\textexclamdown}sub{\textquestiondown}p{\textexclamdown}/sub{\textquestiondown}{\textexclamdown}sup{\textquestiondown}O(log n){\textexclamdown}/sup{\textquestiondown} (p=1,2), called the {\textexclamdown}i{\textquestiondown}Fast-Johnson-Linden-strauss-Transform{\textexclamdown}/i{\textquestiondown}. The FJLT is faster than standard random projections and just as easy to implement. It is based upon the preconditioning of a sparse projection matrix with a randomized Fourier transform. Sparse random projections are unsuitable for low-distortion embeddings. We overcome this handicap by exploiting the "Heisenberg principle" of the Fourier transform, ie, its local-global duality. The FJLT can be used to speed up search algorithms based on low-distortion embeddings in l{\textexclamdown}sub{\textquestiondown}1{\textexclamdown}/sub{\textquestiondown} and l{\textexclamdown}sub{\textquestiondown}2{\textexclamdown}/sub{\textquestiondown}. We consider the case of approximate nearest neighbors in l{\textexclamdown}sub{\textquestiondown}2{\textexclamdown}/sub{\textquestiondown}{\textexclamdown}sup{\textquestiondown}d{\textexclamdown}/sup{\textquestiondown}. We provide a faster algorithm using classical projections, which we then further speed up by plugging in the FJLT. We also give a faster algorithm for searching over the hypercube.},
  citationcount = {529},
  venue = {Symposium on the Theory of Computing}
}

@article{ailonFasterDimensionReduction2010,
  title = {Faster Dimension Reduction},
  author = {Ailon, Nir and Chazelle, B.},
  year = {2010},
  doi = {10.1145/1646353.1646379},
  abstract = {Data represented geometrically in high-dimensional vector spaces can be found in many applications. Images and videos, are often represented by assigning a dimension for every pixel (and time). Text documents may be represented in a vector space where each word in the dictionary incurs a dimension. The need to manipulate such data in huge corpora such as the web and to support various query types gives rise to the question of how to represent the data in a lower-dimensional space to allow more space and time efficient computation. Linear mappings are an attractive approach to this problem because the mapped input can be readily fed into popular algorithms that operate on linear spaces (such as principal-component analysis, PCA) while avoiding the curse of dimensionality. The fact that such mappings even exist became known in computer science following seminal work by Johnson and Lindenstrauss in the early 1980s. The underlying technique is often called "random projection." The complexity of the mapping itself, essentially the product of a vector with a dense matrix, did not attract much attention until recently. In 2006, we discovered a way to "sparsify" the matrix via a computational version of Heisenberg's Uncertainty Principle. This led to a significant speedup, which also retained the practical simplicity of the standard Johnson-Lindenstrauss projection. We describe the improvement in this article, together with some of its applications.},
  citationcount = {76},
  venue = {Communications of the ACM},
  keywords = {query,reduction}
}

@article{ailonTheFastJohnson2009,
  title = {The Fast Johnson--Lindenstrauss Transform and Approximate Nearest Neighbors},
  author = {Ailon, Nir and Chazelle, B.},
  year = {2009},
  doi = {10.1137/060673096},
  abstract = {We introduce a new low-distortion embedding of {$\ell_{2}^d$} into {$\ell_{p}$}\textsuperscript{\{\vphantom\}}O(n)\vphantom\{\} (p=1,2) called the fast Johnson-Lindenstrauss transform (FJLT). The FJLT is faster than standard random projections and just as easy to implement. It is based upon the preconditioning of a sparse projection matrix with a randomized Fourier transform. Sparse random projections are unsuitable for low-distortion embeddings. We overcome this handicap by exploiting the ``Heisenberg principle'' of the Fourier transform, i.e., its local-global duality. The FJLT can be used to speed up search algorithms based on low-distortion embeddings in {$\ell_1$} and {$\ell_2$}. We consider the case of approximate nearest neighbors in {$\ell_{2}^d$}. We provide a faster algorithm using classical projections, which we then speed up further by plugging in the FJLT. We also give a faster algorithm for searching over the hypercube.},
  citationcount = {506},
  venue = {SIAM journal on computing (Print)}
}

@article{ajtaiANonLinear1999,
  title = {A Non-Linear Time Lower Bound for {{Boolean}} Branching Programs},
  author = {Ajtai, M.},
  year = {1999},
  doi = {10.1109/SFFCS.1999.814578},
  abstract = {We prove that for all positive integer k and for all sufficiently small /spl epsiv/{\textquestiondown}0 if n is sufficiently large then there is no Boolean (or 2-way) branching program of size less than 2/sup em/ which for all inputs X/spl sube/\{0, 1, ..., n-1\} computes in time kn the parity of the number of elements of the set of all pairs (x,y) with the property x/spl isin/X, y/spl isin/X, x{\textexclamdown}y, x+y/spl isin/X. For the proof of this fact we show that if A=(/spl alpha//sub i,j/)/sub i=0, j=0//sup n/ is a random n by n matrix over the field with 2 elements with the condition that "/spl forall/, j, k, l/spl isin/\{0, 1, ..., n-1\}, i+j=k+l implies /spl alpha//sub i,j/=/spl alpha//sub k,l/" then with a high probability the rank of each /spl delta/n by /spl delta/n submatrix of A is at least c/spl delta/{\textbar}log /spl delta/{\textbar}/sup -2/n, where c{\textquestiondown}0 is an absolute constant and n is sufficiently large with respect to /spl delta/.},
  citationcount = {104},
  venue = {40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039)}
}

@article{ajtaiDeterminismVersusNon1999,
  title = {Determinism versus Non-Determinism for Linear Time {{RAMs}} (Extended Abstract)},
  author = {Ajtai, M.},
  year = {1999},
  doi = {10.1145/301250.301424},
  abstract = {Our computational model is a random access machine with n read only input registers each containing c log n bits of information and a read and wile memory. We measure the time by the number of accesses to the input registers. We show thal for all k there is an E {\textquestiondown} 0 so that if n is sufficiently large then the elements distinctness problem cannot be solved in time kn with en bits of read and wile memory. that is. there is no machine with this values of the parameters which decides whether there are two different input registers whose contents are identical. Intmduction. One of the main goals of complexity theory is the separation of non-deterministic and deterministic computation. We solve the problem for random access machines with certain restrictions on the size of their working memory. Although the restrictions are strong, the working memory must be smaller than the input. still. under certain circumstances this computational model is realistic as we will explain later. Our seacrh problem. as described in the abstract is the element distinctness problem. that is. we have to decide whether there are different input registers with identical contents. We also show that there is a simple decision problem that can be solved in constant time (actually in IWO steps) using non-deterministic computation, while there is no deterministic linear time algorithm with enlogn bits read and write memory which solves the problem. More precisely if we allow kn time for some fixed constant k, then there is an 6 {\textquestiondown} 0 so that the problem cannot be solved with in log n bits of read and write memory if n is sufficiently large. The decision problem is the following: ``Find two different input registers, so that the Hamming distance of their contents is at most i c log n''. i can be replaced by any fixed 0 {\textexclamdown} 7 {\textexclamdown} 4 if c is sufficiently loge with respect to 7. We actually show that the promise problem : ``decide whether all occurring Hamming dist`ances are greater than (r)clognCopyrightACM1999I-58113.067.8199105...5.00 or there is at least one which is smaller than ye log n'' where 7 {\textquestiondown} 0 is an arbitmrily small constant, cannot be solved by a nonlinear algorithtn with the described limitations even if we know that we only get inputs where one of these conditions hold. (In this case E may depend on y loo). The proof of the theorem about the element distinctness problem is the main contribution of the present paper to the theory of lower bounds. We include the theorem about Hamming distances to give a more complete picture about the determinism versus non-determinism question and also because both in a motivational and technical sense the element distinctness result is built on it. (For a comparison to previously known lower hounds see the remarks about branching programs below.) The element distinctness problem is of great practical and theoretical interest, it has been studied in great detail in various computational models, particularly in the comparison model (see [BFKLT]. [BFW. [Kl, [Yl). A time space tradoff TS = n(n'') for the elements distinctness problem on comparison-based branching programs was conjectured by Borodin et al in [BFKLTI. A. Yao [Yl proved a tradeoff TS = R(na-`(``)), where c(n) = O(l/(logn)+), which is very close IO optimal since Z'S = O(n') is achievable even for sorting in the range c1 log n {\textexclamdown} S {\textexclamdown} czn/ log n (See [PR]). The best upper bounds for the element distinctness problem are given in the RAM model. We can solve the element distinctness problem with bucket sorting in our RAM in linear time with c'n log n bits of read and write memory, where c' is a suitably choose'' constant (see [AHlJl). This is a determinstic (non-probabilistic) algorithm. Our lower bounds are also about non-probabilistic algorithms. For the element distinctness problem we give a probabilistic algorithm which solves it in time kn with en hits of read and write memory provided that k {\textquestiondown} 0 is sufficicntly large with respect to e and n is sufficiently large with respect to k. Moreover our algorithm can be implemented in a random access machine defined in the usual sense. that is where 11x memory is organized into registers and we allow only arithmetic operations etc. (For the exact statement of this result see Theorem 5 in the last section.) This makes it very unlikely that our lower bound for the element distinclness problem can be improved since the ratio between the},
  citationcount = {32},
  venue = {Symposium on the Theory of Computing}
}

@article{ajtaiDeterministicSimulationIn1987,
  title = {Deterministic Simulation in {{LOGSPACE}}},
  author = {Ajtai, M. and Komlos, J. and Szemer{\'e}di, E.},
  year = {1987},
  doi = {10.1145/28395.28410},
  abstract = {In this paper we show that a wide class of probabilistic algorithms can be simulated by deterministic algorithms. Namely if there is a test in LOGSPACE so that a random sequence of length (log n)2 / log log n passes the test with probability at least 1/n then a deterministic sequence can be constructed in LOGSPACE which also passes the test. It is important that the machine performing the test gets each bit of the sequence only once. The theorem remains valid if both the test and the machine constructing the satisfying sequence have access to the same oracle of polynomial size. The sequence that we construct does not really depend on the test, in the sense that a polynomial family of sequences is constructed so that at least one of them passes any test. This family is the same even if the test is allowed to use an oracle of polynomial size, and it can be constructed in LOGSPACE (without using an oracle).},
  citationcount = {248},
  venue = {Symposium on the Theory of Computing}
}

@article{ajtaiHashFunctionsFor1983,
  title = {Hash Functions for Priority Queues},
  author = {Ajtai, M. and Fredman, M. and Komlos, J.},
  year = {1983},
  doi = {10.1109/SFCS.1983.24},
  abstract = {The complexity of priority queue operations is analyzed with respect to the cell probe computational model of A. Yao. A method utilizing families of hash functions is developed which permits priority queue operations to be implemented in constant worst case time provided that a size constraint is satisfied. The minimum necessary size of a family of hash functions for computing the rank function is estimated and contrasted with the minimum size required for perfect hashing.},
  citationcount = {59},
  venue = {24th Annual Symposium on Foundations of Computer Science (sfcs 1983)},
  keywords = {cell probe}
}

@article{ajtaiLowerBoundFinding1988,
  title = {A Lower Bound for Finding Predecessors in {{Yao}}'s Cell Probe Model},
  author = {Ajtai, M.},
  year = {1988},
  month = sep,
  journal = {Combinatorica},
  volume = {8},
  number = {3},
  pages = {235--247},
  issn = {1439-6912},
  doi = {10.1007/BF02126797},
  url = {https://doi.org/10.1007/BF02126797},
  urldate = {2024-11-19},
  abstract = {LetL be the set consisting of the firstq positive integers. We prove in this paper that there does not exist a data structure for representing an arbitrary subsetA ofL which uses poly (A) cells of memory (where each cell holdsc logq bits of information) and which the predecessor inA of an arbitraryx{$\leqq$}q can be determined by probing only a constant (independent ofq) number of cells. Actually our proof gives more: the theorem remains valid if this number is less than{$\varepsilon$} log logq, that is D. E. Willard's algorithm [2] for finding the predecessor inO(log logq) time is optimal up to a constant factor.},
  langid = {english},
  keywords = {cell probe,data structure,lower bound,sorted},
  annotation = {M.Ajtai, M.Fredman and J.Koml{\'o}s, Hash Functions for priority Queues,Proceedings of the 24th Annual Symposium on FOCS, 1983.\\
D. E. Willard, Logarithmic worst case range queries are possible in spaceO(n), Inform. Proc. Letter,17 (1983), 81--89.\\
A. Yao, Should tables be sorted,JACM 28, 3 (July 1981), 615--628.},
  file = {/Users/tulasi/Zotero/storage/FPCJMIXD/Ajtai - 1988 - A lower bound for finding predecessors in Yao's cell probe model.pdf}
}

@article{akmalImprovedMerlinArthur2023,
  title = {Improved Merlin--Arthur Protocols for Central Problems in Fine-Grained Complexity},
  author = {Akmal, Shyan S. and Chen, Lijie and Jin, Ce and Raj, M. and Williams, Ryan},
  year = {2023},
  doi = {10.1007/s00453-023-01102-6},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {Algorithmica}
}

@article{alaamaaloufFastAccurateLeastMeanSquares2019,
  title = {Fast and {{Accurate Least-Mean-Squares Solvers}} for {{High Dimensional Data}}},
  author = {Alaa Maalouf and Ibrahim Jubran and Dan Feldman},
  year = {2019},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/TPAMI.2021.3139612},
  abstract = {Least-mean-squares (LMS) solvers such as Linear / Ridge-Regression and SVD not only solve fundamental machine learning problems, but are also the building blocks in a variety of other methods, such as matrix factorizations. We suggest an algorithm that gets a finite set of {$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$$}n\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}/mml:math{$><$}inline-graphic xlink:href="maalouf-ieq1-3139612.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$} {$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$$}d\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mi{$>$}d{$<$}/mml:mi{$><$}/mml:math{$><$}inline-graphic xlink:href="maalouf-ieq2-3139612.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$}-dimensional real vectors and returns a subset of {$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$$}d+1\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mrow{$><$}mml:mi{$>$}d{$<$}/mml:mi{$><$}mml:mo{$>$}+{$<$}/mml:mo{$><$}mml:mn{$>$}1{$<$}/mml:mn{$><$}/mml:mrow{$><$}/mml:math{$><$}inline-graphic xlink:href="maalouf-ieq3-3139612.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$} vectors with positive weights whose weighted sum is {$<$}italic{$>$}exactly{$<$}/italic{$>$} the same. The constructive proof in Caratheodory's Theorem computes such a subset in {$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$$}O(n{\textasciicircum}2d{\textasciicircum}2)\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mrow{$><$}mml:mi{$>$}O{$<$}/mml:mi{$><$}mml:mo{$>$}({$<$}/mml:mo{$><$}mml:msup{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}mml:mn{$>$}2{$<$}/mml:mn{$><$}/mml:msup{$><$}mml:msup{$><$}mml:mi{$>$}d{$<$}/mml:mi{$><$}mml:mn{$>$}2{$<$}/mml:mn{$><$}/mml:msup{$><$}mml:mo{$>$}){$<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:math{$><$}inline-graphic xlink:href="maalouf-ieq4-3139612.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$} time and thus not used in practice. Our algorithm computes this subset in {$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$$}O(nd+d{\textasciicircum}4{\textbackslash}log \{n\})\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mrow{$><$}mml:mi{$>$}O{$<$}/mml:mi{$><$}mml:mo{$>$}({$<$}/mml:mo{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}mml:mi{$>$}d{$<$}/mml:mi{$><$}mml:mo{$>$}+{$<$}/mml:mo{$><$}mml:msup{$><$}mml:mi{$>$}d{$<$}/mml:mi{$><$}mml:mn{$>$}4{$<$}/mml:mn{$><$}/mml:msup{$><$}mml:mo form="prefix"{$>$}log{$<$}/mml:mo{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}mml:mo{$>$}){$<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:math{$><$}inline-graphic xlink:href="maalouf-ieq5-3139612.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$} time, using {$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$$}O({\textbackslash}log n)\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mrow{$><$}mml:mi{$>$}O{$<$}/mml:mi{$><$}mml:mo{$>$}({$<$}/mml:mo{$><$}mml:mo form="prefix"{$>$}log{$<$}/mml:mo{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}mml:mo{$>$}){$<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:math{$><$}inline-graphic xlink:href="maalouf-ieq6-3139612.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$} calls to Caratheodory's construction on small but ``smart'' subsets. This is based on a novel paradigm of fusion between different data summarization techniques, known as sketches and coresets. For large values of {$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$$}d\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mi{$>$}d{$<$}/mml:mi{$><$}/mml:math{$><$}inline-graphic xlink:href="maalouf-ieq7-3139612.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$}, we suggest a faster construction that takes {$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$$}O(nd)\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mrow{$><$}mml:mi{$>$}O{$<$}/mml:mi{$><$}mml:mo{$>$}({$<$}/mml:mo{$><$}mml:mi{$>$}n{$<$}/mml:mi{$><$}mml:mi{$>$}d{$<$}/mml:mi{$><$}mml:mo{$>$}){$<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:math{$><$}inline-graphic xlink:href="maalouf-ieq8-3139612.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$} time and returns a weighted subset of {$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$$}O(d)\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mrow{$><$}mml:mi{$>$}O{$<$}/mml:mi{$><$}mml:mo{$>$}({$<$}/mml:mo{$><$}mml:mi{$>$}d{$<$}/mml:mi{$><$}mml:mo{$>$}){$<$}/mml:mo{$><$}/mml:mrow{$><$}/mml:math{$><$}inline-graphic xlink:href="maalouf-ieq9-3139612.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$} sparsified input points. Here, a sparsified point means that some of its entries were set to zero. As an application, we show how to boost the performance of existing LMS solvers, such as those in scikit-learn library, up to x100. Generalization for streaming and distributed data is trivial. Extensive experimental results and open source code are provided.},
  annotation = {Citation Count: 24}
}

@article{alankoBufferingUpdatesEnables2021,
  title = {Buffering Updates Enables Efficient Dynamic de {{Bruijn}} Graphs},
  author = {Alanko, Jarno N. and Alipanahi, B. and Settle, J. and Boucher, C. and Gagie, T.},
  year = {2021},
  doi = {10.1016/j.csbj.2021.06.047},
  abstract = {No abstract available},
  citationcount = {10},
  venue = {bioRxiv},
  keywords = {dynamic,update}
}

@article{alegra-galiciaCapturingPointsWith2018,
  title = {Capturing Points with a Rotating Polygon (and a {{3D}} Extension)},
  author = {{Alegr{\'i}a-Galicia}, Carlos and Orden, David and Palios, Leonidas and Seara, C. and Urrutia, J.},
  year = {2018},
  doi = {10.1007/s00224-018-9885-y},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Theory of Computing Systems}
}

@article{alexanderExtremalProblemsOf1974,
  title = {Extremal Problems of Distance Geometry Related to Energy Integrals},
  author = {Alexander, R. and Stolarsky, K.},
  year = {1974},
  doi = {10.1090/S0002-9947-1974-0350629-3},
  abstract = {Let K be a compact set, XT a prescribed family of (possibly signed) Borel measures of total mass one supported by K, and f a continuous real-valued function on K x K. We study the problem of determining for which , e Y1 (if any) the energy integral I(K, A) = IKIKf(X, y)dgXx)dgXy) is maximal, and what this maximum is. The more symmetry K has, the more we can say; our results are best when K is a sphere. In particular, when X is atomic we obtain good upper bounds for the sums of powers of all (2) distances determined by n points on the surface of a sphere. We make use of results from Schoenberg's theory of metric embedding, and of techniques devised by Polya and Szego for the calculation of transfinite diameters. 1. Background and summary of results. In this paper we will investigate a number of extremal problems in distance geometry. Our work is in many ways analogous to the study of energy integrals in classical potential theory. Let K be a compact set in a Euclidean space and 51 be a prescribed family of Borel measures (possibly signed) of total mass one supported by K. Suppose f is a continuous real-valued function on K x K. We consider the family of integrals having the form (1.1) I(K, = {\textbar} f (x, y) d1(x)d1d(y), it E 1. A number of interesting questions naturally arise concerning 1(K), the supremum of the numbers I(K, pi) with it in Vl: (i) What is the numerical value of I(K)? (ii) Does there exist a y0 in V1I such that I(K, Io) = 1(K)? (iii) If y0 exists, is this measure unique? (iv) Can an extremal measure [t be explicitly produced? Received by the editors March 20, 1973 and, in revised form, April 20, 1973. AMS (MOS) subject classifications (1970). Primary 52A25, 52A40.},
  citationcount = {59},
  venue = {No venue available}
}

@article{alexanderfedorovScalableConcurrentAlgorithm2021,
  title = {A {{Scalable Concurrent Algorithm}} for {{Dynamic Connectivity}}},
  author = {Alexander Fedorov and N. Koval and Dan Alistarh},
  year = {2021},
  journal = {ACM Symposium on Parallelism in Algorithms and Architectures},
  doi = {10.1145/3409964.3461810},
  abstract = {Dynamic Connectivity is a fundamental algorithmic graph problem, motivated by a wide range of applications to social and communication networks and used as a building block in various other algorithms, such as the bi-connectivity and the dynamic minimal spanning tree problems. In brief, we wish to maintain the connected components of the graph under dynamic edge insertions and deletions. In the sequential case, the problem has been well-studied from both theoretical and practical perspectives. However, much less is known about efficient concurrent solutions to this problem. This is the gap we address in this paper. We start from one of the classic data structures used to solve this problem, the Euler Tour Tree. Our first contribution is a non-blocking single-writer implementation of it. We leverage this data structure to obtain the first truly concurrent generalization of dynamic connectivity, which preserves the time complexity of its sequential counterpart, but is also scalable in practice. To achieve this, we rely on three main techniques. The first is to ensure that connectivity queries, which usually dominate real-world workloads, are non-blocking. The second non-trivial technique expands the above idea by making all queries that do not change the connectivity structure non-blocking. The third ingredient is applying fine-grained locking for updating the connected components, which allows operations on disjoint components to occur in parallel. We evaluate the resulting algorithm on various workloads, executing on both real and synthetic graphs. The results show the efficiency of each of the proposed optimizations; the most efficient variant improves the performance of a coarse-grained based implementation on realistic scenarios up to 6x on average and up to 30x when connectivity queries dominate.},
  keywords = {communication,data structure,dynamic,query},
  annotation = {Citation Count: 1}
}

@article{alexanderGeneralizedSumsOf1975,
  title = {Generalized Sums of Distances},
  author = {Alexander, R.},
  year = {1975},
  doi = {10.2140/PJM.1975.56.297},
  abstract = {Let K be a compact set in a Euclidean space and let d be a metric on K which is continuous with respect to the usual topology. The generalized energy integral I({$\mu$}) = ff d(x, y) d{$\mu$}(x) d{$\mu$}\{y) is investigated as {$\mu$} is allowed to range over the lamily of signed Borel measures of total mass one concentrated on K. A trick of integral geometry is used to define a class of metrics d, including many standard ones, possessing a number of pleasing properties related to the functional /.\vphantom\}},
  citationcount = {24},
  venue = {No venue available}
}

@article{alexanderGeometricMethodsStudy1990,
  title = {Geometric Methods in the Study of Irregularities of Distribution},
  author = {Alexander, R.},
  year = {1990},
  journal = {Combinatorica},
  volume = {10},
  number = {2},
  pages = {115--136},
  doi = {10.1007/BF02123006},
  annotation = {R. Alexander: On the sum of distances betweenn points on a sphere,Acta Math. Acad. Sci. Hungar.,23 (1972), 443--448.\\
R. Alexander: Generalized sums of distances,Pacific Jour. Math.,56 (1975), 297--304.\\
R. Alexander: On the sum of distances betweenn points on a sphere. II,Acta Math. Acad. Sci. Hungar.,29 (1977), 317--320.\\
R. Alexander: Metric averaging in Euclidean and Hilbert spaces,Pacific Jour. Math.,85 (1979), 1--9.\\
R. Alexander, andK. B. Stolarsky: Extremal problems of distance geometry related to energy integrals,Trans. Amer. Soc.,193 (1973), 1--31.\\
J. Beck: Sums of distances between points on a sphere --- an application of the theory of irregularities of distribution to discrete geometry,Mathematica,31 (1984), 33--41.\\
J. Beck, andW. W. L. Chen:Irregularities of distribution, Cambridge Tracts in Mathematics,89, Cambridge University Press, Cambridge, 1987.\\
L. Kuipers, andH. Niederreiter:Uniform distribution of sequences, John Wiley, New York,1974.\\
K. F. Roth: On irregularities of distribution,Mathematica,1 (1954), 73--79.\\
L. A. Santal{\'o}:Integral geometry and geometric probability, Encyclopedia of Mathematics and its Applications,1, Addison-Wesley, Reading, Mass., 1976.\\
I. J. Schoenberg: On certain metric spaces arising from Euclidean spaces by change of metric and their embedding in Hilbert space,Ann. of Math.,38 (1937), 787--793.\\
W. M. Schmidt: Irregularities of distribution. IV,Invent. Math.,7 (1969), 55--82.\\
W. M. Schmidt: Irregularities of distribution. VII,Acta Arith.,21 (1972), 45--50.\\
K. B. Stolarsky: Sums of distances between points on a sphere,Proc. Amer. Math Soc.,35 (1972), 547--549.\\
K. B. Stolarsky: Sums of distances between points on a sphere. II,Proc. Amer. Math. Soc.,41 (1973), 575--582.\\
K. B. Stolarsky: Spherical distributions ofn points with maximal distance sums are well spaced,Proc. Amer. Math. Soc.,48 (1975), 203--206.\\
K. B.Stolarsky:Discrepancy and sums of distances between points of a metric space, The geometry of metric and linear spaces, Ed. L. M. Kelly, Springer-Verlag,1975, 44--55.\\
H. Weyl: {\"U}ber die Gleichverteilung von Zahlen mod. Eins,Math. Ann.,77 (1916), 313--352.},
  file = {/Users/tulasi/Zotero/storage/3BWP3HLN/Alexander - 1990 - Geometric methods in the study of irregularities of distribution.pdf}
}

@article{alexanderMetricAveragingIn1979,
  title = {Metric Averaging in Euclidean and Hilbert Spaces},
  author = {Alexander, R.},
  year = {1979},
  doi = {10.2140/PJM.1979.85.1},
  abstract = {A number of geometric properties of sets in I 2 can be measured in terms of maxima and minima of quadratic forms subject to side conditions. A method of metric addition which exploits this fact is investigated. The main objective is to develop a flexible method for attacking geometric extremal problems involving sums of distances or other metric constraints.},
  citationcount = {5},
  venue = {No venue available}
}

@article{alexanderOnTheSum1972,
  title = {On the Sum of Distances Betweenn Points on a Sphere. {{II}}},
  author = {Alexander, R.},
  year = {1972},
  doi = {10.1007/BF01895852},
  abstract = {No abstract available},
  citationcount = {52},
  venue = {No venue available}
}

@article{alimohammadlavasaniNewmansTheoremCaratheodory2024,
  title = {Newman's Theorem via {{Carath{\'e}odory}}},
  author = {Ali Mohammad Lavasani and Yaqiao Li and Mehran Shakerinava},
  year = {2024},
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2406.08500},
  abstract = {We give a streamlined short proof of Newman's theorem in communication complexity by applying the classical and the approximate Carath{\textbackslash}'eodory's theorems.},
  keywords = {communication,communication complexity},
  annotation = {Citation Count: 0}
}

@article{almanAlgorithmsAndHardness2020,
  title = {Algorithms and Hardness for Linear Algebra on Geometric Graphs},
  author = {Alman, Josh and Chu, T. and Schild, Aaron and Song, Zhao-quan},
  year = {2020},
  doi = {10.1109/FOCS46700.2020.00057},
  abstract = {For a function {\textexclamdown}tex{\textquestiondown}\{K\}:\{R\}\textsuperscript{\{\vphantom\}}d\vphantom\{\}{\texttimes}\{R\}\textsuperscript{\{\vphantom\}}d\vphantom\{\}{$\rightarrow$}\{R\}\textsubscript{\{\vphantom\}}{$\geq$}0\vphantom\{\}{\textexclamdown}/tex{\textquestiondown}, and a set {\textexclamdown}tex{\textquestiondown}P= x\_\{1\},{\dots},x\_\{n\} {$\subset$}\{R\}\textsuperscript{\{\vphantom\}}d\vphantom\{\}{\textexclamdown}/tex{\textquestiondown} of {\textexclamdown}tex{\textquestiondown}n{\textexclamdown}/tex{\textquestiondown} points, the K graph {\textexclamdown}tex{\textquestiondown}G\textsubscript{\{\vphantom\}}P\vphantom\{\}{\textexclamdown}/tex{\textquestiondown} of {\textexclamdown}tex{\textquestiondown}P{\textexclamdown}/tex{\textquestiondown} is the complete graph on {\textexclamdown}tex{\textquestiondown}n{\textexclamdown}/tex{\textquestiondown} nodes where the weight between nodes {\textexclamdown}tex{\textquestiondown}i{\textexclamdown}/tex{\textquestiondown} and {\textexclamdown}tex{\textquestiondown}j{\textexclamdown}/tex{\textquestiondown} is given by {\textexclamdown}tex{\textquestiondown}\{K\}(x\textsubscript{\{\vphantom\}}i\vphantom\{\},x\textsubscript{\{\vphantom\}}j\vphantom\{\}){\textexclamdown}/tex{\textquestiondown}. In this paper, we initiate the study of when efficient spectral graph theory is possible on these graphs. We investigate whether or not it is possible to solve the following problems in {\textexclamdown}tex{\textquestiondown}n\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}{\textexclamdown}/tex{\textquestiondown} time for a K-graph {\textexclamdown}tex{\textquestiondown}G\textsubscript{\{\vphantom\}}P\vphantom\{\}{\textexclamdown}/tex{\textquestiondown} when {\textexclamdown}tex{\textquestiondown}d\{o(1)\}{\textexclamdown}/tex{\textquestiondown}: {$\bullet$}Multiply a given vector by the adjacency matrix or Laplacian matrix of {\textexclamdown}tex{\textquestiondown}G\textsubscript{\{\vphantom\}}P\vphantom\{\}{\textexclamdown}/tex{\textquestiondown} {$\bullet$}Find a spectral sparsifier of {\textexclamdown}tex{\textquestiondown}G\textsubscript{\{\vphantom\}}P\vphantom\{\}{\textexclamdown}/tex{\textquestiondown} {$\bullet$}Solve a Laplacian system in {\textexclamdown}tex{\textquestiondown}G\textsubscript{\{\vphantom\}}P\vphantom\{\}{\textexclamdown}/tex{\textquestiondown}'s Laplacian matrix For each of these problems, we consider all functions of the form {\textexclamdown}tex{\textquestiondown}\{K\}(u,v)=f({\textbardbl}u-v{\textbardbl}\textsubscript{\{\vphantom\}}2\vphantom\{\}\textsuperscript{\{\vphantom\}}2\vphantom\{\}){\textexclamdown}/tex{\textquestiondown} for a function {\textexclamdown}tex{\textquestiondown}f:\{R\}{$\rightarrow$}\{R\}{\textexclamdown}/tex{\textquestiondown}. We provide algorithms and comparable hardness results for many such K, including the Gaussian kernel, Neural tangent kernels, and more. For example, in dimension {\textexclamdown}tex{\textquestiondown}d={\textohm}(n){\textexclamdown}/tex{\textquestiondown}, we show that there is a parameter associated with the function {\textexclamdown}tex{\textquestiondown}f{\textexclamdown}/tex{\textquestiondown} for which low parameter values imply {\textexclamdown}tex{\textquestiondown}n\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}{\textexclamdown}/tex{\textquestiondown} time algorithms for all three of these problems and high parameter values imply the nonexistence of subquadratic time algorithms assuming Strong Exponential Time Hypothesis (SETH), given natural assumptions on {\textexclamdown}tex{\textquestiondown}f{\textexclamdown}/tex{\textquestiondown}. As part of our results, we also show that the exponential dependence on the dimension {\textexclamdown}tex{\textquestiondown}d{\textexclamdown}/tex{\textquestiondown} in the celebrated fast multi-pole method of Greengard and Rokhlin cannot be improved, assuming SETH, for a broad class of functions {\textexclamdown}tex{\textquestiondown}f{\textexclamdown}/tex{\textquestiondown}. To the best of our knowledge, this is the first formal limitation proven about fast multipole methods.},
  citationcount = {29},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@inproceedings{almanCellprobeLowerBounds2017,
  title = {Cell-Probe Lower Bounds from Online Communication Complexity},
  author = {Alman, Josh and Wang, Joshua R. and Yu, Huacheng},
  year = {2017},
  doi = {10.1145/3188745.3188862},
  abstract = {In this work, we introduce an online model for communication complexity. Analogous to how online algorithms receive their input piece-by-piece, our model presents one of the players, Bob, his input piece-by-piece, and has the players Alice and Bob cooperate to compute a result each time before the next piece is revealed to Bob. This model has a closer and more natural correspondence to dynamic data structures than classic communication models do, and hence presents a new perspective on data structures. We first present a tight lower bound for the online set intersection problem in the online communication model, demonstrating a general approach for proving online communication lower bounds. The online communication model prevents a batching trick that classic communication complexity allows, and yields a stronger lower bound. We then apply the online communication model to prove data structure lower bounds for two dynamic data structure problems: the Group Range problem and the Dynamic Connectivity problem for forests. Both of the problems admit a worst case O(logn)-time data structure. Using online communication complexity, we prove a tight cell-probe lower bound for each: spending o(logn) (even amortized) time per operation results in at best an exp(-{$\delta$}2 n) probability of correctly answering a (1/2+{$\delta$})-fraction of the n queries.},
  citationcount = {4},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {cell probe,communication,communication complexity,data structure,dynamic,lower bound,query,sorted},
  file = {/Users/tulasi/Zotero/storage/FT5BIMGH/Alman et al. - 2018 - Cell-probe lower bounds from online communication complexity.pdf}
}

@article{almanDynamicParameterizedProblems2017,
  title = {Dynamic Parameterized Problems and Algorithms},
  author = {Alman, Josh and Mnich, Matthias and Williams, V. V.},
  year = {2017},
  doi = {10.1145/3395037},
  abstract = {Fixed-parameter algorithms and kernelization are two powerful methods to solve NP-hard problems. Yet so far those algorithms have been largely restricted to static inputs. In this article, we provide fixed-parameter algorithms and kernelizations for fundamental NP-hard problems with dynamic inputs. We consider a variety of parameterized graph and hitting set problems that are known to have f(k)n1+o(1) time algorithms on inputs of size n, and we consider the question of whether there is a data structure that supports small updates (such as edge/vertex/set/element insertions and deletions) with an update time of g(k)no(1); such an update time would be essentially optimal. Update and query times independent of n are particularly desirable. Among many other results, we show that FEEDBACK VERTEX SET and k-PATH admit dynamic algorithms with f(k)log O(1) update and query times for some function f depending on the solution size k only. We complement our positive results by several conditional and unconditional lower bounds. For example, we show that unlike their undirected counterparts, DIRECTED FEEDBACK VERTEX SET and DIRECTED k-PATH do not admit dynamic algorithms with no(1) update and query times even for constant solution sizes k {$\leq$} 3, assuming popular hardness hypotheses. We also show that unconditionally, in the cell probe model, DIRECTED FEEDBACK VERTEX SET cannot be solved with update time that is purely a function of k.},
  citationcount = {26},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {cell probe,data structure,dynamic,lower bound,query,query time,sorted,static,update,update time}
}

@article{almanParameterizedSensitivityOracles2022,
  title = {Parameterized Sensitivity Oracles and Dynamic Algorithms Using Exterior Algebras},
  author = {Alman, Josh and Hirsch, D.},
  year = {2022},
  doi = {10.48550/arXiv.2204.10819},
  abstract = {We design the first efficient sensitivity oracles and dynamic algorithms for a variety of parameterized problems. Our main approach is to modify the algebraic coding technique from static parameterized algorithm design, which had not previously been used in a dynamic context. We particularly build off of the `extensor coding' method of Brand, Dell and Husfeldt [STOC'18], employing properties of the exterior algebra over different fields. For the k-Path detection problem for directed graphs, it is known that no efficient dynamic algorithm exists (under popular assumptions from fine-grained complexity). We circumvent this by designing an efficient sensitivity oracle, which preprocesses a directed graph on n vertices in 2\textsuperscript{k}poly(k)n\textsuperscript{\{\vphantom\}}{$\omega$}+o(1)\vphantom\{\} time, such that, given {$\ell$} updates (mixing edge insertions and deletions, and vertex deletions) to that input graph, it can decide in time {$\ell^2$}2\textsuperscript{k}poly(k) and with high probability, whether the updated graph contains a path of length k. We also give a deterministic sensitivity oracle requiring 4\textsuperscript{k}poly(k)n\textsuperscript{\{\vphantom\}}{$\omega$}+o(1)\vphantom\{\} preprocessing time and {$\ell^2$}2\textsuperscript{\{\vphantom\}}{$\omega$}k+o(k)\vphantom\{\} query time, and obtain a randomized sensitivity oracle for the task of approximately counting the number of k-paths. For k-Path detection in undirected graphs, we obtain a randomized sensitivity oracle with O(1.66\textsuperscript{k}n{$^3$}) preprocessing time and O({$\ell^3$}1.66\textsuperscript{k}) query time, and a better bound for undirected bipartite graphs. In addition, we present the first fully dynamic algorithms for a variety of problems: k-Partial Cover, m-Set k-Packing, t-Dominating Set, d-Dimensional k-Matching, and Exact k-Partial Cover. For example, for k-Partial Cover we show a randomized dynamic algorithm with 2\textsuperscript{k}poly(k)polylog(n) update time, and a deterministic dynamic algorithm with 4\textsuperscript{k}poly(k)polylog(n) update time.},
  citationcount = {3},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {dynamic,query,query time,static,update,update time}
}

@article{almanProbabilisticPolynomialsAnd2015,
  title = {Probabilistic Polynomials and Hamming Nearest Neighbors},
  author = {Alman, Josh and Williams, Ryan},
  year = {2015},
  doi = {10.1109/FOCS.2015.18},
  abstract = {We show how to compute any symmetric Boolean function on n variables over any field (as well as '/ the integers) with a probabilistic polynomial of degree O( {\textsurd}nlog(1/{$\varepsilon$})) and error at most {$\varepsilon$}. The degree dependence on n and {$\varepsilon$} is optimal, matching a lower bound of Razborov (1987) and Smolensky (1987) for the MAJORITY function. The proof is constructive: a low-degree polynomial can be efficiently sampled from the distribution. This polynomial construction is combined with other algebraic ideas to give the first subquadratic time algorithm for computing a (worst-case) batch of Hamming distances in superlogarithmic dimensions, exactly. To illustrate, let c(n) : {$\mathbb{N}$} {$\rightarrow$} {$\mathbb{N}$}. Suppose we are given a database D of n vectors in \{0,1\}{\textexclamdown}sup{\textquestiondown}c(n)logn{\textexclamdown}/sup{\textquestiondown} and a collection of n query vectors Q in the same dimension. For all u {$\in$} Q, we wish to compute a v {$\in$} D with minimum Hamming distance from u. We solve this problem in n{\textexclamdown}sup{\textquestiondown}2-1/O(c(n)log{\textexclamdown}/sup{\textquestiondown}{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}{\textexclamdown}sup{\textquestiondown}c(n)){\textexclamdown}/sup{\textquestiondown} randomized time. Hence, the problem is in ``truly subquadratic'' time for O(logn) dimensions, and in subquadratic time for d = o((log2 n)/(loglogn){\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}). We apply the algorithm to computing pairs with maximum inner product, closest pair in {$\ell$}1 for vectors with bounded integer entries, and pairs with maximum Jaccard coefficients.},
  citationcount = {115},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {lower bound,query}
}

@article{alokhinaFullyDynamicShortest2023,
  title = {Fully Dynamic Shortest Path Reporting against an Adaptive Adversary},
  author = {Alokhina, Anastasiia and {van den Brand}, Jan},
  year = {2023},
  doi = {10.48550/arXiv.2304.07403},
  abstract = {Algebraic data structures are the main subroutine for maintaining distances in fully dynamic graphs in subquadratic time. However, these dynamic algebraic algorithms generally cannot maintain the shortest paths, especially against adaptive adversaries. We present the first fully dynamic algorithm that maintains the shortest paths against an adaptive adversary in subquadratic update time. This is obtained via a combinatorial reduction that allows reconstructing the shortest paths with only a few distance estimates. Using this reduction, we obtain the following: On weighted directed graphs with real edge weights in [1,W], we can maintain (1+{$\epsilon$}) approximate shortest paths in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}1.816\vphantom\{\}{$\epsilon$}\textsuperscript{\{\vphantom\}}-2\vphantom\{\}W) update and O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}1.741\vphantom\{\}{$\epsilon$}\textsuperscript{\{\vphantom\}}-2\vphantom\{\}W) query time. This improves upon the approximate distance data structures from [v.d.Brand, Nanongkai, FOCS'19], which only returned a distance estimate, by matching their complexity and returning an approximate shortest path. On unweighted directed graphs, we can maintain exact shortest paths in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}1.823\vphantom\{\}) update and O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}1.747\vphantom\{\}) query time. This improves upon [Bergamaschi, Henzinger, P.Gutenberg, V.Williams, Wein, SODA'21] who could report the path only against oblivious adversaries. We improve both their update and query time while also handling adaptive adversaries. On unweighted undirected graphs, our reduction holds not just against adaptive adversaries but is also deterministic. We maintain a (1+{$\epsilon$})-approximate st-shortest path in O(n\textsuperscript{\{\vphantom\}}1.529\vphantom\{\}/{$\epsilon^2$}) time per update, and (1+{$\epsilon$})-approximate single source shortest paths in O(n\textsuperscript{\{\vphantom\}}1.764\vphantom\{\}/{$\epsilon^2$}) time per update. Previous deterministic results by [v.d.Brand, Nazari, Forster, FOCS'22] could only maintain distance estimates but no paths.},
  citationcount = {3},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {adaptive,data structure,dynamic,query,query time,reduction,update,update time}
}

@article{alonAlmostKWise2003,
  title = {Almost K-Wise Independence versus k-Wise Independence},
  author = {Alon, N. and Goldreich, Oded and Mansour, Y.},
  year = {2003},
  doi = {10.1016/S0020-0190(03)00359-4},
  abstract = {No abstract available},
  citationcount = {80},
  venue = {Information Processing Letters}
}

@article{alonConstructionOfAsymptotically1991,
  title = {Construction of Asymptotically Good Low-Rate Error-Correcting Codes through Pseudo-Random Graphs},
  author = {Alon, N. and Bruck, Jehoshua and Naor, J. and Naor, M. and Roth, R.},
  year = {1991},
  doi = {10.1109/ISIT.1991.695195},
  abstract = {A novel technique, based on the pseudo-random properties of certain graphs known as expanders, is used to obtain novel simple explicit constructions of asymptotically good codes. In one of the constructions, the expanders are used to enhance Justesen codes by replicating, shuffling, and then regrouping the code coordinates. For any fixed (small) rate, and for a sufficiently large alphabet, the codes thus obtained lie above the Zyablov bound. Using these codes as outer codes in a concatenated scheme, a second asymptotic good construction is obtained which applies to small alphabets (say, GF(2)) as well. Although these concatenated codes lie below the Zyablov bound, they are still superior to previously known explicit constructions in the zero-rate neighborhood. {\textquestiondown}},
  citationcount = {282},
  venue = {Proceedings. 1991 IEEE International Symposium on Information Theory}
}

@article{alonLinearCircuitsOver1990,
  title = {Linear Circuits over {{GF}}(2)},
  author = {Alon, N. and Karchmer, M. and Wigderson, A.},
  year = {1990},
  doi = {10.1137/0219074},
  abstract = {For n=2\textsuperscript{k}, let S be an n{\texttimes}n matrix whose rows and columns are indexed by \{GF\}(2)\textsuperscript{k} and, for i,j{$\in$}\{GF\}(2)\textsuperscript{k},S\textsubscript{\{\vphantom\}}i.j\vphantom\{\}={$\langle$}i,j{$\rangle$}, the standard inner product. Size-depth trade-oils are investigated for computing S\{x\} with circuits using only linear operations. In particular, linear size circuits with depth bounded by the inverse of an Ackerman function are constructed, and it is shown that depth two circuits require {\textohm}(nn) size. The lower bound applies to any Hadamard matrix.},
  citationcount = {33},
  venue = {SIAM journal on computing (Print)}
}

@article{alonPartitioningAndGeometric1987,
  title = {Partitioning and Geometric Embedding of Range Spaces of Finite {{Vapnik-Chervonenkis}} Dimension},
  author = {Alon, N. and Haussler, D. and Welzl, E.},
  year = {1987},
  doi = {10.1145/41958.41994},
  abstract = {Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission.},
  citationcount = {59},
  venue = {SCG '87}
}

@article{alonProblemsAndResults2003,
  title = {Problems and Results in Extremal Combinatorics--{{I}}},
  author = {Alon, N.},
  year = {2003},
  doi = {10.1016/S0012-365X(03)00227-9},
  abstract = {No abstract available},
  citationcount = {250},
  venue = {Discrete Mathematics}
}

@article{alonsoAppendix2004,
  title = {Appendix},
  author = {Alonso, A. and Flytzanis, C. and Krieg, L. and Louis, C. and Sekeris, C.},
  year = {2004},
  doi = {10.1007/BF00777499},
  abstract = {No abstract available},
  citationcount = {1100},
  venue = {Molecular Biology Reports}
}

@article{alonTheApproximateRank2013,
  title = {The Approximate Rank of a Matrix and Its Algorithmic Applications: Approximate Rank},
  author = {Alon, N. and Lee, Troy and Shraibman, A. and Vempala, S.},
  year = {2013},
  doi = {10.1145/2488608.2488694},
  abstract = {We study the {$\varepsilon$}-rank of a real matrix A, defined for any {$\varepsilon$} {\textquestiondown} 0 as the minimum rank over matrices that approximate every entry of A to within an additive {$\varepsilon$}. This parameter is connected to other notions of approximate rank and is motivated by problems from various topics including communication complexity, combinatorial optimization, game theory, computational geometry and learning theory. Here we give bounds on the {$\varepsilon$}-rank and use them for algorithmic applications. Our main algorithmic results are (a) polynomial-time additive approximation schemes for Nash equilibria for 2-player games when the payoff matrices are positive semidefinite or have logarithmic rank and (b) an additive PTAS for the densest subgraph problem for similar classes of weighted graphs. We use combinatorial, geometric and spectral techniques; our main new tool is an algorithm for efficiently covering a convex body with translates of another convex body.},
  citationcount = {66},
  venue = {Symposium on the Theory of Computing}
}

@article{alonTheCoverNumber2014,
  title = {The Cover Number of a Matrix and Its Algorithmic Applications},
  author = {Alon, N. and Lee, Troy and Shraibman, A.},
  year = {2014},
  doi = {10.4230/LIPIcs.APPROX-RANDOM.2014.34},
  abstract = {Given a matrix A, we study how many epsilon-cubes are required to cover the convex hull of the columns of A. We show bounds on this cover number in terms of VC dimension and the gamma\_2 norm and give algorithms for enumerating elements of a cover. This leads to algorithms for computing approximate Nash equilibria that unify and extend several previous results in the literature. Moreover, our approximation algorithms can be applied quite generally to a family of quadratic optimization problems that also includes finding the k-by-k combinatorial rectangle of a matrix. In particular, for this problem we give the first quasi-polynomial time additive approximation algorithm that works for any matrix A in [0,1]{\textasciicircum}\{m x n\}.},
  citationcount = {7},
  venue = {International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques}
}

@inproceedings{alstrupCellProbeLower,
  title = {A Cell Probe Lower Bound for Dynamic Nearest-Neighbour Searching},
  booktitle = {45th {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Alstrup, Stephen and {Husfeldt, Thore} and {Rauhe, Theis}},
  langid = {english},
  keywords = {cell probe,dynamic,lower bound,sorted},
  file = {/Users/tulasi/Zotero/storage/NUFWKNU3/365411.365778.pdf}
}

@article{alstrupDynamicNestedBrackets2004,
  title = {Dynamic Nested Brackets},
  author = {Alstrup, Stephen and Husfeldt, T. and Rauhe, Theis},
  year = {2004},
  doi = {10.1016/j.ic.2004.04.006},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Information and Computation},
  keywords = {dynamic}
}

@inproceedings{alstrupMarkedAncestorProblems1998,
  title = {Marked Ancestor Problems},
  booktitle = {Proceedings 39th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} ({{Cat}}. {{No}}.{{98CB36280}})},
  author = {Alstrup, S. and Husfeldt, T. and Rauhe, T.},
  year = {1998},
  month = nov,
  pages = {534--543},
  issn = {0272-5428},
  doi = {10.1109/SFCS.1998.743504},
  url = {https://ieeexplore.ieee.org/abstract/document/743504},
  urldate = {2025-04-07},
  abstract = {Consider a rooted tree whose nodes can be in two states: marked or unmarked. The marked ancestor problem is to maintain a data structure with the following operations: mark(v) marks node v: unmark(v) removes any marks from node v; firstmarked(v) returns the first marked node on the path from v to the root. We show tight upper and lower bounds for the marked ancestor problem. The lower bounds are proved in the cell probe model, the algorithms run on a unit-cost RAM. As easy corollaries we prove (often optimal) lower bounds on a number of problems. These include planar range searching, including the existential or emptiness problem, priority search trees static tree union-find, and several problems from dynamic computational geometry, including segment intersection, interval maintenance, and ray shooting in the plane. Our upper bounds improve algorithms from various fields, including coloured ancestor problems and maintenance of balanced parentheses.},
  keywords = {cell probe,Computational geometry,data structure,Data structures,DNA,dynamic,lower bound,Probes,Read only memory,static,Upper bound},
  file = {/Users/tulasi/Zotero/storage/MFSHGFA9/Alstrup et al. - 1998 - Marked ancestor problems.pdf}
}

@article{alstrupNearestCommonAncestors2004,
  title = {Nearest Common Ancestors: A Survey and a New Algorithm for a Distributed Environment},
  author = {Alstrup, Stephen and Gavoille, C. and Kaplan, Haim and Rauhe, Theis},
  year = {2004},
  doi = {10.1007/s00224-004-1155-5},
  abstract = {No abstract available},
  citationcount = {78},
  venue = {Theory of Computing Systems}
}

@article{alstrupOptimalStaticRange2001,
  title = {Optimal Static Range Reporting in One Dimension},
  author = {Alstrup, Stephen and Brodal, G. and Rauhe, Theis},
  year = {2001},
  doi = {10.1145/380752.380842},
  abstract = {We consider static one dimensional range searching problems. These problems are to build static data structures for an integer set {\textexclamdown}italic{\textquestiondown}S{\textexclamdown}/italic{\textquestiondown} {$\subseteq$}{\textexclamdown}italic{\textquestiondown}U{\textexclamdown}/italic{\textquestiondown}, where {\textexclamdown}italic{\textquestiondown}U{\textexclamdown}/italic{\textquestiondown} =  0,1,{\dots},2{\textasciicircum}{\textexclamdown}italic{\textquestiondown}w{\textexclamdown}/italic{\textquestiondown}-1 , which support various queries for integer intervals of {\textexclamdown}italic{\textquestiondown}U{\textexclamdown}/italic{\textquestiondown}. For the query of reporting all integers in {\textexclamdown}italic{\textquestiondown}S{\textexclamdown}/italic{\textquestiondown} contained within a query interval, we present an optimal data structure with linear space cost and with query time linear in the number of integers reported. This result holds in the unit cost RAM model with word size {\textexclamdown}italic{\textquestiondown}w{\textexclamdown}/italic{\textquestiondown} and a standard instruction set. We also present a linear space data structure for approximate range counting. A range counting query for an interval returns the number of integers in {\textexclamdown}italic{\textquestiondown}S{\textexclamdown}/italic{\textquestiondown} contained within the interval. For any constant {$\varepsilon$}{\textquestiondown}0, our range counting data structure returns in constant time an approximate answer which is within a factor of at most 1+{$\varepsilon$} of the correct answer.},
  citationcount = {62},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,query,query time,static}
}

@article{alstrupUnionfindConstantTime2005,
  title = {Union-Find with Constant Time Deletions},
  author = {Alstrup, Stephen and G{\o}rtz, Inge Li and Rauhe, Theis and Thorup, M. and Zwick, Uri},
  year = {2005},
  doi = {10.1145/2636922},
  abstract = {A union-find data structure maintains a collection of disjoint sets under the operations makeset, union, and find. Kaplan, Shafrir, and Tarjan [SODA 2002] designed data structures for an extension of the union-find problem in which items of the sets maintained may be deleted. The cost of a delete operation in their implementations is essentially the same as the cost of a find operation; namely, O(log n) worst-case and O({$\alpha\lceil$} M/N{$\rceil$} (n)) amortized, where n is the number of items in the set returned by the find operation, N is the total number of makeset operations performed, M is the total number of find operations performed, and {$\alpha\lceil$} M/N{$\rceil$}(n) is a functional inverse of Ackermann's function. They left open the question whether delete operations can be implemented more efficiently than find operations, for example, in o(log n) worst-case time. We resolve this open problem by presenting a relatively simple modification of the classical union-find data structure that supports delete, as well as makeset and union operations, in constant worst-case time, while still supporting find operations in O(log n) worst-case time and O({$\alpha\lceil$} M/N{$\rceil$} (n)) amortized time. Our analysis supplies, in particular, a very concise potential-based amortized analysis of the standard union-find data structure that yields an O({$\alpha\lceil$} M/N{$\rceil$} (n)) amortized bound on the cost of find operations. All previous potential-based analyses yielded the weaker amortized bound of O({$\alpha\lceil$} M/N{$\rceil$} (N)). Furthermore, our tighter analysis extends to one-path variants of the path compression technique such as path splitting.},
  citationcount = {28},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure}
}

@article{altExactLNearest2001,
  title = {Exact {{L}}{$\infty$} Nearest Neighbor Search in High Dimensions},
  author = {Alt, H. and {Heinrich-Litan}, Laura},
  year = {2001},
  doi = {10.1145/378583.378655},
  abstract = {We present an algorithm for solving the nearest neighbor problem with respect to L\textsubscript{\{\vphantom\}}{$\infty$}\vphantom\{\}-distance. It requires no preprocessing and storage only for the point set P. Its average runtime assuming that the set P of n points is drawn randomly from the unit cube [0,1]\textsuperscript{\{\vphantom\}}d\vphantom\{\} under uniform distribution is essentially {$\Theta$}(nd/ln\;n) thereby improving the brute-force method by a factor of {$\Theta$}(1/ln\;n). Several generalizations of the method are also presented, in particular to other ``well-behaved'' probability distributions and to the important problem of finding the k nearest neighbors to a query point.},
  citationcount = {6},
  venue = {SCG '01}
}

@article{alweissImprovedBoundsFor2019,
  title = {Improved Bounds for the Sunflower Lemma},
  author = {Alweiss, Ryan and Lovett, Shachar and Wu, Kewen and Zhang, Jiapeng},
  year = {2019},
  doi = {10.1145/3357713.3384234},
  abstract = {A sunflower with r petals is a collection of r sets so that the intersection of each pair is equal to the intersection of all. Erd{\H o}s and Rado proved the sunflower lemma: for any fixed r, any family of sets of size w, with at least about w w sets, must contain a sunflower. The famous sunflower conjecture is that the bound on the number of sets can be improved to c w for some constant c. In this paper, we improve the bound to about (logw) w . In fact, we prove the result for a robust notion of sunflowers, for which the bound we obtain is tight up to lower order terms.},
  citationcount = {95},
  venue = {Electron. Colloquium Comput. Complex.}
}

@article{amarilliEnumerationOnTrees2018,
  title = {Enumeration on Trees with Tractable Combined Complexity and Efficient Updates},
  author = {Amarilli, Antoine and Bourhis, P. and Mengel, S. and Niewerth, Matthias},
  year = {2018},
  doi = {10.1145/3294052.3319702},
  abstract = {We give an algorithm to enumerate the results on trees of monadic second-order (MSO) queries represented by nondeterministic tree automata. After linear time preprocessing (in the input tree), we can enumerate answers with linear delay (in each answer). We allow updates on the tree to take place at any time, and we can then restart the enumeration after logarithmic time in the tree. Further, all our combined complexities are polynomial in the automaton. Our result follows our previous circuit-based enumeration algorithms based on deterministic tree automata, and is also inspired by our earlier result on words and nondeterministic sequential extended variable-set automata in the context of document spanners. We extend these results and combine them with a recent tree balancing scheme by Niewerth, so that our enumeration structure supports updates to the underlying tree in logarithmic time (with leaf insertions, leaf deletions, and node relabelings). Our result implies that, for MSO queries with free first-order variables, we can enumerate the results with linear preprocessing and constant-delay and update the underlying tree in logarithmic time, which improves on several known results for words and trees. Building on lower bounds from data structure research, we also show unconditionally that up to a doubly logarithmic factor the update time of our algorithm is optimal. Thus, unlike other settings, there can be no algorithm with constant update time.},
  citationcount = {30},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {data structure,lower bound,query,update,update time}
}

@article{aminiCountingSubgraphsVia2009,
  title = {Counting Subgraphs via Homomorphisms},
  author = {Amini, O. and Fomin, F. and Saurabh, Saket},
  year = {2009},
  doi = {10.1137/100789403},
  abstract = {Counting homomorphisms between graphs has applications in variety of areas, including extremal graph theory, properties of graph products, partition functions in statistical physics and property testing of large graphs. In this work we show a new application of counting graph homomorphisms to the areas of exact and parameterized algorithms. We introduce a generic approach for counting subgraphs in a graph. The main idea is to relate counting subgraphs to counting graph homomorphisms. This approach provides new algorithms and unifies several well known results in algorithms and combinatorics including the recent algorithm of Bjorklund, Husfeldt and Koivisto for computing the chromatic polynomial, the classical algorithm of Kohn, Gottlieb, Kohn, and Karp for counting Hamiltonian cycles, Ryser's formula for counting perfect matchings of a bipartite graph, and color coding based algorithms of Alon, Yuster, and Zwick.},
  citationcount = {64},
  venue = {SIAM Journal on Discrete Mathematics}
}

@article{amirConfigurationsAndMinority2012,
  title = {Configurations and Minority in the String Consensus Problem},
  author = {Amir, A. and Parienty, Haim and Roditty, L.},
  year = {2012},
  doi = {10.1007/s00453-015-9996-7},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Algorithmica}
}

@article{amirDynamicTextAnd2003,
  title = {Dynamic Text and Static Pattern Matching},
  author = {Amir, A. and Landau, G. M. and Lewenstein, Moshe and Sokol, Dina},
  year = {2003},
  doi = {10.1145/1240233.1240242},
  abstract = {In this article, we address a new version of dynamic pattern matching. The {\textexclamdown}i{\textquestiondown}dynamic text and static pattern matching problem{\textexclamdown}/i{\textquestiondown} is the problem of finding a static pattern in a text that is continuously being updated. The goal is to report all new occurrences of the pattern in the text after each text update. We present an algorithm for solving the problem where the text update operation is {\textexclamdown}i{\textquestiondown}changing{\textexclamdown}/i{\textquestiondown} the symbol value of a text location. Given a text of length {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} and a pattern of length {\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown}, our algorithm preprocesses the text in time {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} log log {\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown}), and the pattern in time {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown} log {\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown}). The extra space used is {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown} log {\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown}). Following each text update, the algorithm deletes all prior occurrences of the pattern that no longer match, and reports all new occurrences of the pattern in the text in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log log {\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown}) time. We note that the complexity is not proportional to the number of pattern occurrences, since all new occurrences can be reported in a succinct form.},
  citationcount = {75},
  venue = {TALG},
  keywords = {dynamic,static,update}
}

@article{amirEfficientPatternMatching1990,
  title = {Efficient Pattern Matching with Scaling},
  author = {Amir, A. and Landau, G. M. and Vishkin, U.},
  year = {1990},
  doi = {10.1016/0196-6774(92)90003-U},
  abstract = {No abstract available},
  citationcount = {92},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{amirEfficientRegularData1999,
  title = {Efficient Regular Data Structures and Algorithms for Dilation, Location, and Proximity Problems},
  author = {Amir, A. and Efrat, A. and Indyk, P. and Samet, H.},
  year = {1999},
  doi = {10.1007/s00453-001-0013-y},
  abstract = {No abstract available},
  citationcount = {35},
  venue = {Algorithmica},
  keywords = {data structure}
}

@article{amirEfficientRegularData1999,
  title = {Efficient Regular Data Structures and Algorithms for Location and Proximity Problems},
  author = {Amir, A. and Efrat, A. and Indyk, P. and Samet, H.},
  year = {1999},
  doi = {10.1109/SFFCS.1999.814588},
  abstract = {Investigates data structures obtained by a recursive partitioning of the input domain into regions of equal size. One of the most well-known examples of such a structure is the quadtree, which is used in this paper as a basis for more complex data structures; we also provide multidimensional versions of the stratified tree of P. van Emde Boas (1997). We show that, under the assumption that the input points have limited precision (i.e. are drawn from an integer grid of size u), these data structures yield efficient solutions to many important problems. In particular, they allow us to achieve O(log log u) time per operation for finding the dynamic approximate nearest neighbor (under insertions and deletions) and the exact online closest pair (under insertions only) in any constant dimension. They allow O(log log u) point location in a given planar shape or in its expansion (dilation by a ball of a given radius). Finally, we provide a linear-time (optimal) algorithm for computing the expansion of a shape represented by a quadtree. This result shows that the spatial order imposed by this regular data structure is sufficient to optimize the dilation by a ball operation.},
  citationcount = {22},
  venue = {40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039)}
}

@article{amirm.ben-amramLowerBoundsData1991,
  title = {Lower Bounds for Data Structure Problems on {{RAMs}}},
  author = {{Amir M. Ben-Amram} and Z. Galil},
  year = {1991},
  journal = {[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science},
  doi = {10.1109/SFCS.1991.185428},
  abstract = {A technique is described for deriving lower bounds and tradeoffs for data structure problems. Two quantities are defined. The output variability depends only on the model of computation. It characterizes in some sense the power of a model. The problem variability depends only on the problem under consideration. It characterizes in some sense the difficulty of the problem. The first theorem states that if a model's output variability is smaller than the problem variability, a lower bound on the worst case (average case) time for the problem follows. A RAM that can add, subtract and compare unbounded integers is considered. The second theorem gives an upper bound on the output variability of this model. The two theorems are used to derive lower bounds for the union-find problem in this RAM.{$<<$}ETX{$>>$}},
  keywords = {data structure,lower bound},
  annotation = {Citation Count: 10}
}

@article{amirm.ben-amramPointersAddresses1988,
  title = {On Pointers versus Addresses},
  author = {{Amir M. Ben-Amram} and Z. Galil},
  year = {1988},
  journal = {[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science},
  doi = {10.1145/146637.146666},
  abstract = {The problem of determining the cost of random-access memory (RAM) is addressed by studying the simulation of random addressing by a machine which lacks it, called a pointer machine. The model allows the use of a data type of choice. A RAM program of time t and space s can be simulated in O(t log s) time using a tree. However, this is not an obvious lower bound since a high-level data type can allow the data to be encoded in a more economical way. The major contribution is the formalization of incompressibility for general data types. The definition extends a similar property of strings that underlies the theory of Kolmogorov complexity. The main theorem states that for all incompressible data types an Omega (t log s) lower bound holds. Incompressibility is proved for the real numbers with a set of primitives which includes all functions which are continuously differentiable except on a countable closed set.{$<<$}ETX{$>>$}},
  keywords = {lower bound},
  annotation = {Citation Count: 48}
}

@article{amirm.ben-amramUnitCostPointersLogarithmicCost1994,
  title = {Unit-{{Cost Pointers}} versus {{Logarithmic-Cost Addresses}}},
  author = {{Amir M. Ben-Amram}},
  year = {1994},
  journal = {Theoretical Computer Science},
  doi = {10.1016/0304-3975(93)00079-K},
  annotation = {Citation Count: 1}
}

@article{amirm.ben-amramWhatPointerMachine1995,
  title = {What Is a ``Pointer Machine''?},
  author = {{Amir M. Ben-Amram}},
  year = {1995},
  journal = {SIGA},
  doi = {10.1145/202840.202846},
  abstract = {A "Pointer Machine" is many things. Authors who consider referring to this term are invited to read the following note first.},
  annotation = {Citation Count: 46}
}

@article{amirMindTheGap2016,
  title = {Mind the Gap: {{Essentially}} Optimal Algorithms for Online Dictionary Matching with One Gap},
  author = {Amir, A. and Kopelowitz, T. and Levy, Avivit and Pettie, Seth and Porat, E. and Shalom, B. R.},
  year = {2016},
  doi = {10.4230/LIPIcs.ISAAC.2016.12},
  abstract = {We examine the complexity of the online Dictionary Matching with One Gap Problem (DMOG) which is the following. Preprocess a dictionary D of d patterns, where each pattern contains a special gap symbol that can match any string, so that given a text that arrives online, a character at a time, we can report all of the patterns from D that are suffixes of the text that has arrived so far, before the next character arrives. In more general versions the gap symbols are associated with bounds determining the possible lengths of matching strings. Online DMOG captures the difficulty in a bottleneck procedure for cyber-security, as many digital signatures of viruses manifest themselves as patterns with a single gap. In this paper, we demonstrate that the difficulty in obtaining efficient solutions for the DMOG problem, even in the offline setting, can be traced back to the infamous 3SUM conjecture. We show a conditional lower bound of Omega(delta(G\_D)+op) time per text character, where G\_D is a bipartite graph that captures the structure of D, delta(G\_D) is the degeneracy of this graph, and op is the output size. Moreover, we show a conditional lower bound in terms of the magnitude of gaps for the bounded case, thereby showing that some known offline upper bounds are essentially optimal. We also provide matching upper-bounds (up to sub-polynomial factors), in terms of the degeneracy, for the online DMOG problem. In particular, we introduce algorithms whose time cost depends linearly on delta(G\_D). Our algorithms make use of graph orientations, together with some additional techniques. These algorithms are of practical interest since although delta(G\_D) can be as large as sqrt(d), and even larger if G\_D is a multi-graph, it is typically a very small constant in practice. Finally, when delta(G\_D) is large we are able to obtain even more efficient solutions.},
  citationcount = {19},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {lower bound}
}

@article{amirMindTheGap2018,
  title = {Mind the Gap!},
  author = {Amir, A. and Kopelowitz, T. and Levy, Avivit and Pettie, Seth and Porat, E. and Shalom, B. R.},
  year = {2018},
  doi = {10.1007/s00453-018-0526-2},
  abstract = {No abstract available},
  citationcount = {10},
  venue = {Algorithmica}
}

@article{amirOnHardnessOf2014,
  title = {On Hardness of Jumbled Indexing},
  author = {Amir, A. and Chan, Timothy M. and Lewenstein, Moshe and Lewenstein, Noa},
  year = {2014},
  doi = {10.1007/978-3-662-43948-7_10},
  abstract = {No abstract available},
  citationcount = {76},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{amirOnlineRecognitionOf2020,
  title = {Online Recognition of Dictionary with One Gap},
  author = {Amir, A. and Levy, Avivit and Porat, E. and Shalom, B. R.},
  year = {2020},
  doi = {10.1016/j.ic.2020.104633},
  abstract = {No abstract available},
  citationcount = {6},
  venue = {Prague Stringology Conference}
}

@article{amirOnlineTimestampedText2002,
  title = {Online Timestamped Text Indexing},
  author = {Amir, A. and Landau, G. M. and Ukkonen, E.},
  year = {2002},
  doi = {10.1016/S0020-0190(01)00275-7},
  abstract = {No abstract available},
  citationcount = {14},
  venue = {Information Processing Letters}
}

@article{amirOnTheEfficiency2014,
  title = {On the Efficiency of the Hamming C-Centerstring Problems},
  author = {Amir, A. and Ficler, Jessica and Roditty, L. and Shalom, Oren Sar},
  year = {2014},
  doi = {10.1007/978-3-319-07566-2_1},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {Annual Symposium on Combinatorial Pattern Matching}
}

@article{amitBoxedPermutationPattern2016,
  title = {Boxed Permutation Pattern Matching},
  author = {Amit, M. and Bille, Philip and Cording, Patrick Hagge and G{\o}rtz, Inge Li and Vildh{\o}j, Hjalte Wedel},
  year = {2016},
  doi = {10.4230/LIPIcs.CPM.2016.20},
  abstract = {Given permutations T and P of length n and m, respectively, the Permutation Pattern Matching problem asks to find all m-length subsequences of T that are order-isomorphic to P. This problem has a wide range of applications but is known to be NP-hard. In this paper, we study the special case, where the goal is to only find the boxed subsequences of T that are order-isomorphic to P. This problem was introduced by Bruner and Lackner who showed that it can be solved in O(n{\textasciicircum}3) time. Cho et al. [CPM 2015] gave an O(n{\textasciicircum}2m) time algorithm and improved it to O(n{\textasciicircum}2 log m). In this paper we present a solution that uses only O(n{\textasciicircum}2) time. In general, there are instances where the output size is Omega(n{\textasciicircum}2) and hence our bound is optimal. To achieve our results, we introduce several new ideas including a novel reduction to 2D offline dominance counting. Our algorithm is surprisingly simple and straightforward to implement.},
  citationcount = {1},
  venue = {Annual Symposium on Combinatorial Pattern Matching},
  keywords = {reduction}
}

@article{anconaAlgorithmsAndHardness2018,
  title = {Algorithms and Hardness for Diameter in Dynamic Graphs},
  author = {Ancona, Bertie and Henzinger, Monika and Roditty, L. and Williams, V. V. and Wein, Nicole},
  year = {2018},
  doi = {10.4230/LIPIcs.ICALP.2019.13},
  abstract = {The diameter, radius and eccentricities are natural graph parameters. While these problems have been studied extensively, there are no known dynamic algorithms for them beyond the ones that follow from trivial recomputation after each update or from solving dynamic All-Pairs Shortest Paths (APSP), which is very computationally intensive. This is the situation for dynamic approximation algorithms as well, and even if only edge insertions or edge deletions need to be supported. This paper provides a comprehensive study of the dynamic approximation of Diameter, Radius and Eccentricities, providing both conditional lower bounds, and new algorithms whose bounds are optimal under popular hypotheses in fine-grained complexity. Some of the highlights include: - Under popular hardness hypotheses, there can be no significantly better fully dynamic approximation algorithms than recomputing the answer after each update, or maintaining full APSP. - Nearly optimal partially dynamic (incremental/decremental) algorithms can be achieved via efficient reductions to (incremental/decremental) maintenance of Single-Source Shortest Paths. For instance, a nearly (3/2+{$\epsilon$})-approximation to Diameter in directed or undirected graphs can be maintained decrementally in total time m\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}{\textsurd}\{n\}/{$\epsilon^2$}. This nearly matches the static 3/2-approximation algorithm for the problem that is known to be conditionally optimal.},
  citationcount = {27},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {dynamic,lower bound,reduction,static,update}
}

@article{andersenAMachineVerified1996,
  title = {A Machine Verified Distributed Sorting Algorithm},
  author = {Andersen, J. H. and Harcourt, Edwin A. and Prasad, K. V. S.},
  year = {1996},
  doi = {10.7146/BRICS.V3I4.19967},
  abstract = {We present a verification of a distributed sorting algorithm in ALF, an implementation of Martin L{\textasciidieresis}of's type theory. The implementation is expressed as a program in a prioritized version of CBS, (the Calculus of Broadcasting Systems) which we have implemented in ALF. The specification is expressed in terms of an ALF type which represents the set of all sorted lists and an HML (Hennesey-Milner Logic) formula which expresses that the sorting program will input any number of data until it hears a value triggering the program to begin outputting the data in a sorted fashion. We gain expressive power from the type theory by inheriting the language of data, state expressions, and propositions.},
  citationcount = {14},
  venue = {No venue available}
}

@article{anderssonApproximateIndexedLists1998,
  title = {Approximate Indexed Lists},
  author = {Andersson, A. and Petersson, O.},
  year = {1998},
  doi = {10.1006/jagm.1998.0951},
  abstract = {Let the position of a list element in a list be the number of elements preceding it plus one. Anindexed listsupports the following operations on a list: Insert; delete; return the position of an element; and return the element at a certain position. The order in which the elements appear in the list is completely determined by where the insertions take place; we do not require the presence of any keys that induce the ordering.We considerapproximate indexed listsand show that a tiny relaxation in precision of the query operations allows a considerable improvement in time complexity. The new data structure has applications in two other problems; namely,list labelingandsubset rank.},
  citationcount = {13},
  venue = {J. Algorithms},
  keywords = {data structure,query}
}

@article{anderssonBalancedBinarySearch2007,
  title = {Balanced Binary Search Trees},
  author = {Andersson, Arne and Fagerberg, Rolf and Larsen, Kim S.},
  year = {2007},
  doi = {10.1201/9781420035179.ch10},
  abstract = {No abstract available},
  citationcount = {12},
  venue = {Handbook of Data Structures and Applications}
}

@article{anderssonDictionariesOnAc1997,
  title = {Dictionaries on {{AC}}{\textasciicircum}0 Rams: {{Query}} Time Theta({\textsurd}log n/Log Log n) Is Necessary and Sufficient},
  author = {Andersson, A. and Miltersen, Peter Bro and Riis, S{\o}ren and Thorup, M.},
  year = {1997},
  doi = {10.7146/BRICS.V4I14.21678},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {No venue available},
  keywords = {query,query time}
}

@article{anderssonDynamicOrderedSets2002,
  title = {Dynamic Ordered Sets with Exponential Search Trees},
  author = {Andersson, A. and Thorup, M.},
  year = {2002},
  doi = {10.1145/1236457.1236460},
  abstract = {We introduce exponential search trees as a novel technique for converting static polynomial space search structures for ordered sets into fully-dynamic linear space data structures. This leads to an optimal bound of O({\textsurd}log n/log log n) for searching and updating a dynamic set X of n integer keys in linear space. Searching X for an integer y means finding the maximum key in X which is smaller than or equal to y. This problem is equivalent to the standard text book problem of maintaining an ordered set. The best previous deterministic linear space bound was O(log n/log log n) due to Fredman and Willard from STOC 1990. No better deterministic search bound was known using polynomial space. We also get the following worst-case linear space trade-offs between the number n, the word length W, and the maximal key U {\textexclamdown} 2W: O(min log log n + log n/logW, log log n {$\cdot$} log log U/log log log U). These trade-offs are, however, not likely to be optimal. Our results are generalized to finger searching and string searching, providing optimal results for both in terms of n.},
  citationcount = {113},
  venue = {Journal of the ACM},
  keywords = {data structure,dynamic,static}
}

@article{anderssonFasterDeterministicSorting1996,
  title = {Faster Deterministic Sorting and Searching in Linear Space},
  author = {Andersson, A.},
  year = {1996},
  doi = {10.1109/SFCS.1996.548472},
  abstract = {We present a significant improvement on linear space deterministic sorting and searching. On a unit-cost RAM with word size w, an ordered set of n w-bit keys (viewed as binary strings or integers) can be maintained in O(min\{[/spl radic/(logn)][logn/logw+loglogn][logwloglogn]\}) time per operation, including insert, delete, member search, and neighbour search. The cost for searching is worst-case while the cost for updates is amortized. As an application, n keys can be sorted in linear at O(n/spl radic/(logn)) worst-case cost. The best previous method for deterministic sorting and searching in linear space has been the fusion trees which supports updates and queries in O(logn/loglogn) amortized time and sorting in O(nlogn/loglogn) worst-case time. We also make two minor observations on adapting our data structure to the input distribution and on the complexity of perfect hashing.},
  citationcount = {115},
  venue = {Proceedings of 37th Conference on Foundations of Computer Science},
  keywords = {data structure,query,update}
}

@article{anderssonFusionTreesCan1996,
  title = {Fusion Trees Can Be Implemented with {{AC0}} Instructions Only},
  author = {Andersson, A. and Miltersen, Peter Bro and Thorup, M.},
  year = {1996},
  doi = {10.7146/BRICS.V3I30.20011},
  abstract = {Addressing a problem of Fredman and Willard, we implement fusion trees in deterministic linear space using AC{\textasciicircum}o instructions only. More precisely, we show that a subset of \{0,...,2{\textasciicircum}(w-1)\} of size n ...},
  citationcount = {61},
  venue = {Theoretical Computer Science}
}

@article{anderssonSortingInLinear1995,
  title = {Sorting in Linear Time?},
  author = {Andersson, A. and Hagerup, T. and Nilsson, S. and Raman, R.},
  year = {1995},
  doi = {10.1145/225058.225173},
  abstract = {We show that a unit-cost RAM with a word length of bits can sort integers in the range in time, for arbitrary ! , a significant improvement over the bound of " \# \$ achieved by the fusion trees of Fredman and Willard. Provided that},
  citationcount = {230},
  venue = {Symposium on the Theory of Computing}
}

@article{anderssonStaticDictionariesOn1996,
  title = {Static Dictionaries on {{AC}}/Sup 0/ {{RAMs}}: Query Time /Spl Theta/(/Spl Radic/Log n/Log Log n) Is Necessary and Sufficient},
  author = {Andersson, A. and Miltersen, Peter Bro and Riis, S{\o}ren and Thorup, M.},
  year = {1996},
  doi = {10.1109/SFCS.1996.548503},
  abstract = {In this paper we consider solutions to the static dictionary problem on AC/sup 0/ RAMs, i.e. random access machines where the only restriction on the finite instruction set is that all computational instructions are in AC/sup 0/. Our main result is a tight upper and lower bound of /spl theta/(/spl radic/log n/log log n) on the time for answering membership queries in a set of size n when reasonable space is used for the data structure storing the set; the upper bound can be obtained using O(n) space, and the lower bound holds even if we allow space 2/sup polylog n/. Several variations of this result are also obtained. Among others, we show a tradeoff between time and circuit depth under the unit-cost assumption: any RAM instruction set which permits a linear space, constant query time solution to the static dictionary problem must have an instruction of depth /spl Omega/(log w/log log to), where w is the word size of the machine (and log the size of the universe). This matches the depth of multiplication and integer division, used in the perfect hashing scheme by M.L. Fredman, J. Komlos and E. Szemeredi (1984).},
  citationcount = {24},
  venue = {Proceedings of 37th Conference on Foundations of Computer Science},
  keywords = {data structure,lower bound,query,query time,static}
}

@article{anderssonSublogarithmicSearchingWithout1995,
  title = {Sublogarithmic Searching without Multiplications},
  author = {Andersson, A.},
  year = {1995},
  doi = {10.1109/SFCS.1995.492667},
  abstract = {We show that a unit-cost RAM with word length w can maintain an ordered set of w-bit integers (or binary strings) under the operations search, insert, delete, nearest neighbour in O(/spl radic/(logn)) worst-case time and range queries in O(/spl radic/(logn)+size of output) worst-case time. The operations rely on AC/sup 0/ instructions only, thereby solving an open problem posed by Fredman and Willard. The data structure is simple. We also present a static data structure that can process a set of /spl Theta/O(logn) searches in O(lognloglogn) time.},
  citationcount = {51},
  venue = {Proceedings of IEEE 36th Annual Foundations of Computer Science}
}

@inproceedings{anderssonTighterWorstcaseBounds2000,
  title = {Tight(Er) Worst-Case Bounds on Dynamic Searching and Priority Queues},
  booktitle = {Proceedings of the Thirty-Second Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Andersson, Arne A. and Thorup, Mikkel},
  year = {2000},
  month = may,
  series = {{{STOC}} '00},
  pages = {335--342},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/335305.335344},
  url = {https://dl.acm.org/doi/10.1145/335305.335344},
  urldate = {2025-04-10},
  isbn = {978-1-58113-184-0},
  keywords = {data structure,dynamic,static},
  file = {/Users/tulasi/Zotero/storage/PK87P7LQ/Andersson and Thorup - 2000 - Tight(er) worst-case bounds on dynamic searching and priority queues.pdf}
}

@article{andoniApproximateNearestNeighbor2018,
  title = {Approximate Nearest Neighbor Search in High Dimensions},
  author = {Andoni, Alexandr and Indyk, P. and Razenshteyn, Ilya P.},
  year = {2018},
  doi = {10.1142/9789813272880_0182},
  abstract = {The nearest neighbor problem is defined as follows: Given a set P of n points in some metric space (X,D), build a data structure that, given any point q, returns a point in P that is closest to q (its "nearest neighbor" in P). The data structure stores additional information about the set P, which is then used to find the nearest neighbor without computing all distances between q and P. The problem has a wide range of applications in machine learning, computer vision, databases and other fields. To reduce the time needed to find nearest neighbors and the amount of memory used by the data structure, one can formulate the \{\vphantom\}\emph{approximate}\vphantom\{\}\emph{ nearest neighbor problem, where the the goal is to return any point p'{$\in$}P such that the distance from q to p' is at most c{$\cdot$}\textsubscript{\{\vphantom\}}p{$\in$}P\vphantom\{\}D(q,p), for some c{$\geq$}1. Over the last two decades, many efficient solutions to this problem were developed. In this article we survey these developments, as well as their connections to questions in geometric functional analysis and combinatorial geometry.}},
  citationcount = {142},
  venue = {International Congress of Mathematicans},
  keywords = {data structure}
}

@article{andoniApproximateNearNeighbors2016,
  title = {Approximate near Neighbors for General Symmetric Norms},
  author = {Andoni, Alexandr and Nguyen, Huy L. and Nikolov, Aleksandar and Razenshteyn, Ilya P. and Waingarten, Erik},
  year = {2016},
  doi = {10.1145/3055399.3055418},
  abstract = {We show that every symmetric normed space admits an efficient nearest neighbor search data structure with doubly-logarithmic approximation. Specifically, for every n, d = no(1), and every d-dimensional symmetric norm {\textbar}{\textbar}{$\cdot\vert\vert$}, there exists a data structure for (loglogn)-approximate nearest neighbor search over {\textbar}{\textbar}{$\cdot\vert\vert$} for n-point datasets achieving no(1) query time and n1+o(1) space. The main technical ingredient of the algorithm is a low-distortion embedding of a symmetric norm into a low-dimensional iterated product of top-k norms. We also show that our techniques cannot be extended to general norms.},
  citationcount = {38},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,query,query time}
}

@article{andoniBeyondLocalitySensitive2013,
  title = {Beyond Locality-Sensitive Hashing},
  author = {Andoni, Alexandr and Indyk, P. and Nguyen, Huy L. and Razenshteyn, Ilya P.},
  year = {2013},
  doi = {10.1137/1.9781611973402.76},
  abstract = {We present a new data structure for the c-approximate near neighbor problem (ANN) in the Euclidean space. For n points in Rd, our algorithm achieves Oc(n{$\rho$} + dlogn) query time and Oc(n1+{$\rho$} + dlogn) space, where {$\rho$} {$\leq$} 7/(8c2) + O(1/c3) + oc(1). This is the first improvement over the result by Andoni and Indyk (FOCS 2006) and the first data structure that bypasses a locality-sensitive hashing lower bound proved by O'Donnell, Wu and Zhou (ICS 2011). By a standard reduction we obtain a data structure for the Hamming space and e1 norm with {$\rho$} {$\leq$} 7/(8c)+ O(1/c3/2)+ oc(1), which is the first improvement over the result of Indyk and Motwani (STOC 1998).},
  citationcount = {187},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,lower bound,query,query time,reduction}
}

@article{andoniDataDependentHashing2018,
  title = {Data-Dependent Hashing via Nonlinear Spectral Gaps},
  author = {Andoni, Alexandr and Naor, A. and Nikolov, Aleksandar and Razenshteyn, Ilya P. and Waingarten, Erik},
  year = {2018},
  doi = {10.1145/3188745.3188846},
  abstract = {We establish a generic reduction from \_nonlinear spectral gaps\_ of metric spaces to data-dependent Locality-Sensitive Hashing, yielding a new approach to the high-dimensional Approximate Near Neighbor Search problem (ANN) under various distance functions. Using this reduction, we obtain the following results: * For \_general\_ d-dimensional normed spaces and n-point datasets, we obtain a \_cell-probe\_ ANN data structure with approximation O(logd/{$\varepsilon$}2), space dO(1) n1+{$\varepsilon$}, and dO(1)n{$\varepsilon$} cell probes per query, for any {$\varepsilon$}{\textquestiondown}0. No non-trivial approximation was known before in this generality other than the O({\textsurd}d) bound which follows from embedding a general norm into {$\ell$}2. * For {$\ell$}p and Schatten-p norms, we improve the data structure further, to obtain approximation O(p) and sublinear query \_time\_. For {$\ell$}p, this improves upon the previous best approximation 2O(p) (which required polynomial as opposed to near-linear in n space). For the Schatten-p norm, no non-trivial ANN data structure was known before this work. Previous approaches to the ANN problem either exploit the low dimensionality of a metric, requiring space exponential in the dimension, or circumvent the curse of dimensionality by embedding a metric into a ''tractable'' space, such as {$\ell$}1. Our new generic reduction proceeds differently from both of these approaches using a novel partitioning method.},
  citationcount = {32},
  venue = {Symposium on the Theory of Computing},
  keywords = {cell probe,data structure,query,reduction}
}

@inproceedings{andoniHardnessNearestNeighbor2008,
  title = {Hardness of {{Nearest Neighbor}} under {{L-infinity}}},
  booktitle = {2008 49th {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Andoni, Alexandr and Croitoru, Dorian and Patrascu, Mihai},
  year = {2008},
  month = oct,
  pages = {424--433},
  issn = {0272-5428},
  doi = {10.1109/FOCS.2008.89},
  url = {https://ieeexplore.ieee.org/abstract/document/4690976},
  urldate = {2024-11-20},
  abstract = {Recent years have seen a significant increase in our understanding of high-dimensional nearest neighbor search (NNS) for distances like the lscr1 and lscr2 norms. By contrast, our understanding of the lscrinfin norm is now where it was (exactly) 10 years ago. In FOCSpsila98, Indyk proved the following unorthodox result: there is a data structure (in fact, a decision tree) of size O(nrho), for any rho {$>$} 1, which achieves approximation O(logrho log d) for NNS in the d-dimensional lscr1 metric. In this paper, we provide results that indicate that Indykpsilas unconventional bound might in fact be optimal. Specifically, we show a lower bound for the asymmetric communication complexity of NNS under lscrinfin, which proves that this space/approximation trade-off is optimal for decision trees and for data structures with constant cell-probe complexity.},
  keywords = {Approximation algorithms,cell probe,communication,communication complexity,Complexity theory,Computer science,data structure,Decision trees,Extraterrestrial measurements,Image databases,lower bound,Nearest neighbor searches,Polynomials,Proposals,sorted,Tree data structures},
  file = {/Users/tulasi/Zotero/storage/MFFT4UF9/Andoni et al. - 2008 - Hardness of Nearest Neighbor under L-infinity.pdf;/Users/tulasi/Zotero/storage/VPL64K42/Andoni et al. - 2008 - Hardness of Nearest Neighbor under L-infinity.pdf}
}

@article{andoniNearoptimalHashingAlgorithms2006,
  title = {Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions},
  author = {Andoni, Alexandr and Indyk, P.},
  year = {2006},
  doi = {10.1145/1327452.1327494},
  abstract = {We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n logO(1) n space, with a query time of dnO(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice},
  citationcount = {2493},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {lower bound,query,query time}
}

@article{andoniOptimalDataDependent2015,
  title = {Optimal Data-Dependent Hashing for Approximate near Neighbors},
  author = {Andoni, Alexandr and Razenshteyn, Ilya P.},
  year = {2015},
  doi = {10.1145/2746539.2746553},
  abstract = {We show an optimal data-dependent hashing scheme for the approximate near neighbor problem. For an n-point dataset in a d-dimensional space our data structure achieves query time O(d {$\cdot$} n{$\rho$}+o(1)) and space O(n1+{$\rho$}+o(1) + d {$\cdot$} n), where {$\rho$}=1/(2c2-1) for the Euclidean space and approximation c{\textquestiondown}1. For the Hamming space, we obtain an exponent of {$\rho$}=1/(2c-1). Our result completes the direction set forth in (Andoni, Indyk, Nguyen, Razenshteyn 2014) who gave a proof-of-concept that data-dependent hashing can outperform classic Locality Sensitive Hashing (LSH). In contrast to (Andoni, Indyk, Nguyen, Razenshteyn 2014), the new bound is not only optimal, but in fact improves over the best (optimal) LSH data structures (Indyk, Motwani 1998) (Andoni, Indyk 2006) for all approximation factors c{\textquestiondown}1. From the technical perspective, we proceed by decomposing an arbitrary dataset into several subsets that are, in a certain sense, pseudo-random.},
  citationcount = {280},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,query,query time}
}

@article{andoniOptimalHashingBased2016,
  title = {Optimal Hashing-Based Time-Space Trade-Offs for Approximate near Neighbors},
  author = {Andoni, Alexandr and Laarhoven, Thijs and Razenshteyn, Ilya P. and Waingarten, Erik},
  year = {2016},
  doi = {10.1137/1.9781611974782.4},
  abstract = {[See the paper for the full abstract.] We show tight upper and lower bounds for time-space trade-offs for the c-Approximate Near Neighbor Search problem. For the d-dimensional Euclidean space and n-point datasets, we develop a data structure with space n\textsuperscript{\{\vphantom\}}1+{$\rho$}\textsubscript{u}+o(1)\vphantom\{\}+O(dn) and query time n\textsuperscript{\{\vphantom\}}{$\rho$}\textsubscript{q}+o(1)\vphantom\{\}+dn\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} for every {$\rho$}\textsubscript{u},{$\rho$}\textsubscript{q}{$\geq$}0 such that: \{equation\} c{\textasciicircum}2 {\textsurd}\{{$\rho\_$}q\} + (c{\textasciicircum}2 - 1) {\textsurd}\{{$\rho\_$}u\} = {\textsurd}\{2c{\textasciicircum}2 - 1\}. \{equation\} This is the first data structure that achieves sublinear query time and near-linear space for every approximation factor c{$>$}1, improving upon [Kapralov, PODS 2015]. The data structure is a culmination of a long line of work on the problem for all space regimes; it builds on Spherical Locality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and data-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni, Razenshteyn, STOC 2015]. Our matching lower bounds are of two types: conditional and unconditional. First, we prove tightness of the whole above trade-off in a restricted model of computation, which captures all known hashing-based approaches. We then show unconditional cell-probe lower bounds for one and two probes that match the above trade-off for {$\rho$}\textsubscript{q}=0, improving upon the best known lower bounds from [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first space lower bound (for any static data structure) for two probes which is not polynomially smaller than the one-probe bound. To show the result for two probes, we establish and exploit a connection to locally-decodable codes.},
  citationcount = {125},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,data structure,lower bound,query,query time,sorted,static,time-space}
}

@inproceedings{andoniOptimalityDimensionalityReduction2006,
  title = {On the {{Optimality}} of the {{Dimensionality Reduction Method}}},
  booktitle = {2006 47th {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}}'06)},
  author = {Andoni, Alexandr and Indyk, Piotr and Patrascu, Mihai},
  year = {2006},
  month = oct,
  pages = {449--458},
  issn = {0272-5428},
  doi = {10.1109/FOCS.2006.56},
  url = {https://ieeexplore.ieee.org/abstract/document/4031380},
  urldate = {2024-11-20},
  abstract = {We investigate the optimality of (1+{\i}n )-approximation algorithms obtained via the dimensionality reduction method. We show that: --Any data structure for the (1+{\i}n )-approximate nearest neighbor problem in Hamming space, which uses constant number of probes to answer each query, must use n{\textasciicircum}{\O}mega {\l}eft( 1/ {\i}n {\textasciicircum}2 {\textbackslash}right) space. --Any algorithm for the (1+{\i}n )-approximate closest substring problem must run in time exponential in 1/ {\i}n {\textasciicircum}2 - {\textbackslash}gamma for any {\textbackslash}gamma {$>$} 0 (unless 3SAT can be solved in subexponential time) Both lower bounds are (essentially) tight.},
  keywords = {Algorithm design and analysis,Clustering algorithms,Concrete,data structure,Data structures,Design methodology,Frequency,lower bound,Nearest neighbor searches,Pattern analysis,Polynomials,Probes,query,reduction},
  file = {/Users/tulasi/Zotero/storage/WWALAR5F/Andoni et al. - 2006 - On the Optimality of the Dimensionality Reduction Method.pdf}
}

@article{andoniOptimalityDimensionalityReduction2006a,
  title = {On the Optimality of the Dimensionality Reduction Method},
  author = {Andoni, Alexandr and Indyk, P. and Patrascu, M.},
  year = {2006},
  doi = {10.1109/FOCS.2006.56},
  abstract = {We investigate the optimality of (1+epsi)-approximation algorithms obtained via the dimensionality reduction method. We show that: any data structure for the (1 + epsi)-approximate nearest neighbor problem in Hamming space, which uses constant number of probes to answer each query, must use nOmega(1/epsi2) space; any algorithm for the (1 + epsi)-approximate closest substring problem must run in time exponential in 1/epsi2 - gamma for any gamma {\textquestiondown} 0 (unless 3SAT can be solved in sub-exponential time). Both lower bounds are (essentially) tight},
  citationcount = {84},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,lower bound,query,reduction}
}

@article{andoniOvercomingThe12008,
  title = {Overcoming the ` 1 Non-Embeddability Barrier: {{Algorithms}} for Product Metrics},
  author = {Andoni, Alexandr and {Piotr} and Krauthgamer, Robert},
  year = {2008},
  doi = {10.1137/1.9781611973068.94},
  abstract = {A common approach for solving computational problems over a difficult metric space is to embed the ``hard'' metric into L 1 , which admits efficient algorithms and is thus considered an ``easy'' metric. This approach has proved successful or partially successful for important spaces such as the edit distance, but it also has inherent limitations: it is provably impossible to go below certain approximation for some metrics. We propose a new approach, of embedding the difficult space into richer host spaces, namely iterated products of standard spaces like ` 1 and ` {$\infty$} . We show that this class is rich since it contains useful metric spaces with only a constant distortion, and, at the same time, it is tractable and admits efficient algorithms. Using this approach, we obtain for example the first nearest neighbor data structure with O (log log d ) approximation for edit distance in non-repetitive strings (the Ulam metric). This approximation is exponentially better than the lower bound for embedding into L 1 . Furthermore, we give constant factor approximation for two other computational problems. Along the way, we answer positively a question posed in [Ajtai, Jayram, Kumar, and Sivakumar, STOC 2002]. One of our algorithms has already found applications for smoothed edit distance over 0-1 strings [Andoni and Krauthgamer, ICALP 2008].},
  citationcount = {43},
  venue = {No venue available}
}

@article{andoniTightLowerBounds2015,
  title = {Tight Lower Bounds for Data-Dependent Locality-Sensitive Hashing},
  author = {Andoni, Alexandr and Razenshteyn, Ilya P.},
  year = {2015},
  doi = {10.4230/LIPIcs.SoCG.2016.9},
  abstract = {We prove a tight lower bound for the exponent {$\rho$} for data-dependent Locality-Sensitive Hashing schemes, recently used to design efficient solutions for the c-approximate nearest neighbor search. In particular, our lower bound matches the bound of {$\rho\leq$}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{2c-1\}+o(1) for the {$\ell_1$} space, obtained via the recent algorithm from [Andoni-Razenshteyn, STOC'15]. In recent years it emerged that data-dependent hashing is strictly superior to the classical Locality-Sensitive Hashing, when the hash function is data-independent. In the latter setting, the best exponent has been already known: for the {$\ell_1$} space, the tight bound is {$\rho$}=1/c, with the upper bound from [Indyk-Motwani, STOC'98] and the matching lower bound from [O'Donnell-Wu-Zhou, ITCS'11]. We prove that, even if the hashing is data-dependent, it must hold that {$\rho\geq$}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{2c-1\}-o(1). To prove the result, we need to formalize the exact notion of data-dependent hashing that also captures the complexity of the hash functions (in addition to their collision properties). Without restricting such complexity, we would allow for obviously infeasible solutions such as the Voronoi diagram of a dataset. To preclude such solutions, we require our hash functions to be succinct. This condition is satisfied by all the known algorithmic results.},
  citationcount = {50},
  venue = {International Symposium on Computational Geometry},
  keywords = {lower bound}
}

@article{andreaspavlogiannisCFLDyckReachability2022,
  title = {{{CFL}}/{{Dyck Reachability}}},
  author = {Andreas Pavlogiannis},
  year = {2022},
  journal = {ACM SIGLOG News},
  doi = {10.1145/3583660.3583664},
  abstract = {CFL/Dyck reachability is a simple graph-theoretic problem: given a CFL/Dyck language L over an alphabet {$\Sigma$}, a graph G = (V, E) of {$\Sigma$}-labeled edges, and two distinguished nodes s, t {$\in$} V, does there exist a path from s to t that spells out a word in L? This simple notion of language-based graph reachability serves as the algorithmic formulation of a large number of problems in diverse domains, such as graph databases and program static analysis. This paper takes an algorithmic perspective on CFL/Dyck reachability, and overviews several recent advances concerning the decidability and complexity of the problem and some its close variants, as realized in the areas of automata theory and program verification.},
  keywords = {static},
  annotation = {Citation Count: 4}
}

@article{andreyprokopenkoAdvancesArborXSupport2024,
  title = {Advances in {{ArborX}} to Support Exascale Applications},
  author = {Andrey Prokopenko and Daniel Arndt and {Damien Lebrun-Grandi'e} and Bruno Turcksin and N. Frontiere and J. D. Emberson and M. Buehlmann},
  year = {2024},
  journal = {The international journal of high performance computing applications},
  doi = {10.1177/10943420241298296},
  abstract = {ArborX is a performance portable geometric search library developed as part of the Exascale Computing Project (ECP). In this paper, we explore a collaboration between ArborX and a cosmological simulation code HACC. Large cosmological simulations on exascale platforms encounter a bottleneck due to the in-situ analysis requirements of halo finding, a problem of identifying dense clusters of dark matter (halos). This problem is solved by using a density-based DBSCAN clustering algorithm. With each MPI rank handling hundreds of millions of particles, it is imperative for the DBSCAN implementation to be efficient. In addition, the requirement to support exascale supercomputers from different vendors necessitates performance portability of the algorithm. We describe how this challenge problem guided ArborX development, and enhanced the performance and the scope of the library. We explore the improvements in the basic algorithms for the underlying search index to improve the performance, and describe several implementations of DBSCAN in ArborX. Further, we report the history of the changes in ArborX and their effect on the time to solve a representative benchmark problem, as well as demonstrate the real world impact on production end-to-end cosmology simulations.},
  annotation = {Citation Count: 0}
}

@article{angluinFastProbabilisticAlgorithms1977,
  title = {Fast Probabilistic Algorithms for Hamiltonian Circuits and Matchings},
  author = {Angluin, D. and Valiant, L.},
  year = {1977},
  doi = {10.1145/800105.803393},
  abstract = {The main purpose of this paper is to give techniques for analysing the probabilistic performance of certain kinds of algorithms, and hence to suggest some fast algorithms with provably desirable probabilistic behaviour. The particular problems we consider are: finding Hamiltonian circuits in directed graphs (DHC), finding Hamiltonian circuits in undirected graphs (UHC), and finding perfect matchings in undirected graphs (PM). We show that for each problem there is an algorithm that is extremely fast (0(n(log n)2) for DHC and UHC, and 0(nlog n) for PM), and which with probability tending to one finds a solution in randomly chosen graphs of sufficient density. These results contrast with the known NP-completeness of the first two problems [2,12] and the best worst-case upper bound known of 0(n2.5) for the last [9].},
  citationcount = {717},
  venue = {Symposium on the Theory of Computing}
}

@article{aniketmurhekarNashEquilibriaTwoPlayer2023,
  title = {Nash {{Equilibria}} of {{Two-Player Matrix Games Repeated Until Collision}}},
  author = {Aniket Murhekar and Eklavya Sharma},
  year = {2023},
  journal = {Foundations of Software Technology and Theoretical Computer Science},
  doi = {10.48550/arXiv.2309.15870},
  abstract = {We introduce and initiate the study of a natural class of repeated two-player matrix games, called Repeated-Until-Collision (RUC) games. In each round, both players simultaneously pick an action from a common action set \${\textbackslash}\{1, 2, {\textbackslash}dots, n{\textbackslash}\}\$. Depending on their chosen actions, they derive payoffs given by \$n {\textbackslash}times n\$ matrices \$A\$ and \$B\$, respectively. If their actions collide (i.e., they pick the same action), the game ends, otherwise, it proceeds to the next round. Both players want to maximize their total payoff until the game ends. RUC games can be interpreted as pursuit-evasion games or repeated hide-and-seek games. They also generalize hand cricket, a popular game among children in India. We show that under mild assumptions on the payoff matrices, every RUC game admits a Nash equilibrium (NE). Moreover, we show the existence of a stationary NE, where each player chooses their action according to a probability distribution over the action set that does not change across rounds. Remarkably, we show that all NE are effectively the same as the stationary NE, thus showing that RUC games admit an almost unique NE. Lastly, we also show how to compute (approximate) NE for RUC games.},
  annotation = {Citation Count: 0}
}

@article{antoineamarilliDynamicMembershipRegular2021,
  title = {Dynamic {{Membership}} for {{Regular Languages}}},
  author = {Antoine Amarilli and Louis Jachiet and Charles Paperman},
  year = {2021},
  journal = {International Colloquium on Automata, Languages and Programming},
  doi = {10.4230/LIPIcs.ICALP.2021.116},
  abstract = {We study the dynamic membership problem for regular languages: fix a language L, read a word w, build in time O({\textbar}w{\textbar}) a data structure indicating if w is in L, and maintain this structure efficiently under letter substitutions on w. We consider this problem on the unit cost RAM model with logarithmic word length, where the problem always has a solution in O(log {\textbar}w{\textbar} / log log {\textbar}w{\textbar}) per operation. We show that the problem is in O(log log {\textbar}w{\textbar}) for languages in an algebraically-defined, decidable class QSG, and that it is in O(1) for another such class QLZG. We show that languages not in QSG admit a reduction from the prefix problem for a cyclic group, so that they require \{{\textbackslash}Omega\}(log {\textbar}w{\textbar} / log log {\textbar}w{\textbar}) operations in the worst case; and that QSG languages not in QLZG admit a reduction from the prefix problem for the multiplicative monoid U 1 = \{0, 1\}, which we conjecture cannot be maintained in O(1). This yields a conditional trichotomy. We also investigate intermediate cases between O(1) and O(log log {\textbar}w{\textbar}). Our results are shown via the dynamic word problem for monoids and semigroups, for which we also give a classification. We thus solve open problems of the paper of Skovbjerg Frandsen, Miltersen, and Skyum [30] on the dynamic word problem, and additionally cover regular languages.},
  keywords = {data structure,dynamic,reduction},
  annotation = {Citation Count: 6}
}

@article{antoineamarilliEnumeratingRegularLanguages2022,
  title = {Enumerating {{Regular Languages}} with {{Bounded Delay}}},
  author = {Antoine Amarilli and Mika{\"e}l Monet},
  year = {2022},
  journal = {Symposium on Theoretical Aspects of Computer Science},
  doi = {10.4230/LIPIcs.STACS.2023.8},
  abstract = {We study the task, for a given language \$L\$, of enumerating the (generally infinite) sequence of its words, without repetitions, while bounding the delay between two consecutive words. To allow for delay bounds that do not depend on the current word length, we assume a model where we produce each word by editing the preceding word with a small edit script, rather than writing out the word from scratch. In particular, this witnesses that the language is orderable, i.e., we can write its words as an infinite sequence such that the Levenshtein edit distance between any two consecutive words is bounded by a value that depends only on the language. For instance, \$(a+b){\textasciicircum}*\$ is orderable (with a variant of the Gray code), but \$a{\textasciicircum}* + b{\textasciicircum}*\$ is not. We characterize which regular languages are enumerable in this sense, and show that this can be decided in PTIME in an input deterministic finite automaton (DFA) for the language. In fact, we show that, given a DFA \$A\$, we can compute in PTIME automata \$A\_1, {\textbackslash}ldots, A\_t\$ such that \$L(A)\$ is partitioned as \$L(A\_1) {\textbackslash}sqcup {\textbackslash}ldots {\textbackslash}sqcup L(A\_t)\$ and every \$L(A\_i)\$ is orderable in this sense. Further, we show that the value of \$t\$ obtained is optimal, i.e., we cannot partition \$L(A)\$ into less than \$t\$ orderable languages. In the case where \$L(A)\$ is orderable (i.e., \$t=1\$), we show that the ordering can be produced by a bounded-delay algorithm: specifically, the algorithm runs in a suitable pointer machine model, and produces a sequence of bounded-length edit scripts to visit the words of \$L(A)\$ without repetitions, with bounded delay -- exponential in \${\textbar}A{\textbar}\$ -- between each script. In fact, we show that we can achieve this while only allowing the edit operations push and pop at the beginning and end of the word, which implies that the word can in fact be maintained in a double-ended queue.},
  annotation = {Citation Count: 4}
}

@article{antoineamarilliEnumeratingRegularLanguages2022a,
  title = {Enumerating {{Regular Languages}} in {{Constant Delay}}},
  author = {Antoine Amarilli and Mika{\"e}l Monet},
  year = {2022},
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2209.14878},
  abstract = {We study the task, for a given language L , of enumerating the (generally infinite) sequence of its words, without repetitions, while bounding the delay between two consecutive words. To allow for constant delay bounds, we assume a model where we produce each word by editing the preceding word with a small edit script, rather than writing out the word from scratch. In particular, this witnesses that the language is orderable , i.e., we can write its words as an infinite sequence such that the Levenshtein edit distance between any two consecutive words is bounded by a constant. For instance, p a ` b q {\r{}} is orderable (with a variant of the Gray code), but a {\r{}} ` b {\r{}} is not. We characterize which regular languages are enumerable in this sense, and show that this can be decided in PTIME in an input deterministic finite automaton (DFA) for the language. In fact, we show that, given a DFA A , we can compute in PTIME automata A 1 , . . . , A t such that L p A q is partitioned as L p A 1 q {\textbackslash} . . . {\textbackslash} L p A t q and every L p A i q is orderable in this sense. Further, we show that this is optimal, i.e., we cannot partition L p A q into less than t orderable languages. In the case where L p A q is orderable, we show that the ordering can be computed as a constant-delay algorithm: specifically, the algorithm runs in a suitable pointer machine model, and produces a sequence of constant-length edit scripts to visit the words of L p A q without repetitions, with constant delay between each script. In fact, we show that we can achieve this while only allowing the edit operations push and pop at the beginning and end of the word, which implies that the word can in fact be maintained in a double-ended queue.},
  annotation = {Citation Count: 0}
}

@article{aokiGeneralizingSearchIn1998,
  title = {Generalizing "Search" in Generalized Search Trees},
  author = {Aoki, Paul M.},
  year = {1998},
  doi = {10.1109/ICDE.1998.655801},
  abstract = {The generalized search tree, or GiST, defines a framework of basic interfaces required to construct a hierarchical access method for database systems. As originally specified, GiST only supports record selection. We show how a small number of additional interfaces enable GiST to support a much larger class of operations. Members of this class, which includes, nearest-neighbor and ranked search, user-defined aggregation and index-assisted selectivity estimation, are increasingly common in new database applications. The advantages of implementing these operations in the GiST framework include reduction of user development effort and the ability to use industrial strength concurrency and recovery mechanisms provided by expert implementers.},
  citationcount = {60},
  venue = {Proceedings / International Conference on Data Engineering}
}

@article{applebaumCryptography$NC^0$2006,
  title = {Cryptography in \${{NC}}{\textasciicircum}0\$},
  author = {Applebaum, Benny and Ishai, Yuval and Kushilevitz, Eyal},
  year = {2006},
  month = jan,
  journal = {SIAM J. Comput.},
  volume = {36},
  number = {4},
  pages = {845--888},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/S0097539705446950},
  url = {https://epubs.siam.org/doi/abs/10.1137/S0097539705446950},
  urldate = {2025-03-28},
  abstract = {Pseudorandom generators are fundamental to many theoretical and applied aspects of computing. We show how to construct a pseudorandom generator from any one-way function. Since it is easy to construct a one-way function from a pseudorandom generator, this result shows that there is a pseudorandom generator if and only if there is a one-way function.},
  file = {/Users/tulasi/Zotero/storage/Y9ACKRTB/Applebaum et al. - 2006 - Cryptography in $NC^0$.pdf}
}

@article{arasuObliviousQueryProcessing2013,
  title = {Oblivious Query Processing},
  author = {Arasu, A. and Kaushik, R.},
  year = {2013},
  doi = {10.5441/002/icdt.2014.07},
  abstract = {Motivated by cloud security concerns, there is an increasing interest in database systems that can store and support queries over encrypted data. A common architecture for such systems is to use a trusted component such as a cryptographic co-processor for query processing that is used to securely decrypt data and perform computations in plaintext. The trusted component has limited memory, so most of the (input and intermediate) data is kept encrypted in an untrusted storage and moved to the trusted component on ``demand.'' In this setting, even with strong encryption, the data access pattern from untrusted storage has the potential to reveal sensitive information; indeed, all existing systems that use a trusted component for query processing over encrypted data have this vulnerability. In this paper, we undertake the first formal study of secure query processing, where an adversary having full knowledge of the query (text) and observing the query execution learns nothing about the underlying database other than the result size of the query on the database. We introduce a simpler notion, oblivious query processing, and show formally that a query admits secure query processing iff it admits oblivious query processing. We present oblivious query processing algorithms for a rich class of database queries involving selections, joins, grouping and aggregation. For queries not handled by our algorithms, we provide some initial evidence that designing oblivious (and therefore secure) algorithms would be hard via reductions from two simple, well-studied problems that are generally believed to be hard. Our study of oblivious query processing also reveals interesting connections to database join theory.},
  citationcount = {64},
  venue = {International Conference on Database Theory},
  keywords = {query,reduction}
}

@article{argeOefficientSpatialData2012,
  title = {I/{{O-efficient}} Spatial Data Structures for Range Queries},
  author = {Arge, L. and Larsen, K. G.},
  year = {2012},
  journal = {SIGSPATIAL Special},
  volume = {4},
  number = {2},
  pages = {2--7},
  doi = {10.1145/2367574.2367575},
  keywords = {data structure,query},
  annotation = {L. Arge . External memory geometric data structures , 2005 . "Lecture notes available at http://www.cs.au.dk/{\textasciitilde}large/ioS06/ionotes.pdf". L. Arge. External memory geometric data structures, 2005. "Lecture notes available at http://www.cs.au.dk/{\textasciitilde}large/ioS06/ionotes.pdf".\\
\\
\\
\\
M. J. Atallah and M. Blanton , editors . Algorithms and theory of computation handbook: general concepts and techniques . Chapman \& Hall/CRC , 2 edition, 2010 . M. J. Atallah and M. Blanton, editors. Algorithms and theory of computation handbook: general concepts and techniques. Chapman \& Hall/CRC, 2 edition, 2010.\\
\\
M. de Berg and O. Cheong and M. van Kreveld and M. Overmars . Computational Geometry: Algorithms and Applications . Springer-Verlag , 3 rd edition, 2008 . Chapter 10. M. de Berg and O. Cheong and M. van Kreveld and M. Overmars. Computational Geometry: Algorithms and Applications. Springer-Verlag, 3rd edition, 2008. Chapter 10.\\
\\
\\
\\
K. V. R. Kanth and A. K. Singh . Optimal dynamic range searching in non-replicating index structures . In Proc. International Conference on Database Theory , pages 257 -- 276 , 1997 . K. V. R. Kanth and A. K. Singh. Optimal dynamic range searching in non-replicating index structures. In Proc. International Conference on Database Theory, pages 257--276, 1997.},
  file = {/Users/tulasi/Zotero/storage/33UZRM6Q/Arge and Larsen - 2012 - IO-efficient spatial data structures for range queries.pdf}
}

@article{argeOnTwoDimensional1999,
  title = {On Two-Dimensional Indexability and Optimal Range Search Indexing},
  author = {Arge, L. and Samoladas, V. and Vitter, J.},
  year = {1999},
  doi = {10.1145/303976.304010},
  abstract = {In this paper we settle several longstanding open problems in theory of indexability and external orthogonal range searching. In the rst part of the paper, we apply the theory of indexability to the problem of two-dimensional range searching. We show that the special case of 3-sided querying can be solved with constant redundancy and access overhead. From this, we derive indexing schemes for general 4-sided range queries that exhibit an optimal tradeo between redundancy and access overhead. In the second part of the paper, we develop dynamic external memory data structures for the two query types. Our structure for 3-sided queries occupies O(N=B) disk blocks, and it supports insertions and deletions in O(log B N) I/Os and queries in O(log B N + T=B) I/Os, where B is the disk block size, N is the number of points, and T is the query output size. These bounds are optimal. Our structure for general (4-sided) range searching occupies O (N=B)(log(N=B))= log log B N disk blocks and answers queries in O(log B N + T=B) I/Os, which are optimal. It also supports updates in O (log B N)(log(N=B))= log log B N I/Os. Center for Geometric Computing, Department of Computer Science, Duke University, Box 90129, Durham, NC 27708\{0129. Supported in part by the U.S. Army Research O ce through MURI grant DAAH04\{96\{1\{0013 and by the National Science Foundation through ESS grant EIA\{9870734. Part of this work was done while visiting BRICS, Department of Computer Science, University of Aarhus, Denmark. Email: large@cs.duke.edu. yDepartment of Computer Sciences, University of Texas at Austin, Austin, TX 78712-1188. Email vsam@cs.utexas.edu zCenter for Geometric Computing, Department of Computer Science, Duke University, Box 90129, Durham, NC 27708\{0129. Supported in part by the U.S. Army Research O ce through MURI grant DAAH04\{96\{1\{0013 and by the National Science Foundation through grants CCR\{9522047 and EIA\{9870734. Part of this work was done while visiting BRICS, Department of Computer Science, University of Aarhus, Denmark and I.N.R.I.A., Sophia Antipolis, France. Email: jsv@cs.duke.edu.\vphantom{\}\}\}\}\}\}\}\}\}\}\}}},
  citationcount = {167},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems}
}

@article{argeOptimalDynamicInterval1996,
  title = {Optimal Dynamic Interval Management in External Memory},
  author = {Arge, L. and Vitter, J.},
  year = {1996},
  doi = {10.1109/SFCS.1996.548515},
  abstract = {The authors present a space- and I/O-optimal external-memory data structure for answering stabbing queries on a set of dynamically maintained intervals. The data structure settles an open problem in databases and I/O algorithms by providing the first optimal external-memory solution to the dynamic interval management problem, which is a special case of 2-dimensional range searching and a central problem for object-oriented and temporal databases and for constraint logic programming. The data structure simultaneously uses optimal linear space (that is, O(N/B) blocks of disk space) and achieves the optimal O(log/sub B/ N+T/B) I/O query bound and O(log/sub B/ N) I/O update bound, where B is the I/O block size and T the number of elements in the answer to a query. The structure is also the first optimal external data structure for a 2-dimensional range searching problem that has worst-case as opposed to amortized update bounds. Part of the data structure uses a novel balancing technique for efficient worst-case manipulation of balanced trees, which is of independent interest.},
  citationcount = {168},
  venue = {Proceedings of 37th Conference on Foundations of Computer Science}
}

@article{argeOptimalExternalMemory2003,
  title = {Optimal External Memory Interval Management},
  author = {Arge, L. and Vitter, J.},
  year = {2003},
  doi = {10.1137/S009753970240481X},
  abstract = {In this paper we present the external interval tree, an optimal external memory data structure for answering stabbing queries on a set of dynamically maintained intervals. The external interval tree can be used in an optimal solution to the dynamic interval management problem, which is a central problem for object-oriented and temporal databases and for constraint logic programming. Part of the structure uses a weight-balancing technique for efficient worst-case manipulation of balanced trees, which is of independent interest. The external interval tree, as well as our new balancing technique, have recently been used to develop several efficient external data structures.},
  citationcount = {141},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,dynamic,query}
}

@article{argeOptimalExternalMemory2004,
  title = {Optimal External Memory Planar Point Enclosure},
  author = {Arge, L. and Samoladas, V. and Yi, K.},
  year = {2004},
  doi = {10.1007/s00453-007-9126-2},
  abstract = {No abstract available},
  citationcount = {23},
  venue = {Algorithmica}
}

@article{argeTheBufferTree2003,
  title = {The Buffer Tree: A Technique for Designing Batched External Data Structures},
  author = {Arge, L.},
  year = {2003},
  doi = {10.1007/s00453-003-1021-x},
  abstract = {No abstract available},
  citationcount = {210},
  venue = {Algorithmica}
}

@article{argeThePriorityR2004,
  title = {The Priority {{R-tree}}: {{A}} Practically Efficient and Worst-Case Optimal {{R-tree}}},
  author = {Arge, L. and Berg, M. D. and Haverkort, H. and Yi, K.},
  year = {2004},
  doi = {10.1145/1328911.1328920},
  abstract = {We present the priority R-tree, or PR-tree, which is the first R-tree variant that always answers a window query using {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(({\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown}/{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}){\textexclamdown}sup{\textquestiondown}1-1/{\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sup{\textquestiondown}+{\textexclamdown}i{\textquestiondown}T{\textexclamdown}/i{\textquestiondown}/{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}) I/Os, where {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown} is the number of {\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}-dimensional (hyper-) rectangles stored in the R-tree, {\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown} is the disk block size, and {\textexclamdown}i{\textquestiondown}T{\textexclamdown}/i{\textquestiondown} is the output size. This is provably asymptotically optimal and significantly better than other R-tree variants, where a query may visit all {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown}/{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown} leaves in the tree even when {\textexclamdown}i{\textquestiondown}T{\textexclamdown}/i{\textquestiondown} = 0. We also present an extensive experimental study of the practical performance of the PR-tree using both real-life and synthetic data. This study shows that the PR-tree performs similarly to the best-known R-tree variants on real-life and relatively nicely distributed data, but outperforms them significantly on more extreme data.},
  citationcount = {234},
  venue = {TALG}
}

@article{argyriosdeligkasApproximatingExistentialTheory2018,
  title = {Approximating the {{Existential Theory}} of the {{Reals}}},
  author = {Argyrios Deligkas and John Fearnley and Themistoklis Melissourgos and P. Spirakis},
  year = {2018},
  journal = {Workshop on Internet and Network Economics},
  doi = {10.1007/978-3-030-04612-5_9},
  annotation = {Citation Count: 11}
}

@article{argyriosdeligkasLipschitzContinuityApproximate2015,
  title = {Lipschitz {{Continuity}} and {{Approximate Equilibria}}},
  author = {Argyrios Deligkas and John Fearnley and P. Spirakis},
  year = {2015},
  journal = {Algorithmica},
  doi = {10.1007/s00453-020-00709-3},
  annotation = {Citation Count: 9}
}

@article{arminbiereClausalCongruenceClosure2024,
  title = {Clausal {{Congruence Closure}}},
  author = {Armin Biere and Katalin Fazekas and Mathias Fleury and Nils Froleyks},
  year = {2024},
  journal = {International Conference on Theory and Applications of Satisfiability Testing},
  doi = {10.4230/LIPIcs.SAT.2024.6},
  abstract = {Many practical applications of satisfiability solving employ multiple steps to encode an original problem formulation into conjunctive normal form. Often circuits are used as intermediate representation before encoding those circuits into clausal form. These circuits however might contain redundant isomorphic sub-circuits. If blindly translated into clausal form, this redundancy is retained and increases solving time unless specific preprocessing algorithms are used. Furthermore, such redundant sub-formula structure might only emerge during solving and needs to be addressed by inprocessing. This paper presents a new approach which extracts gate information from the formula and applies congruence closure to match and eliminate redundant gates. Besides new algorithms for gate extraction, we also describe previous unpublished attempts to tackle this problem. Experiments focus on the important problem of combinational equivalence checking for hardware designs and show that our new approach yields a substantial gain in CNF solver performance.},
  keywords = {information},
  annotation = {Citation Count: 2}
}

@article{arminbierePreprocessingSATSolving2021,
  title = {Preprocessing in {{SAT Solving}}},
  author = {Armin Biere and Matti J{\"a}rvisalo and B. Kiesl},
  year = {2021},
  journal = {Handbook of Satisfiability},
  doi = {10.3233/FAIA200992},
  abstract = {Preprocessing has become a key component of the Boolean satisfiability (SAT) solving workflow. In practice, preprocessing is situated between the encoding phase and the solving phase, with the aim of decreasing the total solving time by applying efficient simplification techniques on SAT instances to speed up the search subsequently performed by a SAT solver. In this chapter, we overview key preprocessing techniques proposed in the literature. While the main focus is on techniques applicable to formulas in conjunctive normal form (CNF), we also selectively cover main ideas for preprocessing structural and higher-level SAT instance representations.},
  annotation = {Citation Count: 46}
}

@article{arroyueloAdaptiveSuccinctness2021,
  title = {Adaptive Succinctness},
  author = {Arroyuelo, Diego and Raman, R.},
  year = {2021},
  doi = {10.1007/s00453-021-00872-1},
  abstract = {No abstract available},
  citationcount = {12},
  venue = {Algorithmica},
  keywords = {adaptive}
}

@article{arroyueloFasterDynamicCompressed2019,
  title = {Faster Dynamic Compressed D-Ary Relations},
  author = {Arroyuelo, Diego and {de Bernardo}, Guillermo and Gagie, T. and Navarro, G.},
  year = {2019},
  doi = {10.1007/978-3-030-32686-9_30},
  abstract = {No abstract available},
  citationcount = {5},
  venue = {SPIRE},
  keywords = {dynamic}
}

@article{arroyueloTrieCompressedIntersectable2022,
  title = {Trie-Compressed Intersectable Sets},
  author = {Arroyuelo, Diego and Castillo, J. P.},
  year = {2022},
  doi = {10.48550/arXiv.2212.00946},
  abstract = {We introduce space- and time-efficient algorithms and data structures for the offline set intersection problem. We show that a sorted integer set S{$\subseteq$}[0\{..\}u) of n elements can be represented using compressed space while supporting k-way intersections in adaptive O(k{$\delta$}\{(u/{$\delta$})\}) time, {$\delta$} being the alternation measure introduced by Barbay and Kenyon. Our experimental results suggest that our approaches are competitive in practice, outperforming the most efficient alternatives (Partitioned Elias-Fano indexes, Roaring Bitmaps, and Recursive Universe Partitioning (RUP)) in several scenarios, offering in general relevant space-time trade-offs.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {adaptive,data structure}
}

@article{aryaAccountingForBoundary1995,
  title = {Accounting for Boundary Effects in Nearest-Neighbor Searching},
  author = {Arya, S. and Mount, D. and Narayan, O.},
  year = {1995},
  doi = {10.1145/220279.220315},
  abstract = {Givenn data points ind-dimensional space, nearest-neighbor searching involves determining the nearest of these data points to a given query point. Most averagecase analyses of nearest-neighbor searching algorithms are made under the simplifying assumption thatd is fixed and thatn is so large relative tod thatboundary effects can be ignored. This means that for any query point the statistical distribution of the data points surrounding it is independent of the location of the query point. However, in many applications of nearest-neighbor searching (such as data compression by vector quantization) this assumption is not met, since the number of data pointsn grows roughly as 2d.Largely for this reason, the actual performances of many nearest-neighbor algorithms tend to be much better than their theoretical analyses would suggest. We present evidence of why this is the case. We provide an accurate analysis of the number of cells visited in nearest-neighbor searching by the bucketing andk-d tree algorithms. We assumemdpoints uniformly distributed in dimensiond, wherem is a fixed integer {$\geq$}2. Further, we assume that distances are measured in theL{$\infty$} metric. Our analysis is tight in the limit asd approaches infinity. Empirical evidence is presented showing that the analysis applies even in low dimensions.},
  citationcount = {88},
  venue = {SCG '95}
}

@article{aryaAnOptimalAlgorithm1998,
  title = {An Optimal Algorithm for Approximate Nearest Neighbor Searching Fixed Dimensions},
  author = {Arya, S. and Mount, D. and Netanyahu, N. and Silverman, R. and Wu, A.},
  year = {1998},
  doi = {10.1145/293347.293348},
  abstract = {Consider a set of {\textexclamdown}italic{\textquestiondown}S{\textexclamdown}/italic{\textquestiondown} of {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} data points in real {\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}-dimensional space, R{\textexclamdown}supscrpt{\textquestiondown}d{\textexclamdown}/supscrpt{\textquestiondown}, where distances are measured using any Minkowski metric. In nearest neighbor searching, we preprocess {\textexclamdown}italic{\textquestiondown}S{\textexclamdown}/italic{\textquestiondown} into a data structure, so that given any query point {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}inline-equation{\textquestiondown} {\textexclamdown}f{\textquestiondown}{$\in$}{\textexclamdown}/f{\textquestiondown}{\textexclamdown}/inline-equation{\textquestiondown} R{\textexclamdown}supscrpt{\textquestiondown}d{\textexclamdown}/supscrpt{\textquestiondown}, is the closest point of S to {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown} can be reported quickly. Given any positive real {$\varepsilon$}, data point {\textexclamdown}italic{\textquestiondown}p{\textexclamdown}/italic{\textquestiondown} is a (1 +{$\varepsilon$})-{\textexclamdown}italic{\textquestiondown}approximate nearest neighbor{\textexclamdown}/italic{\textquestiondown} of {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown} if its distance from {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown} is within a factor of (1 + {$\varepsilon$}) of the distance to the true nearest neighbor. We show that it is possible to preprocess a set of {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} points in R{\textexclamdown}supscrpt{\textquestiondown}d{\textexclamdown}/supscrpt{\textquestiondown} in {\textexclamdown}italic{\textquestiondown}O(dn{\textexclamdown}/italic{\textquestiondown} log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) time and {\textexclamdown}italic{\textquestiondown}O(dn){\textexclamdown}/italic{\textquestiondown} space, so that given a query point {\textexclamdown}italic{\textquestiondown} q{\textexclamdown}/italic{\textquestiondown} {\textexclamdown}inline-equation{\textquestiondown} {\textexclamdown}f{\textquestiondown}{$\in$}{\textexclamdown}/f{\textquestiondown}{\textexclamdown}/inline-equation{\textquestiondown} R{\textexclamdown}supscrpt{\textquestiondown}d{\textexclamdown}/supscrpt{\textquestiondown}, and {$\varepsilon$} {\textquestiondown} 0, a (1 + {$\varepsilon$})-approximate nearest neighbor of {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown} can be computed in {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}c{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}subscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}, {$\varepsilon$}{\textexclamdown}/subscrpt{\textquestiondown} log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) time, where {\textexclamdown}italic{\textquestiondown}c{\textexclamdown}subscrpt{\textquestiondown}d,{$\varepsilon$}{\textexclamdown}/subscrpt{\textquestiondown}{\textexclamdown}/italic{\textquestiondown}{$\leq$}{\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown} {\textexclamdown}inline-equation{\textquestiondown} {\textexclamdown}f{\textquestiondown}{\textexclamdown}fen lp="ceil"{\textquestiondown}1 + 6d/{\textexclamdown}g{\textquestiondown}e{\textexclamdown}/g{\textquestiondown}{\textexclamdown}rp post="ceil"{\textquestiondown}{\textexclamdown}/fen{\textquestiondown}{\textexclamdown}/f{\textquestiondown}{\textexclamdown}/inline-equation{\textquestiondown};{\textexclamdown}supscrpt{\textquestiondown}d{\textexclamdown}/supscrpt{\textquestiondown} is a factor depending only on dimension and {$\varepsilon$}. In general, we show that given an integer {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown} {$\geq$} 1, (1 + {$\varepsilon$})-approximations to the {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown} nearest neighbors of {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown} can be computed in additional {\textexclamdown}italic{\textquestiondown}O(kd{\textexclamdown}/italic{\textquestiondown} log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) time.},
  citationcount = {2751},
  venue = {JACM},
  keywords = {data structure,query}
}

@article{aryaTradeoffsInApproximate2008,
  title = {Tradeoffs in Approximate Range Searching Made Simpler},
  author = {Arya, S. and Fonseca, G. D. D. and Mount, D.},
  year = {2008},
  doi = {10.1109/SIBGRAPI.2008.24},
  abstract = {Range searching is a fundamental problem in computational geometry. The problem involves preprocessing a set of n points in R{\textasciicircum}d into a data structure, so that it is possible to determine the subset of points lying within a given query range. In approximate range searching, a parameter eps epsiv {\textquestiondown} 0 is given, and for a given query range R the points lying within distance eps diam(R) of the range's boundary may be counted or not. In this paper we present three results related to the issue of tradeoffs in approximate range searching. First, we introduce the range sketching problem. Next, we present a space-time tradeoff for smooth convex ranges, which generalize spherical ranges. Finally, we show how to modify the previous data structure to obtain a space-time tradeoff for simplex ranges. In contrast to existing results, which are based on relatively complex data structures, all three of our results are based on simple, practical data structures.},
  citationcount = {6},
  venue = {2008 XXI Brazilian Symposium on Computer Graphics and Image Processing}
}

@article{asadiWorstCaseTo2022,
  title = {Worst-Case to Average-Case Reductions via Additive Combinatorics},
  author = {Asadi, Vahid R. and Golovnev, Alexander and Gur, Tom and Shinkar, Igor},
  year = {2022},
  doi = {10.1145/3519935.3520041},
  abstract = {We present a new framework for designing worst-case to average-case reductions. For a large class of problems, it provides an explicit transformation of algorithms running in time T that are only correct on a small (subconstant) fraction of their inputs into algorithms running in time O(T) that are correct on all inputs. Using our framework, we obtain such efficient worst-case to average-case reductions for fundamental problems in a variety of computational models; namely, algorithms for matrix multiplication, streaming algorithms for the online matrix-vector multiplication problem, and static data structures for all linear problems as well as for the multivariate polynomial evaluation problem. Our techniques crucially rely on additive combinatorics. In particular, we show a local correction lemma that relies on a new probabilistic version of the quasi-polynomial Bogolyubov-Ruzsa lemma.},
  citationcount = {8},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {data structure,reduction,static}
}

@article{asharovOptimalObliviousParallel2022,
  title = {Optimal Oblivious Parallel {{RAM}}},
  author = {Asharov, Gilad and Komargodski, Ilan and Lin, Wei-Kai and Peserico, E. and Shi, E.},
  year = {2022},
  doi = {10.1137/1.9781611977073.98},
  abstract = {An oblivious RAM (ORAM), introduced by Goldreich and Ostrovsky (STOC '87 and J. ACM '96), is a technique for hiding RAM's access pattern. That is, for every input the distribution of the observed locations accessed by the machine is essentially independent of the machine's secret inputs. Recent progress culminated in a work of Asharov et al. (EUROCRYPT '20), obtaining an ORAM with (amortized) logarithmic overhead in total work, which is known to be optimal. Oblivious Parallel RAM (OPRAM) is a natural extension of ORAM to the (more realistic) parallel setting where several processors make concurrent accesses to a shared memory. It is known that any OPRAM must incur logarithmic work overhead and for highly parallel RAMs a logarithmic depth blowup (in the balls and bins model). Despite the significant recent advances, there is still a large gap: all existing OPRAM schemes incur a poly-logarithmic overhead either in total work or in depth. Our main result closes the aforementioned gap and provides an essentially optimal OPRAM scheme. Specifically, assuming one-way functions, we show that any Parallel RAM with memory capacity N can be obliviously simulated in space O(N), incurring only O(logN) blowup in (amortized) total work as well as in depth. Our transformation supports all PRAMs in the CRCW mode and the resulting simulation is in the CRCW mode as well. Bar-Ilan University. NTT Research and Hebrew University of Jerusalem. Cornell University. Universit{\`a} degli Studi di Padova. Cornell University and CMU.},
  citationcount = {13},
  venue = {IACR Cryptology ePrint Archive}
}

@article{assadiDecrementalMatchingIn2022,
  title = {Decremental Matching in General Graphs},
  author = {Assadi, Sepehr and Bernstein, A. and Dudeja, Aditi},
  year = {2022},
  doi = {10.4230/LIPIcs.ICALP.2022.11},
  abstract = {We consider the problem of maintaining an approximate maximum integral matching in a dynamic graph G, while the adversary makes changes to the edges of the graph. The goal is to maintain a (1+{$\epsilon$})-approximate maximum matching for constant {$\epsilon>$}0, while minimizing the update time. In the fully dynamic setting, where both edge insertion and deletions are allowed, Gupta and Peng (see \{GP13\}) gave an algorithm for this problem with an update time of O({\textsurd}\{m\}/{$\epsilon^2$}). Motivated by the fact that the O\textsubscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}({\textsurd}\{m\}) barrier is hard to overcome (see Henzinger, Krinninger, Nanongkai, and Saranurak [HKNS15]); Kopelowitz, Pettie, and Porat [KPP16]), we study this problem in the \{decremental\} model, where the adversary is only allowed to delete edges. Recently, Bernstein, Probst-Gutenberg, and Saranurak (see [BPT20]) gave an O\textsubscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}(1) update time decremental algorithm for this problem in \{bipartite graphs\}. However, beating O({\textsurd}\{m\}) update time remained an open problem for \{general graphs\}. In this paper, we bridge the gap between bipartite and general graphs, by giving an O\textsubscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}(1) update time algorithm that maintains a (1+{$\epsilon$})-approximate maximum integral matching under adversarial deletions. Our algorithm is randomized, but works against an adaptive adversary. Together with the work of Grandoni, Leonardi, Sankowski, Schwiegelshohn, and Solomon [GLSSS19] who give an O\textsubscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}(1) update time algorithm for general graphs in the \{incremental\} (insertion-only) model, our result essentially completes the picture for partially dynamic matching.},
  citationcount = {8},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {adaptive,dynamic,update,update time}
}

@article{assadiFullyDynamicMaximal2018,
  title = {Fully Dynamic Maximal Independent Set with Sublinear in n Update Time},
  author = {Assadi, Sepehr and Onak, Krzysztof and Schieber, B. and Solomon, Shay},
  year = {2018},
  doi = {10.5555/3310435.3310551},
  abstract = {The first fully dynamic algorithm for maintaining a maximal independent set (MIS) with update time that is sublinear in the number of edges was presented recently by the authors of this paper [Assadi et al., STOC'18]. The algorithm is deterministic and its update time is O(m3/4), where m is the (dynamically changing) number of edges. Subsequently, Gupta and Khan and independently Du and Zhang [arXiv, April 2018] presented deterministic algorithms for dynamic MIS with update times of O(m2/3) and [MATH HERE], respectively. Du and Zhang also gave a randomized algorithm with update time [MATH HERE]. Moreover, they provided some partial (conditional) hardness results hinting that the update time of m1/2-e, and in particular n1-e for n-vertex dense graphs, is a natural barrier for this problem for any constant e {\textquestiondown} 0, for deterministic and randomized algorithms that satisfy a certain natural property.In this paper, we break this natural barrier and present the first fully dynamic (randomized) algorithm for maintaining an MIS with update time that is always sublinear in the number of vertices, namely, an [MATH HERE] expected amortized update. We also show that a simpler variant of our algorithm can already achieve an O(m1/3) expected amortized update time, which results in an improved performance over our [MATH HERE] update time algorithm for sufficiently sparse graphs, and breaks the m1/2 barrier of Du and Zhang for all values of m.},
  citationcount = {36},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,update,update time}
}

@article{assadiFullyDynamicMaximal2018,
  title = {Fully Dynamic Maximal Independent Set with Sublinear Update Time},
  author = {Assadi, Sepehr and Onak, Krzysztof and Schieber, B. and Solomon, Shay},
  year = {2018},
  doi = {10.1145/3188745.3188922},
  abstract = {A maximal independent set (MIS) can be maintained in an evolving m-edge graph by simply recomputing it from scratch in O(m) time after each update. But can it be maintained in time sublinear in m in fully dynamic graphs? We answer this fundamental open question in the affirmative. We present a deterministic algorithm with amortized update time O(min\{{$\Delta$},m3/4\}), where {$\Delta$} is a fixed bound on the maximum degree in the graph and m is the (dynamically changing) number of edges. We further present a distributed implementation of our algorithm with O(min\{{$\Delta$},m3/4\}) amortized message complexity, and O(1) amortized round complexity and adjustment complexity (the number of vertices that change their output after each update). This strengthens a similar result by Censor-Hillel, Haramaty, and Karnin (PODC'16) that required an assumption of a non-adaptive oblivious adversary.},
  citationcount = {65},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,non-adaptive,update,update time}
}

@article{assadiImprovedBoundsFor2024,
  title = {Improved Bounds for Fully Dynamic Matching via Ordered Ruzsa-Szemer{\'e}di Graphs},
  author = {Assadi, Sepehr and Khanna, Sanjeev},
  year = {2024},
  doi = {10.48550/arXiv.2406.13573},
  abstract = {In a very recent breakthrough, Behnezhad and Ghafari [FOCS'24] developed a novel fully dynamic randomized algorithm for maintaining a (1-{$\epsilon$})-approximation of maximum matching with amortized update time potentially much better than the trivial O(n) update time. The runtime of the BG algorithm is parameterized via the following graph theoretical concept: * For any n, define ORS(n) -- standing for Ordered RS Graph -- to be the largest number of edge-disjoint matchings M{$_1$},{\dots},M\textsubscript{t} of size {$\Theta$}(n) in an n-vertex graph such that for every i{$\in$}[t], M\textsubscript{i} is an induced matching in the subgraph M\textsubscript{\{\vphantom\}}i\vphantom\{\}{$\cup$}M\textsubscript{\{\vphantom\}}i+1\vphantom\{\}{$\cup\dots\cup$}M\textsubscript{t}. Then, for any fixed {$\epsilon>$}0, the BG algorithm runs in  amortized update time with high probability, even against an adaptive adversary. ORS(n) is a close variant of a more well-known quantity regarding RS graphs (which require every matching to be induced regardless of the ordering). It is currently only known that n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\}{$\leq$}ORS(n){$\leq$}n\textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\}, and closing this gap appears to be a notoriously challenging problem. In this work, we further strengthen the result of Behnezhad and Ghafari and push it to limit to obtain a randomized algorithm with amortized update time of  with high probability, even against an adaptive adversary. In the limit, i.e., if current lower bounds for ORS(n)=n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} are almost optimal, our algorithm achieves an n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} update time for (1-{$\epsilon$})-approximation of maximum matching, almost fully resolving this fundamental question. In its current stage also, this fully reduces the algorithmic problem of designing dynamic matching algorithms to a purely combinatorial problem of upper bounding ORS(n) with no algorithmic considerations.},
  citationcount = {5},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {adaptive,dynamic,lower bound,update,update time}
}

@article{assadiRandomizedComposableCoresets2017,
  title = {Randomized Composable Coresets for Matching and Vertex Cover},
  author = {Assadi, Sepehr and Khanna, S.},
  year = {2017},
  doi = {10.1145/3087556.3087581},
  abstract = {A common approach for designing scalable algorithms for massive data sets is to distribute the computation across, say k, machines and process the data using limited communication between them. A particularly appealing framework here is the simultaneous communication model whereby each machine constructs a small representative summary of its own data and one obtains an approximate/exact solution from the union of the representative summaries. If the representative summaries needed for a problem are small, then this results in a communication-efficient and \{round-optimal\} (requiring essentially no interaction between the machines) protocol. Some well-known examples of techniques for creating summaries include sampling, linear sketching, and composable coresets. These techniques have been successfully used to design communication efficient solutions for many fundamental graph problems. However, two prominent problems are notably absent from the list of successes, namely, the maximum matching problem and the minimum vertex cover problem. Indeed, it was shown recently that for both these problems, even achieving a modest approximation factor of \{(n)\} requires using representative summaries of size {\~\{}{\textohm}\vphantom\{\}(n{\textasciicircum}2) i.e. essentially no better summary exists than each machine simply sending its entire input graph. The main insight of our work is that the intractability of matching and vertex cover in the simultaneous communication model is inherently connected to an adversarial partitioning of the underlying graph across machines. We show that when the underlying graph is randomly partitioned across machines, both these problems admit \{randomized composable coresets\} of size {\~\{}O\vphantom\{\}(n) that yield an {\~\{}O\vphantom\{\}(1)-approximate solution\{Here and throughout the paper, we use ({$\cdot$}) notation to suppress \{(n)\} factors, where n is the number of vertices in the graph. In other words, a small subgraph of the input graph at each machine can be identified as its representative summary and the final answer then is obtained by simply running any maximum matching or minimum vertex cover algorithm on these combined subgraphs. This results in an {\~O}(1)-approximation simultaneous protocol for these problems with {\~O}(nk) total communication when the input is randomly partitioned across k machines. We also prove our results are optimal in a very strong sense: we not only rule out existence of smaller randomized composable coresets for these problems but in fact show that our (nk) bound for total communication is optimal for em any simultaneous communication protocol (i.e. not only for randomized coresets) for these two problems. Finally, by a standard application of composable coresets, our results also imply MapReduce algorithms with the same approximation guarantee in one or two rounds of communication, improving the previous best known round complexity for these problems.\vphantom\}},
  citationcount = {48},
  venue = {ACM Symposium on Parallelism in Algorithms and Architectures},
  keywords = {communication}
}

@article{atallahAlgorithmsAndTheory2009,
  title = {Algorithms and Theory of Computation Handbook, Second Edition, Volume 1: {{General}} Concepts and Techniques},
  author = {Atallah, M. and Blanton, Marina},
  year = {2009},
  doi = {10.1201/9781584888239},
  abstract = {No abstract available},
  citationcount = {12},
  venue = {No venue available}
}

@article{atenieseVerifiableCapacityBound2021,
  title = {Verifiable Capacity-Bound Functions: A New Primitive from Kolmogorov Complexity},
  author = {Ateniese, G. and Chen, Long and Francati, Danilo and Papadopoulos, D. and Tang, Qiang},
  year = {2021},
  doi = {10.1007/978-3-031-31371-4_3},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {IACR Cryptology ePrint Archive}
}

@article{availableBibliography2020,
  title = {Bibliography},
  author = {authors {available}, No},
  year = {2020},
  doi = {10.1017/9781108671644.018},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Communication Complexity}
}

@article{availableCircuitsAndProofs2020,
  title = {Circuits and Proofs},
  author = {authors {available}, No},
  year = {2020},
  doi = {10.1017/9781108671644.013},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Communication Complexity}
}

@article{availableCompressingCommunication2020,
  title = {Compressing Communication},
  author = {authors {available}, No},
  year = {2020},
  doi = {10.1017/9781108671644.010},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Communication Complexity},
  keywords = {communication}
}

@article{availableDeterministicProtocols2020,
  title = {Deterministic Protocols},
  author = {authors {available}, No},
  year = {2020},
  doi = {10.1017/9781108671644.004},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Communication Complexity}
}

@article{availableDiscrepancy2020,
  title = {Discrepancy},
  author = {authors {available}, No},
  year = {2020},
  doi = {10.1017/9781108671644.008},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Communication Complexity}
}

@article{availableIndex2020,
  title = {Index},
  author = {authors {available}, No},
  year = {2020},
  doi = {10.1017/9781108671644.019},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Communication Complexity}
}

@article{availableIntroduction2020,
  title = {Introduction},
  author = {authors {available}, No},
  year = {2020},
  doi = {10.1017/9781108671644.002},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Communication Complexity}
}

@article{availablePreface2020,
  title = {Preface},
  author = {authors {available}, No},
  year = {2020},
  doi = {10.1017/9781108671644.001},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Communication Complexity}
}

@article{availableRandomizedProtocols2020,
  title = {Randomized Protocols},
  author = {authors {available}, No},
  year = {2020},
  doi = {10.1017/9781108671644.006},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Communication Complexity}
}

@article{avisTheQuantumLocker2008,
  title = {The Quantum Locker Puzzle},
  author = {Avis, D. and Broadbent, Anne},
  year = {2008},
  doi = {10.1109/ICQNM.2009.13},
  abstract = {The locker puzzle is a game played by multiple players against a referee. It has been previously shown that the best strategy that exists cannot success with probability greater than than 1-ln2 (approximately 0.31), no matter how many players are involved. Our contribution is to show that quantum players can do much better---they can succeed with probability 1. By making the rules of the game significantly stricter, we show a scenario where the quantum players still succeed perfectly, while the classical players win with vanishing probability. Other variants of the locker puzzle are considered, as well as a cheating referee.},
  citationcount = {3},
  venue = {2009 Third International Conference on Quantum, Nano and Micro Technologies}
}

@article{avrimblumSparseApproximationGenerating2015,
  title = {Sparse {{Approximation}} via {{Generating Point Sets}}},
  author = {Avrim Blum and {Sariel Har-Peled} and Benjamin Raichel},
  year = {2015},
  journal = {ACM-SIAM Symposium on Discrete Algorithms},
  doi = {10.1145/3302249},
  abstract = {For a set P of n points in the unit ball b{$\subseteq$} Rd, consider the problem of finding a small subset T{$\subseteq$} P such that its convex-hull {$\varepsilon$}-approximates the convex-hull of the original set. Specifically, the Hausdorff distance between the convex hull of T and the convex hull of P should be at most {$\varepsilon$}. We present an efficient algorithm to compute such an {$\varepsilon\prime$}-approximation of size kalg, where {$\varepsilon$} {$\prime$} is a function of {$\varepsilon$} and kalg is a function of the minimum size kopt of such an {$\varepsilon$}-approximation. Surprisingly, there is no dependence on the dimension d in either of the bounds. Furthermore, every point of P can be {$\varepsilon$}-approximated by a convex-combination of points of T that is O(1/{$\varepsilon$}2)-sparse. Our result can be viewed as a method for sparse, convex autoencoding: approximately representing the data in a compact way using sparse combinations of a small subset T of the original data. The new algorithm can be kernelized, and it preserves sparsity in the original input.},
  annotation = {Citation Count: 35}
}

@article{axiotisFastAndSimple2020,
  title = {Fast and Simple Modular Subset Sum},
  author = {Axiotis, Kyriakos and Backurs, A. and Bringmann, K. and Jin, Ce and Nakos, Vasileios and Tzamos, Christos and Wu, Hongxun},
  year = {2020},
  doi = {10.1137/1.9781611976496.6},
  abstract = {We revisit the Subset Sum problem over the finite cyclic group \{Z\}\textsubscript{m} for some given integer m. A series of recent works has provided asymptotically optimal algorithms for this problem under the Strong Exponential Time Hypothesis. Koiliaris and Xu (SODA'17, TALG'19) gave a deterministic algorithm running in time O\vphantom\{\}(m\textsuperscript{\{\vphantom\}}5/4\vphantom\{\}), which was later improved to O(m{$^7$}m) randomized time by Axiotis et al. (SODA'19). In this work, we present two simple algorithms for the Modular Subset Sum problem running in near-linear time in m, both efficiently implementing Bellman's iteration over \{Z\}\textsubscript{m}. The first one is a randomized algorithm running in time O(m{$^2$}m), that is based solely on rolling hash and an elementary data-structure for prefix sums; to illustrate its simplicity we provide a short and efficient implementation of the algorithm in Python. Our second solution is a deterministic algorithm running in time O(m\{polylog\}m), that uses dynamic data structures for string manipulation. We further show that the techniques developed in this work can also lead to simple algorithms for the All Pairs Non-Decreasing Paths Problem (APNP) on undirected graphs, matching the asymptotically optimal running time of O\vphantom\{\}(n{$^2$}) provided in the recent work of Duan et al. (ICALP'19).},
  citationcount = {14},
  venue = {SIAM Symposium on Simplicity in Algorithms},
  keywords = {data structure,dynamic}
}

@article{azarmehrFullyDynamicMatching2023,
  title = {Fully Dynamic Matching: (2-{\textsurd}2)-Approximation in Polylog Update Time},
  author = {Azarmehr, Amir and Behnezhad, Soheil and Roghani, M.},
  year = {2023},
  doi = {10.48550/arXiv.2307.08772},
  abstract = {We study maximum matchings in fully dynamic graphs, which are graphs that undergo both edge insertions and deletions. Our focus is on algorithms that estimate the size of maximum matching after each update while spending a small time. An important question studied extensively is the best approximation achievable via algorithms that only spend \{poly\}(n) time per update, where n is the number of vertices. The current best bound is a (1/2+{$\varepsilon_0$})-approximation for a small constant {$\varepsilon_{0}>$}0, due to recent works of Behnezhad [SODA'23] ({$\varepsilon_{0}\sim$}0.001) and Bhattacharya, Kiss, Saranurak, Wajc [SODA'23] ({$\varepsilon_{0}\sim$}0.006) who broke the long-standing 1/2-approximation barrier. These works also showed that for any fixed {$\varepsilon>$}0, the approximation can be further improved to (2-{\textsurd}\{2\}-{$\varepsilon$}){$\sim$}.585 for bipartite graphs, leaving a huge gap between general and bipartite graphs. In this work, we close this gap. We show that for any fixed {$\varepsilon>$}0, a (2-{\textsurd}\{2\}-{$\varepsilon$}) approximation can be maintained in \{poly\}(n) time per update even in general graphs. Our techniques also lead to the same approximation for general graphs in two passes of the semi-streaming setting, removing a similar gap in that setting.},
  citationcount = {8},
  venue = {arXiv.org},
  keywords = {dynamic,update,update time}
}

@article{b.bollobasExpectedBehaviorDisjoint1985,
  title = {On the Expected Behavior of Disjoint Set Union Algorithms},
  author = {B. Bollob{\'a}s and Istv{\'a}n Simon},
  year = {1985},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/22145.22171},
  abstract = {We show that the expected time of the Weighted Quickfind (QFW) disjoint set union and find algorithm to perform (n - 1) randomly chosen unions is cn + o(n/log n), where c = 2.0847 {\dots}. This implies, through an observation of Tarjan and Van Leeuwen, linear expected time bounds to perform O(n) unions and finds for a class of other union -find algorithms. We also prove that the expected time of the unweighted Quickfind (QF) algorithm is n2/8 + o(n(log n)2), and set the several related open questions of Knuth and Sch{\"o}nhage.},
  annotation = {Citation Count: 18}
}

@article{b.chazelleComputingFreeTree1984,
  title = {Computing on a Free Tree via Complexity-Preserving Mappings},
  author = {B. Chazelle},
  year = {1984},
  journal = {Algorithmica},
  doi = {10.1007/BF01840366},
  annotation = {Citation Count: 85}
}

@article{b.chazelleFractionalCascadingII1986,
  title = {Fractional Cascading: {{II}}. {{Applications}}},
  author = {B. Chazelle and L. Guibas},
  year = {1986},
  journal = {Algorithmica},
  doi = {10.1007/BF01840441},
  annotation = {Citation Count: 94}
}

@article{b.chazelleFunctionalApproachData1988,
  title = {A {{Functional Approach}} to {{Data Structures}} and {{Its Use}} in {{Multidimensional Searching}}},
  author = {B. Chazelle},
  year = {1988},
  journal = {SIAM journal on computing (Print)},
  doi = {10.1137/0217026},
  abstract = {We establish new upper bounds on the complexity of multidimensional searching. Our results include, in particular, linear-size data structures for range and rectangle counting in two dimensions with logarithmic query time. More generally, we give improved data structures for rectangle problems in any dimension, in a static as well as a dynamic setting. Several of the algorithms we give are simple to implement and might be the solutions of choice in practice. Central to this paper is the nonstandard approach followed to achieve these results. At its root we find a redefinition of data structures in terms of functional specifications.},
  annotation = {Citation Count: 368}
}

@article{b.chazelleLowerBoundsComplexity1986,
  title = {Lower Bounds on the Complexity of Multidimensional Searching},
  author = {B. Chazelle},
  year = {1986},
  journal = {27th Annual Symposium on Foundations of Computer Science (sfcs 1986)},
  doi = {10.1109/SFCS.1986.29},
  abstract = {We establish new lower bounds on the complexity of several searching problems. We show that the time for solving the partial sum problem on n points in d dimensions is at least proportional to (log n/log 2m/n)d-1 in both the worst and average cases; m denotes the amount of storage used. This bound is provably tight for m = {\textohm}(nlogcn) and any c {$\gg$} d- 1. We also prove a lower bound of {\textohm}(n(log n/log log n)d) on the time required for executing n inserts and queries. Other results include a lower bound on the complexity of orthogonal range searching in d dimensions (in report-mode). We show that on a pointer machine a query time of O(s+polylog(n)) time can only be achieved at the expense of {\textohm}(n(log n/log log n)d-1) space, which is optimal; n and s denote respectively the input and output sizes.},
  annotation = {Citation Count: 13}
}

@article{b.chazelleLowerBoundsComplexity1992,
  title = {Lower {{Bounds}} on the {{Complexity}} of {{Simplex Range Reporting}} on a {{Pointer Machine}}},
  author = {B. Chazelle and B. Rosenberg},
  year = {1992},
  journal = {International Colloquium on Automata, Languages and Programming},
  doi = {10.1007/3-540-55719-9_95},
  keywords = {lower bound},
  annotation = {Citation Count: 23}
}

@article{b.chazelleLowerBoundsIntersection2001,
  title = {Lower Bounds for Intersection Searching and Fractional Cascading in Higher Dimension},
  author = {B. Chazelle and Ding Liu},
  year = {2001},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/380752.380818},
  abstract = {Given an {$<$}italic{$>$}n{$<$}/italic{$>$}-edge convex subdivision of the plane, is it possible to report its {$<$}italic{$>$}k{$<$}/italic{$>$} intersections with a query line segment in {$<$}italic{$>$}O(k+\${\textasciitilde}polylog\$(n)){$<$}/italic{$>$} time, using subquadratic storage? If the query is a plane and the input is a polytope with {$<$}italic{$>$}n{$<$}/italic{$>$} vertices, can one achieve {$<$}italic{$>$}O(k+\${\textasciitilde}polylog\$(n)){$<$}/italic{$>$} time with subcubic storage? Does any convex polytope have a boundary dominant Dobkin-Kirkpatrick hierarchy? Can fractional cascading be generalized to planar maps instead of linear lists? We prove that the answer to all of these questions is no, and we derive near-optimal solutions to these classical problems.},
  keywords = {lower bound,query},
  annotation = {Citation Count: 24}
}

@article{b.chazelleLowerBoundsOrthogonal1990,
  title = {Lower Bounds for Orthogonal Range Searching: {{I}}. {{The}} Reporting Case},
  author = {B. Chazelle},
  year = {1990},
  journal = {JACM},
  doi = {10.1145/77600.77614},
  abstract = {We establish lower bounds on the complexity of orthogonal range reporting in the static case. Given a collection of {$<$}italic{$>$}n{$<$}/italic{$>$} points in {$<$}italic{$>$}d{$<$}/italic{$>$}-space and a box [{$<$}italic{$>$}a{$<$}/italic{$><$}subscrpt{$>$}1{$<$}/subscrpt{$>$}, {$<$}italic{$>$}b{$<$}/italic{$><$}subscrpt{$>$}1{$<$}/subscrpt{$>$}] X {\dots} X [{$<$}italic{$>$}a{$<$}subscrpt{$>$}d{$<$}/subscrpt{$><$}/italic{$>$}, {$<$}italic{$>$}b{$<$}subscrpt{$>$}d{$<$}/subscrpt{$><$}/italic{$>$}], report every point whose {$<$}italic{$>$}i{$<$}/italic{$>$}th coordinate lies in [{$<$}italic{$>$}a{$<$}subscrpt{$>$}i{$<$}/subscrpt{$>$}, b{$<$}subscrpt{$>$}i{$<$}/subscrpt{$><$}/italic{$>$}], for each {$<$}italic{$>$}i{$<$}/italic{$>$} = l, {\dots} , {$<$}italic{$>$}d{$<$}/italic{$>$}. The collection of points is fixed once and for all and can be preprocessed. The box, on the other hand, constitutes a query that must be answered online. It is shown that on a pointer machine a query time of {$<$}italic{$>$}O{$<$}/italic{$>$}({$<$}italic{$>$}k{$<$}/italic{$>$} + polylog({$<$}italic{$>$}n{$<$}/italic{$>$})), where {$<$}italic{$>$}k{$<$}/italic{$>$} is the number of points to be reported, can only be achieved at the expense of \&OHgr;({$<$}italic{$>$}n{$<$}/italic{$>$}(log {$<$}italic{$>$}n{$<$}/italic{$>$}/log log {$<$}italic{$>$}n{$<$}/italic{$>$}){$<$}supscrpt{$><$}italic{$>$}d{$<$}/italic{$>$}-1{$<$}/supscrpt{$>$}) storage. Interestingly, these bounds are optimal in the pointer machine model, but they can be improved (ever so slightly) on a random access machine. In a companion paper, we address the related problem of adding up weights assigned to the points in the query box.},
  annotation = {Citation Count: 180}
}

@article{b.chazellePolytopeRangeSearching1987,
  title = {Polytope Range Searching and Integral Geometry},
  author = {B. Chazelle},
  year = {1987},
  journal = {28th Annual Symposium on Foundations of Computer Science (sfcs 1987)},
  doi = {10.1109/SFCS.1987.48},
  abstract = {plexity ofsimplex range searching. We prove that the worst-case query time is 0. (n/vm), for d = 2, and more generally, 0. (nl log n)/m 1 / d ) , for d {\textasciitilde} 3; n is the number of points and m is the amount of stor\- age available. These bounds hold with high probability for a random point-set (from a uniform distribution) and thus are valid in the worst case as well as on the average. Interestingly, they still hold if the query remains congruent to a fixed simplex or even a fixed slab. What is the significance of these lower bounds? From a practical standpoint the news is disheartening but instructive. For the sake of il\- lustration, take d = 11: our results say that with only linear storage the query time will have to be at least 0'(nO. 9 ). To make matters worse, this quasi-linear lower bound also holds on the average, so it is un\- escapable in practice. For the query time to be lowered to, say, O(y'n), one would need g(n S ) storage, and a whopping n(n 10 ) space would be necessary if a polylogarithmic query time were desired. Things are},
  keywords = {lower bound,query,query time},
  annotation = {Citation Count: 15}
}

@article{b.chazelleSlimmingSearchStructures1985,
  title = {Slimming down Search Structures: {{A}} Functional Approach to Algorithm Design},
  author = {B. Chazelle},
  year = {1985},
  journal = {26th Annual Symposium on Foundations of Computer Science (sfcs 1985)},
  doi = {10.1109/SFCS.1985.51},
  abstract = {We establish new upper bounds on the complexity of several "rectangle" problems. Our results include, for instance, optimal algorithms for range counting and rectangle searching in two dimensions. These involve linear space implementations of range trees and segment trees. The algorithms we give are simple and practical; they can be dynamized and taken into higher dimensions. Also of interest is the nonstandard approach which we follow to obtain these results: it involves transforming data structures on the basis of functional specifications.},
  keywords = {data structure},
  annotation = {Citation Count: 15}
}

@article{b.kalyanasundaramCommunicationComplexityLower1992,
  title = {Communication {{Complexity}} and {{Lower Bounds}} for {{Sequential Computation}}},
  author = {B. Kalyanasundaram and G. Schnitger},
  year = {1992},
  journal = {Jahrestagung der Gesellschaft f{\"u}r Informatik},
  doi = {10.1007/978-3-322-95233-2_15},
  keywords = {communication,communication complexity,lower bound},
  annotation = {Citation Count: 8}
}

@article{b.seyboldTechniquesSatisfyingAssembly1999,
  title = {Techniques for Satisfying Assembly Constraints},
  author = {B. Seybold},
  year = {1999},
  doi = {10.3929/ETHZ-A-003821728},
  abstract = {The assembly problem consists in arranging rigid bodies In 3D-spacc such that a given set of geometric constraints is satisfied. Solutions to this problem arc used to understand the kinematic behavior of mecha{\textlnot} nisms. The general assembly problem is NP-hard. A commercial assembly solver for CAD/CAM systems must meet several requirements: it must support modeling and solving practically relevant assemblies efficiently, it must be able to localize mobility, re{\textlnot} dundancy, and inconsistency, and it also needs to have an extendible software architecture. In this thesis, we introduce techniques used by the ROMA-solver for solving assembly problems. In the first part, we concentrate on the design of the solver. The ROMA-solver always runs in polynomial time. Hence, it cannot cover the entire problem class. Nevertheless, the ROMA-solver successfully solves most piactical assemblies. The solv{\textlnot} ing process resembles constructing, Thus, step-wise constructible as{\textlnot} semblies are solvable and unsolvable cases are often badly constructed. Furthermore, the ROMA-solver can easily be extended to cover addi{\textlnot} tional cases. In the second part, we formalize the solving strategy using constraint satisfaction theory. Usually, a constraint satisfaction problem (CSP) is assumed to have finite domains. However, in the assembly they are uncountable. We extend the standard algorithms and show how to use the (possibly imprecise) results to gain insight into the kinematic behavior of the mechanisms. In the third part, we present two novel strategies for consistency enforcement. Both strategics are applicable to general CSPs. They enable the ROMA-solver to run more efficiently and to compute more structural information. First, we show that consistency enforcement can be restricted to biconnected pairs of variables. Second, we present a new approach to deal with immobile pairs of bodies. We provide an elegant and efficient implementation based on the Union/Find data structure. The ROMA-solver was evaluated on practical examples. Because of its design and the strategies mentioned above, the ROMA-solver handles a larger problem class efficiently and provides more information than previous solvers.},
  keywords = {data structure,information},
  annotation = {Citation Count: 2}
}

@article{babaiComplexityClassesIn1986,
  title = {Complexity Classes in Communication Complexity Theory},
  author = {Babai, L. and Frankl, P. and Simon, Janos},
  year = {1986},
  doi = {10.1109/SFCS.1986.15},
  abstract = {We take a complexity theoretic view of A. C. Yao's theory of communication complexity. A rich structure of natural complexity classes is introduced. Besides providing a more structured approach to the complexity of a variety of concrete problems of interest to VLSI, the main objective is to exploit the analogy between Turing machine (TM) and communication complexity (CC) classes. The latter provide a more amicable environment for the study of questions analogous to the most notorious problems in TM complexity. Implicitly, CC classes corresponding to P, NP, coNP, BPP and PP have previously been considered. Surprisingly, pcc = Npcc {$\cap$} coNPcc is known [AUY]. We develop the definitions of PSPACEcc and of the polynomial time hierarchy in CC. Notions of reducibility are introduced and a natural complete member in each class is found. BPPcc {$\subseteq$} {$\Sigma$}2cc {$\cap$} {$\Pi$}2cc [Si2] remains valid. We solve the question that BPPcc ? NPcc by proving an {\textohm}({\textsurd}n) lower bound for the bounded-error complexity of the coNPcc- complete problem "disjointness". Similar lower bounds follow for essentially any nontrivial monotone graph property. Another consequence is that the deterministically exponentially hard "equality" relation is not NPcc-hard with respect to oracle-protocol reductions. We prove that the distributional complexity of the disjointness problem is O({\textsurd}n log n) under any product measure on \{0, 1\}n {\texttimes} \{0, 1\}n. This points to the difficulty of improving the {\textohm}({\textsurd}n) lower bound for the B2PP complexity of "disjointness". The variety of counting and probabilistic classes appears to be greater than in the Turing machine versions. Many of the simplest graph problems (undirected reachability, planarity, bipartiteness, 2-CNF-satisfiability) turn out to be PSPACEcc-hard. The main open problem remains the separation of the hierarchy, more specifically, the conjecture that {$\Sigma$}2cc {$\neq$} {$\Pi$}2cc. Another major problem is to show that PSPACEcc and the probabilistic class UPPcc are not comparable.},
  citationcount = {388},
  venue = {27th Annual Symposium on Foundations of Computer Science (sfcs 1986)},
  keywords = {communication,communication complexity,lower bound,reduction}
}

@article{babaiPaulErds19131997,
  title = {Paul {{Erd{\"o}s}} (1913-996): His Influence on the Theory of Computing},
  author = {Babai, L.},
  year = {1997},
  doi = {10.1145/258533.258624},
  abstract = {Paul Erd6s's oeuw-e encompasses a multitude of areas of mathematics, including combinatorics, set theory, number theory, classical analysis, discrete geometry, probability theory, and more. The theory of computing is conspicuously},
  citationcount = {4},
  venue = {Symposium on the Theory of Computing}
}

@article{babenkoWaveletTreesMeet2014,
  title = {Wavelet Trees Meet Suffix Trees},
  author = {Babenko, M. and Gawrychowski, Pawe{\l} and Kociumaka, Tomasz and Starikovskaya, Tatiana},
  year = {2014},
  doi = {10.1137/1.9781611973730.39},
  abstract = {We present an improved wavelet tree construction algorithm and discuss its applications to a number of rank/select problems for integer keys and strings. Given a string of length n over an alphabet of size {$\sigma$} {$\leq$} n, our method builds the wavelet tree in O(n log {$\sigma$} / [EQUATION]log n) time, improving upon the state-of-the-art algorithm by a factor of [EQUATION]log n. As a consequence, given an array of n integers we can construct in O(n[EQUATION]log n) time a data structure consisting of O(n) machine words and capable of answering rank/select queries for the subranges of the array in O(log n/ log log n) time. This is a log log n-factor improvement in query time compared to Chan and Patrascu (SODA 2010) and a [EQUATION]log n-factor improvement in construction time compared to Brodal et al. (Theor. Comput. Sci. 2011). Next, we switch to stringological context and propose a novel notion of wavelet suffix trees. For a string w of length n, this data structure occupies O(n) words, takes O(n[EQUATION]log n) time to construct, and simultaneously captures the combinatorial structure of substrings of w while enabling efficient top-down traversal and binary search. In particular, with a wavelet suffix tree we are able to answer in O(log{\textbar}x{\textbar}) time the following two natural analogues of rank/select queries for suffixes of substrings: 1) For substrings x and y of w (given by their endpoints) count the number of suffixes of x that are lexicographically smaller than y; 2) For a substring x of w (given by its endpoints) and an integer k, find the k-th lexicographically smallest suffix of x. We further show that wavelet suffix trees allow to compute a run-length-encoded Burrows-Wheeler transform of a substring x of w (again, given by its endpoints) in O(s log {\textbar}x{\textbar}) time, where s denotes the length of the resulting run-length encoding. This answers a question by Cormode and Muthukrishnan (SODA 2005), who considered an analogous problem for Lempel-Ziv compression. All our algorithms, except for the construction of wavelet suffix trees, which additionally requires O(n) time in expectation, are deterministic and operate in the word RAM model.},
  citationcount = {51},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,query,query time}
}

@article{badoiuApproximateClusteringVia2002,
  title = {Approximate Clustering via Core-Sets},
  author = {Badoiu, Mihai and {Har-Peled}, Sariel and Indyk, P.},
  year = {2002},
  doi = {10.1145/509907.509947},
  abstract = {In this paper, we show that for several clustering problems one can extract a small set of points, so that using those core-sets enable us to perform approximate clustering efficiently. The surprising property of those core-sets is that their size is independent of the dimension.Using those, we present a (1+ {$\varepsilon$})-approximation algorithms for the k-center clustering and k-median clustering problems in Euclidean space. The running time of the new algorithms has linear or near linear dependency on the number of points and the dimension, and exponential dependency on 1/{$\varepsilon$} and k. As such, our results are a substantial improvement over what was previously known.We also present some other clustering results including (1+ {$\varepsilon$})-approximate 1-cylinder clustering, and k-center clustering with outliers.},
  citationcount = {450},
  venue = {Symposium on the Theory of Computing}
}

@article{bahredarAMetaHeuristic2010,
  title = {A Meta Heuristic Solution for Closest String Problem Using Ant Colony System},
  author = {Bahredar, Faranak and Erfani, Hossein and Javadi, H. and Masaeli, Nafiseh},
  year = {2010},
  doi = {10.1007/978-3-642-14883-5_70},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {International Symposium on Distributed Computing and Artificial Intelligence}
}

@article{bal'azWheelerMaps2023,
  title = {Wheeler Maps},
  author = {Bal'az, Andrej and Gagie, T. and Goga, Adri{\'a}n and Heumos, Simon and Navarro, G. and Petescia, Alessia and Sir'en, Jouni},
  year = {2023},
  doi = {10.48550/arXiv.2308.09836},
  abstract = {Motivated by challenges in pangenomic read alignment, we propose a generalization of Wheeler graphs that we call Wheeler maps. A Wheeler map stores a text T[1..n] and an assignment of tags to the characters of T such that we can preprocess a pattern P[1..m] and then, given i and j, quickly return all the distinct tags labeling the first characters of the occurrences of P[i..j] in T. For the applications that most interest us, characters with long common contexts are likely to have the same tag, so we consider the number t of runs in the list of tags sorted by their characters' positions in the Burrows-Wheeler Transform (BWT) of T. We show how, given a straight-line program with g rules for T, we can build an O(g+r+t)-space Wheeler map, where r is the number of runs in the BWT of T, with which we can preprocess a pattern P[1..m] in O(mn) time and then return the k distinct tags for P[i..j] in optimal O(k) time for any given i and j. We show various further results related to prioritizing the most frequent tags.},
  citationcount = {6},
  venue = {Latin American Symposium on Theoretical Informatics}
}

@article{balakrishnanSuccinctDataStructure2023,
  title = {Succinct Data Structure for Graphs with D-{{Dimensional}} t-{{Representation}}},
  author = {Balakrishnan, Girish and Chakraborty, Sankardeep and Jo, Seungbum and Narayanaswamy, N. S. and Sadakane, K.},
  year = {2023},
  doi = {10.48550/arXiv.2311.02427},
  abstract = {Erd{\H o}s and West (Discrete Mathematics'85) considered the class of n vertex intersection graphs which have a d-dimensional t-representation (also called a t,d -intersection representation), that is, each vertex of a graph in the class has an associated set consisting of at most t d -dimensional axis-parallel boxes. In particular, for a graph G and for each d {$\geq$} 1, they consider i d ( G ) to be the minimum t for which G has such a representation. For fixed t and d , they consider the class of n vertex labeled graphs for which i d ( G ) {$\leq$} t , and prove an upper bound of (\{2nt+\vphantom\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{2\}\vphantom\{\})dn-(\{n-\vphantom\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{2\}\vphantom\{\})d(4{$\pi$}t) on the logarithm of size of the class. In this work, for fixed t and d we consider the class of n vertex unlabeled graphs which have a d -dimensional t -representation, denoted by \{\{G\}\vphantom\}\textsubscript{\{\vphantom\}}t,d\vphantom\{\}\vphantom\{\} . We address the problem of designing a succinct data structure for the class \{\{G\}\vphantom\}\textsubscript{\{\vphantom\}}t,d\vphantom\{\}\vphantom\{\} in an attempt to generalize the relatively recent results on succinct data structures for interval graphs (Algorithmica'21). Let \{\{G\}\textsubscript{t}\}\{\{and\}\}\{\{G\}\vphantom\}\textsubscript{d}\vphantom\{\} be the class of graphs with bounded interval number and bounded boxicity obtained by setting d = 1 and t = 1 in \{\{G\}\vphantom\}\textsubscript{\{\vphantom\}}t,d\vphantom\{\}\vphantom\{\} , respectively. We have the following results:},
  citationcount = {1},
  venue = {Data Compression Conference},
  keywords = {data structure}
}

@article{ballardGraphExpansionAnd2011,
  title = {Graph Expansion and Communication Costs of Fast Matrix Multiplication: Regular Submission},
  author = {Ballard, Grey and Demmel, J. and Holtz, Olga and Schwartz, O.},
  year = {2011},
  doi = {10.1145/1989493.1989495},
  abstract = {The communication cost of algorithms (also known as I/O-complexity) is shown to be closely related to the expansion properties of the corresponding computation graphs. We demonstrate this on Strassen's and other fast matrix multiplication algorithms, and obtain the first lower bounds on their communication costs. For sequential algorithms these bounds are attainable and so optimal.},
  citationcount = {119},
  venue = {ACM Symposium on Parallelism in Algorithms and Architectures}
}

@article{ballAverageCaseFine2017,
  title = {Average-Case Fine-Grained Hardness},
  author = {Ball, Marshall and Rosen, Alon and Sabin, Manuel and Vasudevan, Prashant Nalini},
  year = {2017},
  doi = {10.1145/3055399.3055466},
  abstract = {We present functions that can be computed in some fixed polynomial time but are hard on average for any algorithm that runs in slightly smaller time, assuming widely-conjectured worst-case hardness for problems from the study of fine-grained complexity. Unconditional constructions of such functions are known from before (Goldmann et al., IPL '94), but these have been canonical functions that have not found further use, while our functions are closely related to well-studied problems and have considerable algebraic structure. Based on the average-case hardness and structural properties of our functions, we outline the construction of a Proof of Work scheme and discuss possible approaches to constructing fine-grained One-Way Functions. We also show how our reductions make conjectures regarding the worst-case hardness of the problems we reduce from (and consequently the Strong Exponential Time Hypothesis) heuristically falsifiable in a sense similar to that of (Naor, CRYPTO '03). We prove our hardness results in each case by showing fine-grained reductions from solving one of three problems - namely, Orthogonal Vectors (OV), 3SUM, and All-Pairs Shortest Paths (APSP) - in the worst case to computing our function correctly on a uniformly random input. The conjectured hardness of OV and 3SUM then gives us functions that require n2-o(1) time to compute on average, and that of APSP gives us a function that requires n3-o(1) time. Using the same techniques we also obtain a conditional average-case time hierarchy of functions.},
  citationcount = {58},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {reduction}
}

@article{baLowerBoundsFor2010,
  title = {Lower Bounds for Sparse Recovery},
  author = {Ba, Khanh Do and Indyk, P. and Price, Eric and Woodruff, David P.},
  year = {2010},
  doi = {10.1137/1.9781611973075.95},
  abstract = {We consider the following {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}-sparse recovery problem: design an {\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown} x {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} matrix {\textexclamdown}i{\textquestiondown}A{\textexclamdown}/i{\textquestiondown}, such that for any signal {\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}, given {\textexclamdown}i{\textquestiondown}Ax{\textexclamdown}/i{\textquestiondown} we can efficiently recover x satisfying {\textbar}{\textbar}{\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown} -- x{\textbar}{\textbar}{\textexclamdown}sub{\textquestiondown}i{\textexclamdown}/sub{\textquestiondown} {$\leq$} {\textexclamdown}i{\textquestiondown}C{\textexclamdown}/i{\textquestiondown} min{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown}-sparse {\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}' {\textbar}{\textbar}{\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown} - {\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}'{\textbar}{\textbar}{\textexclamdown}sub{\textquestiondown}1{\textexclamdown}/sub{\textquestiondown}. It is known that there exist matrices A with this property that have only {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown} log({\textexclamdown}i{\textquestiondown}n/k{\textexclamdown}/i{\textquestiondown})) rows. In this paper we show that this bound is tight. Our bound holds even for the more general {\textexclamdown}i{\textquestiondown}randomized{\textexclamdown}/i{\textquestiondown} version of the problem, where {\textexclamdown}i{\textquestiondown}A{\textexclamdown}/i{\textquestiondown} is a random variable, and the recovery algorithm is required to work for any fixed x with constant probability (over {\textexclamdown}i{\textquestiondown}A{\textexclamdown}/i{\textquestiondown}).},
  citationcount = {199},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{banerjeeFullyDynamicArboricity2019,
  title = {Fully Dynamic Arboricity Maintenance},
  author = {Banerjee, Niranka and Raman, Venkatesh and Saurabh, Saket},
  year = {2019},
  doi = {10.1007/978-3-030-26176-4_1},
  abstract = {No abstract available},
  citationcount = {6},
  venue = {International Computing and Combinatorics Conference},
  keywords = {dynamic}
}

@article{banerjeeMaintainingChordalGraphs2018,
  title = {Maintaining Chordal Graphs Dynamically: {{Improved}} Upper and Lower Bounds},
  author = {Banerjee, Niranka and Raman, Venkatesh and Rao, S.},
  year = {2018},
  doi = {10.1007/978-3-319-90530-3_4},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Computer Science Symposium in Russia},
  keywords = {dynamic,lower bound}
}

@article{bannaiComputingAllDistinct2016,
  title = {Computing All Distinct Squares in Linear Time for Integer Alphabets},
  author = {Bannai, H. and Inenaga, Shunsuke and K{\"o}ppl, Dominik},
  year = {2016},
  doi = {10.4230/LIPIcs.CPM.2017.22},
  abstract = {Given a string on an integer alphabet, we present an algorithm that computes the set of all distinct squares belonging to this string in time linear to the string length. As an application, we show how to compute the tree topology of the minimal augmented suffix tree in linear time. Asides from that, we elaborate an algorithm computing the longest previous table in a succinct representation using compressed working space.},
  citationcount = {22},
  venue = {Annual Symposium on Combinatorial Pattern Matching}
}

@article{bannisterWindowsIntoRelational2012,
  title = {Windows into Relational Events: {{Data}} Structures for Contiguous Subsequences of Edges},
  author = {Bannister, Michael J. and DuBois, Christopher and Eppstein, D. and Smyth, Padhraic},
  year = {2012},
  doi = {10.1137/1.9781611973105.61},
  abstract = {We consider the problem of analyzing social network data sets in which the edges of the network have timestamps, and we wish to analyze the subgraphs formed from edges in contiguous subintervals of these timestamps. We provide data structures for these problems that use near-linear preprocessing time, linear space, and sublogarithmic query time to handle queries that ask for the number of connected components, number of components that contain cycles, number of vertices whose degree equals or is at most some predetermined value, number of vertices that can be reached from a starting set of vertices by time-increasing paths, and related queries.},
  citationcount = {17},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,query,query time}
}

@article{bansalRegularityLemmasCombinatorial2009,
  title = {Regularity Lemmas and Combinatorial Algorithms},
  author = {Bansal, N. and Williams, Ryan},
  year = {2009},
  doi = {10.4086/toc.2012.v008a004},
  abstract = {We present new combinatorial algorithms for Boolean matrix multiplication (BMM) and preprocessing a graph to answer independent set queries. We give the first asymptotic improvements on combinatorial algorithms for dense BMM in many years, improving on the "Four Russians'' O(n{\textasciicircum}3/(wn)) bound for machine models with word size w. (For a pointer machine, we can set w = n.) The algorithms utilize notions from Regularity Lemmas for graphs in a novel way. We give two randomized combinatorial algorithms for BMM. The first algorithm is essentially a reduction from BMM to the \{\vphantom\}\emph{Triangle Removal Lemma}\vphantom\{\}\emph{. The best known bounds for the Triangle Removal Lemma only imply an O((n{\textasciicircum}3 }{$\beta$}\emph{)/(}{$\beta$}\emph{w n)) time algorithm for BMM where }{$\beta$}\emph{= ({\textasciicircum}*n){\textasciicircum}}\{{$\delta$}\}\emph{ for some }{$\delta$}\emph{{$\gg$} 0, but improvements on the Triangle Removal Lemma would yield corresponding runtime improvements. The second algorithm applies the Weak Regularity Lemma of Frieze and Kannan along with several information compression ideas, running in O(n{$^3$}(n){$^2$}/(n)\textsuperscript{\{\vphantom\}}9/4\vphantom\{\})) time with probability exponentially close to 1. When w{$\geq$}n, it can be implemented in O(n{\textasciicircum}3 (n){\textasciicircum}2/(w n){\textasciicircum}}\{\vphantom\}\emph{7/6}\vphantom\{\}\emph{)) time. Our results immediately imply improved combinatorial methods for CFG parsing, detecting triangle-freeness, and transitive closure. Using Weak Regularity, we also give an algorithm for answering queries of the form is S }{$\subseteq$}\emph{V an independent set? in a graph. Improving on prior work, we show how to randomly preprocess a graph in O(n{\textasciicircum}}\{\vphantom\}\emph{2+}\vphantom\{\}\emph{) time (for all {$\gg$} 0) so that with high probability, all subsequent batches of n independent set queries can be answered deterministically in O(n{\textasciicircum}2 (n){\textasciicircum}2/((n){\textasciicircum}}\{\vphantom\}\emph{5/4}\vphantom\{\}\emph{)) time. When w }{$\geq$}\emph{n, w queries can be answered in O(n{\textasciicircum}2 (n){\textasciicircum}2/((n){\textasciicircum}}\{\vphantom\}\emph{7/6}\vphantom\{\}\emph{)) time. In addition to its nice applications, this problem is interesting in that it is not known how to do better than O(n{\textasciicircum}2) using "algebraic'' methods.}},
  citationcount = {93},
  venue = {2009 50th Annual IEEE Symposium on Foundations of Computer Science},
  keywords = {query,reduction}
}

@article{bar-natanFaultTolerantDistance2021,
  title = {Fault-Tolerant Distance Labeling for Planar Graphs},
  author = {{Bar-Natan}, Aviv and Charalampopoulos, P. and Gawrychowski, Pawe{\l} and Mozes, S. and Weimann, Oren},
  year = {2021},
  doi = {10.1007/978-3-030-79527-6_18},
  abstract = {No abstract available},
  citationcount = {5},
  venue = {Colloquium on Structural Information \& Communication Complexity}
}

@article{bar-yossefAnInformationStatistics2002,
  title = {An Information Statistics Approach to Data Stream and Communication Complexity},
  author = {{Bar-Yossef}, Ziv and Jayram, T. S. and Kumar, Ravi and Sivakumar, D.},
  year = {2002},
  doi = {10.1109/SFCS.2002.1181944},
  abstract = {We present a new method for proving strong lower bounds in communication complexity. This method is based on the notion of the conditional information complexity of a function which is the minimum amount of information about the inputs that has to be revealed by a communication protocol for the function. While conditional information complexity is a lower bound on the communication complexity, we show that it also admits a direct sum theorem. Direct sum decomposition reduces our task to that of proving (conditional) information complexity lower bounds for simple problems (such as the AND of two bits). For the latter, we develop novel techniques based on Hellinger distance and its generalizations.},
  citationcount = {657},
  venue = {The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.},
  keywords = {communication,communication complexity,lower bound}
}

@article{bar-yossefInformationStatisticsApproach2004,
  title = {An Information Statistics Approach to Data Stream and Communication Complexity},
  author = {{Bar-Yossef}, Ziv and Jayram, T. S. and Kumar, Ravi and Sivakumar, D.},
  year = {2004},
  journal = {Journal of Computer and System Sciences},
  volume = {68},
  number = {4},
  pages = {702--732},
  doi = {10.1016/j.jcss.2003.11.006},
  keywords = {communication,communication complexity},
  annotation = {M. Ajtai, T.S. Jayram, R. Kumar, D. Sivakumar, Approximate counting of inversions in a data stream, in: Proceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC), Montr{\'e}al, QC, Canada, 2002, pp. 370--379.\\
\\
\\
L. Babai, P. Frankl, J. Simon, Complexity classes in communication complexity theory (preliminary version), in: Proceedings of the 27th IEEE Annual Symposium on Foundations of Computer Science (FOCS), Toronto, ON, Canada, 1986, pp. 337--347.\\
\\
A. Chakrabarti, S. Khot, X. Sun, Near-optimal lower bounds on the multi-party communication complexity of set-disjointness, in: Proceedings of the 18th Annual IEEE Conference on Computational Complexity (CCC), Aarhus, Denmark, 2003.\\
A. Chakrabarti, Y. Shi, A. Wirth, A.C-C. Yao, Informational complexity and the direct sum problem for simultaneous message complexity, in: Proceedings of the 42nd IEEE Annual Symposium on Foundations of Computer Science (FOCS), Las Vegas, NV, 2001, pp. 270--278.\\
\\
\\
A. Gilbert, S. Guha, P. Indyk, Y. Kotidis, S. Muthukrishnan, M. Strauss, Fast, small-space algorithms for approximate histogram maintenance, in: Proceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC), Montr{\'e}al, QC, Canada, 2002, pp. 389--398.\\
S. Guha, N. Mishra, R. Motwani, L. O'Callaghan, Clustering data streams, in: Proceedings of the 41st Annual IEEE Symposium on Foundations of Computer Science (FOCS), Redondo Beach, CA, 2000, pp. 359--366.\\
P. Indyk, Stable distributions, pseudorandom generators, embeddings, and data stream computations, in: Proceedings of the 41st Annual IEEE Symposium on Foundations of Computer Science (FOCS), San Diego, CA, 2000, pp. 189--197.\\
T.S. Jayram, R. Kumar, D. Sivakumar, Two applications of information complexity, in: Proceedings of the 35th Annual ACM Symposium on Theory of Computing (STOC), Cambridge, MA, 2003, pp. 673--682.\\
R. Jain, J. Radhakrishnan, P. Sen, A lower bound for bounded round quantum communication complexity of set disjointness, in: Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science (FOCS), Cambridge, MA, 2003, pp. 220--229.\\
\\
\\
\\
\\
\\
L. Le Cam, G.L. Yang, Asymptotics in Statistics---Some Basic Concepts, Springer, Berlin, 1990, pp. 24--30.\\
N. Nisan, I. Segal, The communication complexity of efficient allocation problems, in: DIMACS Workshop on Computational Issues in Game Theory and Mechanism Design, Piscataway, NJ, 2001.\\
\\
\\
\\
A. R{\'e}nyi, On measures of entropy and information, in: Proceedings of the 4th Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, CA, 1960, pp. 547--561.\\
M. Saks, X. Sun, Space lower bounds for distance approximation in the data stream model, in: Proceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC), Montr{\'e}al, QC, Canada, 2002, pp. 360--369.\\
\\
A.C-C. Yao, Some complexity questions related to distributive computing, in: Proceedings of the 11th Annual ACM Symposium on Theory of Computing (STOC), Atlanta, CA, 1979, pp. 209--213.},
  file = {/Users/tulasi/Zotero/storage/6WDHFATN/Bar-Yossef et al. - 2004 - An information statistics approach to data stream and communication complexity.pdf}
}

@article{barakHowToCompress2010,
  title = {How to Compress Interactive Communication},
  author = {Barak, B. and Braverman, M. and Chen, X. and Rao, Anup},
  year = {2010},
  doi = {10.1145/1806689.1806701},
  abstract = {We describe new ways to simulate 2-party communication protocols to get protocols with potentially smaller communication. We show that every communication protocol that communicates C bits and reveals I bits of information about the inputs to the participating parties can be simulated by a new protocol involving at most  O({\textsurd}CI) bits of communication. If the protocol reveals I bits of information about the inputs to an observer that watches the communication in the protocol, we show how to carry out the simulation with  O(I) bits of communication. These results lead to a direct sum theorem for randomized communication complexity. Ignoring polylogarithmic factors, we show that for worst case computation, computing n copies of a function requires {\textsurd}n times the communication required for computing one copy of the function. For average case complexity, given any distribution {$\mu$} on inputs, computing n copies of the function on n inputs sampled independently according to {$\mu$} requires {\textsurd}n times the communication for computing one copy. If {$\mu$} is a product distribution, computing n copies on n independent inputs sampled according to {$\mu$} requires n times the communication required for computing the function. We also study the complexity of computing the sum (or parity) of n evaluations of f, and obtain results analogous to those above.},
  citationcount = {246},
  venue = {Symposium on the Theory of Computing}
}

@article{baranSubquadraticAlgorithmsFor2005,
  title = {Subquadratic Algorithms for {{3SUM}}},
  author = {Baran, Ilya and Demaine, E. and Patrascu, M.},
  year = {2005},
  doi = {10.1007/s00453-007-9036-3},
  abstract = {No abstract available},
  citationcount = {138},
  venue = {Algorithmica}
}

@article{barbaSubquadraticAlgorithmsAlgebraic2018,
  title = {Subquadratic Algorithms for Algebraic {{3SUM}}},
  author = {Barba, Luis and Cardinal, J. and Iacono, J. and Langerman, S. and Ooms, Aur{\'e}lien and Solomon, Noam},
  year = {2018},
  doi = {10.1007/s00454-018-0040-y},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Discrete \& Computational Geometry}
}

@article{barbaSubquadraticAlgorithmsFor2016,
  title = {Subquadratic Algorithms for Algebraic Generalizations of {{3SUM}}},
  author = {Barba, Luis and Cardinal, J. and Iacono, J. and Langerman, S. and Ooms, Aur{\'e}lien and Solomon, Noam},
  year = {2016},
  doi = {10.4230/LIPIcs.SoCG.2017.13},
  abstract = {The 3SUM problem asks if an input n-set of real numbers contains a triple whose sum is zero. We consider the 3POL problem, a natural generalization of 3SUM where we replace the sum function by a constant-degree polynomial in three variables. The motivations are threefold. Raz, Sharir, and de Zeeuw gave a O(n\textsuperscript{\{\vphantom\}}11/6\vphantom\{\}) upper bound on the number of solutions of trivariate polynomial equations when the solutions are taken from the cartesian product of three n-sets of real numbers. We give algorithms for the corresponding problem of counting such solutions. Gronlund and Pettie recently designed subquadratic algorithms for 3SUM. We generalize their results to 3POL. Finally, we shed light on the General Position Testing (GPT) problem: "Given n points in the plane, do three of them lie on a line?", a key problem in computational geometry. We prove that there exist bounded-degree algebraic decision trees of depth O(n\textsuperscript{\{\{\vphantom{\}\}}}{\textfractionsolidus}{$_1$}2\vphantom\{\}\{7\}+{$\varepsilon$}\vphantom\{\}) that solve 3POL, and that 3POL can be solved in O(n{$^2$}\{(n)\}\{3\}\{2\}/\{(n)\}\{1\}\{2\}) time in the real-RAM model. Among the possible applications of those results, we show how to solve GPT in subquadratic time when the input points lie on o(\{(n)\}\{1\}\{6\}/\{(n)\}\{1\}\{2\}) constant-degree polynomial curves. This constitutes a first step towards closing the major open question of whether GPT can be solved in subquadratic time. To obtain these results, we generalize important tools --- such as batch range searching and dominance reporting --- to a polynomial setting. We expect these new tools to be useful in other applications.},
  citationcount = {5},
  venue = {International Symposium on Computational Geometry}
}

@article{barbayAlternationAndRedundancy2008,
  title = {Alternation and Redundancy Analysis of the Intersection Problem},
  author = {Barbay, J{\'e}r{\'e}my F{\'e}lix and Mathieu, Claire},
  year = {2008},
  doi = {10.1145/1328911.1328915},
  abstract = {The intersection of sorted arrays problem has applications in search engines such as Google. Previous work has proposed and compared deterministic algorithms for this problem, in an adaptive analysis based on the encoding size of a certificate of the result (cost analysis). We define the alternation analysis, based on the nondeterministic complexity of an instance. In this analysis we prove that there is a deterministic algorithm asymptotically performing as well as any randomized algorithm in the comparison model. We define the redundancy analysis, based on a measure of the internal redundancy of the instance. In this analysis we prove that any algorithm optimal in the redundancy analysis is optimal in the alternation analysis, but that there is a randomized algorithm which performs strictly better than any deterministic algorithm in the comparison model. Finally, we describe how these results can be extended beyond the comparison model.},
  citationcount = {34},
  venue = {TALG},
  keywords = {adaptive}
}

@article{barbayCompactBinaryRelation2012,
  title = {Compact Binary Relation Representations with Rich Functionality},
  author = {Barbay, J{\'e}r{\'e}my F{\'e}lix and Claude, Francisco and Navarro, G.},
  year = {2012},
  doi = {10.1016/j.ic.2013.10.003},
  abstract = {No abstract available},
  citationcount = {28},
  venue = {Information and Computation}
}

@article{barbayLrmTreesCompressed2010,
  title = {{{LRM-Trees}}: {{Compressed}} Indices, Adaptive Sorting, and Compressed Permutations},
  author = {Barbay, J{\'e}r{\'e}my F{\'e}lix and Fischer, J. and Navarro, G.},
  year = {2010},
  doi = {10.1007/978-3-642-21458-5_25},
  abstract = {No abstract available},
  citationcount = {22},
  venue = {Theoretical Computer Science},
  keywords = {adaptive}
}

@article{barbaySuccinctIndexesFor2007,
  title = {Succinct Indexes for Strings, Binary Relations and Multilabeled Trees},
  author = {Barbay, J{\'e}r{\'e}my F{\'e}lix and He, Meng and Munro, J. and Rao, S.},
  year = {2007},
  doi = {10.1145/2000807.2000820},
  abstract = {We define and design succinct indexes for several abstract data types (ADTs). The concept is to design auxiliary data structures that ideally occupy asymptotically less space than the information-theoretic lower bound on the space required to encode the given data, and support an extended set of operations using the basic operators defined in the ADT. The main advantage of succinct indexes as opposed to succinct (integrated data/index) encodings is that we make assumptions only on the ADT through which the main data is accessed, rather than the way in which the data is encoded. This allows more freedom in the encoding of the main data. In this article, we present succinct indexes for various data types, namely strings, binary relations and multilabeled trees. Given the support for the interface of the ADTs of these data types, we can support various useful operations efficiently by constructing succinct indexes for them. When the operators in the ADTs are supported in constant time, our results are comparable to previous results, while allowing more flexibility in the encoding of the given data. Using our techniques, we design a succinct encoding that represents a string of length {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} over an alphabet of size {$\sigma$} using {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}i{\textquestiondown}H{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown}({\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown}) + lg {$\sigma$} {$\cdot$} {\textexclamdown}i{\textquestiondown}o{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) + {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} lg {$\sigma$}/lg lg lg {$\sigma$}) bits to support access/rank/select operations in {\textexclamdown}i{\textquestiondown}o{\textexclamdown}/i{\textquestiondown}((lg lg {$\sigma$}){\textexclamdown}sup{\textquestiondown}1+{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown}) time, for any fixed constant {$\varepsilon$} {\textquestiondown} 0. We also design a succinct text index using {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} {\textexclamdown}i{\textquestiondown}H{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}0{\textexclamdown}/sub{\textquestiondown}({\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown}) + {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} lg {$\sigma$}/lg lg {$\sigma$}) bits that supports finding all the occ occurrences of a given pattern of length {\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown} in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown} lg lg {$\sigma$} + occ lg {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}/lg{\textexclamdown}sup{\textquestiondown}{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown} {$\sigma$}) time, for any fixed constant 0 {\textexclamdown} {$\varepsilon$} {\textexclamdown} 1. Previous results on these two problems either have a lg {$\sigma$} factor instead of lg lg {$\sigma$} in the running time, or are not compressed. Finally, we present succinct encodings of binary relations and multi-labeled trees that are more compact than previous structures.},
  citationcount = {82},
  venue = {TALG},
  keywords = {data structure,information theoretic,lower bound}
}

@article{barbuClusterSamplingMethods2020,
  title = {Cluster Sampling Methods},
  author = {Barbu, Adrian and Zhu, Song-Chun},
  year = {2020},
  doi = {10.1007/978-981-13-2971-5_6},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {No venue available}
}

@article{barhumOnThePower2016,
  title = {On the Power of Laconic Advice in Communication Complexity},
  author = {Barhum, Kfir and Hromkovic, J.},
  year = {2016},
  doi = {10.1007/978-3-662-49192-8_13},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Conference on Current Trends in Theory and Practice of Informatics},
  keywords = {communication,communication complexity}
}

@article{barkolTighterLowerBounds2002,
  title = {Tighter {{Lower Bounds}} for {{Nearest Neighbor Search}} and {{Related Problems}} in the {{Cell Probe Model}}},
  author = {Barkol, Omer and Rabani, Yuval},
  year = {2002},
  month = jun,
  journal = {Journal of Computer and System Sciences},
  volume = {64},
  number = {4},
  pages = {873--896},
  issn = {00220000},
  doi = {10.1006/jcss.2002.1831},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022000002918313},
  urldate = {2024-11-19},
  abstract = {We prove new lower bounds for nearest neighbor search in the Hamming cube. Our lower bounds are for randomized, two-sided error, algorithms in Yao's cell probe model. Our bounds are in the form of a tradeoff among the number of cells, the size of a cell, and the search time. For example, suppose we are searching among n points in the d dimensional cube, we use poly(n, d) cells, each containing poly(d, log n) bits. We get a lower bound of W(d/log n) on the search time, a significant improvement over the recent bound of W(log d) of Borodin et al. This should be contrasted with the upper bound of O(log log d) for approximate search (and O(1) for a decision version of the problem; our lower bounds hold in that case). By previous results, the bounds for the cube imply similar bounds for nearest neighbor search in high dimensional Euclidean space, and for other geometric problems.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  keywords = {cell probe,lower bound,sorted},
  annotation = {M. Ajtai, A non-linear time lower bound for Boolean branching programs, preprint, April 1999.\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
R. Fagin, Fuzzy queries in multimedia database systems, in, Proc. of PODS, 1998.\\
\\
\\
P. Indyk, Dimensionality reduction techniques for proximity problems, manuscript, Aug. 1999, to appear in SODA 2000.\\
\\
\\
\\
\\
\\
\\
\\
A. Pentland, R. W. Picard, and, S. Sclaroff, Photobook: tools for content-based manipulation of image databases, in, Proc. SPIE Conf. on Storage and Retrieval of Image and Video Databases II, 1994.},
  file = {/Users/tulasi/Zotero/storage/S32HMLPV/1-s2.0-S0022000002918313-main.pdf}
}

@inproceedings{barmanApproximatingNashEquilibria2015,
  title = {Approximating {{Nash Equilibria}} and {{Dense Bipartite Subgraphs}} via an {{Approximate Version}} of {{Caratheodory}}'s {{Theorem}}},
  booktitle = {Proceedings of the Forty-Seventh Annual {{ACM}} Symposium on {{Theory}} of {{Computing}}},
  author = {Barman, Siddharth},
  year = {2015},
  month = jun,
  series = {{{STOC}} '15},
  pages = {361--369},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2746539.2746566},
  url = {https://doi.org/10.1145/2746539.2746566},
  urldate = {2025-03-08},
  abstract = {We present algorithmic applications of an approximate version of Caratheodory's theorem. The theorem states that given a set of vectors X in Rd, for every vector in the convex hull of X there exists an {$\varepsilon$}-close (under the p-norm distance, for 2 {$\leq$} p {$<$} {$\infty$}) vector that can be expressed as a convex combination of at most b vectors of X, where the bound b depends on {$\varepsilon$} and the norm p and is independent of the dimension d. This theorem can be derived by instantiating Maurey's lemma, early references to which can be found in the work of Pisier (1981) and Carl (1985). However, in this paper we present a self-contained proof of this result. Using this theorem we establish that in a bimatrix game with n x n payoff matrices A, B, if the number of non-zero entries in any column of A+B is at most s then an {$\varepsilon$}-Nash equilibrium of the game can be computed in time nO(log s/{$\varepsilon$}2\vphantom\{\}). This, in particular, gives us a polynomial-time approximation scheme for Nash equilibrium in games with fixed column sparsity s. Moreover, for arbitrary bimatrix games---since s can be at most n---the running time of our algorithm matches the best-known upper bound, which was obtained by Lipton, Markakis, and Mehta (2003). The approximate Carath{\'e}odory's theorem also leads to an additive approximation algorithm for the densest k-bipartite subgraph problem. Given a graph with n vertices and maximum degree d, the developed algorithm determines a k x k bipartite subgraph with density within {$\varepsilon$} (in the additive sense) of the optimal density in time nO(log d/{$\varepsilon$}2).},
  isbn = {978-1-4503-3536-2},
  annotation = {N. Alon S. Arora R. Manokaran D. Moshkovitz and O. Weinstein. Inapproximability of densest {$\kappa$}-subgraph from average case hardness. http://www.tau.ac.il/ nogaa/PDFS/dks8.pdf 2011.  N. Alon S. Arora R. Manokaran D. Moshkovitz and O. Weinstein. Inapproximability of densest {$\kappa$}-subgraph from average case hardness. http://www.tau.ac.il/ nogaa/PDFS/dks8.pdf 2011.\\
Reference DOIs: 10.1007/11944874\_24, 10.1007/978-3-540-73420-8\_52, 10.1007/978-3-642-17572-5\_31, 10.1007/BFB0090048, 10.1007/S00199-009-0436-2, 10.1007/s10107-009-0330-5, 10.1016/0022-247X(64)90021-6, 10.1016/j.tcs.2008.12.031, 10.1016/j.tcs.2008.12.033, 10.1016/j.tcs.2009.09.023, 10.1017/CBO9780511755217.005, 10.1017/CBO9780511800481, 10.1080/15427951.2008.10129172, 10.1137/090766991, 10.1137/1.9781611973730.66, 10.1142/9789814578554\_0004, 10.1145/1132516.1132527, 10.1145/1250910.1250961, 10.1145/1250910.1250962, 10.1145/1516512.1516516, 10.1145/1536414.1536427, 10.1145/2483699.2483703, 10.1145/2488608.2488694, 10.1145/779928.779933, 10.4230/LIPIcs.APPROX-RANDOM.2014.34, 10.5802/AIF.1020},
  file = {/Users/tulasi/Zotero/storage/I2IDKNZS/Barman - 2015 - Approximating Nash Equilibria and Dense Bipartite Subgraphs via an Approximate Version of Caratheodo.pdf;/Users/tulasi/Zotero/storage/U5G78P4P/Barman - 2015 - Approximating Nash Equilibria and Dense Bipartite Subgraphs via an Approximate Version of Caratheodo.pdf}
}

@article{barnesPatchtable2015,
  title = {{{PatchTable}}},
  author = {Barnes, Connelly and Zhang, Fang-Lue and Lou, Li-ming and Wu, X. and Hu, Shimin},
  year = {2015},
  doi = {10.1145/2766934},
  abstract = {This paper presents a data structure that reduces approximate nearest neighbor query times for image patches in large datasets. Previous work in texture synthesis has demonstrated real-time synthesis from small exemplar textures. However, high performance has proved elusive for modern patch-based optimization techniques which frequently use many exemplar images in the tens of megapixels or above. Our new algorithm, PatchTable, offloads as much of the computation as possible to a pre-computation stage that takes modest time, so patch queries can be as efficient as possible. There are three key insights behind our algorithm: (1) a lookup table similar to locality sensitive hashing can be precomputed, and used to seed sufficiently good initial patch correspondences during querying, (2) missing entries in the table can be filled during pre-computation with our fast Voronoi transform, and (3) the initially seeded correspondences can be improved with a precomputed k-nearest neighbors mapping. We show experimentally that this accelerates the patch query operation by up to 9{\texttimes} over k-coherence, up to 12{\texttimes} over TreeCANN, and up to 200{\texttimes} over PatchMatch. Our fast algorithm allows us to explore efficient and practical imaging and computational photography applications. We show results for artistic video stylization, light field super-resolution, and multi-image editing.},
  citationcount = {69},
  venue = {ACM Transactions on Graphics},
  keywords = {data structure,query,query time}
}

@article{baroneFundamentalDataStructures2013,
  title = {Fundamental Data Structures},
  author = {Barone, L. and Marinari, E. and Organtini, G. and Tersenghi, Federico Ricci},
  year = {2013},
  doi = {10.1142/9789814513418_0006},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {No venue available},
  keywords = {data structure}
}

@article{barringtonSearchingConstantWidth1997,
  title = {Searching Constant Width Mazes Captures the {{AC0}} Hierarchy},
  author = {Barrington, D. A. and Lu, Chi-Jen and Miltersen, Peter Bro and Skyum, Sven},
  year = {1997},
  doi = {10.1007/BFb0028550},
  abstract = {No abstract available},
  citationcount = {45},
  venue = {Symposium on Theoretical Aspects of Computer Science}
}

@article{bartalApproximateNearestNeighbor2018,
  title = {Approximate Nearest Neighbor Search for {$\ell\_$}p -Spaces (2 via Embeddings},
  author = {Bartal, Y. and Gottlieb, Lee-Ad},
  year = {2018},
  doi = {10.1007/978-3-319-77404-6_10},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Latin American Symposium on Theoretical Informatics}
}

@article{bartalApproximateNearestNeighbor2019,
  title = {Approximate Nearest Neighbor Search for {$\ell$}p-Spaces (2},
  author = {Bartal, Y. and Gottlieb, Lee-Ad},
  year = {2019},
  doi = {10.1016/j.tcs.2018.07.011},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Theoretical Computer Science}
}

@article{bartalFastPreciseAnd2010,
  title = {Fast, Precise and Dynamic Distance Queries},
  author = {Bartal, Y. and Gottlieb, Lee-Ad and Kopelowitz, T. and Lewenstein, Moshe and Roditty, L.},
  year = {2010},
  doi = {10.1137/1.9781611973082.66},
  abstract = {We present an approximate distance oracle for a point set {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown} with {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} points and doubling dimension {$\lambda$}. For every {$\varepsilon$} {\textquestiondown} 0, the oracle supports (1 + {$\varepsilon$})-approximate distance queries in (universal) constant time, occupies space [{$\varepsilon$}{\textexclamdown}sup{\textquestiondown}-{\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({$\lambda$}){\textexclamdown}/sup{\textquestiondown} + 2{\textexclamdown}sup{\textquestiondown}{\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({$\lambda$} log {$\lambda$}){\textexclamdown}/sup{\textquestiondown}]{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}, and can be constructed in [2{\textexclamdown}sup{\textquestiondown}{\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({$\lambda$}){\textexclamdown}/sup{\textquestiondown} log{\textexclamdown}sup{\textquestiondown}3{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {$\varepsilon$}{\textexclamdown}sup{\textquestiondown}-{\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({$\lambda$}){\textexclamdown}/sup{\textquestiondown} +2{\textexclamdown}sup{\textquestiondown}{\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({$\lambda$} log {$\lambda$}){\textexclamdown}/sup{\textquestiondown}]{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} expected time. This improves upon the best previously known constructions, presented by Har-Peled and Mendel [13]. Furthermore, the oracle can be made fully dynamic with expected {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(1) query time and only 2{\textexclamdown}sup{\textquestiondown}{\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({$\lambda$}){\textexclamdown}/sup{\textquestiondown} log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {$\varepsilon$}{\textexclamdown}sup{\textquestiondown}-{\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({$\lambda$}){\textexclamdown}/sup{\textquestiondown} +2{\textexclamdown}sup{\textquestiondown}{\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({$\lambda$} log {$\lambda$}){\textexclamdown}/sup{\textquestiondown} update time. This is the first fully dynamic (1 + {$\varepsilon$})-distance oracle.},
  citationcount = {21},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,query,query time,update,update time}
}

@article{bartonCombinatorialGeometry2012,
  title = {Combinatorial Geometry},
  author = {Barton, Reid},
  year = {2012},
  doi = {10.1016/0898-1221(96)87331-7},
  abstract = {No abstract available},
  citationcount = {252},
  venue = {No venue available}
}

@article{basatApproximateQueryProcessing2018,
  title = {Approximate Query Processing over Static Sets and Sliding Windows},
  author = {Basat, Ran Ben and Jo, Seungbum and Satti, S. R. and Ugare, Shubham},
  year = {2018},
  doi = {10.4230/LIPIcs.ISAAC.2018.54},
  abstract = {Indexing of static and dynamic sets is fundamental to a large set of applications such as information retrieval and caching. Denoting the characteristic vector of the set by B, we consider the problem of encoding sets and multisets to support approximate versions of the operations rank(i) (i.e., computing sum\_\{j = i\}) queries. We study multiple types of approximations (allowing an error in the query or the result) and present lower bounds and succinct data structures for several variants of the problem. We also extend our model to sliding windows, in which we process a stream of elements and compute suffix sums. This is a generalization of the window summation problem that allows the user to specify the window size at query time. Here, we provide an algorithm that supports updates and queries in constant time while requiring just (1+o(1)) factor more space than the fixed-window summation algorithms.},
  citationcount = {1},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {data structure,dynamic,lower bound,query,query time,static,update}
}

@article{basavarajuOnTheKernelization2018,
  title = {On the Kernelization Complexity of String Problems},
  author = {Basavaraju, Manu and Panolan, Fahad and Rai, Ashutosh and Ramanujan, M. and Saurabh, Saket},
  year = {2018},
  doi = {10.1016/J.TCS.2018.03.024},
  abstract = {No abstract available},
  citationcount = {10},
  venue = {Theoretical Computer Science}
}

@article{baswanaAnEfficientStrongly2016,
  title = {An Efficient Strongly Connected Components Algorithm in the Fault Tolerant Model},
  author = {Baswana, Surender and Choudhary, Keerti and Roditty, L.},
  year = {2016},
  doi = {10.1007/s00453-018-0452-3},
  abstract = {No abstract available},
  citationcount = {16},
  venue = {Algorithmica}
}

@article{baswanaDynamicDfsIn2015,
  title = {Dynamic {{DFS}} in Undirected Graphs: Breaking the {{O}}(m) Barrier},
  author = {Baswana, Surender and Chaudhury, Shreejit Ray and Choudhary, Keerti and Khan, Shahbaz},
  year = {2015},
  doi = {10.1137/1.9781611974331.ch52},
  abstract = {Given an undirected graph G = (V, E) on n vertices and m edges, we address the problem of maintaining a DFS tree when the graph is undergoing updates (insertion and deletion of vertices or edges). We present the following results for this problem. 1. Fault tolerant DFS tree: There exists a data structure of size O(m) 1 such that given any set F of failed vertices or edges, a DFS tree of the graph Gcan be reported in O(n{\textbar}F{\textbar}) time. 2. Fully dynamic DFS tree: There exists a fully dynamic algorithm for maintaining a DFS tree that takes worst case O([EQUATION]) time per update for any arbitrary online sequence of updates. 3. Incremental DFS tree: Given any arbitrary online sequence of edge insertions, we can maintain a DFS tree in O(n) worst case time per edge insertion. These are the first o(m) worst case time results for maintaining a DFS tree in a dynamic environment. Moreover, our fully dynamic algorithm provides, in a seamless manner, the first deterministic algorithm with O(1) query time and o(m) worst case update time for the dynamic subgraph connectivity, biconnectivity, and 2-edge connectivity.},
  citationcount = {40},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,query,query time,update,update time}
}

@article{baswanaFullyDynamicMaximal2011,
  title = {Fully Dynamic Maximal Matching in {{O}} (Log n) Update Time},
  author = {Baswana, Surender and Gupta, Manoj and Sen, Sandeep},
  year = {2011},
  doi = {10.1137/130914140},
  abstract = {We present an algorithm for maintaining maximal matching in a graph under addition and deletion of edges. Our data structure is randomized that takes O(n) expected amortized time for each edge update where n is the number of vertices in the graph. While there is a trivial O(n) algorithm for edge update, the previous best known result for this problem was due to Ivkovi{\'c} and Llyod\{llyod\}. For a graph with n vertices and m edges, they give an O(\{(n+m)\}\textsuperscript{\{\vphantom\}}0.7072\vphantom\{\}) update time algorithm which is sub linear only for a sparse graph.},
  citationcount = {153},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,dynamic,update,update time}
}

@article{baswanaFullyDynamicMaximal2018,
  title = {Fully Dynamic Maximal Matching in {{O}}(Log n) Update Time (Corrected Version)},
  author = {Baswana, Surender and Gupta, Manoj and Sen, Sandeep},
  year = {2018},
  doi = {10.1137/16M1106158},
  abstract = {We present an algorithm for maintaining a maximal matching in a graph under addition and deletion of edges. Our algorithm is randomized and it takes expected amortized O(n) time for each edge update, where n is the number of vertices in the graph. Moreover, for any sequence of t edge updates, the total time taken by the algorithm is O(tn+n{$^2$}n) with high probability.},
  citationcount = {4},
  venue = {SIAM journal on computing (Print)},
  keywords = {dynamic,update,update time}
}

@article{battistaAlgorithmsForDrawing1988,
  title = {Algorithms for Drawing Graphs: An Annotated Bibliography},
  author = {Battista, G. and Eades, P. and Tamassia, R. and Tollis, I.},
  year = {1988},
  doi = {10.1016/0925-7721(94)00014-X},
  abstract = {No abstract available},
  citationcount = {1056},
  venue = {Computational geometry}
}

@article{baudinCliquePercolationMethod2021,
  title = {Clique Percolation Method: Memory Efficient Almost Exact Communities},
  author = {Baudin, Alexis and Danisch, Maximilien and Kirgizov, Sergey and Magnien, Cl{\'e}mence and Ghanem, M.},
  year = {2021},
  doi = {10.1007/978-3-030-95408-6_9},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {International Conference on Advanced Data Mining and Applications}
}

@article{baumgartenDynamicPointLocation1992,
  title = {Dynamic Point Location in General Subdivisions},
  author = {Baumgarten, Hanna and Jung, Hermann and Mehlhorn, K.},
  year = {1992},
  doi = {10.1006/jagm.1994.1040},
  abstract = {The {\textexclamdown}italic{\textquestiondown}dynamic planar point location problem{\textexclamdown}/italic{\textquestiondown} is the task of maintaining a dynamic set {\textexclamdown}italic{\textquestiondown}S{\textexclamdown}/italic{\textquestiondown} of {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} non-intersecting, except possibly at endpoints, line segments in the plane under the following operations: {\textexclamdown}list{\textquestiondown}{\textexclamdown}item{\textquestiondown}{$\bullet$}Locate({\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown} point): Report the segment immediately above {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown}, i.e., the first segment intersected by an upward vertical ray starting at {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown}; {\textexclamdown}/item{\textquestiondown}{\textexclamdown}item{\textquestiondown}{$\bullet$}Insert({\textexclamdown}italic{\textquestiondown}s{\textexclamdown}/italic{\textquestiondown} segment): Add segment {\textexclamdown}italic{\textquestiondown}s{\textexclamdown}/italic{\textquestiondown} to the collection {\textexclamdown}italic{\textquestiondown}S{\textexclamdown}/italic{\textquestiondown} segments; {\textexclamdown}/item{\textquestiondown}{\textexclamdown}item{\textquestiondown}{$\bullet$}Delete({\textexclamdown}italic{\textquestiondown}s{\textexclamdown}/italic{\textquestiondown} segment): Remove segment {\textexclamdown}italic{\textquestiondown}s{\textexclamdown}/italic{\textquestiondown} from the collection {\textexclamdown}italic{\textquestiondown}S{\textexclamdown}/italic{\textquestiondown} of segments. {\textexclamdown}/item{\textquestiondown}{\textexclamdown}/list{\textquestiondown}We present a solution which requires space {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}), has query and insertion time {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} loglog {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) and deletion time {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(log{\textexclamdown}supscrpt{\textquestiondown}2{\textexclamdown}/supscrpt{\textquestiondown} {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}). A query time below {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(log{\textexclamdown}supscrpt{\textquestiondown}2{\textexclamdown}/supscrpt{\textquestiondown} {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) was previously only known for monotone subdivisions and horizontal segments and required non-linear space.},
  citationcount = {62},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,query,query time}
}

@article{baurTheComplexityOf1983,
  title = {The Complexity of Partial Derivatives},
  author = {Baur, W. and Strassen, V.},
  year = {1983},
  doi = {10.1016/0304-3975(83)90110-X},
  abstract = {No abstract available},
  citationcount = {214},
  venue = {Theoretical Computer Science}
}

@article{bayerOrganizationAndMaintenance1972,
  title = {Organization and Maintenance of Large Ordered Indexes},
  author = {Bayer, R. and McCreight, E.},
  year = {1972},
  doi = {10.1007/BF00288683},
  abstract = {No abstract available},
  citationcount = {1037},
  venue = {Acta Informatica}
}

@article{bazziEndcodingComplexityVersus2005,
  title = {Endcoding Complexity versus Minimum Distance},
  author = {Bazzi, L. and Mitter, S.},
  year = {2005},
  doi = {10.1109/TIT.2005.847727},
  abstract = {A bound on the minimum distance of a binary error-correcting code is established given constraints on the computational time-space complexity of its encoder where the encoder is modeled as a branching program. The bound obtained asserts that if the encoder uses linear time and sublinear memory in the most general sense, then the minimum distance of the code cannot grow linearly with the block length when the rate is nonvanishing, that is, the minimum relative distance of the code tends to zero in such a setting. The setting is general enough to include nonserially concatenated turbo-like codes and various generalizations. Our argument is based on branching program techniques introduced by Ajtai. The case of constant-depth AND-OR circuit encoders with unbounded fanins are also considered.},
  citationcount = {32},
  venue = {IEEE Transactions on Information Theory}
}

@article{bazziTheMinimumDistance2009,
  title = {The Minimum Distance of Turbo-like Codes},
  author = {Bazzi, L. and Mahdian, Mohammad and Spielman, D.},
  year = {2009},
  doi = {10.1109/TIT.2008.2008114},
  abstract = {Worst-case upper bounds are derived on the minimum distance of parallel concatenated turbo codes, serially concatenated convolutional codes, repeat-accumulate codes, repeat-convolute codes, and generalizations of these codes obtained by allowing nonlinear and large-memory constituent codes. It is shown that parallel-concatenated turbo codes and repeat-convolute codes with sub-linear memory are asymptotically bad. It is also shown that depth-two serially concatenated codes with constant-memory outer codes and sublinear-memory inner codes are asymptotically bad. Most of these upper bounds hold even when the convolutional encoders are replaced by general finite-state automata encoders. In contrast, it is proven that depth-three serially concatenated codes obtained by concatenating a repetition code with two accumulator codes through random permutations can be asymptotically good.},
  citationcount = {33},
  venue = {IEEE Transactions on Information Theory}
}

@article{beameAStrongDirect2006,
  title = {A Strong Direct Product Theorem for Corruption and the Multiparty Communication Complexity of Disjointness},
  author = {Beame, P. and Pitassi, T. and Segerlind, Nathan and Wigderson, A.},
  year = {2006},
  doi = {10.1007/s00037-007-0220-2},
  abstract = {No abstract available},
  citationcount = {48},
  venue = {Computational Complexity}
}

@article{beameHardnessAmplificationIn2009,
  title = {Hardness Amplification in Proof Complexity},
  author = {Beame, P. and Huynh, Trinh N. D. and Pitassi, T.},
  year = {2009},
  doi = {10.1145/1806689.1806703},
  abstract = {We present a general method for converting any family of unsatisfiable CNF formulas that is hard for one of the simplest proof systems -- tree resolution -- into formulas that require large rank in very strong proof systems, including any proof system that manipulates polynomials of degree at most k (known as Th(k) proofs). These include high degree versions of Lovasz-Schrijver and Cutting Planes proofs. We introduce two very general families of these proof systems, denoted Tcc(k) and Rcc(k). The proof lines of Tcc(k) are arbitrary Boolean functions, each of which can be evaluated by an efficient k-party randomized communication protocol. Tcc(k) proofs include Th(k-1) proofs as a special case. Rcc(k) proofs generalize Tcc(k) proofs and require only that each inference be checkable by a short k-party protocol. For all k in O(loglog n), our main results are as follows: First, any unsatisfiable CNF formula of high resolution rank can be efficiently transformed into another CNF formula requiring high rank in all Rcc(k) systems, and exponential tree size in all Tcc(k) systems. Secondly, there are strict rank hierarchies for all Rcc(k) systems, and strict tree-size hierarchies for all Tcc(k) systems. Finally, we apply our general method to give optimal integrality gaps for low rank Rcc(2) proofs for MAX-2t-SAT, which imply optimal integrality gaps for low rank Cutting Planes and Th(1) proofs.},
  citationcount = {35},
  venue = {Symposium on the Theory of Computing}
}

@article{beameMassivelyParallelSimilarity2016,
  title = {Massively-Parallel Similarity Join, Edge-Isoperimetry, and Distance Correlations on the Hypercube},
  author = {Beame, P. and Rashtchian, Cyrus},
  year = {2016},
  doi = {10.1137/1.9781611974782.19},
  abstract = {We study distributed protocols for finding all pairs of similar vectors in a large dataset. Our results pertain to a variety of discrete metrics, and we give concrete instantiations for Hamming distance. In particular, we give improved upper bounds on the overhead required for similarity defined by Hamming distance r {\textquestiondown} 1 and prove a lower bound showing qualitative optimality of the overhead required for similarity over any Hamming distance r. Our main conceptual contribution is a connection between similarity search algorithms and certain graph-theoretic quantities. For our upper bounds, we exhibit a general method for designing one-round protocols using edge-isoperimetric shapes in similarity graphs. For our lower bounds, we define a new combinatorial optimization problem, which can be stated in purely graph-theoretic terms yet also captures the core of the analysis in previous theoretical work on distributed similarity joins. As one of our main technical results, we prove new bounds on distance correlations in subsets of the Hamming cube.},
  citationcount = {5},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {lower bound}
}

@inproceedings{beameOptimalBoundsPredecessor1999,
  title = {Optimal Bounds for the Predecessor Problem},
  booktitle = {Proceedings of the Thirty-First Annual {{ACM}} Symposium on {{Theory}} of {{Computing}}},
  author = {Beame, Paul and Fich, Faith E.},
  year = {1999},
  month = may,
  pages = {295--304},
  publisher = {ACM},
  address = {Atlanta Georgia USA},
  doi = {10.1145/301250.301323},
  url = {https://dl.acm.org/doi/10.1145/301250.301323},
  urldate = {2025-03-26},
  isbn = {978-1-58113-067-6},
  langid = {english}
}

@article{beameOptimalBoundsPredecessor2002,
  title = {Optimal Bounds for the Predecessor Problem and Related Problems},
  author = {Beame, Paul and Fich, Faith E.},
  year = {2002},
  journal = {Journal of Computer and System Sciences},
  volume = {65},
  number = {1},
  pages = {38--72},
  doi = {10.1006/jcss.2002.1822},
  abstract = {We obtain matching upper and lower bounds for the amount of time to find the predecessor of a given element among the elements of a fixed compactly stored set. Our algorithms are for the unit-cost word RAM with multiplication and are extended to give dynamic algorithms. The lower bounds are proved for a large class of problems, including both static and dynamic predecessor problems, in a much stronger communication game model, but they apply to the cell probe and RAM models.},
  keywords = {cell probe,communication,dynamic,lower bound,sorted,static},
  annotation = {S. Alstrup, G. Brodal, and T. Rauhe, Optimal static range reporting in one dimension, in Proceedings of the Thirty-Third Annual ACM Symposium on Theory of Computing, Hersonissos, Crete, Greece, July 2001, pp. 476--482.\\
S. Alstrup, T. Husfeldt, and T. Rauhe, Marked ancestor problems, in Proceedings 39th Annual Symposium on Foundations of Computer Science, Palo Alto, CA, November 1998, pp. 534--543, IEEE Comput. Soc. Press, Los Alamitos, CA.\\
A. Amir, A. Efrat, P. Indyk, and H. Samet, Efficient regular data structures and algorithms for location and proximity problems, in Proceedings 40th Annual Symposium on Foundations of Computer Science, New York, NY, October 1999, pp. 160--170.\\
A. Andersson, Sublogarithmic seaching without multiplications, in Proceedings 36th Annual Symposium on Foundations of Computer Science, Milwaukee, WI, October 1995, pp. 655--665, IEEE Comput. Soc. Press, Los Alamitos, CA.\\
A. Andersson, Faster deterministic sorting and seaching in linear space, in Proceedings 37th Annual Symposium on Foundations of Computer Science, Burlington, VT, October 1996, pp. 135--141, IEEE Comput. Soc. Press, Los Alamitos, CA.\\
A. Andersson, P. B. Miltersen, S. Riis, and M. Thorup, Static dictionaries on AC0 RAMs: query time {$\Theta$}({\textbackslash}log {\textbackslash}, n / {\textbackslash}log {\textbackslash}, {\textbackslash}log /, n) is necessary and sufficient, in Proceedings 37th Annual Symposium on Foundations of Computer Science, Burlington, VT, October 1996, pp. 441--450, IEEE Comput. Soc. Press, Los Alamitos, CA.\\
\\
A. Andersson and M. Thorup, Tight(er) worst-case bounds on dynamic searching and priority queues, in Proceedings of the Thirty-Second Annual ACM Symposium on Theory of Computing, Portland, OR, May 2000, pp. 335--342.\\
\\
A. Brodnik, P. B. Miltersen, and I. Munro, Trans-dichotomous algorithms without multiplications--some upper and lower bounds, in Proceedings of the 5th Workshop on Algorithms and Data Structures, Halifax, NS, Canada, 1997, Lecture Notes in Computer Science, Vol. 1272, pp. 426--439, Springer-Verlag, Berlin/New York.\\
\\
\\
A. Chakrabarti, B. Chazelle, B. Gum, and A. Lvov, A good neighbor is hard to find, in Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing, Atlanta, GA, May 1999, pp. 305--311.\\
\\
\\
\\
\\
\\
\\
M. Dietzfelbinger and F. Meyer auf der Heide, A new universal class of hash functions and dynamic hashing in real time, in Automata, Languages, and Programming: 17th International Colloquium, Warwick University, England, July 1990 (M. S. Paterson, Ed.), Lecture Notes in Computer Science, Vol. 443, pp. 6--17, Springer-Verlag, Berlin/New York. [Informatik-Festschrift, zum 60, Geburtstag von Gunter Hotz (J. Buchmann, H. Ganzinger, and W. J. Paul, Eds.), B. G. Teubner, 1992, pp. 95--119]\\
\\
G. S. Frandsen, P. B. Miltersen, and S. Skyum, Dynamic word problems, in Proceedings 34th Annual Symposium on Foundations of Computer Science, Palo Alto, CA, November 1993, pp. 470--479, IEEE Comput. Soc. Press, Los Alamitos, CA. [J. Assoc. Comput. Mach.44 (1997), 257--71]\\
\\
M. Fredman and M. Saks, The cell probe complexity of dynamic data structures, in Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing, Seattle, WA, May 1989, pp. 345--354.\\
\\
T. Hagerup, Sorting and searching on a word RAM, in (STACS) 98: 15th Annual Symposium on Theoretical Aspects of Computer Science, Paris, France, February 1998, Lecture Notes in Computer Science, Vol. 1373, pp. 366--398, Springer-Verlag, Berlin/New York.\\
\\
M. Karchmer and A. Wigderson, Monotone circuits for connectivity require super-logarithmic depth, in Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, Chicago, IL, May 1988, pp. 539--550.\\
J. A. La Poutr{\'e}, Lower bounds for the Union-Find and the Split-Find problem on pointer machines, in Proceedings of the Twenty-Second Annual ACM Symposium on Theory of Computing, Baltimore, MD, May 1990, pp. 34--44.\\
\\
\\
P. B. Miltersen, The bit probe complexity measure revisited, in STACS 93: 10th Annual Symposium on Theoretical Aspects of Computer Science, Wurzburg, Germany, February 1993, (A. Finkel, P. Enjalbert, and K. W. Wagner, Eds.), Lecture Notes in Computer Science, Vol. 665, pp. 662--671, Springer-Verlag, Berlin/New York.\\
P. B. Miltersen, Lower bounds for Union-Split-Find related problems on random access machines, in Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing, Montr{\'e}al, Qu{\'e}bec, Canada, May 1994, pp. 625--634.\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
A. C. Yao, Some complexity questions related to distributive computing, in Conference Record of the Eleventh Annual ACM Symposium on Theory of Computing, Atlanta, GA, April-May 1979, pp. 209--213.},
  file = {/Users/tulasi/Zotero/storage/XIGQ5Q2P/Beame and Fich - 2002 - Optimal bounds for the predecessor problem and related problems.pdf}
}

@article{beameTimeSpaceTradeoffs1998,
  title = {Time-Space Tradeoffs for Branching Programs},
  author = {Beame, P. and Saks, M. and Thathachar, Jayram S.},
  year = {1998},
  doi = {10.1109/SFCS.1998.743453},
  abstract = {We obtain the first non-trivial time-space tradeoff lower bound for functions f: \{0,1\}/sup n//spl rarr/\{0,1\} on general branching programs by exhibiting a Boolean function f that requires exponential size to be computed by any branching program of length (1+/spl epsiv/)n, for some constant /spl epsiv/{\textquestiondown}0. We also give the first separation result between the syntactic and semantic read-k models for k{\textquestiondown}1 by showing that polynomial-size semantic read-twice branching programs can compute functions that require exponential size on any syntactic read-k branching program. We also show a time-space tradeoff result on the more general R-way branching program model: for any k, we give a function that requires exponential size to be computed by length kn q-way branching programs, for some q=q(k).},
  citationcount = {89},
  venue = {Proceedings 39th Annual Symposium on Foundations of Computer Science (Cat. No.98CB36280)}
}

@article{beameTimespaceTradeoffsMultiparty2002,
  title = {Time-Space Tradeoffs, Multiparty Communication Complexity, and Nearest-Neighbor Problems},
  author = {Beame, P. and Vee, Erik},
  year = {2002},
  doi = {10.1145/509907.510006},
  abstract = {The first non-trivial time-space tradeoff lower bounds have been shown for decision problems in P using notions derived from the study of two-party communication complexity. These results are proven directly for branching programs, natural generalizations of decision trees to directed graphs that provide elegant models of both non-uniform time T and space S simultaneously. We develop a new lower bound criterion, based on extending two-party communication complexity ideas to multiparty communication complexity. Applying this criterion to an explicit Boolean function based on a multilinear form over F/sub 2/. for suitable s, we show lower bounds that yield T = /spl Omega/(n log/sup 2/ n) when S /spl les/ n/sup 1-/spl epsi// log {\textbar}D{\textbar} for large input domain D. Finally, we develop lower bounds for nearest-neighbor problems involving n data points in a variety of d-dimensional metric spaces.},
  citationcount = {37},
  venue = {Proceedings 17th IEEE Annual Conference on Computational Complexity},
  keywords = {communication,communication complexity,lower bound,time-space}
}

@article{becchettiObliviousDimensionReduction2019,
  title = {Oblivious Dimension Reduction for K-Means: Beyond Subspaces and the {{Johnson-Lindenstrauss}} Lemma},
  author = {Becchetti, L. and Bury, Marc and {Cohen-Addad}, Vincent and Grandoni, F. and Schwiegelshohn, Chris},
  year = {2019},
  doi = {10.1145/3313276.3316318},
  abstract = {We show that for n points in d-dimensional Euclidean space, a data oblivious random projection of the columns onto m{$\in$} O((logk+loglogn){$\varepsilon-$}6log1/{$\varepsilon$}) dimensions is sufficient to approximate the cost of all k-means clusterings up to a multiplicative (1{\textpm}{$\varepsilon$}) factor. The previous-best upper bounds on m are O(logn{$\cdot$} {$\varepsilon-$}2) given by a direct application of the Johnson-Lindenstrauss Lemma, and O(k{$\varepsilon-$}2) given by [Cohen et al.-STOC'15].},
  citationcount = {72},
  venue = {Symposium on the Theory of Computing},
  keywords = {reduction}
}

@article{beckBalancedTwocoloringsFinite1981,
  title = {Balanced Two-Colorings of Finite Sets in the Square {{I}}},
  author = {Beck, J.},
  year = {1981},
  journal = {Combinatorica},
  volume = {1},
  number = {4},
  pages = {327--335},
  doi = {10.1007/BF02579453},
  annotation = {J. Beck, Roth's estimate of the discrepancy of integer sequences is nearly sharp,Combinatorica 1(4) (1981), 319--325.\\
J. Beck andT. Fiala, ``Integer-Making'' theorems,Discrete Applied Math. 3 (1981), 1--8.\\
P. Erd{\H o}s andJ. Spencer,Probabilistic methods in combinatorics, Akad{\'e}miai Kiad{\'o}, Budapest, 1974.\\
J. H. Halton, On the efficiency of certain quasirandom sequences of points in evaluating multi-dimensional integrals,Num. Math. 2 (1960), 84--90.\\
P. Major, On the invariance principle for sums of independent identically distributed random variables,Journal of Multivariate Analysis 8 (1978), 487--517.\\
V. V. Petrov,Sums of independent random variables (in Russian), Moscow, Nauka, 1972 (see Ch. X, {\S}1, Lemma 1).\\
K. F. Roth, On irregularities of distribution,Mathematika 7 (1954), 73--79.\\
W. M. Schmidt, Irregularities of distribution IV.,Inv. Math. 7 (1969), 55--82.\\
W. M. Schmidt, Irregularities of distribution VII.,Acta Arithmetica 21 (1972), 45--50.\\
W. M. Schmidt,On irregularities of distribution, Tata Inst. of Fund. Res. Lectures on Math. and Phys.56 (1977).\\
J. G. van der Corput, Verteilungsfunktionen I.,Proc. Kon. Ned. Akad. v. Wetensch. 38 (1935), 813--821.},
  file = {/Users/tulasi/Zotero/storage/XPENVJDK/Beck - 1981 - Balanced two-colorings of finite sets in the square I.pdf}
}

@article{beckerAddingAReferee2010,
  title = {Adding a Referee to an Interconnection Network: {{What}} Can(Not) Be Computed in One Round},
  author = {Becker, Florent and Matamala, M. and Nisse, Nicolas and Rapaport, I. and Suchan, Karol and Todinca, Ioan},
  year = {2010},
  doi = {10.1109/IPDPS.2011.55},
  abstract = {In this paper we ask which properties of a distributed network can be computed from a few amount of local information provided by its nodes. The distributed model we consider is a restriction of the classical \{CONGEST\} (distributed) model and it is close to the simultaneous messages (communication complexity) model defined by Babai, Kimmel and Lokam. More precisely, each of these n nodes-which only knows its own ID and the IDs of its neighbors- is allowed to send a message of O(n) bits to some central entity, called the referee. Is it possible for the referee to decide some basic structural properties of the network topology G? We show that simple questions like, "does G contain a square?", "does G contain a triangle?" or "Is the diameter of G at most 3?" cannot be solved in general. On the other hand, the referee can decode the messages in order to have full knowledge of G when G belongs to many graph classes such as planar graphs, bounded tree width graphs and, more generally, bounded degeneracy graphs. We leave open questions related to the connectivity of arbitrary graphs.},
  citationcount = {28},
  venue = {IEEE International Parallel and Distributed Processing Symposium},
  keywords = {communication,communication complexity}
}

@article{beckerAnAsymptoticallyOptimal1996,
  title = {An Asymptotically Optimal Multiversion {{B-tree}}},
  author = {Becker, Bruno and Gschwind, S. and Ohler, T. and Seeger, B. and Widmayer, P.},
  year = {1996},
  doi = {10.1007/s007780050028},
  abstract = {No abstract available},
  citationcount = {424},
  venue = {The VLDB journal}
}

@article{beckIntegermakingTheorems1981,
  title = {Integer-Making Theorems},
  author = {Beck, J. and Fiala, T.},
  year = {1981},
  journal = {Discrete Applied Mathematics},
  volume = {3},
  pages = {1--8},
  doi = {10.1016/0166-218X(81)90022-6},
  file = {/Users/tulasi/Zotero/storage/TANT6GD5/Beck and Fiala - 1981 - Integer-making theorems.pdf}
}

@article{beckIrregularitiesOfDistribution1987,
  title = {Irregularities of Distribution},
  author = {Beck, J. and Chen, William W. L.},
  year = {1987},
  doi = {10.1017/CBO9780511565984.012},
  abstract = {Part I. The Classical Problem: 1. Van der Corput's conjecture 2. Lower bounds - Roth's method 3. Upper bounds 4. Lower bounds - a combinatorial method of Schmidt Part II. Generalization of the Classical Problem: 5. Schmidt's work 6. A Fourier transform approach 7. Further applications of the Fourier transform method 8. More upper bounds Part III. More Problems!: 9. Miscellaneous questions.},
  citationcount = {207},
  venue = {No venue available}
}

@article{beckRothSEstimate1981,
  title = {Roth's Estimate of the Discrepancy of Integer Sequences Is Nearly Sharp},
  author = {Beck, J.},
  year = {1981},
  doi = {10.1007/BF02579452},
  abstract = {No abstract available},
  citationcount = {121},
  venue = {Comb.}
}

@article{beckSumsOfDistances1984,
  title = {Sums of Distances between Points on a Sphere --- an Application of the Theory of Irregularities of Distribution to Discrete {{Geometry}}},
  author = {Beck, J.},
  year = {1984},
  doi = {10.1112/S0025579300010639},
  abstract = {This paper is concerned with the solution of the following interesting geometrical problem. For what set of n points on the sphere is the sum of all Euclidean distances between points maximal, and what is the maximum? Our starting point is the following surprising ``invariance principle'' due to K. B. Stolarsky: The sum of the distances between points plus the quadratic average of a discrepancy type quantity is constant. Thus the sum of distances is maximized by a well distributed set of points. We now introduce some notation to make the statement more precise.},
  citationcount = {94},
  venue = {No venue available}
}

@article{bednarchakNoteBeckFialaTheorem1997,
  title = {A Note on the {{Beck-Fiala}} Theorem},
  author = {Bednarchak, D. and Helm, M.},
  year = {1997},
  journal = {Combinatorica},
  volume = {17},
  number = {1},
  pages = {147--149},
  doi = {10.1007/BF01196138},
  annotation = {P. K. Agarwal, andJ. Pach:Combinatorial Geometry, John Wiley \& Sons (New York) (1995).\\
N. Alon, P. Erd?s, andJ. H. Spencer:The Probabilistic method, John Wiley \& Sons (New York) (1992).\\
J. Beck, andT. Fiala: Integer-making theorems,Discrete Applied Mathematics,3, 1-8.},
  file = {/Users/tulasi/Zotero/storage/XKJJGH4R/Bednarchak and Helm - 1997 - A note on the Beck-Fiala theorem.pdf}
}

@article{beelenFastEncodingOf2020,
  title = {Fast Encoding of {{AG}} Codes over Cab Curves},
  author = {Beelen, Peter and Rosenkilde, J. and Solomatov, Grigory},
  year = {2020},
  doi = {10.1109/TIT.2020.3042248},
  abstract = {We investigate algorithms for encoding of one-point algebraic geometry (AG) codes over certain plane curves called {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}C\textsubscript{\{\vphantom\}}ab\vphantom\{\} {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} curves, as well as algorithms for inverting the encoding map, which we call ``unencoding''. Some {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}C\textsubscript{\{\vphantom\}}ab\vphantom\{\} {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} curves have many points or are even maximal, e.g. the Hermitian curve. Our encoding resp. unencoding algorithms have complexity {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}\{O\}\vphantom\{\}(\{n\}\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}) {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} resp. {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}\{O\}\vphantom\{\}(\{\{qn\}\}) {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} for AG codes over any {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}C\textsubscript{\{\vphantom\}}ab\vphantom\{\} {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} curve satisfying very mild assumptions, where n is the code length and q the base field size, and {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}\{O\}\vphantom\{\} {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} ignores constants and logarithmic factors in the estimate. For codes over curves whose evaluation points lie on a grid-like structure, for example the Hermitian curve and norm-trace curves, we show that our algorithms have quasi-linear time complexity {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}\{O\}\vphantom\{\}(\{n\}) {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} for both operations. For infinite families of curves whose number of points is a constant factor away from the Hasse-Weil bound, our encoding and unencoding algorithms have complexities {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}\{O\}\vphantom\{\}(\{n\}\textsuperscript{\{\vphantom\}}5/4\vphantom\{\}) {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} and {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}\{O\}\vphantom\{\}(\{n\}\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}) {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} respectively.},
  citationcount = {10},
  venue = {IEEE Transactions on Information Theory}
}

@article{behnezhadDynamicAlgorithmsFor2022,
  title = {Dynamic Algorithms for Maximum Matching Size},
  author = {Behnezhad, Soheil},
  year = {2022},
  doi = {10.48550/arXiv.2207.07607},
  abstract = {We study fully dynamic algorithms for maximum matching. This is a well-studied problem, known to admit several update-time/approximation trade-offs. For instance, it is known how to maintain a 1/2-approximate matching in \textsuperscript{\{\vphantom\}}O(1)\vphantom\{\}n update time or a 2/3-approximate matching in O({\textsurd}\{n\}) update time, where n is the number of vertices. It has been a long-standing open problem to determine whether either of these bounds can be improved. In this paper, we show that when the goal is to maintain just the size of the matching (and not its edge-set), then these bounds can indeed be improved. First, we give an algorithm that takes \textsuperscript{\{\vphantom\}}O(1)\vphantom\{\}n update-time and maintains a .501-approximation (.585-approximation if the graph is bipartite). Second, we give an algorithm that maintains a (2/3+{\textohm}(1))-approximation in O({\textsurd}\{n\}) time for bipartite graphs. Our results build on new connections to sublinear time algorithms. In particular, a key tool for both is an algorithm of the author for estimating the size of maximal matchings in O\vphantom\{\}(n) time [Behnezhad; FOCS 2021]. Our second result also builds on the edge-degree constrained subgraph (EDCS) of Bernstein and Stein [ICALP'15, SODA'16]. In particular, while it has been known that EDCS may not include a better than 2/3-approximation, we give a new characterization of such tight instances which allows us to break it. We believe this characterization might be of independent interest.},
  citationcount = {22},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,update,update time}
}

@article{behnezhadFullyDynamicMatching2024,
  title = {Fully Dynamic Matching and Ordered Ruzsa-Szemer{\'e}di Graphs},
  author = {Behnezhad, Soheil and Ghafari, Alma},
  year = {2024},
  doi = {10.1109/FOCS61266.2024.00027},
  abstract = {We study the fully dynamic maximum matching problem. In this problem, the goal is to efficiently maintain an approximate maximum matching of a graph that is subject to edge insertions and deletions. Our focus is particularly on algorithms that maintain the edges of a (1-{$\varepsilon$}) -approximate maximum matching for an arbitrarily small constant {$\varepsilon>$}0. Until recently, the fastest known algorithm for this problem required {$\Theta$}(n) time per update where n is the number of vertices. This bound was slightly improved to n/(\textsuperscript{\{\vphantom\}}{$\ast$}\vphantom\{\}n)\textsuperscript{\{\vphantom\}}{\textohm}(1)\vphantom\{\} by Assadi, Behnezhad, Khanna, and Li [STOC'23] and very recently to n/2\textsubscript{\{\vphantom\}}-\vphantom\{\}\textsuperscript{\{\vphantom\}}{\textohm}({\textsurd}\{n\})\vphantom\{\} by Liu [FOCS'24]. Whether this can be improved to n\textsuperscript{\{\vphantom\}}1-{\textohm}(1)\vphantom\{\} remains a major open problem. In this paper, we introduce Ordered Ruzsa-Szemer{\'e}di (ORS) graphs (a generalization of Ruzsa-Szemer{\'e}di graphs) and show that the complexity of dynamic matching is closely tied to them. For {$\delta>$}0, define ORS ({$\delta$}n) to be the maximum number of matchings M\textsubscript{\{\vphantom\}}1\vphantom\{\},{\dots},1M\textsubscript{\{\vphantom\}}t\vphantom\{\}, each of size {$\delta$}n, that one can pack in an n-vertex graph such that each matching M\textsubscript{\{\vphantom\}}i\vphantom\{\} is an induced matching in subgraph M\textsubscript{\{\vphantom\}}1\vphantom\{\}{$\cup\dots\cup$}M\textsubscript{\{\vphantom\}}i\vphantom\{\}. We show that there is a randomized algorithm that maintains a (1-{$\varepsilon$}) -approximate maximum matching of a fully dynamic graph in amortized update-time. While the value of \{ORS\}({$\Theta$}(n)) remains unknown and is only upper bounded by n\textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\}, the densest construction known from more than two decades ago only achieves ORS({$\Theta$}(n)){$\geq$}n\textsuperscript{\{\vphantom\}}1/{$\Theta$}(n)\vphantom\{\}=n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} [Fischer et al. STOC'02]. If this is close to the right bound, then our algorithm achieves an update-time of {\textsurd}\{n\vphantom\}\textsuperscript{\{\vphantom\}}1+O({$\varepsilon$})\vphantom\{\}\vphantom\{\}\textsuperscript{\{\vphantom\}}-\vphantom\{\}, resolving the aforementioned longstanding open problem in dynamic algorithms in a strong sense.},
  citationcount = {5},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,update,update time}
}

@article{behnezhadNewTradeOffs2022,
  title = {New Trade-Offs for Fully Dynamic Matching via Hierarchical {{EDCS}}},
  author = {Behnezhad, Soheil and Khanna, S.},
  year = {2022},
  doi = {10.1137/1.9781611977073.140},
  abstract = {We study the maximum matching problem in fully dynamic graphs: a graph is undergoing both edge insertions and deletions, and the goal is to efficiently maintain a large matching after each edge update. This problem has received considerable attention in recent years. The known algorithms naturally exhibit a trade-off between the quality of the matching maintained (i.e., the approximation ratio) and the time needed per update. While several interesting results have been obtained, the optimal behavior of this trade-off remains largely unclear. Our main contribution is a new approach to designing fully dynamic approximate matching algorithms that in a unified manner not only (essentially) recovers all previously known trade-offs that were achieved via very different techniques, but reveals some new ones as well. As our main tool to achieve this, we introduce a generalization of the edge-degree constrained subgraph (EDCS) of Bernstein and Stein (2015) that we call the hierarchical EDCS (HEDCS).},
  citationcount = {18},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,update}
}

@article{behnezhadSublinearTimeAlgorithms2022,
  title = {Sublinear Time Algorithms and Complexity of Approximate Maximum Matching},
  author = {Behnezhad, Soheil and Roghani, M. and Rubinstein, A.},
  year = {2022},
  doi = {10.1145/3564246.3585231},
  abstract = {Sublinear time algorithms for approximating maximum matching size have long been studied. Much of the progress over the last two decades on this problem has been on the algorithmic side. For instance, an algorithm of [Behnezhad; FOCS'21] obtains a 1/2-approximation in O(n) time for n-vertex graphs. A more recent algorithm by [Behnezhad, Roghani, Rubinstein, and Saberi; SODA'23] obtains a slightly-better-than-1/2 approximation in O(n1+{\cyrchar\cyrie}) time (for arbitrarily small constant {$\varepsilon$}{\textquestiondown}0). On the lower bound side, [Parnas and Ron; TCS'07] showed 15 years ago that obtaining any constant approximation of maximum matching size requires {\textohm}(n) time. Proving any super-linear in n lower bound, even for (1-{\cyrchar\cyrie})-approximations, has remained elusive since then. In this paper, we prove the first super-linear in n lower bound for this problem. We show that at least n1.2 - o(1) queries in the adjacency list model are needed for obtaining a (2/3 + {\textohm}(1))-approximation of the maximum matching size. This holds even if the graph is bipartite and is promised to have a matching of size {$\Theta$}(n). Our lower bound argument builds on techniques such as correlation decay that to our knowledge have not been used before in proving sublinear time lower bounds. We complement our lower bound by presenting two algorithms that run in strongly sublinear time of n2-{\textohm}(1). The first algorithm achieves a (2/3-{$\varepsilon$})-approximation (for any arbitrarily small constant {$\varepsilon$}{\textquestiondown}0); this significantly improves prior close-to-1/2 approximations. Our second algorithm obtains an even better approximation factor of (2/3+{\textohm}(1)) for bipartite graphs. This breaks 2/3-approximation which has been a barrier in various settings of the matching problem, and importantly shows that our n1.2-o(1) time lower bound for (2/3+{\textohm}(1))-approximations cannot be improved all the way to n2-o(1).},
  citationcount = {18},
  venue = {Symposium on the Theory of Computing},
  keywords = {lower bound,query}
}

@article{beigelOnAcc1994,
  title = {On {{ACC}}},
  author = {Beigel, R. and Tarui, J.},
  year = {1994},
  doi = {10.1007/BF01263423},
  abstract = {No abstract available},
  citationcount = {127},
  venue = {Computational Complexity}
}

@article{beinekeModernGraphTheory2013,
  title = {Modern Graph Theory},
  author = {Beineke, L. and Wilson, Robin J.},
  year = {2013},
  doi = {10.1093/acprof:oso/9780199656592.003.0015},
  abstract = {No abstract available},
  citationcount = {1627},
  venue = {No venue available}
}

@article{beisShapeIndexingUsing1997,
  title = {Shape Indexing Using Approximate Nearest-Neighbour Search in High-Dimensional Spaces},
  author = {Beis, J. and Lowe, D.},
  year = {1997},
  doi = {10.1109/CVPR.1997.609451},
  abstract = {Shape indexing is a way of making rapid associations between features detected in an image and object models that could have produced them. When model databases are large, the use of high-dimensional features is critical, due to the improved level of discrimination they can provide. Unfortunately, finding the nearest neighbour to a query point rapidly becomes inefficient as the dimensionality of the feature space increases. Past indexing methods have used hash tables for hypothesis recovery, but only in low-dimensional situations. In this paper we show that a new variant of the k-d tree search algorithm makes indexing in higher-dimensional spaces practical. This Best Bin First, or BBF search is an approximate algorithm which finds the nearest neighbour for a large fraction of the queries, and a very close neighbour in the remaining cases. The technique has been integrated into a fully developed recognition system, which is able to detect complex objects in real, cluttered scenes in just a few seconds.},
  citationcount = {1132},
  venue = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  keywords = {query}
}

@article{belazzouguiBlockTrees2021,
  title = {Block Trees},
  author = {Belazzougui, Djamal and C{\'a}ceres, Manuel and Gagie, T. and Gawrychowski, Pawe{\l} and K{\"a}rkk{\"a}inen, Juha and Navarro, G. and Pereira, Alberto Ord{\'o}{\~n}ez and Puglisi, S. and Tabei, Yasuo},
  year = {2021},
  doi = {10.1016/j.jcss.2020.11.002},
  abstract = {No abstract available},
  citationcount = {20},
  venue = {Journal of computer and system sciences (Print)}
}

@article{belazzouguiFastPrefixSearch2010,
  title = {Fast Prefix Search in Little Space, with Applications},
  author = {Belazzougui, Djamal and Boldi, P. and Pagh, R. and Vigna, S.},
  year = {2010},
  doi = {10.1007/978-3-642-15775-2_37},
  abstract = {No abstract available},
  citationcount = {54},
  venue = {Embedded Systems and Applications}
}

@article{belazzouguiNewLowerAnd2011,
  title = {New Lower and Upper Bounds for Representing Sequences},
  author = {Belazzougui, Djamal and Navarro, G.},
  year = {2011},
  doi = {10.1007/978-3-642-33090-2_17},
  abstract = {No abstract available},
  citationcount = {42},
  venue = {Embedded Systems and Applications}
}

@article{belazzouguiOptimalLowerAnd2011,
  title = {Optimal Lower and Upper Bounds for Representing Sequences},
  author = {Belazzougui, Djamal and Navarro, G.},
  year = {2011},
  doi = {10.1145/2629339},
  abstract = {Sequence representations supporting the queries access, select, and rank are at the core of many data structures. There is a considerable gap between the various upper bounds and the few lower bounds known for such representations, and how they relate to the space used. In this article, we prove a strong lower bound for rank, which holds for rather permissive assumptions on the space used, and give matching upper bounds that require only a compressed representation of the sequence. Within this compressed space, the operations access and select can be solved in constant or almost-constant time, which is optimal for large alphabets. Our new upper bounds dominate all of the previous work in the time/space map.},
  citationcount = {75},
  venue = {TALG},
  keywords = {data structure,lower bound,query}
}

@article{belazzouguiPredecessorSearchString2016,
  title = {Predecessor Search, String Algorithms and Data Structures},
  author = {Belazzougui, Djamal},
  year = {2016},
  doi = {10.1007/978-1-4939-2864-4_632},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Encyclopedia of Algorithms},
  keywords = {data structure}
}

@article{belazzouguiWeightedAncestorsIn2021,
  title = {Weighted Ancestors in Suffix Trees Revisited},
  author = {Belazzougui, Djamal and Kosolobov, D. and Puglisi, S. and Raman, R.},
  year = {2021},
  doi = {10.4230/LIPIcs.CPM.2021.8},
  abstract = {The weighted ancestor problem is a well-known generalization of the predecessor problem to trees. It is known that it requires {\textohm}(n) time for queries provided O(n\{\{polylog\}\}n) space is available and weights are from [0..n], where n is the number of tree nodes. However, when applied to suffix trees, the problem, surprisingly, admits an O(n)-space solution with constant query time as was shown by Gawrychowski, Lewenstein, and Nicholson. This variant of the problem can be reformulated as follows: given the suffix tree of a string s, we need a data structure that can locate in the tree any substring s[p..q] of s in O(1) time (as if one descended from the root reading s[p..q] along the way). Unfortunately, the data structure of Gawrychowski et al. has no efficient construction algorithm, which apparently prevents its wide usage. In this paper we resolve this issue describing a data structure for weighted ancestors in suffix trees with constant query time and a linear construction algorithm. Our solution is based on a novel approach using so-called irreducible LCP values.},
  citationcount = {20},
  venue = {Annual Symposium on Combinatorial Pattern Matching},
  keywords = {data structure,query,query time}
}

@article{ben-amramGeneralizationLowerBound2001,
  title = {A Generalization of a Lower Bound Technique Due to {{Fredman}} and {{Saks}}},
  author = {{Ben-Amram}, Amir M. and Galil, Zvi},
  year = {2001},
  journal = {Algorithmica},
  volume = {30},
  number = {1},
  pages = {34--66},
  doi = {10.1007/s004530010077},
  abstract = {In a seminal paper of 1989, Fredman and Saks proved lower bounds for some important datastructure problems in the cell probe model. In particular, lower bounds were established on worst-case and amortized operation cost for the union-find problem and the prefix sum problem. The goal of this paper is to turn their proof technique into a general tool that can be applied to different problems and computational models. To this end we define two quantities: Output Variability depends only on the model of computation. It indicates how much variation can be found in the results of a program with certain resource bounds. This measures in some sense the power of a model. Problem Variability characterizes in a similar sense the difficulty of the problem. Our Main Theorem shows that by comparing a model's output variability to a problem's problem variability, lower bounds on the complexity of solving the problem on the given model may be inferred. The theorem thus shows how to separate the analysis of the model of computation from that of the problem when proving lower bounds. We show how the results of Fredman and Saks fit our framework by computing the output variability of the cell probe model and the problem variability for problems considered in their paper. This allows us to reprove their lower bound results, and slightly extend them. The main purpose of this paper though is to present the generalized technique.},
  keywords = {cell probe,Chronogram method,data structures,dynamic,lower bound,Prefix sums.,sorted,Union-find},
  file = {/Users/tulasi/Zotero/storage/RGVL6UCQ/Ben-Amram and Galil - 2001 - A Generalization of a Lower Bound Technique due to Fredman and Saks.pdf}
}

@article{ben-amramLowerBoundsDynamic2002,
  title = {Lower Bounds for Dynamic Data Structures on Algebraic Rams},
  author = {{Ben-Amram}, Amir M. and Galil, Z.},
  year = {2002},
  doi = {10.1007/s00453-001-0079-6},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Algorithmica},
  keywords = {data structure,dynamic,lower bound}
}

@article{ben-davidLimitationsOfLearning2003,
  title = {Limitations of Learning via Embeddings in Euclidean Half Spaces},
  author = {{Ben-David}, Shai and Eiron, Nadav and Simon, H.},
  year = {2003},
  doi = {10.1007/3-540-44581-1_25},
  abstract = {No abstract available},
  citationcount = {109},
  venue = {Journal of machine learning research}
}

@article{ben-eliezerApproximateCountingOf2024,
  title = {Approximate Counting of Permutation Patterns},
  author = {{Ben-Eliezer}, Omri and Mitrovi'c, Slobodan and Srivastava, Pranjal},
  year = {2024},
  doi = {10.48550/arXiv.2411.04718},
  abstract = {We consider the problem of counting the copies of a length-k pattern {$\sigma$} in a sequence f[n]{$\rightarrow$}\{R\}, where a copy is a subset of indices i{$_{1}<\dots$}j){$\ell$}) if and only if {$\sigma$}(j){$<\sigma$}({$\ell$}). This problem is motivated by a range of connections and applications in ranking, nonparametric statistics, combinatorics, and fine-grained complexity, especially when k is a small fixed constant. Recent advances have significantly improved our understanding of counting and detecting patterns. Guillemot and Marx [2014] demonstrated that the detection variant is solvable in O(n) time for any fixed k. Their proof has laid the foundations for the discovery of the twin-width, a concept that has notably advanced parameterized complexity in recent years. Counting, in contrast, is harder: it has a conditional lower bound of n\textsuperscript{\{\vphantom\}}{\textohm}(k/k)\vphantom\{\} [Berendsohn, Kozma, and Marx 2019] and is expected to be polynomially harder than detection as early as k=4, given its equivalence to counting 4-cycles in graphs [Dudek and Gawrychowski, 2020]. In this work, we design a deterministic near-linear time (1+{$\varepsilon$})-approximation algorithm for counting {$\sigma$}-copies in f for all k{$\leq$}5. Combined with the conditional lower bound for k=4, this establishes the first known separation between approximate and exact algorithms for pattern counting. Interestingly, our algorithm leverages the Birg{\'e} decomposition -- a sublinear tool for monotone distributions widely used in distribution testing -- which, to our knowledge, has not been applied in a pattern counting context before.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {lower bound}
}

@article{ben-orLowerBoundsFor1983,
  title = {Lower Bounds for Algebraic Computation Trees},
  author = {{Ben-Or}, M.},
  year = {1983},
  doi = {10.1145/800061.808735},
  abstract = {A topological method is given for obtaining lower bounds for the height of algebraic computation trees, and algebraic decision trees. Using this method we are able to generalize, and present in a uniform and easy way, almost all the known nonlinear lower bounds for algebraic computations. Applying the method to decision trees we extend all the apparently known lower bounds for linear decision trees to bounded degree algebraic decision trees, thus answering the open questions raised by Steele and Yao [20]. We also show how this new method can be used to establish lower bounds on the complexity of constructions with ruler and compass in plane Euclidean geometry.},
  citationcount = {582},
  venue = {Symposium on the Theory of Computing}
}

@article{ben-orProbabilisticAlgorithmsIn1980,
  title = {Probabilistic Algorithms in Finite Fields},
  author = {{Ben-Or}, M.},
  year = {1980},
  doi = {10.1109/SFCS.1981.37},
  abstract = {We present probabilistic algorithms for the problems of finding an irreducible polynomial of degree n over a finite field, finding roots of a polynomial, and factoring a polynomial into its irreducible factors over a finite field. All of these problems are of importance in algebraic coding theory, algebraic symbol manipulation, and number theory. These algorithms have a very transparent, easy to program structure. For finite fields of large characteristic p, so that exhaustive search through \{\{Z\}\}\textsubscript{p}, is not feasible, our algorithms are of lower order in the degrees of the polynomial and fields in question, than previously published algorithms.},
  citationcount = {299},
  venue = {22nd Annual Symposium on Foundations of Computer Science (sfcs 1981)}
}

@article{benderANewApproach2009,
  title = {A New Approach to Incremental Topological Ordering},
  author = {Bender, M. A. and Fineman, Jeremy T. and Gilbert, Seth},
  year = {2009},
  doi = {10.1137/1.9781611973068.120},
  abstract = {Let G = (V, E) be a directed acyclic graph (dag) with n = {\textbar}V{\textbar} and m = {\textbar}E{\textbar}. We say that a total ordering p on vertices V is a topological ordering if for every edge (u, v) {$\in$} E, we have u {$\in$} v. In this paper, we consider the problem of maintaining a topological ordering subject to dynamic changes to the underlying graph. That is, we begin with an empty graph G = (V, o) consisting of n nodes. The adversary adds m edges to the graph G, one edge at a time. Throughout this process, we maintain an online topological ordering of the graph G. In this paper, we present a new algorithm that has a total cost of O(n2logn) for maintaining the topological ordering throughout all the edge additions. At the heart of our algorithm is a new approach for maintaining the ordering. Instead of attempting to place the nodes in an ordered list, we assign each node a label that is consistent with the ordering, and yet can be updated efficiently as edges are inserted. When the graph is dense, our algorithm is more efficient than existing algorithms. By way of contrast, the best known prior algorithms achieve only O(min(m1.5, n2.5)) cost.},
  citationcount = {33},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{benderIncrementalEdgeOrientation2021,
  title = {Incremental Edge Orientation in Forests},
  author = {Bender, M. A. and Kopelowitz, T. and Kuszmaul, William and Porat, E. and Stein, C.},
  year = {2021},
  doi = {10.4230/LIPIcs.ESA.2021.12},
  abstract = {For any forest G=(V,E) it is possible to orient the edges E so that no vertex in V has out-degree greater than 1. This paper considers the incremental edge-orientation problem, in which the edges E arrive over time and the algorithm must maintain a low-out-degree edge orientation at all times. We give an algorithm that maintains a maximum out-degree of 3 while flipping at most O(n) edge orientations per edge insertion, with high probability in n. The algorithm requires worst-case time O(nn) per insertion, and takes amortized time O(1). The previous state of the art required up to O(n/n) edge flips per insertion. We then apply our edge-orientation results to the problem of dynamic Cuckoo hashing. The problem of designing simple families \{H\} of hash functions that are compatible with Cuckoo hashing has received extensive attention. These families \{H\} are known to satisfy \{static guarantees\}, but do not come typically with \{dynamic guarantees\} for the running time of inserts and deletes. We show how to transform static guarantees (for 1-associativity) into near-state-of-the-art dynamic guarantees (for O(1)-associativity) in a black-box fashion. Rather than relying on the family \{H\} to supply randomness, as in past work, we instead rely on randomness within our table-maintenance algorithm.},
  citationcount = {4},
  venue = {Embedded Systems and Applications},
  keywords = {dynamic,static}
}

@article{benderLowestCommonAncestors2005,
  title = {Lowest Common Ancestors in Trees and Directed Acyclic Graphs},
  author = {Bender, M. A. and {Farach-Colton}, Mart{\'i}n and Pemmasani, Giridhar and Skiena, S. and Sumazin, Pavel},
  year = {2005},
  doi = {10.1016/J.JALGOR.2005.08.001},
  abstract = {No abstract available},
  citationcount = {242},
  venue = {J. Algorithms}
}

@article{benderTheLevelAncestor2002,
  title = {The Level Ancestor Problem Simplified},
  author = {Bender, M. A. and {Farach-Colton}, Mart{\'i}n},
  year = {2002},
  doi = {10.1007/3-540-45995-2_44},
  abstract = {No abstract available},
  citationcount = {213},
  venue = {Latin American Symposium on Theoretical Informatics}
}

@article{benderTwoSimplifiedAlgorithms2002,
  title = {Two Simplified Algorithms for Maintaining Order in a List},
  author = {Bender, M. A. and Cole, R. and Demaine, E. and {Farach-Colton}, Mart{\'i}n and Zito, Jack},
  year = {2002},
  doi = {10.1007/3-540-45749-6_17},
  abstract = {No abstract available},
  citationcount = {168},
  venue = {Embedded Systems and Applications}
}

@article{benomarLearningAugmentedPriority2024,
  title = {Learning-Augmented Priority Queues},
  author = {Benomar, Ziyad and Coester, Christian},
  year = {2024},
  doi = {10.48550/arXiv.2406.04793},
  abstract = {Priority queues are one of the most fundamental and widely used data structures in computer science. Their primary objective is to efficiently support the insertion of new elements with assigned priorities and the extraction of the highest priority element. In this study, we investigate the design of priority queues within the learning-augmented framework, where algorithms use potentially inaccurate predictions to enhance their worst-case performance. We examine three prediction models spanning different use cases, and show how the predictions can be leveraged to enhance the performance of priority queue operations. Moreover, we demonstrate the optimality of our solution and discuss some possible applications.},
  citationcount = {5},
  venue = {Neural Information Processing Systems},
  keywords = {data structure}
}

@article{bentertParameterizedAspectsOf2017,
  title = {Parameterized Aspects of Triangle Enumeration},
  author = {Bentert, M. and Fluschnik, T. and Nichterlein, A. and Niedermeier, R.},
  year = {2017},
  doi = {10.1007/978-3-662-55751-8_9},
  abstract = {No abstract available},
  citationcount = {26},
  venue = {International Symposium on Fundamentals of Computation Theory}
}

@article{bentleyDecomposableSearchingProblems1979,
  title = {Decomposable Searching Problems},
  author = {Bentley, J.},
  year = {1979},
  doi = {10.1016/0020-0190(79)90117-0},
  abstract = {No abstract available},
  citationcount = {367},
  venue = {Information Processing Letters}
}

@article{bentleyDecomposableSearchingProblems1980,
  title = {Decomposable Searching Problems {{I}}: {{Static-to-dynamic}} Transformation},
  author = {Bentley, J. and Saxe, J.},
  year = {1980},
  doi = {10.1016/0196-6774(80)90015-2},
  abstract = {No abstract available},
  citationcount = {412},
  venue = {J. Algorithms}
}

@article{bentleyMultidimensionalBinarySearch1975,
  title = {Multidimensional Binary Search Trees Used for Associative Searching},
  author = {Bentley, J.},
  year = {1975},
  doi = {10.1145/361002.361007},
  abstract = {This paper develops the multidimensional binary search tree (or {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}-d tree, where {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown} is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}-d tree is defined and examples are given. It is shown to be quite efficient in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently. Various utility algorithms are developed; their proven average running times in an {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} record file are: insertion, {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}); deletion of the root, {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}({\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}-1)/{\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown}); deletion of a random node, {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}); and optimization (guarantees logarithmic performance of searches), {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}). Search algorithms are given for partial match queries with {\textexclamdown}italic{\textquestiondown}t{\textexclamdown}/italic{\textquestiondown} keys specified [proven maximum running time of {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}({\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}-{\textexclamdown}italic{\textquestiondown}t{\textexclamdown}/italic{\textquestiondown})/{\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown})] and for nearest neighbor queries [empirically observed average running time of {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}).] These performances far surpass the best currently known algorithms for these tasks. An algorithm is presented to handle any general intersection query. The main focus of this paper is theoretical. It is felt, however, that {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}-d trees could be quite useful in many applications, and examples of potential uses are given.},
  citationcount = {7720},
  venue = {CACM}
}

@article{bentleyMultidimensionalDivideAnd1980,
  title = {Multidimensional Divide-and-Conquer},
  author = {Bentley, J.},
  year = {1980},
  doi = {10.1145/358841.358850},
  abstract = {Most results in the field of algorithm design are single algorithms that solve single problems. In this paper we discuss multidimensional divide-and-conquer, an algorithmic paradigm that can be instantiated in many different ways to yield a number of algorithms and data structures for multidimensional problems. We use this paradigm to give best-known solutions to such problems as the ECDF, maxima, range searching, closest pair, and all nearest neighbor problems. The contributions of the paper are on two levels. On the first level are the particular algorithms and data structures given by applying the paradigm. On the second level is the more novel contribution of this paper: a detailed study of an algorithmic paradigm that is specific enough to be described precisely yet general enough to solve a wide variety of problems.},
  citationcount = {728},
  venue = {CACM}
}

@article{beraLinearTimeSubgraph2019,
  title = {Linear Time Subgraph Counting, Graph Degeneracy, and the Chasm at Size Six},
  author = {Bera, Suman Kalyan and Pashanasangi, Noujan and Comandur, Seshadhri},
  year = {2019},
  doi = {10.4230/LIPIcs.ITCS.2020.38},
  abstract = {We consider the problem of counting all k-vertex subgraphs in an input graph, for any constant k. This problem (denoted sub-cnt\textsubscript{k}) has been studied extensively in both theory and practice. In a classic result, Chiba and Nishizeki (SICOMP 85) gave linear time algorithms for clique and 4-cycle counting for bounded degeneracy graphs. This is a rich class of sparse graphs that contains, for example, all minor-free families and preferential attachment graphs. The techniques from this result have inspired a number of recent practical algorithms for sub-cnt\textsubscript{k}. Towards a better understanding of the limits of these techniques, we ask: for what values of k can sub-cnt\textsubscript{k} be solved in linear time? We discover a chasm at k=6. Specifically, we prove that for k{$<$}6, sub-cnt\textsubscript{k} can be solved in linear time. Assuming a standard conjecture in fine-grained complexity, we prove that for all k{$\geq$}6, sub-cnt\textsubscript{k} cannot be solved even in near-linear time.},
  citationcount = {31},
  venue = {Information Technology Convergence and Services}
}

@article{bergamaschiNewTechniquesAnd2020,
  title = {New Techniques and Fine-Grained Hardness for Dynamic near-Additive Spanners},
  author = {Bergamaschi, Thiago and Henzinger, Monika and Gutenberg, Maximilian Probst and Williams, V. V. and Wein, Nicole},
  year = {2020},
  doi = {10.1137/1.9781611976465.110},
  abstract = {Maintaining and updating shortest paths information in a graph is a fundamental problem with many applications. As computations on dense graphs can be prohibitively expensive, and it is preferable to perform the computations on a sparse skeleton of the given graph that roughly preserves the shortest paths information. Spanners and emulators serve this purpose. This paper develops fast dynamic algorithms for sparse spanner and emulator maintenance and provides evidence from fine-grained complexity that these algorithms are tight. Under the popular OMv conjecture, we show that there can be no decremental or incremental algorithm that maintains an n\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\} edge (purely additive) +n\textsuperscript{\{\vphantom\}}{$\delta$}\vphantom\{\}-emulator for any {$\delta<$}1/2 with arbitrary polynomial preprocessing time and total update time m\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}. Also, under the Combinatorial k-Clique hypothesis, any fully dynamic combinatorial algorithm that maintains an n\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\} edge (1+{$\epsilon$},n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\})-spanner or emulator must either have preprocessing time mn\textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\} or amortized update time m\textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\}. Both of our conditional lower bounds are tight. As the above fully dynamic lower bound only applies to combinatorial algorithms, we also develop an algebraic spanner algorithm that improves over the m\textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\} update time for dense graphs. For any constant {$\epsilon\in$}(0,1], there is a fully dynamic algorithm with worst-case update time O(n\textsuperscript{\{\vphantom\}}1.529\vphantom\{\}) that whp maintains an n\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\} edge (1+{$\epsilon$},n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\})-spanner. Our new algebraic techniques and spanner algorithms allow us to also obtain (1) a new fully dynamic algorithm for All-Pairs Shortest Paths (APSP) with update and path query time O(n\textsuperscript{\{\vphantom\}}1.9\vphantom\{\}); (2) a fully dynamic (1+{$\epsilon$})-approximate APSP algorithm with update time O(n\textsuperscript{\{\vphantom\}}1.529\vphantom\{\}); (3) a fully dynamic algorithm for near-2-approximate Steiner tree maintenance.},
  citationcount = {28},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,lower bound,query,query time,update,update time}
}

@article{bergTwoAndThree1992,
  title = {Two- and Three-Dimensional Point Location in Rectangular Subdivisions (Extended Abstract)},
  author = {Berg, M. D. and Kreveld, M. V. and Snoeyink, J.},
  year = {1992},
  doi = {10.1007/3-540-55706-7_32},
  abstract = {No abstract available},
  citationcount = {46},
  venue = {Scandinavian Workshop on Algorithm Theory}
}

@article{berkholzAnsweringConjunctiveQueries2017,
  title = {Answering Conjunctive Queries under Updates},
  author = {Berkholz, Christoph and Keppeler, Jens and Schweikardt, Nicole},
  year = {2017},
  doi = {10.1145/3034786.3034789},
  abstract = {We consider the task of enumerating and counting answers to k-ary conjunctive queries against relational databases that may be updated by inserting or deleting tuples. We exhibit a new notion of q-hierarchical conjunctive queries and show that these can be maintained efficiently in the following sense. During a linear time pre-processing phase, we can build a data structure that enables constant delay enumeration of the query results; and when the database is updated, we can update the data structure and restart the enumeration phase within constant time. For the special case of self-join free conjunctive queries we obtain a dichotomy: if a query is not q-hierarchical, then query enumeration with sublinear *) delay and sublinear update time (and arbitrary preprocessing time) is impossible. For answering Boolean conjunctive queries and for the more general problem of counting the number of solutions of k-ary queries we obtain complete dichotomies: if the query's homomorphic core is q-hierarchical, then size of the the query result can be computed in linear time and maintained with constant update time. Otherwise, the size of the query result cannot be maintained with sublinear update time. All our lower bounds rely on the OMv-conjecture, a conjecture on the hardness of online matrix-vector multiplication that has recently emerged in the field of fine-grained complexity to characterise the hardness of dynamic problems. The lower bound for the counting problem additionally relies on the orthogonal vectors conjecture, which in turn is implied by the strong exponential time hypothesis.*) By sublinear we mean O(n(1-{$\varepsilon$}) for some {$\varepsilon$} {\textquestiondown} 0, where n is the size of the active domain of the current database.},
  citationcount = {103},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {data structure,dynamic,lower bound,query,update,update time}
}

@article{berkholzAnsweringUcqsUnder2017,
  title = {Answering {{UCQs}} under Updates and in the Presence of Integrity Constraints},
  author = {Berkholz, Christoph and Keppeler, Jens and Schweikardt, Nicole},
  year = {2017},
  doi = {10.4230/LIPIcs.ICDT.2018.8},
  abstract = {We investigate the query evaluation problem for fixed queries over fully dynamic databases where tuples can be inserted or deleted. The task is to design a dynamic data structure that can immediately report the new result of a fixed query after every database update. We consider unions of conjunctive queries (UCQs) and focus on the query evaluation tasks testing (decide whether an input tuple belongs to the query result), enumeration (enumerate, without repetition, all tuples in the query result), and counting (output the number of tuples in the query result). We identify three increasingly restrictive classes of UCQs which we call t-hierarchical, q-hierarchical, and exhaustively q-hierarchical UCQs. Our main results provide the following dichotomies: If the query's homomorphic core is t-hierarchical (q-hierarchical, exhaustively q-hierarchical), then the testing (enumeration, counting) problem can be solved with constant update time and constant testing time (delay, counting time). Otherwise, it cannot be solved with sublinear update time and sublinear testing time (delay, counting time), unless the OV-conjecture and/or the OMv-conjecture fails. We also study the complexity of query evaluation in the dynamic setting in the presence of integrity constraints, and we obtain according dichotomy results for the special case of small domain constraints (i.e., constraints which state that all values in a particular column of a relation belong to a fixed domain of constant size).},
  citationcount = {32},
  venue = {International Conference on Database Theory},
  keywords = {data structure,dynamic,query,update,update time}
}

@article{berkholzConstantDelayEnumeration2020,
  title = {Constant Delay Enumeration for Conjunctive Queries},
  author = {Berkholz, Christoph and Gerhardt, Fabian and Schweikardt, Nicole},
  year = {2020},
  doi = {10.1145/3385634.3385636},
  abstract = {This paper is the tutorial we wish we had had available when starting our own research on constant delay enumeration for conjunctive queries. It provides precise statements and detailed, self-contained proofs of the fundamental results in this area.},
  citationcount = {19},
  venue = {ACM SIGLOG News},
  keywords = {query}
}

@article{berkholzProbabilisticDatabasesUnder2021,
  title = {Probabilistic Databases under Updates: {{Boolean}} Query Evaluation and Ranked Enumeration},
  author = {Berkholz, Christoph and Merz, M.},
  year = {2021},
  doi = {10.1145/3452021.3458326},
  abstract = {We consider tuple-independent probabilistic databases in a dynamic setting, where tuples can be inserted or deleted. In this context we are interested in efficient data structures for maintaining the query result of Boolean as well as non-Boolean queries. For Boolean queries, we show how the known lifted inference rules can be made dynamic, so that they support single-tuple updates with only a constant number of arithmetic operations. As a consequence, we obtain that the probability of every safe UCQ can be maintained with constant update time. For non-Boolean queries, our task is to enumerate all result tuples ranked by their probability. We develop lifted inference rules for non-Boolean queries, and, based on these rules, provide a dynamic data structure that allows both log-time updates and ranked enumeration with logarithmic delay. As an application, we identify a fragment of non-repeating conjunctive queries that supports log-time updates as well as log-delay ranked enumeration. This characterisation is tight under the OMv-conjecture.},
  citationcount = {5},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {data structure,dynamic,query,update,update time}
}

@article{berkmanRecursiveStarTree1993,
  title = {Recursive Star-Tree Parallel Data Structure},
  author = {Berkman, O. and Vishkin, U.},
  year = {1993},
  doi = {10.1137/0222017},
  abstract = {This paper introduces a novel parallel data structure called the recursive star-tree (denoted ``\{\}\textsuperscript{*}-tree''). For its definition a generalization of the * functional is used (where for a function f*f(n)= \{i{\textbar}f{\textasciicircum}\{(i)\}(n){$\leq$}1\}  and f\textsuperscript{\{\vphantom\}}(i)\vphantom\{\} is the ith iterate of f). Recursive \{\}\textsuperscript{*}-trees are derived by using recursion in the spirit of the inverse Ackermann function.The recursive \{\}\textsuperscript{*}-tree data structure leads to a new design paradigm for parallel algorithms. This paradigm allows for extremely fast parallel computations, specifically, O({$\alpha$}(n)) time (where {$\alpha$}(n) is the inverse of the Ackermann function), using an optimal number of processors on the (weakest) concurrent-read, concurrent-write parallel random-access machine (CRCW PRAM).These computations need only constant time, and use an optimal number of processors if the following nonstandard assumption about the model of parallel computation is added to the CRCW PRAM: an extremely small number of processor...},
  citationcount = {199},
  venue = {SIAM journal on computing (Print)}
}

@article{bernApproximateClosestPoint1993,
  title = {Approximate Closest-Point Queries in High Dimensions},
  author = {Bern, M.},
  year = {1993},
  doi = {10.1016/0020-0190(93)90222-U},
  abstract = {No abstract available},
  citationcount = {95},
  venue = {Information Processing Letters},
  keywords = {query}
}

@article{bernardiniElasticDegenerateString2019,
  title = {Elastic-Degenerate String Matching via Fast Matrix Multiplication},
  author = {Bernardini, G. and Gawrychowski, Pawe{\l} and Pisanti, N. and Pissis, S. and Rosone, Giovanna},
  year = {2019},
  doi = {10.1137/20m1368033},
  abstract = {An elastic-degenerate (ED) string is a sequence of n sets of strings of total length N, which was recently proposed to model a set of similar sequences. The ED string matching (EDSM) problem is to find all occurrences of a pattern of length m in an ED text. An O(nm\textsuperscript{\{\vphantom\}}1.5\vphantom\{\}{\textsurd}\{m\}+N)-time algorithm for EDSM is known [Aoyama et al., CPM 2018]. The standard assumption in the prior work on this question is that N is substantially larger than both n and m, and thus we would like to have a linear dependency on the former. Under this assumption, the natural open problem is whether we can decrease the 1.5 exponent in the time complexity, similarly as in the related (but, to the best of our knowledge, not equivalent) word break problem [Backurs and Indyk, FOCS 2016]. Our starting point is a conditional lower bound for EDSM. We use the popular combinatorial Boolean Matrix Multiplication (BMM) conjecture stating that there is no truly subcubic combinatorial algorithm for BMM [Abboud and Williams, FOCS 2014]. By designing an appropriate reduction we show that a combinatorial algorithm solving the EDSM problem in O(nm\textsuperscript{\{\vphantom\}}1.5-e\vphantom\{\}+N) time, for any e{$>$}0, refutes this conjecture. Our reduction should be understood as an indication that decreasing the exponent requires fast matrix multiplication. String periodicity and fast Fourier transform are two standard tools in string algorithms. Our main technical contribution is that we successfully combine these tools with fast matrix multiplication to design a non-combinatorial O\vphantom\{\}(nm\textsuperscript{\{\vphantom\}}{$\omega$}-1\vphantom\{\}+N)-time algorithm for EDSM, where {$\omega$} denotes the matrix multiplication exponent. To the best of our knowledge, we are the first to combine these tools. In particular, using the fact that {$\omega<$}2.373 [Le Gall, ISSAC 2014; Williams, STOC 2012], we obtain an O(nm\textsuperscript{\{\vphantom\}}1.373\vphantom\{\}+N)-time algorithm for EDSM.},
  citationcount = {17},
  venue = {SIAM journal on computing (Print)},
  keywords = {lower bound,reduction}
}

@article{bernardiniEvenFasterElastic2019,
  title = {Even Faster Elastic-Degenerate String Matching via Fast Matrix Multiplication},
  author = {Bernardini, G. and Gawrychowski, Pawe{\l} and Pisanti, N. and Pissis, S. and Rosone, Giovanna},
  year = {2019},
  doi = {10.4230/LIPIcs.ICALP.2019.21},
  abstract = {An elastic-degenerate (ED) string is a sequence of n sets of strings of total length N, which was recently proposed to model a set of similar sequences. The ED string matching (EDSM) problem is to find all occurrences of a pattern of length m in an ED text. The EDSM problem has recently received some attention in the combinatorial pattern matching community, and an \{O\}(nm\textsuperscript{\{\vphantom\}}1.5\vphantom\{\}{\textsurd}\{m\}+N)-time algorithm is known [Aoyama et al., CPM 2018]. The standard assumption in the prior work on this question is that N is substantially larger than both n and m, and thus we would like to have a linear dependency on the former. Under this assumption, the natural open problem is whether we can decrease the 1.5 exponent in the time complexity, similarly as in the related (but, to the best of our knowledge, not equivalent) word break problem [Backurs and Indyk, FOCS 2016]. Our starting point is a conditional lower bound for the EDSM problem. We use the popular combinatorial Boolean matrix multiplication (BMM) conjecture stating that there is no truly subcubic combinatorial algorithm for BMM [Abboud and Williams, FOCS 2014]. By designing an appropriate reduction we show that a combinatorial algorithm solving the EDSM problem in \{O\}(nm\textsuperscript{\{\vphantom\}}1.5-{$\epsilon$}\vphantom\{\}+N) time, for any {$\epsilon>$}0, refutes this conjecture. Of course, the notion of combinatorial algorithms is not clearly defined, so our reduction should be understood as an indication that decreasing the exponent requires fast matrix multiplication. Two standard tools used in algorithms on strings are string periodicity and fast Fourier transform. Our main technical contribution is that we successfully combine these tools with fast matrix multiplication to design a non-combinatorial \{O\}(nm\textsuperscript{\{\vphantom\}}1.381\vphantom\{\}+N)-time algorithm for EDSM. To the best of our knowledge, we are the first to do so.},
  citationcount = {24},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {lower bound,reduction}
}

@article{bernsteinADeamortizationApproach2018,
  title = {A Deamortization Approach for Dynamic Spanner and Dynamic Maximal Matching},
  author = {Bernstein, A. and Forster, S. and Henzinger, Monika},
  year = {2018},
  doi = {10.1145/3469833},
  abstract = {Many dynamic graph algorithms have an amortized update time, rather than a stronger worst-case guarantee. But amortized data structures are not suitable for real-time systems, where each individual operation has to be executed quickly. For this reason, there exist many recent randomized results that aim to provide a guarantee stronger than amortized expected. The strongest possible guarantee for a randomized algorithm is that it is always correct (Las Vegas) and has high-probability worst-case update time, which gives a bound on the time for each individual operation that holds with high probability. In this article, we present the first polylogarithmic high-probability worst-case time bounds for the dynamic spanner and the dynamic maximal matching problem. (1) For dynamic spanner, the only known o(n) worst-case bounds were O(n3/4) high-probability worst-case update time for maintaining a 3-spanner and O(n5/9) for maintaining a 5-spanner. We give a O(1)k log3 (n) high-probability worst-case time bound for maintaining a (2k-1)-spanner, which yields the first worst-case polylog update time for all constant k. (All the results above maintain the optimal tradeoff of stretch 2k-1 and {\~O}(n1+1/k) edges.) (2) For dynamic maximal matching, or dynamic 2-approximate maximum matching, no algorithm with o(n) worst-case time bound was known and we present an algorithm with O(log 5 (n)) high-probability worst-case time; similar worst-case bounds existed only for maintaining a matching that was (2+{$\epsilon$})-approximate, and hence not maximal. Our results are achieved using a new approach for converting amortized guarantees to worst-case ones for randomized data structures by going through a third type of guarantee, which is a middle ground between the two above: An algorithm is said to have worst-case expected update time  if for every update {$\sigma$}, the expected time to process {$\sigma$} is at most . Although stronger than amortized expected, the worst-case expected guarantee does not resolve the fundamental problem of amortization: A worst-case expected update time of O(1) still allows for the possibility that every 1/f(n) updates requires {\textTheta} (f(n)) time to process, for arbitrarily high f(n). In this article, we present a black-box reduction that converts any data structure with worst-case expected update time into one with a high-probability worst-case update time: The query time remains the same, while the update time increases by a factor of O(log 2(n)). Thus, we achieve our results in two steps: (1) First, we show how to convert existing dynamic graph algorithms with amortized expected polylogarithmic running times into algorithms with worst-case expected polylogarithmic running times. (2) Then, we use our black-box reduction to achieve the polylogarithmic high-probability worst-case time bound. All our algorithms are Las-Vegas-type algorithms.},
  citationcount = {65},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,query,query time,reduction,update,update time}
}

@article{bernsteinAFrameworkFor2021,
  title = {A Framework for Dynamic Matching in Weighted Graphs},
  author = {Bernstein, A. and Dudeja, Aditi and Langley, Zachary},
  year = {2021},
  doi = {10.1145/3406325.3451113},
  abstract = {We introduce a new framework for computing approximate maximum weight matchings. Our primary focus is on the fully dynamic setting, where there is a large gap between the guarantees of the best known algorithms for computing weighted and unweighted matchings. Indeed, almost all current weighted matching algorithms that reduce to the unweighted problem lose a factor of two in the approximation ratio. In contrast, in other sublinear models such as the distributed and streaming models, recent work has largely closed this weighted/unweighted gap. For bipartite graphs, we almost completely settle the gap with a general reduction that converts any algorithm for {$\alpha$}-approximate unweighted matching to an algorithm for (1-){$\alpha$}-approximate weighted matching, while only increasing the update time by an O(logn) factor for constant . We also show that our framework leads to significant improvements for non-bipartite graphs, though not in the form of a universal reduction. In particular, we give two algorithms for weighted non-bipartite matching: 1. A randomized (Las Vegas) fully dynamic algorithm that maintains a (1/2-)-approximate maximum weight matching in worst-case update time O(polylog n) with high probability against an adaptive adversary. Our bounds are essentially the same as those of the unweighted algorithm of Wajc [STOC 2020]. 2. A deterministic fully dynamic algorithm that maintains a (2/3-)-approximate maximum weight matching in amortized update time O(m1/4). Our bounds are essentially the same as those of the unweighted algorithm of Bernstein and Stein [SODA 2016]. A key feature of our framework is that it uses existing algorithms for unweighted matching as black-boxes. As a result, our framework is simple and versatile. Moreover, our framework easily translates to other models, and we use it to derive new results for the weighted matching problem in streaming and communication complexity models.},
  citationcount = {25},
  venue = {Symposium on the Theory of Computing},
  keywords = {adaptive,communication,communication complexity,dynamic,reduction,update,update time}
}

@article{bernsteinDeterministicDecrementalReachability2020,
  title = {Deterministic Decremental Reachability, {{SCC}}, and Shortest Paths via Directed Expanders and Congestion Balancing},
  author = {Bernstein, A. and Gutenberg, Maximilian Probst and Saranurak, Thatchaphol},
  year = {2020},
  doi = {10.1109/FOCS46700.2020.00108},
  abstract = {Let G=(V,E,w) be a weighted, directed graph subject to a sequence of adversarial edge deletions. In the decremental single-source reachability problem (SSR), we are given a fixed source s and the goal is to maintain a data structure that can answer path-queries sv for any v{$\in$}V. In the more general single-source shortest paths (SSSP) problem the goal is to return an approximate shortest path to v, and in the SCC problem the goal is to maintain strongly connected components of G and to answer path queries within each component. All of these problems have been very actively studied over the past two decades, but all the fast algorithms are randomized and, more significantly, they can only answer path queries if they assume a weaker model: they assume an oblivious adversary which is not adaptive and must fix the update sequence in advance. This assumption significantly limits the use of these data structures, most notably preventing them from being used as subroutines in static algorithms. All the above problems are notoriously difficult in the adaptive setting. In fact, the state-of-the-art is still the Even and Shiloach tree, which dates back all the way to 1981 [1] and achieves total update time O(mn). We present the first algorithms to break through this barrier. {$\bullet$}deterministic decremental SSR/SSC with total update time mn\textsuperscript{\{\vphantom\}}2/3+o(1)\vphantom\{\} {$\bullet$}deterministic decremental SSSP with total update time n\textsuperscript{\{\vphantom\}}2+2/3+o(1)\vphantom\{\} To achieve these results, we develop two general techniques for working with dynamic graphs. The first generalizes expander-based tools to dynamic directed graphs. While these tools have already proven very successful in undirected graphs, the underlying expander decomposition they rely on does not exist in directed graphs. We thus need to develop an efficient framework for using expanders in directed graphs, as well as overcome several technical challenges in processing directed expanders. We establish several powerful primitives that we hope will pave the way for other expander-based algorithms in directed graphs. The second technique, which we call congestion balancing, provides a new method for maintaining flow under adversarial deletions. The results above use this technique to maintain an embedding of an expander. The technique is quite general, and to highlight its power, we use it to achieve the following additional result: {$\bullet$}The first near-optimal algorithm for decremental bipartite matching},
  citationcount = {45},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {adaptive,data structure,dynamic,query,static,update,update time}
}

@article{bernsteinDeterministicDecrementalSingle2016,
  title = {Deterministic Decremental Single Source Shortest Paths: Beyond the o(Mn) Bound},
  author = {Bernstein, A. and Chechik, S.},
  year = {2016},
  doi = {10.1145/2897518.2897521},
  abstract = {In this paper we consider the decremental single-source shortest paths (SSSP) problem, where given a graph G and a source node s the goal is to maintain shortest paths between s and all other nodes in G under a sequence of online adversarial edge deletions. In their seminal work, Even and Shiloach [JACM 1981] presented an exact solution to the problem with only O(mn) total update time over all edge deletions. Their classic algorithm was the best known result for the decremental SSSP problem for three decades, even when approximate shortest paths are allowed. The first improvement over the Even-Shiloach algorithm was given by Bernstein and Roditty [SODA 2011], who for the case of an unweighted and undirected graph presented an approximate (1+) algorithm with constant query time and a total update time of O(n2+O(1/{\textsurd}logn)). This work triggered a series of new results, culminating in a recent breakthrough of Henzinger, Krinninger and Nanongkai [FOCS 14], who presented a -approximate algorithm whose total update time is near linear O(m1+ O(1/{\textsurd}logn)). In this paper they posed as a major open problem the question of derandomizing their result. In fact, all known improvements over the Even-Shiloach algorithm are randomized. All these algorithms maintain some truncated shortest path trees from a small subset of nodes. While in the randomized setting it is possible to ``hide'' these nodes from the adversary, in the deterministic setting this is impossible: the adversary can delete all edges touching these nodes, thus forcing the algorithm to choose a new set of nodes and incur a new computation of shortest paths. In this paper we present the first deterministic decremental SSSP algorithm that breaks the Even-Shiloach bound of O(mn) total update time, for unweighted and undirected graphs. Our algorithm is (1 + {\cyrchar\cyrie}) approximate and achieves a total update time of {\~O}(n2). Our algorithm can also achieve the same bounds in the incremental setting. It is worth mentioning that for dense instances where m = {\textohm}(n2 - 1/{\textsurd}log(n)), our algorithm is also faster than all existing randomized algorithms.},
  citationcount = {57},
  venue = {Symposium on the Theory of Computing},
  keywords = {query,query time,update,update time}
}

@article{bernsteinDeterministicDecrementalSssp2021,
  title = {Deterministic Decremental {{SSSP}} and Approximate Min-Cost Flow in Almost-Linear Time},
  author = {Bernstein, A. and Gutenberg, Maximilian Probst and Saranurak, Thatchaphol},
  year = {2021},
  doi = {10.1109/FOCS52979.2021.00100},
  abstract = {In the decremental single-source shortest paths problem, the goal is to maintain distances from a fixed source s to every vertex v in an m-edge graph undergoing edge deletions. In this paper, we conclude a long line of research on this problem by showing a near-optimal deterministic data structure that maintains (1 + E) -approximate distance estimates and runs in m1+o(1)total update time. Our result, in particular, removes the oblivious adversary assumption required by the previous breakthrough result by Henzinger et al. [FOCS'14], which leads to our second result: the first almost-linear time algorithm for (1 - E) -approximate min-cost flow in undirected graphs where capacities and costs can be taken over edges and vertices. Previously, algorithms for max flow with vertex capacities, or min-cost flow with any capacities required super-linear time. Our result essentially completes the picture for approximate flow in undirected graphs. The key technique of the first result is a novel framework that allows us to treat low-diameter graphs like expanders. This allows us to harness expander properties while bypassing shortcomings of expander decomposition, which almost all previous expander-based algorithms needed to deal with. For the second result, we break the notorious flow-decomposition barrier from the multiplicative-weight-update framework using randomization.},
  citationcount = {48},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,update,update time}
}

@article{bernsteinDeterministicPartiallyDynamic2017,
  title = {Deterministic Partially Dynamic Single Source Shortest Paths in Weighted Graphs},
  author = {Bernstein, A.},
  year = {2017},
  doi = {10.4230/LIPIcs.ICALP.2017.44},
  abstract = {In this paper we consider the decremental single-source shortest paths (SSSP) problem, where given a graph G and a source node s the goal is to maintain shortest distances between s and all other nodes in G under a sequence of online adversarial edge deletions. In their seminal work, Even and Shiloach [JACM 1981] presented an exact solution to the problem in unweighted graphs with only O(mn) total update time over all edge deletions. Their classic algorithm was the state of the art for the decremental SSSP problem for three decades, even when approximate shortest paths are allowed. A series of results showed how to improve upon O(mn) if approximation is allowed, culminating in a recent breakthrough of Henzinger, Krinninger and Nanongkai [FOCS 14], who presented a (1+{$\epsilon$})-approximate algorithm for undirected weighted graphs whose total update time is near linear: O(m\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}(W)), where W is the ratio of the heaviest to the lightest edge weight in the graph. In this paper they posed as a major open problem the question of derandomizing their result. Until very recently, all known improvements over the Even-Shiloach algorithm were randomized and required the assumption of a non-adaptive adversary. In STOC 2016, Bernstein and Chechik showed the first \{deterministic\} algorithm to go beyond O(mn) total update time: the algorithm is also (1+{$\epsilon$})-approximate, and has total update time O\vphantom\{\}(n{$^2$}). In SODA 2017, the same authors presented an algorithm with total update time O\vphantom\{\}(mn\textsuperscript{\{\vphantom\}}3/4\vphantom\{\}). However, both algorithms are restricted to undirected, unweighted graphs. We present the \{first\} deterministic algorithm for \{weighted\} undirected graphs to go beyond the O(mn) bound. The total update time is O\vphantom\{\}(n{$^2$}(W)).},
  citationcount = {31},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {dynamic,non-adaptive,update,update time}
}

@article{bernsteinDeterministicPartiallyDynamic2017,
  title = {Deterministic Partially Dynamic Single Source Shortest Paths for Sparse Graphs},
  author = {Bernstein, A. and Chechik, S.},
  year = {2017},
  doi = {10.5555/3039686.3039715},
  abstract = {In this paper we consider the decremental single-source shortest paths (SSSP) problem, where given a graph G and a source node s the goal is to maintain shortest paths between s and all other nodes in G under a sequence of online adversarial edge deletions. (Our algorithm can also be modified to work in the incremental setting, where the graph is initially empty and subject to a sequence of online adversarial edge insertions.)In their seminal work, Even and Shiloach [JACM 1981] presented an exact solution to the problem with only O(mn) total update time over all edge deletions. Later papers presented conditional lower bounds showing that O(mn) is optimal up to log factors.In SODA 2011, Bernstein and Roditty showed how to bypass these lower bounds and improve upon the Even and Shiloach O(mn) total update time bound by allowing a (1 + {$\epsilon$}) approximation. This triggered a series of new results, culminating in a recent breakthrough of Henzinger, Krinninger and Nanongkai [FOCS 14], who presented a (1 + {$\epsilon$})-approximate algorithm whose total update time is near linear: [EQUATION].However, every single one of these improvements over the Even-Shiloach algorithm was randomized and assumed a non-adaptive adversary. This additional assumption meant that the algorithms were not suitable for certain settings and could not be used as a black box data structure. Very recently Bernstein and Chechik presented in STOC 2016 the first deterministic improvement over Even and Shiloach, that did not rely on randomization or assumptions about the adversary: in an undirected unweighted graph the algorithm maintains (1+{$\epsilon$})-approximate distances and has total update time O(n2).In this paper, we present a new deterministic algorithm for the problem with total update time [EQUATION]: it returns a (1 + {$\epsilon$}) approximation, and is limited to undirected unweighted graphs. Although this result is still far from matching the randomized near-linear total update time, it presents important progress towards that direction, because unlike the STOC 2016 O(n2) algorithm it beats the Even and Shiloach O(mn) bound for all graphs, not just sufficiently dense ones. In particular, the O(n2) algorithm relied entirely on a new sparsification technique, and so could not hope to yield an improvement for sparse graphs. We present the first deterministic improvement for sparse graphs by significantly extending some of the ideas from the O(n2) algorithm and combining them with the hop-set technique used in several earlier dynamic shortest path papers.Also, because decremental single source shortest paths is often used as a building block for fully dynamic all pairs shortest paths, using our new algorithm as a black box yields new deterministic algorithms for fully dynamic approximate all pairs shortest paths.},
  citationcount = {37},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,lower bound,non-adaptive,update,update time}
}

@article{bernsteinDynamicAlgorithmsFor2016,
  title = {Dynamic Algorithms for Shortest Paths and Matching},
  author = {Bernstein, A.},
  year = {2016},
  doi = {10.7916/D8QF8T2W},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {No venue available},
  keywords = {dynamic}
}

@article{bernsteinFasterFullyDynamic2016,
  title = {Faster Fully Dynamic Matchings with Small Approximation Ratios},
  author = {Bernstein, A. and Stein, Cliff},
  year = {2016},
  doi = {10.1137/1.9781611974331.CH50},
  abstract = {Maximum cardinality matching is a fundamental algorithmic problem with many algorithms and applications. The fully dynamic version, in which edges are inserted and deleted over time has also been the subject of much attention. Existing algorithms for dynamic matching (in general n-vertex m-edge graphs) fall into two groups: there are fast (mostly randomized) algorithms that achieve a 2-approximation or worse, and there are slow algorithms with {\textohm}([EQUATION]) update time that achieve a better-than-2 approximation. Thus the obvious question is whether we can design an algorithm that achieves a tradeoff between these two: a o([EQUATION]) update time and a better-than-2 approximation simultaneously. We answer this question in the affirmative. Previously, such bounds were only known for the special case of bipartite graphs. Our main result is a fully dynamic deterministic algorithm that maintains a (3/2 + e)-approximation in amortized update time O(m1/4e--2.5). In addition to achieving the trade-off described above, our algorithm manages to be polynomially faster than all existing deterministic algorithms (excluding an existing log n-approximation of Onak and Rubinfeld), while still maintaining a better-than-2 approximation. We also give stronger results for graphs whose arboricity is at most {$\alpha$}. We show how to maintain a (1 + e)-approximate fractional matching or a (3/2 + e)-approximate integral matching in worst-case time O({$\alpha$}({$\alpha$} + log n)) for constant e. When the arboricity is constant, this bound is O(log n) and when the arboricity is polylogarithmic the update time is also polylogarithmic. Previous results for small arboricity non-bipartite graphs could only maintain a maximal matching (2-approximation). We maintain the approximate matching without explicitly using augmenting paths. We define an intermediate graph, called an EDCS and show that the EDCS H contains a large matching, and show how to maintain an EDCS in G. The EDCS was used in previous works on bipartite graphs, however the details and proofs are completely different in general graphs. The algorithm for bipartite graphs relies on ideas from flows and cuts to non-constructively prove the existence of a good matching in H, but these ideas do not seem to extend to non-bipartite graphs. In this paper we instead explicitly construct a large fractional matching in H. In some cases we can guarantee that this fractional matching is {$\gamma$}-restricted, which means that it only uses values either in the range [0, {$\gamma$}] or 1. We then combine this matching with a new structural property of maximum matchings in non-bipartite graphs, which is analogous to the cut induced by maximum matchings in bipartite graphs.},
  citationcount = {114},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,update,update time}
}

@article{bernsteinMaintainingShortestPaths2013,
  title = {Maintaining Shortest Paths under Deletions in Weighted Directed Graphs: [Extended Abstract]},
  author = {Bernstein, A.},
  year = {2013},
  doi = {10.1145/2488608.2488701},
  abstract = {We present an improved algorithm for maintaining all-pairs 1 + {$\varepsilon$} approximate shortest paths under deletions and weight-increases. The previous state of the art for this problem was total update time  O (n2{\textsurd}m/{$\varepsilon$}) for directed, unweighted graphs [2], and  O(mn/{$\varepsilon$}) for undirected, unweighted graphs [12]. Both algorithms were randomized and had constant query time. Note that  O(mn) is a natural barrier because even with a (1 + {$\varepsilon$}) approximation, there is no o(mn) combinatorial algorithm for the static all-pairs shortest path problem. Our algorithm works on directed, weighted graphs and has total (randomized) update time  O (mn log(R)/{$\varepsilon$}) where R is the ratio of the largest edge weight ever seen in the graph, to the smallest such weight (our query time is constant). Note that log(R) = O(log(n)) as long as weights are polynomial in n. Although  O(mn log(R)/{$\varepsilon$}) is the total time over all updates, our algorithm also requires a clearly unavoidable constant time per update. Thus, we effectively expand the  O(mn) total update time bound from undirected, unweighted graphs to directed graphs with polynomial weights. This is in fact the first non-trivial algorithm for decremental all-pairs shortest paths that works on weighted graphs (previous algorithms could only handle small integer weights). By a well known reduction from decremental algorithms to fully dynamic ones [9], our improved decremental algorithm leads to improved query-update tradeoffs for fully dynamic (1 + {$\varepsilon$}) approximate APSP algorithm in directed graphs.},
  citationcount = {29},
  venue = {Symposium on the Theory of Computing}
}

@article{bernsteinMaintainingShortestPaths2016,
  title = {Maintaining Shortest Paths under Deletions in Weighted Directed Graphs},
  author = {Bernstein, A.},
  year = {2016},
  doi = {10.1137/130938670},
  abstract = {We present an improved algorithm for maintaining all-pairs (1+{$\epsilon$}) approximate shortest paths under deletions and weight-increases. The previous state of the art for this problem is total update time O\vphantom\{\}(n{$^2$}{\textsurd}\{m\}/{$\epsilon$}) over all updates for directed unweighted graphs [S. Baswana, R. Hariharan, and S. Sen, J. Algorithms, 62 (2007), pp. 74--92], and O\vphantom\{\}(mn/{$\epsilon$}) for undirected unweighted graphs [L. Roditty and U. Zwick, in Proceedings of the 45th FOCS, Rome, Italy, 2004, pp. 499--508]. Both algorithms are randomized and have constant query time. Very recently, Henzinger, Krinninger, and Nanongkai presented a deterministic version of the latter algorithm [M. Henzinger, S. Krinninger, and D. Nanongkai, in IEEE FOCS, 2013, pp. 538--547]. Note that O\vphantom\{\}(mn) is a natural barrier because even with a (1+{$\epsilon$}) approximation, there is no o(mn) combinatorial algorithm for the static all-pairs shortest path problem. Our algorithm works on directed weighte...},
  citationcount = {68},
  venue = {SIAM journal on computing (Print)}
}

@article{bernsteinMatchingCompositionAnd2024,
  title = {Matching Composition and Efficient Weight Reduction in Dynamic Matching},
  author = {Bernstein, Aaron and Chen, Jiale and Dudeja, Aditi and Langley, Zachary and Sidford, Aaron and Tu, Ta-Wei},
  year = {2024},
  doi = {10.48550/arXiv.2410.18936},
  abstract = {We consider the foundational problem of maintaining a (1-{$\varepsilon$})-approximate maximum weight matching (MWM) in an n-node dynamic graph undergoing edge insertions and deletions. We provide a general reduction that reduces the problem on graphs with a weight range of \{poly\}(n) to \{poly\}(1/{$\varepsilon$}) at the cost of just an additive \{poly\}(1/{$\varepsilon$}) in update time. This improves upon the prior reduction of Gupta-Peng (FOCS 2013) which reduces the problem to a weight range of {$\varepsilon$}\textsuperscript{\{\vphantom\}}-O(1/{$\varepsilon$})\vphantom\{\} with a multiplicative cost of O(n). When combined with a reduction of Bernstein-Dudeja-Langley (STOC 2021) this yields a reduction from dynamic (1-{$\varepsilon$})-approximate MWM in bipartite graphs with a weight range of \{poly\}(n) to dynamic (1-{$\varepsilon$})-approximate maximum cardinality matching in bipartite graphs at the cost of a multiplicative \{poly\}(1/{$\varepsilon$}) in update time, thereby resolving an open problem in [GP'13; BDL'21]. Additionally, we show that our approach is amenable to MWM problems in streaming, shared-memory work-depth, and massively parallel computation models. We also apply our techniques to obtain an efficient dynamic algorithm for rounding weighted fractional matchings in general graphs. Underlying our framework is a new structural result about MWM that we call the"matching composition lemma"and new dynamic matching subroutines that may be of independent interest.},
  citationcount = {1},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,reduction,update,update time}
}

@article{bernsteinNearOptimalDecremental2020,
  title = {Near-Optimal Decremental {{SSSP}} in Dense Weighted Digraphs},
  author = {Bernstein, A. and Gutenberg, Maximilian Probst and {Wulff-Nilsen}, Christian},
  year = {2020},
  doi = {10.1109/FOCS46700.2020.00107},
  abstract = {In the decremental Single-Source Shortest Path problem (SSSP), we are given a weighted directed graph G=(V,E,w) undergoing edge deletions and a source vertex r{$\in$}V; let n={\textbar}V{\textbar},m={\textbar}E{\textbar} and W be the aspect ratio of the graph. The goal is to obtain a data structure that maintains shortest paths from r to all vertices in V and can answer distance queries in O(1) time, as well as return the corresponding path P in O({\textbar}P{\textbar}) time. This problem was first considered by Even and Shiloach [JACM'81], who provided an algorithm with total update time O(mn) for unweighted undirected graphs; this was later extended to directed weighted graphs [FOCS'95, STOC'99]. There are conditional lower bounds showing that O(mn) is in fact near-optimal [ESA'04, FOCS'14, STOC'15, STOC'20]. In a breakthrough result, Forster et al. showed that total update time  m{\textasciicircum}\{7/6\}n{\textasciicircum}\{2/3+o(1)\},m{\textasciicircum}\{3/4\}n{\textasciicircum}\{5/4+o(1)\} \{polylog\}(W)=mn\textsuperscript{\{\vphantom\}}0.9+o(1)\vphantom\{\}\{polylog\}(W), is possible if the algorithm is allowed to return (1+{$\epsilon$})-approximate paths, instead of exact ones [STOC'14, ICALP'15]. No further progress was made until Probst Gutenberg and Wulff-Nilsen [SODA'20] provided a new approach for the problem, which yields total time O\vphantom\{\}( m{\textasciicircum}\{2/3\}n{\textasciicircum}\{4/3\}W,(mn){\textasciicircum}\{7/8\}W )=O\vphantom\{\}( n{\textasciicircum}\{8/3\}W,mn{\textasciicircum}\{3/4\}W ). Our result builds on this recent approach, but overcomes its limitations by introducing a significantly more powerful abstraction, as well as a different core subroutine. Our new framework yields a decremental (1+{$\epsilon$})-approximate SSSP data structure with total update time O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2\vphantom\{\}\textsuperscript{\{\vphantom\}}4\vphantom\{\}W/{$\epsilon$}). Our algorithm is thus near-optimal for dense graphs with polynomial edge-weights. Our framework can also be applied to sparse graphs to obtain total update time O\vphantom\{\}(mn\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}\textsuperscript{\{\vphantom\}}3\vphantom\{\}W/{$\epsilon$}). Combined, these data structures dominate all previous results. Like all previous o(mn) algorithms that can return a path (not just a distance estimate), our result is randomized and assumes an oblivious adversary. Our framework effectively allows us to reduce SSSP in general graphs to the same problem in directed acyclic graphs (DAGs). We believe that our framework has significant potential to influence future work on directed SSSP, both in the dynamic model and in others.},
  citationcount = {26},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,dynamic,lower bound,query,update,update time}
}

@article{bernsteinNegativeWeightSingle2022,
  title = {Negative-Weight Single-Source Shortest Paths in near-Linear Time},
  author = {Bernstein, A. and Nanongkai, Danupon and {Wulff-Nilsen}, Christian},
  year = {2022},
  doi = {10.1109/FOCS54457.2022.00063},
  abstract = {We present a randomized algorithm that computes single-source shortest paths (SSSP) in O(m\textsuperscript{\{\vphantom\}}8\vphantom\{\}(n)W) time when edge weights are integral and can be negative. 1 This essentially resolves the classic negative-weight SSSP problem. The previous bounds are O\vphantom\{\}((m+n\textsuperscript{\{\vphantom\}}1.5\vphantom\{\})W) [BLNPSSSW FOCS'20] and m\textsuperscript{\{\vphantom\}}4/3+o(1)\vphantom\{\}W [AMV FOCS'20]. Near-linear time algorithms were known previously only for the special case of planar directed graphs [Fakcharoenphol and Rao FOCS'01]. In contrast to all recent developments that rely on sophisticated continuous optimization methods and dynamic algorithms, our algorithm is simple: it requires only a simple graph decomposition and elementary combinatorial tools. In fact, ours is the first combinatorial algorithm for negative-weight SSSP to break through the classic O(m{\textsurd}\{n\}W) bound from over three decades ago [Gabow and Tarjan SICOMP'89].},
  citationcount = {33},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{beyneOutOfOddity2020,
  title = {Out of Oddity - New Cryptanalytic Techniques against Symmetric Primitives Optimized for Integrity Proof Systems},
  author = {Beyne, Tim and Canteaut, A. and Dinur, Itai and Eichlseder, Maria and Leander, G. and Leurent, G. and {Naya-Plasencia}, M. and Perrin, L{\'e}o and Sasaki, Yu and Todo, Yosuke and Wiemer, Friedrich},
  year = {2020},
  doi = {10.1007/978-3-030-56877-1_11},
  abstract = {No abstract available},
  citationcount = {70},
  venue = {IACR Cryptology ePrint Archive}
}

@article{bhargavaFastAlgebraicMultivariate2021,
  title = {Fast, Algebraic Multivariate Multipoint Evaluation in Small Characteristic and Applications},
  author = {Bhargava, Vishwas and Ghosh, Sumanta K and Kumar, Mrinal and Mohapatra, C. K.},
  year = {2021},
  doi = {10.1145/3519935.3519968},
  abstract = {Multipoint evaluation is the computational task of evaluating a polynomial given as a list of coefficients at a given set of inputs. Besides being a natural and fundamental question in computer algebra on its own, fast algorithms for this problem are also closely related to fast algorithms for other natural algebraic questions like polynomial factorization and modular composition. And while nearly linear time algorithms have been known for the univariate instance of multipoint evaluation for close to five decades due to a work of Borodin and Moenck, fast algorithms for the multivariate version have been much harder to come by. In a significant improvement to the state of art for this problem, Umans and Kedlaya \& Umans gave nearly linear time algorithms for this problem over field of small characteristic and over all finite fields respectively, provided that the number of variables n is at most do(1) where the degree of the input polynomial in every variable is less than d. They also stated the question of designing fast algorithms for the large variable case (i.e. n {$\notin$} do(1)) as an open problem. In this work, we show that there is a deterministic algorithm for multivariate multipoint evaluation over a field Fq of characteristic p which evaluates an n-variate polynomial of degree less than d in each variable on N inputs in time (N + dn)1 + o(1)(logq, d, n, p) provided that p is at most do(1), and q is at most (exp({$\cdots$} (exp(d)))), where the height of this tower of exponentials is fixed. When the number of variables is large (e.g. n {$\notin$} do(1)), this is the first nearly linear time algorithm for this problem over any (large enough) field. Our algorithm is based on elementary algebraic ideas and this algebraic structure naturally leads to the following two independently interesting applications. We show that there is an algebraic data structure for univariate polynomial evaluation with nearly linear space complexity and sublinear time complexity over finite fields of small characteristic and quasipolynomially bounded size. This provides a counterexample to a conjecture of Milterson who conjectured that over small finite fields, any algebraic data structure for polynomial evaluation using polynomial space must have linear query complexity. We also show that over finite fields of small characteristic and quasipolynomially bounded size, Vandermonde matrices are not rigid enough to yield size-depth tradeoffs for linear circuits via the current quantitative bounds in Valiant's program. More precisely, for every fixed prime p, we show that for every constant {\cyrchar\cyrie} {\textquestiondown} 0, and large enough n, the rank of any n {\texttimes} n Vandermonde matrix V over the field pa can be reduced to (n/exp({\textohm}(({\cyrchar\cyrie}){\textsurd}logn))) by changing at most n{$\Theta$}({\cyrchar\cyrie}) entries in every row of V, provided a {$\leq$} (logn). Prior to this work, similar upper bounds on rigidity were known only for special Vandermonde matrices. For instance, the Discrete Fourier Transform matrices and Vandermonde matrices with generators in a geometric progression.},
  citationcount = {14},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {data structure,query,query complexity}
}

@article{bhargavaFastAlgebraicMultivariate2023,
  title = {Fast, Algebraic Multivariate Multipoint Evaluation in Small Characteristic and Applications},
  author = {Bhargava, Vishwas and Ghosh, Sumanta and Kumar, Mrinal and Mohapatra, C. K.},
  year = {2023},
  doi = {10.1145/3625226},
  abstract = {Multipoint evaluation is the computational task of evaluating a polynomial given as a list of coefficients at a given set of inputs. Besides being a natural and fundamental question in computer algebra on its own, fast algorithms for this problem are also closely related to fast algorithms for other natural algebraic questions such as polynomial factorization and modular composition. And while nearly linear time algorithms have been known for the univariate instance of multipoint evaluation for close to five decades due to a work of Borodin and Moenck [7], fast algorithms for the multivariate version have been much harder to come by. In a significant improvement to the state-of-the-art for this problem, Umans [25] and Kedlaya \& Umans [16] gave nearly linear time algorithms for this problem over field of small characteristic and over all finite fields, respectively, provided that the number of variables n is at most d\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} where the degree of the input polynomial in every variable is less than d. They also stated the question of designing fast algorithms for the large variable case (i.e., n{$\notin$}d\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} ) as an open problem. In this work, we show that there is a deterministic algorithm for multivariate multipoint evaluation over a field \{F\}\textsubscript{\{\vphantom\}}q\vphantom\{\} of characteristic p, which evaluates an n-variate polynomial of degree less than d in each variable on N inputs in time \{equation*\}((N+d{$^n$})\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}\{poly\}(q,d,n,p)),\{equation*\} provided that p is at most do(1), and q is at most (exp (exp (exp (...(exp (d))))), where the height of this tower of exponentials is fixed. When the number of variables is large (e.g., n {$\notin$} do(1)), this is the first nearly linear time algorithm for this problem over any (large enough) field. Our algorithm is based on elementary algebraic ideas, and this algebraic structure naturally leads to the following two independently interesting applications: --- We show that there is an algebraic data structure for univariate polynomial evaluation with nearly linear space complexity and sublinear time complexity over finite fields of small characteristic and quasipolynomially bounded size. This provides a counterexample to a conjecture of Miltersen [21] who conjectured that over small finite fields, any algebraic data structure for polynomial evaluation using polynomial space must have linear query complexity. --- We also show that over finite fields of small characteristic and quasipolynomially bounded size, Vandermonde matrices are not rigid enough to yield size-depth tradeoffs for linear circuits via the current quantitative bounds in Valiant's program [26]. More precisely, for every fixed prime p, we show that for every constant {$\varepsilon$} {\textquestiondown} 0, and large enough n, the rank of any n{\texttimes}n Vandermonde matrix V over the field \{F\}\textsubscript{\{\vphantom\}}p\textsuperscript{a}\vphantom\{\} can be reduced to (n/exp ({\textohm} (poly({$\varepsilon$})log0.53 n))) by changing at most n{$\Theta$} ({$\varepsilon$}) entries in every row of V, provided a {$\leq$} poly(log n). Prior to this work, similar upper bounds on rigidity were known only for special Vandermonde matrices. For instance, the Discrete Fourier Transform matrices and Vandermonde matrices with generators in a geometric progression [9].},
  citationcount = {3},
  venue = {Journal of the ACM},
  keywords = {data structure,query,query complexity}
}

@article{bhattacharyaANewDeterministic2019,
  title = {A New Deterministic Algorithm for Dynamic Set Cover},
  author = {Bhattacharya, Sayan and Henzinger, Monika and Nanongkai, Danupon},
  year = {2019},
  doi = {10.1109/FOCS.2019.00033},
  abstract = {We present a deterministic dynamic algorithm for maintaining a (1+{$\varepsilon$})f-approximate minimum cost set cover with O(f log(Cn)/{$\varepsilon$}{\textasciicircum}2) amortized update time, when the input set system is undergoing element insertions and deletions. Here, n denotes the number of elements, each element appears in at most f sets, and the cost of each set lies in the range [1/C, 1]. Our result, together with that of Gupta et al. [STOC'17], implies that there is a deterministic algorithm for this problem with O(f log(Cn)) amortized update time and O(min(log n, f)) -approximation ratio, which nearly matches the polynomial-time hardness of approximation for minimum set cover in the static setting. Our update time is only O(log (Cn)) away from a trivial lower bound. Prior to our work, the previous best approximation ratio guaranteed by deterministic algorithms was O(f{\textasciicircum}2), which was due to Bhattacharya et al. [ICALP`15]. In contrast, the only result that guaranteed O(f) -approximation was obtained very recently by Abboud et al. [STOC`19], who designed a dynamic algorithm with (1+{$\varepsilon$})f-approximation ratio and O(f{\textasciicircum}2 log n/{$\varepsilon$}) amortized update time. Besides the extra O(f) factor in the update time compared to our and Gupta et al.'s results, the Abboud et al. algorithm is randomized, and works only when the adversary is oblivious and the sets are unweighted (each set has the same cost). We achieve our result via the primal-dual approach, by maintaining a fractional packing solution as a dual certificate. This approach was pursued previously by Bhattacharya et al. and Gupta et al., but not in the recent paper by Abboud et al. Unlike previous primal-dual algorithms that try to satisfy some local constraints for individual sets at all time, our algorithm basically waits until the dual solution changes significantly globally, and fixes the solution only where the fix is needed.},
  citationcount = {22},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,lower bound,static,update,update time}
}

@article{bhattacharyaAnImprovedAlgorithm2018,
  title = {An Improved Algorithm for Incremental Cycle Detection and Topological Ordering in Sparse Graphs},
  author = {Bhattacharya, Sayan and Kulkarni, Janardhan},
  year = {2018},
  doi = {10.1137/1.9781611975994.153},
  abstract = {We consider the problem of incremental cycle detection and topological ordering in a directed graph G=(V,E) with {\textbar}V{\textbar}=n nodes. In this setting, initially the edge-set E of the graph is empty. Subsequently, at each time-step an edge gets inserted into G. After every edge-insertion, we have to report if the current graph contains a cycle, and as long as the graph remains acyclic, we have to maintain a topological ordering of the node-set V. Let m be the total number of edges that get inserted into G. We present a randomized algorithm for this problem with O\vphantom\{\}(m\textsuperscript{\{\vphantom\}}4/3\vphantom\{\}) total expected update time.},
  citationcount = {15},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {update,update time}
}

@article{bhattacharyaCoarseGrainedComplexity2020,
  title = {Coarse-Grained Complexity for Dynamic Algorithms},
  author = {Bhattacharya, Sayan and Nanongkai, Danupon and Saranurak, Thatchaphol},
  year = {2020},
  doi = {10.1137/1.9781611975994.29},
  abstract = {To date, the only way to argue polynomial lower bounds for dynamic algorithms is via fine-grained complexity arguments. These arguments rely on strong assumptions about specific problems such as the Strong Exponential Time Hypothesis (SETH) and the Online Matrix-Vector Multiplication Conjecture (OMv). While they have led to many exciting discoveries, dynamic algorithms still miss out some benefits and lessons from the traditional "coarse-grained" approach that relates together classes of problems such as P and NP. In this paper we initiate the study of coarse-grained complexity theory for dynamic algorithms. Below are among questions that this theory can answer. What if dynamic Orthogonal Vector (OV) is easy in the cell-probe model? A research program for proving polynomial unconditional lower bounds for dynamic OV in the cell-probe model is motivated by the fact that many conditional lower bounds can be shown via reductions from the dynamic OV problem (e.g. [Abboud, V.-Williams, FOCS 2014]). Since the cell-probe model is more powerful than word RAM and has historically allowed smaller upper bounds (e.g. [Larsen, Williams, SODA 2017; Chakraborty, Kamma, Larsen, STOC 2018]), it might turn out that dynamic OV is easy in the cell-probe model, making this research direction infeasible. Our theory implies that if this is the case, there will be very interesting algorithmic consequences: If dynamic OV can be maintained in polylogarithmic worst-case update time in the cell-probe model, then so are several important dynamic problems such as k-edge connectivity, (1 + {$\epsilon$})-approximate mincut, (1 + {$\epsilon$})-approximate matching, planar nearest neighbors, Chan's subset union and 3-vs-4 diameter. The same conclusion can be made when we replace dynamic OV by, e.g., subgraph connectivity, single source reachability, Chan's subset union, and 3-vs-4 diameter. Lower bounds for k-edge connectivity via dynamic OV? The ubiquity of reductions from dynamic OV raises a question whether we can prove conditional lower bounds for, e.g., k-edge connectivity, approximate mincut, and approximate matching, via the same approach. Our theory provides a method to refute such possibility (the so-called non-reducibility). In particular, we show that there are no "efficient" reductions (in both cell-probe and word RAM models) from dynamic OV to k-edge connectivity under an assumption about the classes of dynamic algorithms whose analogue in the static setting is widely believed. We are not aware of any existing assumptions that can play the same role. (The NSETH of Carmosino et al. [ITCS 2016] is the closest one, but is not enough.) To show similar results for other problems, one only need to develop efficient randomized verification protocols for such problems.},
  citationcount = {Unknown},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,dynamic,lower bound,reduction,sorted,static,update,update time}
}

@article{bhattacharyaDeterministicDynamicMatching2019,
  title = {Deterministic Dynamic Matching in {{O}}(1) Update Time},
  author = {Bhattacharya, Sayan and Chakrabarty, Deeparnab and Henzinger, Monika},
  year = {2019},
  doi = {10.1007/s00453-019-00630-4},
  abstract = {No abstract available},
  citationcount = {10},
  venue = {Algorithmica},
  keywords = {dynamic,update,update time}
}

@article{bhattacharyaDeterministicFullyDynamic2016,
  title = {Deterministic Fully Dynamic Approximate Vertex Cover and Fractional Matching in {{O}}(1) Amortized Update Time},
  author = {Bhattacharya, Sayan and Chakrabarty, Deeparnab and Henzinger, Monika},
  year = {2016},
  doi = {10.1007/978-3-319-59250-3_8},
  abstract = {No abstract available},
  citationcount = {48},
  venue = {Conference on Integer Programming and Combinatorial Optimization},
  keywords = {dynamic,update,update time}
}

@article{bhattacharyaDeterministicRoundingOf2021,
  title = {Deterministic Rounding of Dynamic Fractional Matchings},
  author = {Bhattacharya, Sayan and Kiss, P.},
  year = {2021},
  doi = {10.4230/LIPIcs.ICALP.2021.27},
  abstract = {We present a framework for deterministically rounding a dynamic fractional matching. Applying our framework in a black-box manner on top of existing fractional matching algorithms, we derive the following new results: (1) The first deterministic algorithm for maintaining a (2-{$\delta$})-approximate maximum matching in a fully dynamic bipartite graph, in arbitrarily small polynomial update time. (2) The first deterministic algorithm for maintaining a (1+{$\delta$})-approximate maximum matching in a decremental bipartite graph, in polylogarithmic update time. (3) The first deterministic algorithm for maintaining a (2+{$\delta$})-approximate maximum matching in a fully dynamic general graph, in small polylogarithmic (specifically, O({$^4$}n)) update time. These results are respectively obtained by applying our framework on top of the fractional matching algorithms of Bhattacharya et al. [STOC'16], Bernstein et al. [FOCS'20], and Bhattacharya and Kulkarni [SODA'19]. Prior to our work, there were two known general-purpose rounding schemes for dynamic fractional matchings. Both these schemes, by Arar et al. [ICALP'18] and Wajc [STOC'20], were randomized. Our rounding scheme works by maintaining a good \{\vphantom\}\emph{matching-sparsifier}\vphantom\{\}\emph{ with bounded arboricity, and then applying the algorithm of Peleg and Solomon [SODA'16] to maintain a near-optimal matching in this low arboricity graph. To the best of our knowledge, this is the first dynamic matching algorithm that works on general graphs by using an algorithm for low-arboricity graphs as a black-box subroutine. This feature of our rounding scheme might be of independent interest.}},
  citationcount = {23},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {dynamic,update,update time}
}

@article{bhattacharyaDynamicAlgorithmsFor2017,
  title = {Dynamic Algorithms for Graph Coloring},
  author = {Bhattacharya, Sayan and Chakrabarty, Deeparnab and Henzinger, Monika and Nanongkai, Danupon},
  year = {2017},
  doi = {10.1137/1.9781611975031.1},
  abstract = {We design fast dynamic algorithms for proper vertex and edge colorings in a graph undergoing edge insertions and deletions. In the static setting, there are simple linear time algorithms for ({$\Delta$}+1)- vertex coloring and (2{$\Delta$}-1)-edge coloring in a graph with maximum degree {$\Delta$}. It is natural to ask if we can efficiently maintain such colorings in the dynamic setting as well. We get the following three results. (1) We present a randomized algorithm which maintains a ({$\Delta$}+1)-vertex coloring with O({$\Delta$}) expected amortized update time. (2) We present a deterministic algorithm which maintains a (1+o(1)){$\Delta$}-vertex coloring with O(\{poly\}{$\Delta$}) amortized update time. (3) We present a simple, deterministic algorithm which maintains a (2{$\Delta$}-1)-edge coloring with O({$\Delta$}) worst-case update time. This improves the recent O({$\Delta$})-edge coloring algorithm with O\vphantom\{\}({\textsurd}\{{$\Delta$}\}) worst-case update time by Barenboim and Maimon.},
  citationcount = {70},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,static,update,update time}
}

@article{bhattacharyaDynamicMatchingWith2022,
  title = {Dynamic Matching with Better-than-2 Approximation in Polylogarithmic Update Time},
  author = {Bhattacharya, Sayan and Kiss, P. and Saranurak, Thatchaphol and Wajc, David},
  year = {2022},
  doi = {10.48550/arXiv.2207.07438},
  abstract = {We present dynamic algorithms with polylogarithmic update time for estimating the size of the maximum matching of a graph undergoing edge insertions and deletions with approximation ratio strictly better than 2 . Specifically, we obtain a 1+\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{{\textsurd}\{2\}\}+{$\epsilon\approx$}1.707+{$\epsilon$} approximation in bipartite graphs and a 1.973 + {$\epsilon$} approximation in general graphs. We thus answer in the affirmative the value version of the major open question repeatedly asked in the dynamic graph algorithms literature. Our randomized algorithms' approximation and worst-case update time bounds both hold w.h.p. against adaptive adversaries. Our algorithms are based on simulating new two-pass streaming matching algorithms in the dynamic setting. Our key new idea is to invoke the recent sublinear-time matching algorithm of Behnezhad (FOCS'21) in a white-box manner to efficiently simulate the second pass of our streaming algorithms, while bypassing the well-known vertex-update barrier.},
  citationcount = {25},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {adaptive,dynamic,update,update time}
}

@article{bhattacharyaDynamicSetCover2020,
  title = {Dynamic Set Cover: {{Improved}} Amortized and Worst-Case Update Time},
  author = {Bhattacharya, Sayan and Henzinger, Monika and Nanongkai, Danupon and Wu, Xiaowei},
  year = {2020},
  doi = {10.1137/1.9781611976465.150},
  abstract = {In the dynamic minimum set cover problem, a challenge is to minimize the update time while guaranteeing close to the optimal (O(n),f) approximation factor. (Throughout, m, n, f, and C are parameters denoting the maximum number of sets, number of elements, frequency, and the cost range.) In the high-frequency range, when f={\textohm}(n), this was achieved by a deterministic O(n)-approximation algorithm with O(fn) amortized update time [Gupta et al. STOC'17]. In the low-frequency range, the line of work by Gupta et al. [STOC'17], Abboud et al. [STOC'19], and Bhattacharya et al. [ICALP'15, IPCO'17, FOCS'19] led to a deterministic (1+{$\epsilon$})f-approximation algorithm with O(f(Cn)/{$\epsilon^2$}) amortized update time. In this paper we improve the latter update time and provide the first bounds that subsume (and sometimes improve) the state-of-the-art dynamic vertex cover algorithms. We obtain: 1. (1+{$\epsilon$})f-approximation ratio in O(f{$^2$}(Cn)/{$\epsilon^3$}) worst-case update time: No non-trivial worst-case update time was previously known for dynamic set cover. Our bound subsumes and improves by a logarithmic factor the O({$^3$}n/\{poly\}({$\epsilon$})) worst-case update time for unweighted dynamic vertex cover (i.e., when f=2 and C=1) by Bhattacharya et al. [SODA'17]. 2. (1+{$\epsilon$})f-approximation ratio in O((f{$^2$}/{$\epsilon^3$})+(f/{$\epsilon^2$})C) amortized update time: This result improves the previous O(f(Cn)/{$\epsilon^2$}) update time bound for most values of f in the low-frequency range, i.e. whenever f=o(n). It is the first that is independent of m and n. It subsumes the constant amortized update time of Bhattacharya and Kulkarni [SODA'19] for unweighted dynamic vertex cover (i.e., when f=2 and C=1).},
  citationcount = {12},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,update,update time}
}

@article{bhattacharyaFullyDynamicApproximate2017,
  title = {Fully Dynamic Approximate Maximum Matching and Minimum Vertex Cover in {{O}}(Log3 n) Worst Case Update Time},
  author = {Bhattacharya, Sayan and Henzinger, Monika and Nanongkai, Danupon},
  year = {2017},
  doi = {10.1137/1.9781611974782.30},
  abstract = {We consider the problem of maintaining an approximately maximum (fractional) matching and an approximately minimum vertex cover in a dynamic graph. Starting with the seminal paper by Onak and Rubinfeld [STOC 2010], this problem has received significant attention in recent years. There remains, however, a polynomial gap between the best known worst case update time and the best known amortised update time for this problem, even after allowing for randomisation. Specifically, Bernstein and Stein [ICALP 2015, SODA 2016] have the best known worst case update time. They present a deterministic data structure with approximation ratio (3/2 + {$\epsilon$}) and worst case update time O(m1/4 / {$\epsilon$}2), where m is the number of edges in the graph. In recent past, Gupta and Peng [FOCS 2013] gave a deterministic data structure with approximation ratio (1 + {$\epsilon$}) and worst case update time [EQUATION]. No known randomised data structure beats the worst case update times of these two results. In contrast, the paper by Onak and Rubinfeld [STOC 2010] gave a randomised data structure with approximation ratio O(1) and amortised update time O(log2 n), where n is the number of nodes in the graph. This was later improved by Baswana, Gupta and Sen [FOCS 2011] and Solomon [FOCS 2016], leading to a randomised date structure with approximation ratio 2 and amortised update time O(1). We bridge the polynomial gap between the worst case and amortised update times for this problem, without using any randomisation. We present a deterministic data structure with approximation ratio (2 + {$\epsilon$}) and worst case update time O(log3 n), for all sufficiently small constants {$\epsilon$}.},
  citationcount = {71},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,update,update time}
}

@article{bhattacharyaNearOptimalDynamic2023,
  title = {Near-Optimal Dynamic Rounding of Fractional Matchings in Bipartite Graphs},
  author = {Bhattacharya, Sayan and Kiss, Peter and Sidford, Aaron and Wajc, David},
  year = {2023},
  doi = {10.1145/3618260.3649648},
  abstract = {We study dynamic (1-{\cyrchar\cyrie})-approximate rounding of fractional matchings---a key ingredient in numerous breakthroughs in the dynamic graph algorithms literature. Our first contribution is a surprisingly simple deterministic rounding algorithm in bipartite graphs with amortized update time O({\cyrchar\cyrie}-1 log2 ({\cyrchar\cyrie}-1 {$\cdot$} n)), matching an (unconditional) recourse lower bound of {\textohm}({\cyrchar\cyrie}-1) up to logarithmic factors. Moreover, this algorithm's update time improves provided the minimum (non-zero) weight in the fractional matching is lower bounded throughout. Combining this algorithm with novel dynamic partial rounding algorithms to increase this minimum weight, we obtain a number of algorithms that improve this dependence on n. For example, we give a high-probability randomized algorithm with {\~O}({\cyrchar\cyrie}-1 {$\cdot$} (loglogn)2)-update time against adaptive adversaries. Using our rounding algorithms, we also round known (1-{\cyrchar\cyrie})-decremental fractional bipartite matching algorithms with no asymptotic overhead, thus improving on state-of-the-art algorithms for the decremental bipartite matching problem. Further, we provide extensions of our results to general graphs and to maintaining almost-maximal matchings.},
  citationcount = {5},
  venue = {Symposium on the Theory of Computing},
  keywords = {adaptive,dynamic,lower bound,update,update time}
}

@article{bhattacharyaNewAmortizedCell2019,
  title = {New Amortized Cell-Probe Lower Bounds for Dynamic Problems},
  author = {Bhattacharya, Sayan and Henzinger, Monika and Neumann, S.},
  year = {2019},
  doi = {10.1016/J.TCS.2019.01.043},
  abstract = {We build upon the recent papers by Weinstein and Yu [11], Larsen [7], and Clifford et al. [3] to present a general framework that gives amortized lower bounds on the update and query times of dynamic data structures. Using our framework, we present two concrete results. 1. For the dynamic polynomial evaluation problem, where the polynomial is defined over a finite field of size n1+  (1) and has degree n, any dynamic data structure must either have an amortized update time of  ((lg n/ lg lg n)2) or an amortized query time of  ((lg n/ lg lg n)2). 2. For the dynamic online matrix vector multiplication problem, where we get an n {\texttimes} n matrix whose entires are drawn from a finite field of size n  (1), any dynamic data structure must either have an amortized update time of  ((lg n/ lg lg n)2) or an amortized query time of  (n {$\cdot$} (lg n/ lg lg n)2). For these two problems, the previous works by Larsen [7] and Clifford et al. [3] gave the same lower bounds, but only for worst case update and query times. Our bounds match the highest unconditional lower bounds known till date for any dynamic problem in the cell-probe model.},
  citationcount = {1},
  venue = {Theoretical Computer Science},
  keywords = {cell probe,dynamic,lower bound,sorted},
  file = {/Users/tulasi/Zotero/storage/JMH4BE6K/1-s2.0-S0304397519300842-main.pdf}
}

@article{bhattacharyaNewAmortizedCellprobe2019,
  title = {New Amortized Cell-Probe Lower Bounds for Dynamic Problems},
  author = {Bhattacharya, Sayan and Henzinger, Monika and Neumann, Stefan},
  year = {2019},
  month = aug,
  journal = {Theoretical Computer Science},
  volume = {779},
  pages = {72--87},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2019.01.043},
  url = {https://www.sciencedirect.com/science/article/pii/S0304397519300842},
  urldate = {2025-03-26},
  abstract = {We build upon the recent papers by Weinstein and Yu [11], Larsen [7], and Clifford et al. [3] to present a general framework that gives amortized lower bounds on the update and query times of dynamic data structures. Using our framework, we present two concrete results.1.For the dynamic polynomial evaluation problem, where the polynomial is defined over a finite field of size n1+{\textohm}(1) and has degree n, any dynamic data structure must either have an amortized update time of {\textohm}((lgn/lglgn)2) or an amortized query time of {\textohm}((lgn/lglgn)2).2.For the dynamic online matrix vector multiplication problem, where we get an n{\texttimes}n matrix whose entires are drawn from a finite field of size n{$\Theta$}(1), any dynamic data structure must either have an amortized update time of {\textohm}((lgn/lglgn)2) or an amortized query time of {\textohm}(n{$\cdot$}(lgn/lglgn)2). For these two problems, the previous works by Larsen [7] and Clifford et al. [3] gave the same lower bounds, but only for worst case update and query times. Our bounds match the highest unconditional lower bounds known till date for any dynamic problem in the cell-probe model.},
  keywords = {Cell-probe lower bounds,Dynamic algorithms,Online matrix vector multiplication,Polynomial evaluation}
}

@article{bhattacharyaNewAmortizedCellprobe2019a,
  title = {New Amortized Cell-Probe Lower Bounds for Dynamic Problems},
  author = {Bhattacharya, Sayan and Henzinger, Monika and Neumann, Stefan},
  year = {2019},
  month = aug,
  journal = {Theor. Comput. Sci.},
  volume = {779},
  number = {C},
  pages = {72--87},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2019.01.043},
  url = {https://doi.org/10.1016/j.tcs.2019.01.043},
  urldate = {2025-03-26}
}

@article{bhattacharyaNewDeterministicApproximation2016,
  title = {New Deterministic Approximation Algorithms for Fully Dynamic Matching},
  author = {Bhattacharya, Sayan and Henzinger, Monika and Nanongkai, Danupon},
  year = {2016},
  doi = {10.1145/2897518.2897568},
  abstract = {We present two deterministic dynamic algorithms for the maximum matching problem. (1) An algorithm that maintains a (2+{\cyrchar\cyrie})-approximate maximum matching in general graphs with O(poly(logn, 1/{\cyrchar\cyrie})) update time. (2) An algorithm that maintains an {$\alpha$}K approximation of the value of the maximum matching with O(n2/K) update time in bipartite graphs, for every sufficiently large constant positive integer K. Here, 1{$\leq$} {$\alpha$}K {\textexclamdown} 2 is a constant determined by the value of K. Result (1) is the first deterministic algorithm that can maintain an o(logn)-approximate maximum matching with polylogarithmic update time, improving the seminal result of Onak et al. [STOC 2010]. Its approximation guarantee almost matches the guarantee of the best randomized polylogarithmic update time algorithm [Baswana et al. FOCS 2011]. Result (2) achieves a better-than-two approximation with arbitrarily small polynomial update time on bipartite graphs. Previously the best update time for this problem was O(m1/4) [Bernstein et al. ICALP 2015], where m is the current number of edges in the graph.},
  citationcount = {96},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,update,update time}
}

@article{bhattacharyaSpaceTimeefficientAlgorithm2015,
  title = {Space- and Time-Efficient Algorithm for Maintaining Dense Subgraphs on One-Pass Dynamic Streams},
  author = {Bhattacharya, Sayan and Henzinger, Monika and Nanongkai, Danupon and Tsourakakis, Charalampos E.},
  year = {2015},
  doi = {10.1145/2746539.2746592},
  abstract = {While in many graph mining applications it is crucial to handle a stream of updates efficiently in terms of both time and space, not much was known about achieving such type of algorithm. In this paper we study this issue for a problem which lies at the core of many graph mining applications called densest subgraph problem. We develop an algorithm that achieves time- and space-efficiency for this problem simultaneously. It is one of the first of its kind for graph problems to the best of our knowledge. Given an input graph, the densest subgraph is the subgraph that maximizes the ratio between the number of edges and the number of nodes. For any {$\varepsilon$}{\textquestiondown}0, our algorithm can, with high probability, maintain a (4+{$\varepsilon$})-approximate solution under edge insertions and deletions using  O(n) space and  O(1) amortized time per update; here, n is the number of nodes in the graph and  O hides the O(polylog\_\{1+{$\varepsilon$}\} n) term. The approximation ratio can be improved to (2+{$\varepsilon$}) with more time. It can be extended to a (2+{$\varepsilon$})-approximation sublinear-time algorithm and a distributed-streaming algorithm. Our algorithm is the first streaming algorithm that can maintain the densest subgraph in one pass. Prior to this, no algorithm could do so even in the special case of an incremental stream and even when there is no time restriction. The previously best algorithm in this setting required O(log n) passes [BahmaniKV12]. The space required by our algorithm is tight up to a polylogarithmic factor.},
  citationcount = {138},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,update}
}

@article{billeDynamicRangeMinimum2024,
  title = {Dynamic Range Minimum Queries on the Ultra-Wide Word {{RAM}}},
  author = {Bille, Philip and G{\o}rtz, Inge Li and Stordalen, Tord and L'opez, M'aximo P'erez},
  year = {2024},
  doi = {10.48550/arXiv.2411.16281},
  abstract = {We consider the dynamic range minimum problem on the ultra-wide word RAM model of computation. This model extends the classic w-bit word RAM model with special ultrawords of length w{$^2$} bits that support standard arithmetic and boolean operation and scattered memory access operations that can access w (non-contiguous) locations in memory. The ultra-wide word RAM model captures (and idealizes) modern vector processor architectures. Our main result is a linear space data structure that supports range minimum queries and updates in O(n) time. This exponentially improves the time of existing techniques. Our result is based on a simple reduction to prefix minimum computations on sequences O(n) words combined with a new parallel, recursive implementation of these.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {data structure,dynamic,query,reduction,update}
}

@article{billeDynamicRelativeCompression2015,
  title = {Dynamic Relative Compression, Dynamic Partial Sums, and Substring Concatenation},
  author = {Bille, Philip and Cording, Patrick Hagge and G{\o}rtz, Inge Li and Skjoldjensen, Frederik Rye and Vildh{\o}j, Hjalte Wedel and Vind, S{\o}ren},
  year = {2015},
  doi = {10.1007/s00453-017-0380-7},
  abstract = {No abstract available},
  citationcount = {23},
  venue = {Algorithmica},
  keywords = {dynamic}
}

@article{billeFastDynamicArrays2017,
  title = {Fast Dynamic Arrays},
  author = {Bille, Philip and Christiansen, Anders Roy and Ettienne, Mikko Berggren and G{\o}rtz, Inge Li},
  year = {2017},
  doi = {10.4230/LIPIcs.ESA.2017.16},
  abstract = {We present a highly optimized implementation of tiered vectors, a data structure for maintaining a sequence of n elements supporting access in time O(1) and insertion and deletion in time O(n{\textasciicircum}e) for e {\textquestiondown} 0 while using o(n) extra space. We consider several different implementation optimizations in C++ and compare their performance to that of vector and set from the standard library on sequences with up to 10{\textasciicircum}8 elements. Our fastest implementation uses much less space than set while providing speedups of 40x for access operations compared to set and speedups of 10.000x compared to vector for insertion and deletion operations while being competitive with both data structures for all other operations.},
  citationcount = {2},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,dynamic}
}

@article{billeLongestCommonExtensions2014,
  title = {Longest Common Extensions in Trees},
  author = {Bille, Philip and Gawrychowski, Pawe{\l} and G{\o}rtz, Inge Li and Landau, G. M. and Weimann, Oren},
  year = {2014},
  doi = {10.1007/978-3-319-19929-0_5},
  abstract = {No abstract available},
  citationcount = {6},
  venue = {Annual Symposium on Combinatorial Pattern Matching}
}

@article{billePartialSumsOn2019,
  title = {Partial Sums on the Ultra-Wide Word {{RAM}}},
  author = {Bille, Philip and G{\o}rtz, Inge Li and Skjoldjensen, Frederik Rye},
  year = {2019},
  doi = {10.1007/978-3-030-59267-7_2},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Theory and Applications of Models of Computation}
}

@article{billePredecessorOnThe2022,
  title = {Predecessor on the Ultra-Wide Word {{RAM}}},
  author = {Bille, Philip and G{\o}rtz, Inge Li and Stordalen, Tord},
  year = {2022},
  doi = {10.4230/LIPIcs.SWAT.2022.18},
  abstract = {We consider the predecessor problem on the ultra-wide word RAM model of computation, which extends the word RAM model with ultrawords consisting of  w 2 bits (TAMC, 2015). The model supports arithmetic and boolean operations on ultrawords, in addition to scattered memory operations that access or modify w (potentially non-contiguous) memory addresses simultaneously. The ultra-wide word RAM model captures (and idealizes) modern vector processor architectures. Our main result is a simple, linear space data structure that supports predecessor in constant time and updates in amortized, expected constant time. This improves the space of the previous constant time solution that uses space in the order of the size of the universe. Our result holds even in a weaker model where ultrawords consist of  w 1 + {$\epsilon$} bits for any  {$\epsilon$} {\textquestiondown} 0 . It is based on a new implementation of the classic x-fast trie data structure of Willard (Inform Process Lett 17(2):81--84, https://doi.org/10.1016/0020-0190(83)90075-3, 1983) combined with a new dictionary data structure that supports fast parallel lookups.},
  citationcount = {1},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {data structure,update}
}

@article{billeRandomAccessIn2020,
  title = {Random Access in Persistent Strings},
  author = {Bille, Philip and G{\o}rtz, Inge Li},
  year = {2020},
  doi = {10.4230/LIPIcs.ISAAC.2020.48},
  abstract = {We consider compact representations of collections of similar strings that support random access queries. The collection of strings is given by a rooted tree where edges are labeled by an edit operation (inserting, deleting, or replacing a character) and a node represents the string obtained by applying the sequence of edit operations on the path from the root to the node. The goal is to compactly represent the entire collection while supporting fast random access to any part of a string in the collection. This problem captures natural scenarios such as representing the past history of a edited document or representing highly-repetitive collections. Given a tree with n nodes, we show how to represent the corresponding collection in O(n) space and optimal O(n/n) query time. This improves the previous time-space trade-offs for the problem. To obtain our results, we introduce new techniques and ideas, including a reduction to a new geometric line segment selection together with an efficient solution.},
  citationcount = {4},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {query,query time,reduction,time-space}
}

@article{billeRandomAccessIn2022,
  title = {Random Access in Persistent Strings and Segment Selection},
  author = {Bille, Philip and G{\o}rtz, Inge Li},
  year = {2022},
  doi = {10.1007/s00224-022-10109-5},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Theory of Computing Systems}
}

@article{billeRandomAccessTo2010,
  title = {Random Access to Grammar-Compressed Strings},
  author = {Bille, Philip and Landau, G. M. and Weimann, Oren},
  year = {2010},
  doi = {10.1137/130936889},
  abstract = {Let {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown} be a string of length {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown} compressed into a context-free grammar {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown} of size {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}. We present two representations of {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown} achieving {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown}) random access time, and either {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} {$\cdot$} {$\alpha$}{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown})) construction time and space on the pointer machine model, or {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) construction time and space on the RAM. Here, {$\alpha$}{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) is the inverse of the {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}{\textexclamdown}i{\textquestiondown}th{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sup{\textquestiondown} row of Ackermann's function. Our representations also efficiently support decompression of any substring in {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown}: we can decompress any substring of length {\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown} in the same complexity as a single random access query and additional {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown}) time. Combining these results with fast algorithms for uncompressed approximate string matching leads to several efficient algorithms for approximate string matching on grammar-compressed strings without decompression. For instance, we can find all approximate occurrences of a pattern {\textexclamdown}i{\textquestiondown}P{\textexclamdown}/i{\textquestiondown} with at most {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown} errors in time {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}(min\{{\textbar}{\textexclamdown}i{\textquestiondown}P{\textexclamdown}/i{\textquestiondown}{\textbar}{\textexclamdown}i{\textquestiondown}k, k{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}4{\textexclamdown}/sup{\textquestiondown} +{\textbar}{\textexclamdown}i{\textquestiondown}P{\textexclamdown}/i{\textquestiondown}{\textbar}\} +log {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown}) + occ), where occ is the number of occurrences of {\textexclamdown}i{\textquestiondown}P{\textexclamdown}/i{\textquestiondown} in {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown}. Finally, we are able to generalize our results to navigation and other operations on grammar-compressed {\textexclamdown}i{\textquestiondown}trees{\textexclamdown}/i{\textquestiondown}. All of the above bounds significantly improve the currently best known results. To achieve these bounds, we introduce several new techniques and data structures of independent interest, including a predecessor data structure, two "biased" weighted ancestor data structures, and a compact representation of heavy-paths in grammars.},
  citationcount = {160},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{billeSizeConstrainedWeighted2023,
  title = {Size-Constrained Weighted Ancestors with Applications},
  author = {Bille, Philip and Nekrich, Yakov and Pissis, S.},
  year = {2023},
  doi = {10.48550/arXiv.2311.15777},
  abstract = {The weighted ancestor problem on a rooted node-weighted tree T is a generalization of the classic predecessor problem: construct a data structure for a set of integers that supports fast predecessor queries. Both problems are known to require {\textohm}(n) time for queries provided \{O\}(n\{poly\}n) space is available, where n is the input size. The weighted ancestor problem has attracted a lot of attention by the combinatorial pattern matching community due to its direct application to suffix trees. In this formulation of the problem, the nodes are weighted by string depth. This research has culminated in a data structure for weighted ancestors in suffix trees with \{O\}(1) query time and an \{O\}(n)-time construction algorithm [Belazzougui et al., CPM 2021]. In this paper, we consider a different version of the weighted ancestor problem, where the nodes are weighted by any function \{weight\} that maps the nodes of T to positive integers, such that \{weight\}(u){$\leq$}\{size\}(u) for any node u and \{weight\}(u{$_1$}){$\leq$}\{weight\}(u{$_2$}) if node u{$_1$} is a descendant of node u{$_2$}, where \{size\}(u) is the number of nodes in the subtree rooted at u. In the size-constrained weighted ancestor (SWA) problem, for any node u of T and any integer k, we are asked to return the lowest ancestor w of u with weight at least k. We show that for any rooted tree with n nodes, we can locate node w in \{O\}(1) time after \{O\}(n)-time preprocessing. In particular, this implies a data structure for the SWA problem in suffix trees with \{O\}(1) query time and \{O\}(n)-time preprocessing, when the nodes are weighted by \{weight\}. We also show several string-processing applications of this result.},
  citationcount = {Unknown},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {data structure,query,query time}
}

@article{billeSubstringRangeReporting2011,
  title = {Substring Range Reporting},
  author = {Bille, Philip and G{\o}rtz, Inge Li},
  year = {2011},
  doi = {10.1007/s00453-012-9733-4},
  abstract = {No abstract available},
  citationcount = {26},
  venue = {Algorithmica}
}

@article{billeSuccinctPartialSums2017,
  title = {Succinct Partial Sums and Fenwick Trees},
  author = {Bille, Philip and Christiansen, Anders Roy and Prezza, N. and Skjoldjensen, Frederik Rye},
  year = {2017},
  doi = {10.1007/978-3-319-67428-5_8},
  abstract = {No abstract available},
  citationcount = {10},
  venue = {SPIRE}
}

@article{billeTheComplexityOf2022,
  title = {The Complexity of the Co-Occurrence Problem},
  author = {Bille, Philip and G{\o}rtz, Inge Li and Stordalen, Tord},
  year = {2022},
  doi = {10.48550/arXiv.2206.10383},
  abstract = {Let S be a string of length n over an alphabet {$\Sigma$} and let Q be a subset of {$\Sigma$} of size q {$\geq$} 2. The co-occurrence problem is to construct a compact data structure that supports the following query: given an integer w return the number of length- w substrings of S that contain each character of Q at least once. This is a natural string problem with applications to, e.g., data mining, natural language processing, and DNA analysis. The state of the art is an O ( {\textsurd} nq ) space data structure that --- with some minor additions --- supports queries in O (log log n ) time [CPM 2021]. Our contributions are as follows. Firstly, we analyze the problem in terms of a new, natural parameter d , giving a simple data structure that uses O ( d ) space and supports queries in O (log log n ) time. The preprocessing algorithm does a single pass over S , runs in expected O ( n ) time, and uses O ( d + q ) space in addition to the input. Furthermore, we show that O ( d ) space is optimal and that O (log log n )-time queries are optimal given optimal space. Secondly, we bound d = O ( {\textsurd} nq ), giving clean bounds in terms of n and q that match the state of the art. Furthermore, we prove that {\textohm}( {\textsurd} nq ) bits of space is necessary in the worst case, meaning that the O ( {\textsurd} nq ) upper bound is tight to within polylogarithmic factors. All of our results are based on simple and intuitive combinatorial ideas that simplify the state of the art.},
  citationcount = {Unknown},
  venue = {SPIRE},
  keywords = {data structure,query}
}

@article{bilykSmallBallInequality2008,
  title = {On the Small Ball Inequality in All Dimensions},
  author = {Bilyk, D. and Lacey, M. T. and Vagharshakyan, A.},
  year = {2008},
  journal = {Journal of Functional Analysis},
  volume = {254},
  pages = {2470--2502},
  doi = {10.1016/j.jfa.2007.09.010},
  annotation = {Dmitriy Bilyk, Michael T. Lacey, On the Small Ball Inequality in three dimensions, Duke Math. J., (2006), in press arXiv: math.CA/0609815},
  file = {/Users/tulasi/Zotero/storage/RZPK2HDY/Bilyk et al. - 2008 - On the small ball inequality in all dimensions.pdf}
}

@article{bingolMemorySize2020,
  title = {Memory Size},
  author = {Bingol, H.},
  year = {2020},
  doi = {10.1017/9781108671644.014},
  abstract = {It is a fact that there are many web pages for one to know all. Similarly there are many songs than we can keep in our mp3 players. It is not very difficult to increase examples of "too many itemstoo little memory" cases. We try to model the case where the memory is too small to keep all the items. We consider agents with a limited memory of size m. We let them interact by means of recommending web pages among the ones that they know. The giver agent "remembers" an item that is stored in its memory. The giver recommends the item to the taker agent. Since the taker does not have empty space in its memory, it "forgets" something and uses the emptied slot for the item recommended. We surprisingly observed that this process leads to many items become completely forgotten by the society while a few of them become extremely well known as the ratio of the memory size to the number of items decreases. We interpret this result as the emergence of fame. This may explain why there are so many web pages but only a small percentage is widely known. We also observed that some metrics such as the percentage of completely forgotten items, maximum fame, minimum fame changes linearly with the memory ratio.},
  citationcount = {Unknown},
  venue = {Communication Complexity}
}

@article{bishnuCountingAndSampling2022,
  title = {Counting and Sampling from Substructures Using Linear Algebraic Queries},
  author = {Bishnu, Arijit and Ghosh, Arijit and Mishra, Gopinath and Paraashar, Manaswi},
  year = {2022},
  doi = {10.4230/LIPIcs.FSTTCS.2022.8},
  abstract = {For an unknown n {\texttimes} n matrix A having non-negative entries, the inner product ( IP ) oracle takes as inputs a specified row (or a column) of A and a vector v {$\in$} R n with non-negative entries, and returns their inner product. Given two input vectors x and y in R n with non-negative entries, and an unknown matrix A with non-negative entries with IP oracle access, we design almost optimal sublinear time algorithms for the following two fundamental matrix problems: Find an estimate X for the bilinear form x T A y such that X {$\approx$} x T A y . Designing a sampler Z for the entries of the matrix A such that P ( Z = ( i, j )) {$\approx$} x i A ij y j / (cid:0) x T A y (cid:1) , where x i and y j are i -th and j -th coordinate of x and y respectively. As special cases of the above results, for any submatrix of an unknown matrix with non-negative entries and IP oracle access, we can efficiently estimate the sum of the entries of any submatrix, and also sample a random entry from the submatrix with probability proportional to its weight. We will show that the above results imply that if we are given IP oracle access to the adjacency matrix of a graph, with non-negative weights on the edges, then we can design sublinear time algorithms for the following two fundamental graph problems:},
  citationcount = {1},
  venue = {Foundations of Software Technology and Theoretical Computer Science},
  keywords = {query}
}

@article{biswasMassivelyParallelAlgorithms2020,
  title = {Massively Parallel Algorithms for Small Subgraph Counting},
  author = {Biswas, Amartya Shankha and Eden, T. and Liu, Quanquan C. and Rubinfeld, R. and Mitrovi'c, Slobodan},
  year = {2020},
  doi = {10.4230/LIPIcs.APPROX/RANDOM.2022.39},
  abstract = {Over the last two decades, frameworks for distributed-memory parallel computation, such as MapReduce, Hadoop, Spark and Dryad, have gained significant popularity with the growing prevalence of large network datasets. The Massively Parallel Computation (MPC) model is the de-facto standard for studying graph algorithms in these frameworks theoretically. Subgraph counting is one such fundamental problem in analyzing massive graphs, with the main algorithmic challenges centering on designing methods which are both scalable and accurate. Given a graph G=(V,E) with n vertices, m edges and T triangles, our first result is an algorithm that outputs a (1+{$\varepsilon$})-approximation to T, with asymptotically \{optimal round and total space complexity\} provided any S{$\geq$}\{({\textsurd}m,n{$^2$}/m)\} space per machine and assuming T={\textohm}({\textsurd}\{m/n\}). Our result gives a quadratic improvement on the bound on T over previous works. We also provide a simple extension of our result to counting \{any\} subgraph of k size for constant k{$\geq$}1. Our second result is an O\textsubscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}(n)-round algorithm for exactly counting the number of triangles, whose total space usage is parametrized by the \{arboricity\} {$\alpha$} of the input graph. We extend this result to exactly counting k-cliques for any constant k. Finally, we prove that a recent result of Bera, Pashanasangi and Seshadhri (ITCS 2020) for exactly counting all subgraphs of size at most 5 can be implemented in the MPC model in total space.},
  citationcount = {8},
  venue = {International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques}
}

@article{bjrklundExactCoversVia2010,
  title = {Exact Covers via Determinants},
  author = {Bj{\"o}rklund, Andreas},
  year = {2010},
  doi = {10.4230/LIPIcs.STACS.2010.2447},
  abstract = {Given a k-uniform hypergraph on n vertices, partitioned in k equal parts such that every hyperedge includes one vertex from each part, the k-Dimensional Matching problem asks whether there is a disjoint collection of the hyperedges which covers all vertices. We show it can be solved by a randomized polynomial space algorithm in O\textsuperscript{*}(2\textsuperscript{\{\vphantom\}}n(k-2)/k\vphantom\{\}) time. The O\textsuperscript{*}() notation hides factors polynomial in n and k. The general Exact Cover by k-Sets problem asks the same when the partition constraint is dropped and arbitrary hyperedges of cardinality k are permitted. We show it can be solved by a randomized polynomial space algorithm in O\textsuperscript{*}(c\textsubscript{k}{$^n$}) time, where c{$_3$}=1.496,c{$_4$}=1.642,c{$_5$}=1.721, and provide a general bound for larger k. Both results substantially improve on the previous best algorithms for these problems, especially for small k. They follow from the new observation that Lov{\'a}sz' perfect matching detection via determinants (Lov{\'a}sz, 1979) admits an embedding in the recently proposed inclusion--exclusion counting scheme for set covers, \{despite\} its inability to count the perfect matchings.},
  citationcount = {18},
  venue = {Symposium on Theoretical Aspects of Computer Science}
}

@article{bjrklundHowProofsAre2016,
  title = {How Proofs Are Prepared at Camelot: {{Extended}} Abstract},
  author = {Bj{\"o}rklund, Andreas and Kaski, P.},
  year = {2016},
  doi = {10.1145/2933057.2933101},
  abstract = {We study a design framework for robust, independently verifiable, and workload-balanced distributed algorithms working on a common input. The framework builds on recent noninteractive Merlin--Arthur proofs of batch evaluation of Williams [31st IEEE Colloquium on Computational Complexity (CCC'16, May 29-June 1, 2016, Tokyo), to appear] with the basic observation that Merlin's magic is not needed for batch evaluation: mere Knights can prepare the independently verifiable proof, in parallel, and with intrinsic error-correction. As our main technical result, we show that the k-cliques in an n-vertex graph can be counted and verified in per-node O(n({$\omega$}+{$\varepsilon$})k/6) time and space on O(n({$\omega$}+{$\varepsilon$})k/6) compute nodes, for any constant {$\varepsilon$}{\textquestiondown}0 and positive integer k divisible by 6, where 2 {$\leq$} {$\omega$} {\textexclamdown} 2.3728639 is the exponent of square matrix multiplication over the integers. This matches in total running time the best known sequential algorithm, due to Ne{\v s}et{\v r}il and Poljak [Comment. Math. Univ. Carolin. 26 (1985) 415--419], and considerably improves its space usage and parallelizability. Further results (only partly presented in this extended abstract) include novel algorithms for counting triangles in sparse graphs, computing the chromatic polynomial of a graph, and computing the Tutte polynomial of a graph.},
  citationcount = {8},
  venue = {ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing}
}

@article{bjrklundListingTriangles2014,
  title = {Listing Triangles},
  author = {Bj{\"o}rklund, Andreas and Pagh, R. and Williams, V. V. and Zwick, Uri},
  year = {2014},
  doi = {10.1007/978-3-662-43948-7_19},
  abstract = {No abstract available},
  citationcount = {77},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{bjrklundShortestCycleThrough2012,
  title = {Shortest Cycle through Specified Elements},
  author = {Bj{\"o}rklund, Andreas and Husfeldt, T. and Taslaman, Nina},
  year = {2012},
  doi = {10.5555/2095116.2095255},
  abstract = {We give a randomized algorithm that finds a shortest simple cycle through a given set of k vertices or edges in an n-vertex undirected graph in time 2knO(1).},
  citationcount = {45},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{blellochANewCombinatorial2008,
  title = {A New Combinatorial Approach for Sparse Graph Problems},
  author = {Blelloch, G. and Williams, V. V. and Williams, Ryan},
  year = {2008},
  doi = {10.1007/978-3-540-70575-8_10},
  abstract = {No abstract available},
  citationcount = {24},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{blikstadNearlyOptimalCommunication2022,
  title = {Nearly Optimal Communication and Query Complexity of Bipartite Matching},
  author = {Blikstad, Joakim and {van den Brand}, Jan and Efron, Yuval and Mukhopadhyay, Sagnik and Nanongkai, Danupon},
  year = {2022},
  doi = {10.1109/FOCS54457.2022.00113},
  abstract = {We settle the complexities of the maximum-cardinality bipartite matching problem (BMM) up to polylogarithmic factors in five models of computation: the two-party communication, AND query, OR query, XOR query, and quantum edge query models. Our results answer open problems that have been raised repeatedly since at least three decades ago [Hajnal, Maass, and Turan STOC'88; Ivanyos, Klauck, Lee, Santha, and de Wolf FSTTCS'12; Dobzinski, Nisan, and Oren STOC'14; Nisan SODA'21] and tighten the lower bounds shown by Beniamini and Nisan [STOC'21] and Zhang [ICALP'04]. We also settle the communication complexity of the generalizations of BMM, such as maximum-cost bipartite b-matching and transshipment; and the query complexity of unique bipartite perfect matching (answering an open question by Beniamini [2022]). Our algorithms and lower bounds follow from simple applications of known techniques such as cutting planes methods and set disjointness.},
  citationcount = {11},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {communication,communication complexity,lower bound,query,query complexity}
}

@article{bloomSpaceTimeTrade1970,
  title = {Space/Time Trade-Offs in Hash Coding with Allowable Errors},
  author = {Bloom, B.},
  year = {1970},
  doi = {10.1145/362686.362692},
  abstract = {In this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash-coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency. The new methods are intended to reduce the amount of space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods. In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to ``catch'' the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods. Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time.},
  citationcount = {7905},
  venue = {CACM}
}

@article{blumLinearTimeBounds1972,
  title = {Linear Time Bounds for Median Computations},
  author = {Blum, M. and Floyd, R. W. and Pratt, V. and Rivest, R. and Tarjan, R.},
  year = {1972},
  doi = {10.1145/800152.804904},
  abstract = {New upper and lower bounds are presented for the maximum number of comparisons, f(i,n), required to select the i-th largest of n numbers. An upper bound is found, by an analysis of a new selection algorithm, to be a linear function of n: f(i,n) {$\leq$} 103n/18 {\textexclamdown} 5.73n, for 1 {$\leq$} i {$\leq$} n. A lower bound is shown deductively to be: f(i,n) {$\geq$} n+min(i,n-i+l) + [log2(n)] - 4, for 2 {$\leq$} i {$\leq$} n-1, or, for the case of computing medians: f([n/2],n) {$\geq$} 3n/2 - 3},
  citationcount = {53},
  venue = {Symposium on the Theory of Computing},
  keywords = {lower bound}
}

@article{blumOnTheSingle1986,
  title = {On the Single-Operation Worst-Case Time Complexity of the Disjoint Set Union Problem},
  author = {Blum, Norbert},
  year = {1986},
  doi = {10.1137/0215072},
  abstract = {We give an algorithm for the disjoint set union problem, within the class of algorithms defined by Tarjan, which has O(n/n) single-operation time complexity in the worst case. Also we define a class of algorithms for the disjoint set union problem, which includes the class of algorithms defined by Tarjan. We prove that any algorithm from this class has at least {\textohm}(\{\{n\}/\{n\}\}) single-operation time complexity in the worst case.},
  citationcount = {44},
  venue = {SIAM journal on computing (Print)}
}

@article{blumTimeBoundsFor1973,
  title = {Time Bounds for Selection},
  author = {Blum, M. and Floyd, R. W. and Pratt, V. and Rivest, R. and Tarjan, R.},
  year = {1973},
  doi = {10.1016/S0022-0000(73)80033-9},
  abstract = {No abstract available},
  citationcount = {1342},
  venue = {Journal of computer and system sciences (Print)}
}

@article{boasDesignImplementationEfficient1977,
  title = {Design and Implementation of an Efficient Priority Queue},
  author = {Boas, Peter van Emde and Kaas, R. and Zijlstra, E.},
  year = {1977},
  journal = {Mathematical Systems Theory},
  volume = {10},
  pages = {99--127},
  doi = {10.1007/BF01683268},
  keywords = {Computational Mathematic,Data Structure,Full Detail,Processing Time,Time Requirement},
  annotation = {Aho, A. V., J. E. Hopcroft andJ. D. Ullman,The design and analysis of computer Algorithms, Addison Wesley, Reading, Mass. (1974).\\
Aho, A. V., J. E. Hopcroft andJ. D. Ullman,On finding lowest common ancestors in tress, Proc. 5-th ACM symp. Theory of Computing (1973), 253--265.\\
Emde Boas, P. Van,An O (n log log n) On-Line Algorithm for the Insert-Extract Min Problem, Rep. TR 74-221 Dept. of Comp. Sci., Cornell Univ., Ithaca 14853, N.Y. Dec. 1974.\\
Even, S. andO. Kariv,Oral Commun., Berkeley, October 1975.\\
Fischer, M. J.,Efficiency of equivalence algorithms, in: R. E. Miller andJ. W. Thatcher (eds.),Complexity of Computer Computations, Plenum Press, New York (1972), 158--168.\\
Hopcroft, J. andJ. D. Ullman,Set-merging Algorithms, SIAM J. Comput. 2 (Dec. 1973), 294--303.\\
Hunt, J. W. andT. G. Szymanski,A fast algorithm for computing longest common subsequences. Manuscript. Dept. Electr. Eng. Princeton Univ. Princeton, N.J. 08540. Oct. 1975.\\
Tarjan, R. E.,Applications of path compression on balanced trees. Manuscript. Stanford Oct. 75. (Submitted toJACM).\\
Tarjan, R. E.,Efficiency of a good but non linear set union algorithm,J. Assoc. Comput. Mach. 22 (1975), 215--224.\\
Tarjan, R. E.,Edge disjoint spanning trees, dominators and depth first search, Rep. CS-74-455 (Sept. 1974), Stanford.\\
Wirth, N.,The Programming Language PASCAL (revised report), in K. Jensen and N. WirthPASCAL User Manual and Report, Lecture Notes in Computer Science 18, Springer, Berlin (1974).},
  file = {/Users/tulasi/Zotero/storage/FXWT5BWZ/van Emde Boas et al. - 1976 - Design and implementation of an efficient priority queue.pdf}
}

@article{boasPreservingOrderIn1975,
  title = {Preserving Order in a Forest in Less than Logarithmic Time},
  author = {Boas, P.},
  year = {1975},
  doi = {10.1109/SFCS.1975.26},
  abstract = {We present a data structure, based upon a stratified binary tree, which enables us to manipulate on-line a priority queue whose priorities are selected from the interval 1...n, with an average and worst case processing time of O(log log n) per instruction. The structure is used to obtain a mergeable heap whose time requirements are about as good.},
  citationcount = {538},
  venue = {16th Annual Symposium on Foundations of Computer Science (sfcs 1975)}
}

@article{boasPreservingOrderIn1977,
  title = {Preserving Order in a Forest in Less than Logarithmic Time and Linear Space},
  author = {{van Emde Boas}, Peter},
  year = {1977},
  doi = {10.1016/0020-0190(77)90031-X},
  abstract = {No abstract available},
  citationcount = {83},
  venue = {Information Processing Letters}
}

@article{boffaALearnedApproach2021,
  title = {A "Learned" Approach to Quicken and Compress Rank/Select Dictionaries},
  author = {Boffa, Antonio and Ferragina, P. and Vinciguerra, Giorgio},
  year = {2021},
  doi = {10.1137/1.9781611976472.4},
  abstract = {We address the well-known problem of designing, implementing and experimenting compressed data structures for supporting rank and select queries over a dictionary of integers. This problem has been studied far and wide since the end of the `80s with tons of important theoretical and practical results. Following a recent line of research on the so-called learned data structures, we first show that this problem has a surprising connection with the geometry of a set of points in the Cartesian plane suitably derived from the input integers. We then build upon some classical results in computational geometry to introduce the first ``learned'' scheme for implementing a compressed rank/select dictionary. We prove theoretical bounds on its time and space performance both in the worst case and in the case of input distributions with finite mean and variance. We corroborate these theoretical results with a large set of experiments over datasets originating from a variety of sources and applications (Web, DNA sequencing, information retrieval and natural language processing), and we show that a carefully engineered version of our approach provides new interesting space-time trade-offs with respect to several well-established implementations of Elias-Fano, RRR-vector, and random-access vectors of Elias {$\gamma$} / {$\delta$} -coded gaps.},
  citationcount = {22},
  venue = {Workshop on Algorithm Engineering and Experimentation},
  keywords = {data structure,query}
}

@article{boffaALearnedApproach2022,
  title = {A Learned Approach to Design Compressed Rank/Select Data Structures},
  author = {Boffa, Antonio and Ferragina, P. and Vinciguerra, Giorgio},
  year = {2022},
  doi = {10.1145/3524060},
  abstract = {We address the problem of designing, implementing, and experimenting with compressed data structures that support rank and select queries over a dictionary of integers. We shine a new light on this classical problem by showing a connection between the input integers and the geometry of a set of points in a Cartesian plane suitably derived from them. We then build upon some results in computational geometry to introduce the first compressed rank/select dictionary based on the idea of ``learning'' the distribution of such points via proper linear approximations (LA). We therefore call this novel data structure the la\_vector. We prove time and space complexities of the la\_vector in several scenarios: in the worst case, in the case of input distributions with finite mean and variance, and taking into account the kth order entropy of some of its building blocks. We also discuss improved hybrid data structures, namely, ones that suitably orchestrate known compressed rank/select dictionaries with the la\_vector. We corroborate our theoretical results with a large set of experiments over datasets originating from a variety of applications (Web search, DNA sequencing, information retrieval, and natural language processing) and show that our approach provides new interesting space-time tradeoffs with respect to many well-established compressed rank/select dictionary implementations. In particular, we show that our select is the fastest, and our rank is on the space-time Pareto frontier.},
  citationcount = {25},
  venue = {ACM Trans. Algorithms},
  keywords = {data structure,query}
}

@article{bollobsTheProbabilisticMethod2015,
  title = {The Probabilistic Method},
  author = {Bollob{\'a}s, B.},
  year = {2015},
  doi = {10.1112/BLMS/28.1.108},
  abstract = {``Shortly after the celebration of the four thousandth anniversary of the opening of space, Angary J. Gustible discovered Gustible's planet. The discovery turned out to be a tragic mistake. Gustible's planet was inhabited by highly intelligent life forms. They had moderate telepathic powers. They immediately mind-read Angary J. Gustible's entire mind and life history, and embarrassed him very deeply by making up an opera concerning his recent divorce.''},
  citationcount = {4234},
  venue = {No venue available}
}

@article{boningerNonAdaptiveData2017,
  title = {Non-Adaptive Data Structure Bounds for Dynamic Predecessor},
  author = {Boninger, Joseph and Brody, Joshua and Kephart, O.},
  year = {2017},
  doi = {10.4230/LIPIcs.FSTTCS.2017.20},
  abstract = {In this work, we continue the examination of the role non-adaptivity plays in maintaining dynamic data structures, initiated by Brody and Larsen. We consider non-adaptive data structures for predecessor search in the w-bit cell probe model. In this problem, the goal is to dynamically maintain a subset T of up to n elements from \{1, ..., m\}, while supporting insertions, deletions, and a predecessor query Pred(x), which returns the largest element in T that is less than or equal to x. Predecessor search is one of the most well-studied data structure problems. For this problem, using non-adaptivity comes at a steep price. We provide exponential cell probe complexity separations between (i) adaptive and non-adaptive data structures and (ii) non-adaptive and memoryless data structures for predecessor search. A classic data structure of van Emde Boas solves dynamic predecessor search in log(log(m)) probes; this data structure is adaptive. For dynamic data structures which make non-adaptive updates, we show the cell probe complexity is O(log(m)/log(w/log(m))). We also give a nearly-matching Omega(log(m)/log(w)) lower bound. We also give an m/w lower bound for memoryless data structures. Our lower bound technique is tailored to non-adaptive (as opposed to memoryless) updates and might be of independent interest.},
  citationcount = {6},
  venue = {Foundations of Software Technology and Theoretical Computer Science},
  keywords = {cell probe,data structure,dynamic,lower bound,non-adaptive,query,update}
}

@article{boppanaTheAverageSensitivity1997,
  title = {The Average Sensitivity of Bounded-Depth Circuits},
  author = {Boppana, R.},
  year = {1997},
  doi = {10.1016/S0020-0190(97)00131-2},
  abstract = {No abstract available},
  citationcount = {125},
  venue = {Information Processing Letters}
}

@article{borodinComputingAndLife2013,
  title = {Computing (and Life) Is All about Tradeoffs - a Small Sample of Some Computational Tradeoffs},
  author = {Borodin, A.},
  year = {2013},
  doi = {10.1007/978-3-642-40273-9_9},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Space-Efficient Data Structures, Streams, and Algorithms}
}

@article{borodinEfficientSearchingUsing1981,
  title = {Efficient Searching Using Partial Ordering},
  author = {Borodin, A. and Guibas, L. and Lynch, N. and Yao, A.},
  year = {1981},
  doi = {10.1016/0020-0190(81)90005-3},
  abstract = {No abstract available},
  citationcount = {36},
  venue = {Information Processing Letters}
}

@inproceedings{borodinLowerBoundsHigh1999,
  title = {Lower Bounds for High Dimensional Nearest Neighbor Search and Related Problems},
  booktitle = {Proceedings of the Thirty-First Annual {{ACM}} Symposium on {{Theory}} of {{Computing}}},
  author = {Borodin, Allan and Ostrovsky, Rafail and Rabani, Yuval},
  year = {1999},
  month = may,
  series = {{{STOC}} '99},
  pages = {312--321},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/301250.301330},
  url = {https://dl.acm.org/doi/10.1145/301250.301330},
  urldate = {2024-11-19},
  abstract = {The curse of dimensionality describes the phenomenon  whereby (in spite of extensive and continuing research)  for various geometric search problems we only have algorithms with performance that grows exponentially in  the dimension. Recent results [31, 30, 331 show that in  some sense it is possible to avoid the curse of dimensionality for the approximate nearest neighbor search  problem. But must the exact nearest neighbor search  problem suffer this curse? We provide some evidence  in support of the curse. Specifically we investigate the  exact nearest neighbor search problem and the related  problem of exact partial match within the asymmetric communication model first used by Miltersen [36] to  study data structure problems. We derive non-trivial  asymptotic lower bounds for the exact problem that  stand in contrast to known algorithms for approximate  nearest neighbor search.},
  isbn = {978-1-58113-067-6},
  keywords = {communication,data structure,lower bound,sorted},
  annotation = {M. Ajtai . A lower bound for finding predecessors in Yao's cell probe model. Combinatorica, 8: 235- {\textasciitilde}47 , 1988 . M. Ajtai. A lower bound for finding predecessors in Yao's cell probe model. Combinatorica, 8:235- {\textasciitilde}47, 1988.\\
S. Arya and D. Mount . Approximate nearest neighbor searching . In Proc. of gih SODA , pp. 271 - 280 , 1993 . S. Arya and D. Mount. Approximate nearest neighbor searching. In Proc. of gih SODA, pp. 271-280, 1993.\\
S. Arya , D. Mount , N. N etanyahu, R. Silverman , and A. Wu . An optimal algorithm for approximate nearest neighbor searching in fixed dimensions . In Proc. of 5{\textasciitilde}h SODA , pp. 573 - 582 , 1994 . S. Arya, D. Mount, N. N etanyahu, R. Silverman, and A. Wu. An optimal algorithm for approximate nearest neighbor searching in fixed dimensions. In Proc. of 5{\textasciitilde}h SODA, pp. 573-582, 1994.\\
P. Beame , M. Saks , and J.S. Thathachar . Timespace tradeoffs for branching programs . In Proc. of 39th FOCS , pp. 254 - 263 , 1998 . P. Beame, M. Saks, and J.S. Thathachar. Timespace tradeoffs for branching programs. In Proc. of 39th FOCS, pp. 254-263, 1998.\\
J.S. Beis and D.G. Lowe . Shape indexing using approximate nearest-neighbor search in highdimensional spaces . In Proc. IEEE Conj. Comp. Vision Part. Recog. , pages 1000 - 1006 , 1997 . J.S. Beis and D.G. Lowe. Shape indexing using approximate nearest-neighbor search in highdimensional spaces. In Proc. IEEE Conj. Comp. Vision Part. Recog., pages 1000-1006, 1997.\\
J.L. Bentley and R. Sedgewick . Fast algorithms for sorting and search strings . In Proc. of 8th SODA , pp. 360 - 369 , 1997 . J.L. Bentley and R. Sedgewick. Fast algorithms for sorting and search strings. In Proc. of 8th SODA, pp. 360-369, 1997.\\
\\
M. de Berg , M. van Kreveld , M. Overmars , O. Schwarzkopf . Computational Geometry, Algorithms and Applications . Springer , 1997 . M. de Berg, M. van Kreveld, M. Overmars, O. Schwarzkopf. Computational Geometry, Algorithms and Applications. Springer, 1997.\\
\\
A. Chakrabaxti , B. Chazelle , B. Gum , A. Lvov . A good neighbor is hard to find . These Proceedings. A. Chakrabaxti, B. Chazelle, B. Gum, A. Lvov. A good neighbor is hard to find. These Proceedings.\\
\\
B. Chazelle. Private communication.  B. Chazelle. Private communication.\\
\\
\\
S. Deerwester , S.T. Dumais , G.W. Furnas , T.K. Landauer , and R. tIarshman. Indexing by latent semantic analysis. J. Amer. Soc. lnfo. \$ci., 41(6):391-407 , 1990 . S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. tIarshman. Indexing by latent semantic analysis. J. Amer. Soc. lnfo. \$ci., 41(6):391-407, 1990.\\
\\
D. Dolev , Y. Harari , and M. Pumas . Finding'the neighborhood of a query in a dictionary . In Proc. of {\textasciitilde}nd ISTC\$ , 1993 . D. Dolev, Y. Harari, and M. Pumas. Finding'the neighborhood of a query in a dictionary. In Proc. of {\textasciitilde}nd ISTC\$, 1993.\\
D. Dolev , Y. Harari , N. Linial , N. Nisan , and M. Pumas . Neighborhood preserving hashing and approximate queries . In Proc. of 5th SODA , pp. 251 - 259 , 1994 . D. Dolev, Y. Harari, N. Linial, N. Nisan, and M. Pumas. Neighborhood preserving hashing and approximate queries. In Proc. of 5th SODA, pp. 251-259, 1994.\\
J. Erickson . New lower bounds for Hopcroft's problem. Discrete Comput. {\textasciitilde}eom., 16:389-418 , 1996 . J. Erickson. New lower bounds for Hopcroft's problem. Discrete Comput. {\textasciitilde}eom., 16:389-418, 1996.\\
\\
\\
\\
M.L. Fredman . Lower bounds on the complexity of some optimal data structures . SIAM J. Computing , I0 : 1 - 10 , 198i M.L. Fredman. Lower bounds on the complexity of some optimal data structures. SIAM J. Computing, I0:1-10, 198i\\
\\
\\
T. H astie and R. Tibshirani. Discriminant adaptive nearest neighbor classification . In Is! International Conf. on Knowledge Discovery and Data Mining , 1995 . T. H astie and R. Tibshirani. Discriminant adaptive nearest neighbor classification. In Is! International Conf. on Knowledge Discovery and Data Mining, 1995.\\
\\
\\
\\
\\
\\
\\
\\
\\
P.B. Miltersen . The bit probe complexity measure revisited . In Pvoc. of l Oth STA C\$ , pp. 662 - 671 , 1993 . P.B. Miltersen. The bit probe complexity measure revisited. In Pvoc. of l Oth STA C\$, pp. 662- 671, 1993.\\
\\
\\
\\
\\
\\
G. Salton . Automatic Text Processing . Addison- Wesley , 1989 . G. Salton. Automatic Text Processing. Addison- Wesley, 1989.\\
A.W.M. Smeulders and R. Jain (eds). Proc. 1st Workshop on Image Databases and Multi-Media Search , 1996 . A.W.M. Smeulders and R. Jain (eds). Proc. 1st Workshop on Image Databases and Multi-Media Search, 1996.\\
B. Xiao . New Bounds in Cell Probe Model . Ph. D. thesis , UC San Diego , 1992 . B. Xiao. New Bounds in Cell Probe Model. Ph.D. thesis, UC San Diego, 1992.},
  file = {/Users/tulasi/Zotero/storage/MBMTU649/Borodin et al. - 1999 - Lower bounds for high dimensional nearest neighbor search and related problems.pdf}
}

@article{borodinLowerBoundsHigh1999a,
  title = {Lower Bounds for High Dimensional Nearest Neighbor Search and Related Problems},
  author = {Borodin, A. and Ostrovsky, R. and Rabani, Y.},
  year = {1999},
  doi = {10.1145/301250.301330},
  abstract = {In spite of extensive and continuing research, for various geometric search problems (such as nearest neighbor search), the best algorithms known have performance that degrades exponentially in the dimension. This phenomenon is sometimes called the curse of dimensionality. Recent results [37, 38, 40] show that in some sense it is possible to avoid the curse of dimensionality for the approximate nearest neighbor search problem. But must the exact nearest neighbor search problem suffer this curse? We provide some evidence in support of the curse. Specifically we investigate the exact nearest neighbor search problem and the related problem of exact partial match within the asymmetric communication model first used by Miltersen [43] to study data structure problems. We derive non-trivial asymptotic lower bounds for the exact problem that stand in contrast to known algorithms for approximate nearest neighbor search.},
  citationcount = {114},
  venue = {Symposium on the Theory of Computing},
  keywords = {communication,data structure,lower bound}
}

@article{borradaileConnectivityOraclesFor2012,
  title = {Connectivity Oracles for Planar Graphs},
  author = {Borradaile, G. and Pettie, Seth and {Wulff-Nilsen}, Christian},
  year = {2012},
  doi = {10.1007/978-3-642-31155-0_28},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Scandinavian Workshop on Algorithm Theory}
}

@article{boseAHistoryOf2013,
  title = {A History of Distribution-Sensitive Data Structures},
  author = {Bose, P. and Howat, John and Morin, Pat},
  year = {2013},
  doi = {10.1007/978-3-642-40273-9_10},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Space-Efficient Data Structures, Streams, and Algorithms},
  keywords = {data structure}
}

@article{boseApproximateRangeMode2005,
  title = {Approximate Range Mode and Range Median Queries},
  author = {Bose, P. and Kranakis, E. and Morin, Pat and Tang, Yihui},
  year = {2005},
  doi = {10.1007/978-3-540-31856-9_31},
  abstract = {No abstract available},
  citationcount = {32},
  venue = {Symposium on Theoretical Aspects of Computer Science}
}

@article{boseBiasedPredecessorSearch2011,
  title = {Biased Predecessor Search},
  author = {Bose, P. and Fagerberg, Rolf and Howat, John and Morin, Pat},
  year = {2011},
  doi = {10.1007/s00453-016-0146-7},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Algorithmica}
}

@article{boseFastLocalSearches2013,
  title = {Fast Local Searches and Updates in Bounded Universes},
  author = {Bose, P. and Dou{\"i}eb, Karim and Dujmovi{\'c}, V. and Howat, John and Morin, Pat},
  year = {2013},
  doi = {10.1016/j.comgeo.2012.01.002},
  abstract = {No abstract available},
  citationcount = {14},
  venue = {Canadian Conference on Computational Geometry},
  keywords = {update}
}

@article{bosekOnlineBipartiteMatching2014,
  title = {Online Bipartite Matching in Offline Time},
  author = {Bosek, B. and Leniowski, Dariusz and Sankowski, P. and Zych, Anna},
  year = {2014},
  doi = {10.1109/FOCS.2014.48},
  abstract = {This paper investigates the problem of maintaining maximum size matchings in incremental bipartite graphs. In this problem a bipartite graph G between n clients and n servers is revealed online. The clients arrive in an arbitrary order and request to be matched to a subset of servers. In our model we allow the clients to switch between servers and want to maximize the matching size between them, i.e., after a client arrives we find an augmenting path from a client to a free server. Our goals in this model are twofold. First, we want to minimize the number of times clients are reallocated between the servers. Second, we want to give fast algorithms that recompute such reallocation. As for the number of changes, we propose a greedy algorithm that chooses an augmenting path {$\pi$} that minimizes the maximum number of times each server in {$\pi$} was used by augmenting paths so far. We show that in this algorithm each server has its client reassigned O({\textsurd}n) times. This gives an O(n3/2) bound on the total number of changes, what gives a progress towards the main open question risen by Chaudhuri et al. (INFOCOM'09) who asked to prove O(n log n) upper bound. Next, we argue that the same bound holds in the decremental case. Moreover, we show incremental and decremental algorithms that maintain (1 - {$\varepsilon$})-approximate matching with total of O({$\varepsilon$}-1n) reallocations, for any {$\varepsilon$} {\textquestiondown} 0. Finally, we address the question of how to efficiently compute paths given by this greedy algorithm. We show that by introducing proper amortization we can obtain an incremental algorithm that maintains the maximum size matching in total O({\textsurd}nm) time. This matches the running time of one of the fastest static maximum matching algorithms that was given by Hopcroft and Karp (SIAM J. Comput '73). We extend our result to decremental case where we give the same total bound on the running time. Additionally, we show O({$\varepsilon$}-1m) time incremental and decremental algorithms that maintain (1 - {$\varepsilon$})-approximate matching for any {$\varepsilon$} {\textquestiondown} 0. Observe that this bound matches the running time of the fastest approximate static solution as well.},
  citationcount = {59},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {static}
}

@article{bosseNewAlgorithmsFor2007,
  title = {New Algorithms for Approximate {{Nash}} Equilibria in Bimatrix Games},
  author = {Bosse, Hartwig and Byrka, J. and Markakis, E.},
  year = {2007},
  doi = {10.1016/j.tcs.2009.09.023},
  abstract = {No abstract available},
  citationcount = {113},
  venue = {Theoretical Computer Science}
}

@article{bostanAlgorithmesRapidesPour2010,
  title = {Algorithmes Rapides Pour Les Polyn{\^o}mes, S{\'e}ries Formelles et Matrices},
  author = {Bostan, Alin},
  year = {2010},
  doi = {10.5802/CCIRM.9},
  abstract = {Le calcul formel calcule des objets mathematiques exacts. Ce cours explore deux directions : la calculabilite et la complexite. La calculabilite etudie les classes d'objets mathematiques sur lesquelles des reponses peuvent etre obtenues algorithmiquement. La complexite donne ensuite des outils pour comparer des algorithmes du point de vue de leur efficacite. Ce cours passe en revue l'algorithmique efficace sur les objets fondamentaux que sont les entiers, les polynomes, les matrices, les series et les solutions d'equations differentielles ou de recurrences lineaires. On y montre que de nombreuses questions portant sur ces objets admettent une reponse en complexite (quasi-)optimale, en insistant sur les principes generaux de conception d'algorithmes efficaces. Ces notes sont derivees du cours " Algorithmes efficaces en calcul formel " du Master Parisien de Recherche en Informatique (2004-2010), co-ecrit avec Frederic Chyzak, Marc Giusti, Romain Lebreton, Bruno Salvy et Eric Schost. Le support de cours complet est disponible a l'url https://wikimpri.dptinfo.ens-cachan.fr/doku.php?id=cours:c-2-22},
  citationcount = {9},
  venue = {No venue available}
}

@article{bostanTellegenSPrinciple2003,
  title = {Tellegen's Principle into Practice},
  author = {Bostan, Alin and Lecerf, Gr{\'e}goire and Schost, {\'E}.},
  year = {2003},
  doi = {10.1145/860854.860870},
  abstract = {The transposition principle, also called Tellegen's principle, is a set of transformation rules for linear programs. Yet, though well known, it is not used systematically, and few practical implementations rely on it. In this article, we propose explicit transposed versions of polynomial multiplication and division but also new faster algorithms for multipoint evaluation, interpolation and their transposes. We report on their implementation in Shoup's NTL C++ library.},
  citationcount = {117},
  venue = {International Symposium on Symbolic and Algebraic Computation}
}

@article{boucherOnApproximatingString2012,
  title = {On Approximating String Selection Problems with Outliers},
  author = {Boucher, C. and Landau, G. M. and Levy, Avivit and Pritchard, David and Weimann, Oren},
  year = {2012},
  doi = {10.1007/978-3-642-31265-6_34},
  abstract = {No abstract available},
  citationcount = {14},
  venue = {Annual Symposium on Combinatorial Pattern Matching}
}

@article{boucherTheBoundedSearch2011,
  title = {The Bounded Search Tree Algorithm for the Closest String Problem Has Quadratic Smoothed Complexity},
  author = {Boucher, C.},
  year = {2011},
  doi = {10.1007/978-3-642-22993-0_17},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {International Symposium on Mathematical Foundations of Computer Science}
}

@article{boucherWhyLargeClosest2010,
  title = {Why Large Closest String Instances Are Easy to Solve in Practice},
  author = {Boucher, C. and Wilkie, K.},
  year = {2010},
  doi = {10.1007/978-3-642-16321-0_10},
  abstract = {No abstract available},
  citationcount = {6},
  venue = {SPIRE}
}

@article{bourgainOnLipschitzEmbedding1985,
  title = {On Lipschitz Embedding of Finite Metric Spaces in {{Hilbert}} Space},
  author = {Bourgain, J.},
  year = {1985},
  doi = {10.1007/BF02776078},
  abstract = {No abstract available},
  citationcount = {756},
  venue = {No venue available}
}

@article{bourgainOnTheDuality1989,
  title = {On the Duality Problem for Entropy Numbers of Operators},
  author = {Bourgain, J. and Pajor, A. and Szarek, S. and {Tomczak-Jaegermann}, N.},
  year = {1989},
  doi = {10.1007/BFB0090048},
  abstract = {No abstract available},
  citationcount = {70},
  venue = {No venue available}
}

@article{boyarSpaceEfficientData2013,
  title = {Space-Efficient Data Structures, Streams, and Algorithms},
  author = {Boyar, J. and Ellen, Faith},
  year = {2013},
  doi = {10.1007/978-3-642-40273-9},
  abstract = {No abstract available},
  citationcount = {26},
  venue = {Lecture Notes in Computer Science},
  keywords = {data structure}
}

@article{bozanisNewResultsOn1997,
  title = {New Results on Intersection Query Problems},
  author = {Bozanis, Panayiotis and Kitsios, Nectarios and Makris, C. and Tsakalidis, A.},
  year = {1997},
  doi = {10.1093/comjnl/40.1.22},
  abstract = {We present simple algorithms for three problems belonging to the class of intersection query problems. The first algorithm deals with the static rectangle enclosure problem and can easily be extended to d dimensions, the second algorithm copes with the generalized c-oriented polygon intersection searching problem in two dimensions, while the third solves the static 2-dimensional dominance searching problem with respect to a set of obstacles. All algorithms are simple, are based on persistence and improve previous bounds. Also, as a corollary of the first algorithm, we present a result for the static d-dimensional range searching problem.},
  citationcount = {25},
  venue = {Computer/law journal}
}

@article{bozanisNewUpperBounds1995,
  title = {New Upper Bounds for Generalized Intersection Searching Problems},
  author = {Bozanis, Panayiotis and Kitsios, Nectarios and Makris, C. and Tsakalidis, A.},
  year = {1995},
  doi = {10.1007/3-540-60084-1_97},
  abstract = {No abstract available},
  citationcount = {44},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{bradleyAnalysingDistributedInternet2008,
  title = {Analysing Distributed {{Internet}} Worm Attacks Using Continuous State-Space Approximation of Process Algebra Models},
  author = {Bradley, Jeremy T. and Gilmore, S. and Hillston, J.},
  year = {2008},
  doi = {10.1016/j.jcss.2007.07.005},
  abstract = {No abstract available},
  citationcount = {388},
  venue = {Journal of computer and system sciences (Print)}
}

@article{brakerskiOnTheHardness2020,
  title = {On the Hardness of Average-Case k-{{SUM}}},
  author = {Brakerski, Zvika and {Stephens-Davidowitz}, N. and Vaikuntanathan, V.},
  year = {2020},
  doi = {10.4230/LIPIcs.APPROX/RANDOM.2021.29},
  abstract = {In this work, we show the first worst-case to average-case reduction for the classical k-SUM problem. A k-SUM instance is a collection of m integers, and the goal of the k-SUM problem is to find a subset of k elements that sums to 0. In the average-case version, the m elements are chosen uniformly at random from some interval [-u,u]. We consider the total setting where m is sufficiently large (with respect to u and k), so that we are guaranteed (with high probability) that solutions must exist. Much of the appeal of k-SUM, in particular connections to problems in computational geometry, extends to the total setting. The best known algorithm in the average-case total setting is due to Wagner (following the approach of Blum-Kalai-Wasserman), and achieves a run-time of u\textsuperscript{\{\vphantom\}}O(1/k)\vphantom\{\}, which beats the known (conditional) lower bounds for worst-case k-SUM. One could wonder whether this can be improved even further. But, we show a matching average-case lower-bound, by showing a reduction from worst-case lattice problems, thus introducing a new family of techniques into the field of fine-grained complexity. In particular, we show that any algorithm solving average-case k-SUM on m elements in time u\textsuperscript{\{\vphantom\}}o(1/k)\vphantom\{\} will give a super-polynomial improvement in the complexity of algorithms for lattice problems.},
  citationcount = {5},
  venue = {International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques},
  keywords = {lower bound,reduction}
}

@article{brandAlgorithmAndHardness2023,
  title = {Algorithm and Hardness for Dynamic Attention Maintenance in Large Language Models},
  author = {{van den Brand}, Jan and Song, Zhao and Zhou, Tianyi},
  year = {2023},
  doi = {10.48550/arXiv.2304.02207},
  abstract = {Large language models (LLMs) have made fundamental changes in human life. The attention scheme is one of the key components over all the LLMs, such as BERT, GPT-1, Transformers, GPT-2, 3, 3.5 and 4. Inspired by previous theoretical study of static version of the attention multiplication problem [Zandieh, Han, Daliri, and Karbasi arXiv 2023, Alman and Song arXiv 2023]. In this work, we formally define a dynamic version of attention matrix multiplication problem. There are matrices Q,K,V{$\in$}\{R\}\textsuperscript{\{\vphantom\}}n{\texttimes}d\vphantom\{\}, they represent query, key and value in LLMs. In each iteration we update one entry in K or V. In the query stage, we receive (i,j){$\in$}[n]{\texttimes}[d] as input, and want to answer (D\textsuperscript{\{\vphantom\}}-1\vphantom\{\}AV)\textsubscript{\{\vphantom\}}i,j\vphantom\{\}, where A:=(QK\textsuperscript{{$\top$}}){$\in$}\{R\}\textsuperscript{\{\vphantom\}}n{\texttimes}n\vphantom\{\} is a square matrix and D:=\{diag\}(A\{1\}\textsubscript{n}){$\in$}\{R\}\textsuperscript{\{\vphantom\}}n{\texttimes}n\vphantom\{\} is a diagonal matrix. Here \{1\}\textsubscript{n} denote a length-n vector that all the entries are ones. We provide two results: an algorithm and a conditional lower bound. {$\bullet$} On one hand, inspired by the lazy update idea from [Demetrescu and Italiano FOCS 2000, Sankowski FOCS 2004, Cohen, Lee and Song STOC 2019, Brand SODA 2020], we provide a data-structure that uses O(n\textsuperscript{\{\vphantom\}}{$\omega$}(1,1,{$\tau$})-{$\tau$}\vphantom\{\}) amortized update time, and O(n\textsuperscript{\{\vphantom\}}1+{$\tau$}\vphantom\{\}) worst-case query time. {$\bullet$} On the other hand, show that unless the hinted matrix vector multiplication conjecture [Brand, Nanongkai and Saranurak FOCS 2019] is false, there is no algorithm that can use both O(n\textsuperscript{\{\vphantom\}}{$\omega$}(1,1,{$\tau$})-{$\tau$}-{\textohm}(1)\vphantom\{\}) amortized update time, and O(n\textsuperscript{\{\vphantom\}}1+{$\tau$}-{\textohm}(1)\vphantom\{\}) worst query time. In conclusion, our algorithmic result is conditionally optimal unless hinted matrix vector multiplication conjecture is false.},
  citationcount = {31},
  venue = {International Conference on Machine Learning},
  keywords = {data structure,dynamic,lower bound,query,query time,static,update,update time}
}

@article{brandAlmostLinearTime2024,
  title = {Almost-Linear Time Algorithms for Decremental Graphs: {{Min-cost}} Flow and More via Duality},
  author = {{van den Brand}, Jan and Chen, Li and Kyng, Rasmus and Liu, Yang P. and Meierhans, Simon and Gutenberg, Maximilian Probst and Sachdeva, Sushant},
  year = {2024},
  doi = {10.1109/FOCS61266.2024.00120},
  abstract = {We give the first almost-linear total time algorithm for deciding if a flow of cost at most F still exists in a directed graph, with edge costs and capacities, undergoing decremental updates, i.e., edge deletions, capacity decreases, and cost increases. This implies almost-linear time algorithms for approximating the minimum-cost flow value and s-t distance on such decremental graphs. Our framework additionally allows us to maintain decremental strongly connected components in almost-linear time deterministically. These algorithms also improve over the current best known runtimes for statically computing minimum-cost flow, in both the randomized and deterministic settings. We obtain our algorithms by taking the dual perspective, which yields cut-based algorithms. More precisely, our algorithm computes the flow via a sequence of m\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}-dynamic min-ratio cut problems, the dual analog of the dynamic min-ratio cycle problem that underlies recent fast algorithms for minimum-cost flow. Our main technical contribution is a new data structure that returns an approximately optimal min-ratio cut in amortized m\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} time by maintaining a tree-cut sparsifier. This is achieved by devising a new algorithm to maintain the dynamic expander hierarchy of [\{Goranci-Racke-\} SaranurakTan, SODA 2021] that also works in capacitated graphs. All our algorithms are deterministc, though they can be sped up further using randomized techniques while still working against an adaptive adversary.},
  citationcount = {1},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {adaptive,data structure,dynamic,static,update}
}

@article{brandDeterministicFullyDynamic2023,
  title = {Deterministic Fully Dynamic {{SSSP}} and More},
  author = {{van den Brand}, Jan and Karczmarz, Adam},
  year = {2023},
  doi = {10.1109/FOCS57990.2023.00142},
  abstract = {We present the first non-trivial fully dynamic algorithm maintaining exact single-source distances in unweighted graphs. This resolves an open problem stated by Sankowski [COCOON 2005] and van den Brand and Nanongkai [FOCS 2019]. Previous fully dynamic single-source distances data structures were all approximate, but so far, non-trivial dynamic algorithms for the exact setting could only be ruled out for polynomially weighted graphs (Abboud and Vassilevska Williams, [FOCS 2014]). The exact unweighted case remained the main case for which neither a subquadratic dynamic algorithm nor a quadratic lower bound was known.Our dynamic algorithm works on directed graphs and is deterministic, and can report a single-source shortest paths tree in subquadratic time as well. Thus we also obtain the first deterministic fully dynamic data structure for reachability (transitive closure) with subquadratic update and query time. This answers an open problem of van den Brand, Nanongkai, and Saranurak [FOCS 2019]. Finally, using the same framework we obtain the first fully dynamic data structure maintaining all-pairs (1+{$\epsilon$})-approximate distances within non-trivial sub-n\textsuperscript{\{\vphantom\}}{$\omega$}\vphantom\{\} worst-case update time while supporting optimal-time approximate shortest path reporting at the same time. This data structure is also deterministic and therefore implies the first known non-trivial deterministic worst-case bound for recomputing the transitive closure of a digraph.},
  citationcount = {1},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,dynamic,lower bound,query,query time,update,update time}
}

@article{brandDynamicApproximateShortest2019,
  title = {Dynamic Approximate Shortest Paths and beyond: {{Subquadratic}} and Worst-Case Update Time},
  author = {{van den Brand}, Jan and Nanongkai, Danupon},
  year = {2019},
  doi = {10.1109/FOCS.2019.00035},
  abstract = {Consider the following distance query for an n-node graph G undergoing edge insertions and deletions: given two sets of nodes I and J, return the distances between every pair of nodes in I {\texttimes} J.},
  citationcount = {39},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,query,update,update time}
}

@article{brandDynamicMatrixInverse2019,
  title = {Dynamic Matrix Inverse: {{Improved}} Algorithms and Matching Conditional Lower Bounds},
  author = {{van den Brand}, Jan and Nanongkai, Danupon and Saranurak, Thatchaphol},
  year = {2019},
  doi = {10.1109/FOCS.2019.00036},
  abstract = {The dynamic matrix inverse problem is to maintain the inverse of a matrix undergoing element and column updates. It is the main subroutine behind the best algorithms for many dynamic problems whose complexity is not yet well-understood, such as maintaining the largest eigenvalue, rank and determinant of a matrix and maintaining reachability, distances, maximum matching size, and k-paths/cycles in a graph. Understanding the complexity of dynamic matrix inverse is a key to understand these problems. In this paper, we present (i) improved algorithms for dynamic matrix inverse and their extensions to some incremental/look-ahead variants, and (ii) variants of the Online Matrix-Vector conjecture [Henzinger et al. STOC'15] that, if true, imply that these algorithms are tight. Our algorithms automatically lead to faster dynamic algorithms for the aforementioned problems, some of which are also tight under our conjectures, e.g. reachability and maximum matching size (closing the gaps for these two problems was in fact asked by Abboud and V. Williams [FOCS'14]). Prior best bounds for most of these problems date back to more than a decade ago [Sankowski FOCS'04, COCOON'05, SODA'07; Kavitha FSTTCS'08; Mucha and Sankowski Algorithmica'10; Bosek et al. FOCS'14]. Our improvements stem mostly from the ability to use fast matrix multiplication ``one more time'', to maintain a certain transformation matrix which could be maintained only combinatorially previously (i.e. without fast matrix multiplication). Oddly, unlike other dynamic problems where this approach, once successful, could be repeated several times (``bootstrapping''), our conjectures imply that this is not the case for dynamic matrix inverse and some related problems. However, when a small additional ``look-ahead'' information is provided we can perform such repetition to drive the bounds down further.},
  citationcount = {69},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,lower bound,update}
}

@article{brandDynamicMaxflowVia2022,
  title = {Dynamic Maxflow via Dynamic Interior Point Methods},
  author = {{van den Brand}, Jan and Liu, Y. and Sidford, Aaron},
  year = {2022},
  doi = {10.1145/3564246.3585135},
  abstract = {In this paper we provide an algorithm for maintaining a (1-{\cyrchar\cyrie})-approximate maximum flow in a dynamic, capacitated graph undergoing edge insertions. Over a sequence of m insertions to an n-node graph where every edge has capacity O(poly(m)) our algorithm runs in time O(m {\textsurd}n {$\cdot$} {\cyrchar\cyrie}-1). To obtain this result we design dynamic data structures for the more general problem of detecting when the value of the minimum cost circulation in a dynamic graph undergoing edge insertions achieves value at most F (exactly) for a given threshold F. Over a sequence m insertions to an n-node graph where every edge has capacity O(poly(m)) and cost O(poly(m)) we solve this thresholded minimum cost flow problem in O(m {\textsurd}n). Both of our algorithms succeed with high probability against an adaptive adversary. We obtain these results by dynamizing the recent interior point method by [Chen et al.\;FOCS 2022] used to obtain an almost linear time algorithm for minimum cost flow, and introducing a new dynamic data structure for maintaining minimum ratio cycles in an undirected graph that succeeds with high probability against adaptive adversaries.},
  citationcount = {6},
  venue = {Symposium on the Theory of Computing},
  keywords = {adaptive,data structure,dynamic}
}

@article{brandFastConvolutionsFor2023,
  title = {Fast Convolutions for Near-Convex Sequences},
  author = {Brand, Cornelius and Lassota, Alexandra},
  year = {2023},
  doi = {10.4230/LIPIcs.ISAAC.2023.16},
  abstract = {We develop algorithms for (min , +)- Convolution and related convolution problems such as Super Additivity Testing , Convolution 3-Sum and Minimum Consecutive Subsums which use the degree of convexity of the instance as a parameter. Assuming the min-plus conjecture (K{\"u}nnemann-Paturi-Schneider, ICALP'17 and Cygan et al., ICALP'17), our results interpolate in an optimal manner between fully convex instances, which can be solved in near-linear time using Legendre transformations, and general non-convex sequences, where the trivial quadratic-time algorithm is conjectured to be best possible, up to subpolynomial factors.},
  citationcount = {Unknown},
  venue = {International Symposium on Algorithms and Computation}
}

@article{brandOnDynamicGraph2023,
  title = {On Dynamic Graph Algorithms with Predictions},
  author = {{van den Brand}, Jan and Forster, S. and Nazari, Yasamin and Polak, Adam},
  year = {2023},
  doi = {10.48550/arXiv.2307.09961},
  abstract = {We study dynamic algorithms in the model of algorithms with predictions. We assume the algorithm is given imperfect predictions regarding future updates, and we ask how such predictions can be used to improve the running time. This can be seen as a model interpolating between classic online and offline dynamic algorithms. Our results give smooth tradeoffs between these two extreme settings. First, we give algorithms for incremental and decremental transitive closure and approximate APSP that take as an additional input a predicted sequence of updates (edge insertions, or edge deletions, respectively). They preprocess it in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}(3+{$\omega$})/2\vphantom\{\}) time, and then handle updates in O\vphantom\{\}(1) worst-case time and queries in O\vphantom\{\}({$\eta^2$}) worst-case time. Here {$\eta$} is an error measure that can be bounded by the maximum difference between the predicted and actual insertion (deletion) time of an edge, i.e., by the {$\ell_\infty$}-error of the predictions. The second group of results concerns fully dynamic problems with vertex updates, where the algorithm has access to a predicted sequence of the next n updates. We show how to solve fully dynamic triangle detection, maximum matching, single-source reachability, and more, in O(n\textsuperscript{\{\vphantom\}}{$\omega$}-1\vphantom\{\}+n{$\eta$}\textsubscript{i}) worst-case update time. Here {$\eta$}\textsubscript{i} denotes how much earlier the i-th update occurs than predicted. Our last result is a reduction that transforms a worst-case incremental algorithm without predictions into a fully dynamic algorithm which is given a predicted deletion time for each element at the time of its insertion. As a consequence we can, e.g., maintain fully dynamic exact APSP with such predictions in O\vphantom\{\}(n{$^2$}) worst-case vertex insertion time and O\vphantom\{\}(n{$^2$}(1+{$\eta$}\textsubscript{i})) worst-case vertex deletion time (for the prediction error {$\eta$}\textsubscript{i} defined as above).},
  citationcount = {8},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,query,reduction,update,update time}
}

@article{braunInfoGreedySequential2014,
  title = {Info-Greedy Sequential Adaptive Compressed Sensing},
  author = {Braun, G{\'a}bor and Pokutta, S. and Xie, Yao},
  year = {2014},
  doi = {10.1109/JSTSP.2015.2400428},
  abstract = {We present an information-theoretic framework for sequential adaptive compressed sensing, Info-Greedy Sensing, where measurements are chosen to maximize the extracted information conditioned on the previous measurements. We show that the widely used bisection approach is Info-Greedy for a family of k-sparse signals by connecting compressed sensing and blackbox complexity of sequential query algorithms, and present Info-Greedy algorithms for Gaussian and Gaussian mixture model (GMM) signals, as well as ways to design sparse Info-Greedy measurements. Numerical examples demonstrate the good performance of the proposed algorithms using simulated and real data: Info-Greedy Sensing shows significant improvement over random projection for signals with sparse and low-rank covariance matrices, and adaptivity brings robustness when there is a mismatch between the assumed and the true distributions.},
  citationcount = {33},
  venue = {IEEE Journal on Selected Topics in Signal Processing},
  keywords = {adaptive,information theoretic,query}
}

@article{braunLowerBoundsOn2014,
  title = {Lower Bounds on the Oracle Complexity of Nonsmooth Convex Optimization via Information Theory},
  author = {Braun, G{\'a}bor and Guzm{\'a}n, Crist{\'o}bal and Pokutta, S.},
  year = {2014},
  doi = {10.1109/TIT.2017.2701343},
  abstract = {We present an information-theoretic approach to lower bound the oracle complexity of nonsmooth black box convex optimization, unifying previous lower bounding techniques by identifying a combinatorial problem, namely string guessing, as a single source of hardness. As a measure of complexity, we use distributional oracle complexity, which subsumes randomized oracle complexity as well as worst case oracle complexity. We obtain strong lower bounds on distributional oracle complexity for the box {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}[-1,1]\textsuperscript{\{\vphantom\}}n\vphantom\{\} {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}, as well as for the {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}L\textsuperscript{\{\vphantom\}}p\vphantom\{\} {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}-ball for {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}p{$\geq$}1 {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} (for both low-scale and large-scale regimes), matching worst case upper bounds, and hence we close the gap between distributional complexity, and in particular, randomized complexity and worst case complexity. Furthermore, the bounds remain essentially the same for high-probability and bounded-error oracle complexity, and even for combination of the two, i.e., bounded-error high-probability oracle complexity. This considerably extends the applicability of known bounds.},
  citationcount = {26},
  venue = {IEEE Transactions on Information Theory},
  keywords = {information theoretic,lower bound}
}

@article{braunsteinSurveyPropagationAn2005,
  title = {Survey Propagation: {{An}} Algorithm for Satisfiability},
  author = {Braunstein, A. and M{\'e}zard, Marc and Zecchina, R.},
  year = {2005},
  doi = {10.1002/RSA.V27:2},
  abstract = {We study the satisfiability of randomly generated formulas formed by M clauses of exactly K literals over N Boolean variables. For a given value of N the problem is known to be most difficult when {$\alpha$} = M/N is close to the experimental threshold {$\alpha$}c separating the region where almost all formulas are SAT from the region where all formulas are UNSAT. Recent results from a statistical physics analysis suggest that the difficulty is related to the existence of a clustering phenomenon of the solutions when {$\alpha$} is close to (but smaller than) {$\alpha$}c. We introduce a new type of message passing algorithm which allows to find efficiently a satisfying assignment of the variables in this difficult region. This algorithm is iterative and composed of two main parts. The first is a message-passing procedure which generalizes the usual methods like Sum-Product or Belief Propagation: It passes messages that may be thought of as surveys over clusters of the ordinary messages. The second part uses the detailed probabilistic information obtained from the surveys in order to fix variables and simplify the problem. Eventually, the simplified problem that remains is solved by a conventional heuristic. {\copyright} 2005 Wiley Periodicals, Inc. Random Struct. Alg., 2005},
  citationcount = {66},
  venue = {No venue available}
}

@article{bravermanApproximatingTheBest2015,
  title = {Approximating the Best Nash Equilibrium in No(Log n)-Time Breaks the Exponential Time Hypothesis},
  author = {Braverman, M. and {Kun-Ko}, Young and Weinstein, Omri},
  year = {2015},
  doi = {10.1137/1.9781611973730.66},
  abstract = {The celebrated PPAD hardness result for finding an exact Nash equilibrium in a two-player game initiated a quest for finding approximate Nash equilibria efficiently, and is one of the major open questions in algorithmic game theory. We study the computational complexity of finding an e-approximate Nash equilibrium with good social welfare. Hazan and Krauthgamer and subsequent improvements showed that finding an e-approximate Nash equilibrium with good social welfare in a two player game and many variants of this problem is at least as hard as finding a planted clique of size O(log n) in the random graph G(n, 1/2). We show that any polynomial time algorithm that finds an e-approximate Nash equilibrium with good social welfare refutes (the worst-case) Exponential Time Hypothesis by Impagliazzo and Paturi, confirming the recent conjecture by Aaronson, Impagliazzo and Moshkovitz. Specifically it would imply a 2O(n1/2) algorithm for SAT. Our lower bound matches the quasi-polynomial time algorithm by Lipton, Markakis and Mehta for solving the problem. Our key tool is a reduction from the PCP machinery to finding Nash equilibrium via free games, the framework introduced in the recent work by Aaronson, Impagliazzo and Moshkovitz. Techniques developed in the process may be useful for replacing planted clique hardness with ETH-hardness in other applications.},
  citationcount = {65},
  venue = {Electron. Colloquium Comput. Complex.}
}

@article{bravermanDirectProductsIn2013,
  title = {Direct Products in Communication Complexity},
  author = {Braverman, M. and Rao, Anup and Weinstein, Omri and Yehudayoff, A.},
  year = {2013},
  doi = {10.1109/FOCS.2013.85},
  abstract = {We give exponentially small upper bounds on the success probability for computing the direct product of any function over any distribution using a communication protocol. Let suc({$\mu$}, f, C) denote the maximum success probability of a 2-party communication protocol for computing the boolean function f(x, y) with C bits of communication, when the inputs (x, y) are drawn from the distribution {$\mu$}. Let {$\mu$}{\textexclamdown}sup{\textquestiondown}n{\textexclamdown}/sup{\textquestiondown} be the product distribution on n inputs and f{\textexclamdown}sup{\textquestiondown}n{\textexclamdown}/sup{\textquestiondown} denote the function that computes n copies of f on these inputs. We prove that if T log{\textexclamdown}sup{\textquestiondown}3/2{\textexclamdown}/sup{\textquestiondown} T {$\ll$} (C - 1){\textsurd}n and suc({$\mu$}, f, C) {\textexclamdown}; 2/3, then suc({$\mu$}{\textexclamdown}sup{\textquestiondown}n{\textexclamdown}/sup{\textquestiondown}, f{\textexclamdown}sup{\textquestiondown}n{\textexclamdown}/sup{\textquestiondown}, T) {$\leq$} exp(-{\textohm}(n)). When {$\mu$} is a product distribution, we prove a nearly optimal result: as long as T log{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} T {$\ll$} Cn, we must have suc({$\mu$}{\textexclamdown}sup{\textquestiondown}n{\textexclamdown}/sup{\textquestiondown}, f{\textexclamdown}sup{\textquestiondown}n{\textexclamdown}/sup{\textquestiondown}, T) {$\leq$} exp(-{\textohm}(n)).},
  citationcount = {82},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{bravermanOnInformationComplexity2015,
  title = {On Information Complexity in the Broadcast Model},
  author = {Braverman, M. and Oshman, R.},
  year = {2015},
  doi = {10.1145/2767386.2767425},
  abstract = {Information complexity is the extension of classical information theory to the interactive setting, where instead of one-way transmission we are interested in back-and-forth communication. This approach has been very influential in communication complexity, where it enables us to prove powerful lower bounds by quantifying the amount of information the participants in the computation must reveal about their inputs. In this paper we study information complexity in the classical broadcast model: k parties with private inputs wish to compute some function of their inputs, and they communicate by sending messages (one at a time) over a broadcast channel. We measure how much information the players reveal about their inputs to an external observer. This is called external information cost. Using this approach, we prove a tight lower bound of {\textohm}(n log k + k) on the communication complexity of set disjointness, a fundamental problem in communication complexity. We also give a deterministic matching upper bound. Next we study compression, a central question in information complexity: given a protocol with low information cost (but possibly high communication), can we compress the protocol so that its communication cost matches its information cost? In the two-player setting, it is known that every protocol can be compressed to roughly its external information cost. We show that for the multi-party case this is no longer true: there is a gap of at least {\textohm}(k/log k) between external information and communication. However, if we wish to compress many independent instances of the same protocol, then it is possible to do so with an amortized per-copy cost that approaches the information cost as the number of copies goes to infinity.},
  citationcount = {17},
  venue = {ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing},
  keywords = {communication,communication complexity,lower bound}
}

@article{bravermanPolyLogarithmicIndependence2011,
  title = {Poly-Logarithmic Independence Fools Bounded-Depth Boolean Circuits},
  author = {Braverman, M.},
  year = {2011},
  doi = {10.1145/1924421.1924446},
  abstract = {1. intRoDuCtion The question of determining which (weak) forms of random-ness " fool " (or seem totally random to) a given algorithm is a basic and fundamental question in the modern theory of computer science. In this work we report progress on this question by showing that any " k-wise independent " collection of random bits, for some k = (log n)},
  citationcount = {36},
  venue = {Communications of the ACM}
}

@article{bravermanSemiDirectSumTheorem2018,
  title = {Semi-{{Direct Sum Theorem}} and {{Nearest Neighbor}} under L\_infty},
  author = {Braverman, Mark and Ko, Young Kun},
  editor = {Blais, Eric and Jansen, Klaus and Rolim, Jos{\'e} D. P. and Steurer, David},
  year = {2018},
  journal = {LIPIcs, Volume 116, APPROX/RANDOM 2018},
  volume = {116},
  pages = {6:1-6:17},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  issn = {1868-8969},
  doi = {10.4230/LIPICS.APPROX-RANDOM.2018.6},
  url = {https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.APPROX-RANDOM.2018.6},
  urldate = {2025-04-07},
  abstract = {We introduce semi-direct sum theorem as a framework for proving asymmetric communication lower bounds for the functions of the form V\_\{i=1\}{\textasciicircum}n f(x,y\_i). Utilizing tools developed in proving direct sum theorem for information complexity, we show that if the function is of the form V\_\{i=1\}{\textasciicircum}n f(x,y\_i) where Alice is given x and Bob is given y\_i's, it suffices to prove a lower bound for a single f(x,y\_i). This opens a new avenue of attack other than the conventional combinatorial technique (i.e. "richness lemma" from [Miltersen et al., 1995]) for proving randomized lower bounds for asymmetric communication for functions of such form. As the main technical result and an application of semi-direct sum framework, we prove an information lower bound on c-approximate Nearest Neighbor (ANN) under l\_infty which implies that the algorithm of [Indyk, 2001] for c-approximate Nearest Neighbor under l\_infty is optimal even under randomization for both decision tree and cell probe data structure model (under certain parameter assumption for the latter). In particular, this shows that randomization cannot improve [Indyk, 2001] under decision tree model. Previously only a deterministic lower bound was known by [Andoni et al., 2008] and randomized lower bound for cell probe model by [Kapralov and Panigrahy, 2012]. We suspect further applications of our framework in exhibiting randomized asymmetric communication lower bounds for big data applications.},
  copyright = {Creative Commons Attribution 3.0 Unported license, info:eu-repo/semantics/openAccess},
  isbn = {9783959770859},
  langid = {english},
  keywords = {Asymmetric Communication Lower Bound,cell probe,communication,communication complexity,data structure,Data Structure Lower Bound,lower bound,Nearest Neighbor Search},
  file = {/Users/tulasi/Zotero/storage/GBLANXEI/Braverman and Ko - 2018 - Semi-Direct Sum Theorem and Nearest Neighbor under l_infty.pdf}
}

@article{bremler-barrEncodingShortRanges2016,
  title = {Encoding Short Ranges in {{TCAM}} without Expansion: {{Efficient}} Algorithm and Applications},
  author = {{Bremler-Barr}, A. and Harchol, Yotam and Hay, David and {Hel-Or}, Y.},
  year = {2016},
  doi = {10.1145/2935764.2935769},
  abstract = {We present range encoding with no expansion (REN{\'E})--- a novel encoding scheme for short ranges on Ternary content addressable memory (TCAM), which, unlike previous solutions, does not impose row expansion, and uses bits proportionally to the maximal range length. We provide theoretical analysis to show that our encoding is the closest to the lower bound of number of bits used. In addition, we show several applications of our technique in the field of packet classification, and also, how the same technique could be used to efficiently solve other hard problems, such as the nearest-neighbor search problem and its variants. We show that using TCAM, one could solve such problems in much higher rates than previously suggested solutions, and outperform known lower bounds in traditional memory models. We show by experiments that the translation process of REN{\'E} on switch hardware induces only a negligible 2.5},
  citationcount = {24},
  venue = {IEEE/ACM Transactions on Networking},
  keywords = {lower bound}
}

@article{bremler-barrUltraFastSimilarity2015,
  title = {Ultra-Fast Similarity Search Using Ternary Content Addressable Memory},
  author = {{Bremler-Barr}, A. and Harchol, Yotam and Hay, David and {Hel-Or}, Y.},
  year = {2015},
  doi = {10.1145/2771937.2771938},
  abstract = {Similarity search, and specifically the nearest-neighbor search (NN) problem is widely used in many fields of computer science such as machine learning, computer vision and databases. However, in many settings such searches are known to suffer from the notorious curse of dimensionality, where running time grows exponentially with d. This causes severe performance degradation when working in high-dimensional spaces. Approximate techniques such as locality-sensitive hashing [2] improve the performance of the search, but are still computationally intensive. In this paper we propose a new way to solve this problem using a special hardware device called ternary content addressable memory (TCAM). TCAM is an associative memory, which is a special type of computer memory that is widely used in switches and routers for very high speed search applications. We show that the TCAM computational model can be leveraged and adjusted to solve NN search problems in a single TCAM lookup cycle, and with linear space. This concept does not suffer from the curse of dimensionality and is shown to improve the best known approaches for NN by more than four orders of magnitude. Simulation results demonstrate dramatic improvement over the best known approaches for NN, and suggest that TCAM devices may play a critical role in future large-scale databases and cloud applications.},
  citationcount = {14},
  venue = {International Workshop on Data Management on New Hardware}
}

@article{brentFastAlgorithmsFor1978,
  title = {Fast Algorithms for Manipulating Formal Power Series},
  author = {Brent, R. and Kung, H. T.},
  year = {1978},
  doi = {10.1145/322092.322099},
  abstract = {The classical algorithms require order n   operations to compute the first n terms in the reversion of a power series or the composition of two series, and order nelog n operations if the fast Founer transform is used for power series multiplication In this paper we show that the composition and reversion problems are equivalent (up to constant factors), and we give algorithms which require only order (n log n)  /2 operations In many cases of practical importance only order n log n operations are required, these include certain special functions of power series and power series solution of certain differential equations Applications to root-finding methods which use inverse mterpolauon and to queuemg theory are described, some results on multivariate power series are stated, and several open questions are mentioned},
  citationcount = {295},
  venue = {JACM}
}

@article{brieulleComputingIsomorphismsAnd2017,
  title = {Computing Isomorphisms and Embeddings of Finite Fields},
  author = {Brieulle, Ludovic and Feo, L. D. and Doliskani, Javad and Flori, Jean-Pierre and Schost, {\'E}.},
  year = {2017},
  doi = {10.1090/mcom/3363},
  abstract = {Let {\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown} be a prime power and let F{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown} be a field with {\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown} elements. Let {\textexclamdown}i{\textquestiondown}f{\textexclamdown}/i{\textquestiondown} and {\textexclamdown}i{\textquestiondown}g{\textexclamdown}/i{\textquestiondown} be irreducible polynomials in F{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown}[{\textexclamdown}i{\textquestiondown}X{\textexclamdown}/i{\textquestiondown}], with deg {\textexclamdown}i{\textquestiondown}f{\textexclamdown}/i{\textquestiondown} dividing deg {\textexclamdown}i{\textquestiondown}g.{\textexclamdown}/i{\textquestiondown} Define {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown} = F{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown}[{\textexclamdown}i{\textquestiondown}X{\textexclamdown}/i{\textquestiondown}]/{\textexclamdown}i{\textquestiondown}f{\textexclamdown}/i{\textquestiondown} and {\textexclamdown}i{\textquestiondown}K{\textexclamdown}/i{\textquestiondown} = F{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown}[{\textexclamdown}i{\textquestiondown}X{\textexclamdown}/i{\textquestiondown}]/{\textexclamdown}i{\textquestiondown}g{\textexclamdown}/i{\textquestiondown}, then there is an embedding {\textexclamdown}i{\textquestiondown}{$\varphi$}{\textexclamdown}/i{\textquestiondown} : {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown} [EQUATION] {\textexclamdown}i{\textquestiondown}K{\textexclamdown}/i{\textquestiondown}, unique up to F{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown}-automorphisms of {\textexclamdown}i{\textquestiondown}k.{\textexclamdown}/i{\textquestiondown} Our goal is to describe algorithms to efficiently represent and evaluate one such embedding.},
  citationcount = {8},
  venue = {ACCA}
}

@article{bringmannAStructuralInvestigation2022,
  title = {A Structural Investigation of the Approximability of Polynomial-Time Problems},
  author = {Bringmann, K. and Cassis, Alejandro and Fischer, N. and K{\"u}nnemann, Marvin},
  year = {2022},
  doi = {10.48550/arXiv.2204.11681},
  abstract = {We initiate the systematic study of a recently introduced polynomial-time analogue of MaxSNP, which includes a large number of well-studied problems (including Nearest and Furthest Neighbor in the Hamming metric, Maximum Inner Product, optimization variants of k-XOR and Maximum k-Cover). Specifically, MaxSP\textsubscript{k} denotes the class of O(m\textsuperscript{k})-time problems of the form \textsubscript{\{\vphantom\}}x{$_1$},{\dots},x\textsubscript{k}\vphantom\{\}\# y:{$\phi$}(x\_1,{\dots},x\_k,y)  where {$\phi$} is a quantifier-free first-order property and m denotes the size of the relational structure. Assuming central hypotheses about clique detection in hypergraphs and MAX3SAT, we show that for any MaxSP\textsubscript{k} problem definable by a quantifier-free m-edge graph formula {$\phi$}, the best possible approximation guarantee in faster-than-exhaustive-search time O(m\textsuperscript{\{\vphantom\}}k-{$\delta$}\vphantom\{\}) falls into one of four categories: * optimizable to exactness in time O(m\textsuperscript{\{\vphantom\}}k-{$\delta$}\vphantom\{\}), * an (inefficient) approximation scheme, i.e., a (1+{$\epsilon$})-approximation in time O(m\textsuperscript{\{\vphantom\}}k-f({$\epsilon$})\vphantom\{\}), * a (fixed) constant-factor approximation in time O(m\textsuperscript{\{\vphantom\}}k-{$\delta$}\vphantom\{\}), or * an m\textsuperscript{{$\epsilon$}}-approximation in time O(m\textsuperscript{\{\vphantom\}}k-f({$\epsilon$})\vphantom\{\}). We obtain an almost complete characterization of these regimes, for MaxSP\textsubscript{k} as well as for an analogously defined minimization class MinSP\textsubscript{k}. As our main technical contribution, we rule out approximation schemes for a large class of problems admitting constant-factor approximations, under the Sparse MAX3SAT hypothesis posed by (Alman, Vassilevska Williams'20). As general trends for the problems we consider, we find: (1) Exact optimizability has a simple algebraic characterization, (2) only few maximization problems do not admit a constant-factor approximation; these do not even have a subpolynomial-factor approximation, and (3) constant-factor approximation of minimization problems is equivalent to deciding whether the optimum is equal to 0.},
  citationcount = {2},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{bringmannExploringTheApproximability2024,
  title = {Exploring the Approximability Landscape of {{3SUM}}},
  author = {Bringmann, K. and Ghazy, Ahmed and K{\"u}nnemann, Marvin},
  year = {2024},
  doi = {10.4230/LIPIcs.ESA.2024.34},
  abstract = {Since an increasing number of problems in P have conditional lower bounds against exact algorithms, it is natural to study which of these problems can be efficiently approximated. Often, however, there are many potential ways to formulate an approximate version of a problem. We ask: How sensitive is the (in)approximability of a problem in P to its precise formulation? To this end, we perform a case study using the popular 3SUM problem. Its many equivalent formulations give rise to a wide range of potential approximate relaxations. Specifically, to obtain an approximate relaxation in our framework, one can choose among the options: (a) 3SUM or Convolution 3SUM, (b) monochromatic or trichromatic, (c) allowing under-approximation, over-approximation, or both, (d) approximate decision or approximate optimization, (e) single output or multiple outputs and (f) implicit or explicit target (given as input). We show general reduction principles between some variants and find that we can classify the remaining problems (over polynomially bounded positive integers) into three regimes: 1. (1 + {$\epsilon$} )-approximable in near-linear time e O ( n + 1 /{$\epsilon$} ), 2. (1 + {$\epsilon$} )-approximable in near-quadratic time e O ( n/{$\epsilon$} ) or e O ( n + 1 /{$\epsilon$} 2 ), or 3. non-approximable, i.e., requiring time n 2 {\textpm} o (1) even for any approximation factor. In each of these three regimes, we provide matching upper and conditional lower bounds. To prove our results, we establish two results that may be of independent interest: Over polynomially bounded integers, we show subquadratic equivalence of (min , +)-convolution and polyhedral 3SUM, and we prove equivalence of the Strong 3SUM conjecture and the Strong Convolution 3SUM conjecture.},
  citationcount = {Unknown},
  venue = {Embedded Systems and Applications},
  keywords = {lower bound,reduction}
}

@article{bringmannSparseNonnegativeConvolution2021,
  title = {Sparse Nonnegative Convolution Is Equivalent to Dense Nonnegative Convolution},
  author = {Bringmann, K. and Fischer, N. and Nakos, Vasileios},
  year = {2021},
  doi = {10.1145/3406325.3451090},
  abstract = {Computing the convolution A {$\star$} B of two length-n vectors A,B is an ubiquitous computational primitive, with applications in a variety of disciplines. Within theoretical computer science, applications range from string problems to Knapsack-type problems, and from 3SUM to All-Pairs Shortest Paths. These applications often come in the form of nonnegative convolution, where the entries of A,B are nonnegative integers. The classical algorithm to compute A{$\star$} B uses the Fast Fourier Transform (FFT) and runs in time O(n logn). However, in many cases A and B might satisfy sparsity conditions, and hence one could hope for significant gains compared to the standard FFT algorithm. The ideal goal would be an O(k logk)-time algorithm, where k is the number of non-zero elements in the output, i.e., the size of the support of A {$\star$} B. This problem is referred to as sparse nonnegative convolution, and has received a considerable amount of attention in the literature; the fastest algorithms to date run in time O(k log2 n). The main result of this paper is the first O(k logk)-time algorithm for sparse nonnegative convolution. Our algorithm is randomized and assumes that the length n and the largest entry of A and B are subexponential in k. Surprisingly, we can phrase our algorithm as a reduction from the sparse case to the dense case of nonnegative convolution, showing that, under some mild assumptions, sparse nonnegative convolution is equivalent to dense nonnegative convolution for constant-error randomized algorithms. Specifically, if D(n) is the time to convolve two nonnegative length-n vectors with success probability 2/3, and S(k) is the time to convolve two nonnegative vectors with output size k with success probability 2/3, then S(k) = O(D(k) + k (loglogk)2). Our approach uses a variety of new techniques in combination with some old machinery from linear sketching and structured linear algebra, as well as new insights on linear hashing, the most classical hash function.},
  citationcount = {17},
  venue = {Symposium on the Theory of Computing},
  keywords = {reduction}
}

@inproceedings{bringmannSuccinctSamplingDiscrete2013,
  title = {Succinct Sampling from Discrete Distributions},
  booktitle = {Proc. 45th {{ACM Symposium}} on {{Theory}} of {{Computation}}},
  author = {Bringmann, K. and Larsen, K. G.},
  year = {2013},
  doi = {10.1145/2488608.2488707},
  annotation = {L. Devroye . Nonuniform random variate generation . Springer , New York , 1986 . L. Devroye. Nonuniform random variate generation. Springer, New York, 1986.\\
\\
J. Fischer and V. Heun . A new succinct representation of RMQ-information and improvements in the enhanced suffix array . In Combinatorics, Algorithms, Probabilistic and Experimental Methodologies , volume 4614 of Lecture Notes in Computer Science , pages 459 -- 470 . Springer , 2007 . J. Fischer and V. Heun. A new succinct representation of RMQ-information and improvements in the enhanced suffix array. In Combinatorics, Algorithms, Probabilistic and Experimental Methodologies, volume 4614 of Lecture Notes in Computer Science, pages 459--470. Springer, 2007.\\
\\
\\
\\
\\
\\
\\
D. E. Knuth . The Art of Computer Programming . Vol. 2 : Seminumerical Algorithms . Addison-Wesley Publishing Co. , Reading, Mass ., third edition, 2009 . D. E. Knuth. The Art of Computer Programming. Vol. 2: Seminumerical Algorithms. Addison-Wesley Publishing Co., Reading, Mass., third edition, 2009.\\
D. E. Knuth and A. C. Yao . The complexity of nonuniform random number generation . In Algorithms and Complexity: New Directions and Recent Results , pages 357 -- 428 . Academic Press , 1976 . D. E. Knuth and A. C. Yao. The complexity of nonuniform random number generation. In Algorithms and Complexity: New Directions and Recent Results, pages 357--428. Academic Press, 1976.\\
\\
\\
\\
\\
M. Patra{\c s}cu. Webdiarios de motocicleta sampling a discrete distribution. http://infoweekly.blogspot.de/2011/09/follow-up-sampling-discrete.html 2011.  M. Patra{\c s}cu. Webdiarios de motocicleta sampling a discrete distribution. http://infoweekly.blogspot.de/2011/09/follow-up-sampling-discrete.html 2011.},
  file = {/Users/tulasi/Zotero/storage/TEQE6ARK/Bringmann and Larsen - 2013 - Succinct sampling from discrete distributions.pdf}
}

@article{bringmannTheNfaAcceptance2023,
  title = {The {{NFA}} Acceptance Hypothesis: {{Non-combinatorial}} and Dynamic Lower Bounds},
  author = {Bringmann, K. and J{\o}rgensen, A. and K{\"u}nnemann, Marvin and Larsen, Kasper Green},
  year = {2023},
  doi = {10.46298/theoretics.24.22},
  abstract = {We pose the fine-grained hardness hypothesis that the textbook algorithm for the NFA Acceptance problem is optimal up to subpolynomial factors, even for dense NFAs and fixed alphabets. We show that this barrier appears in many variations throughout the algorithmic literature by introducing a framework of Colored Walk problems. These yield fine-grained equivalent formulations of the NFA Acceptance problem as problems concerning detection of an s-t-walk with a prescribed color sequence in a given edge- or node-colored graph. For NFA Acceptance on sparse NFAs (or equivalently, Colored Walk in sparse graphs), a tight lower bound under the Strong Exponential Time Hypothesis has been rediscovered several times in recent years. We show that our hardness hypothesis, which concerns dense NFAs, has several interesting implications: - It gives a tight lower bound for Context-Free Language Reachability. This proves conditional optimality for the class of 2NPDA-complete problems, explaining the cubic bottleneck of interprocedural program analysis. - It gives a tight (n+nm\textsuperscript{\{\vphantom\}}1/3\vphantom\{\})\textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\} lower bound for the Word Break problem on strings of length n and dictionaries of total size m. - It implies the popular OMv hypothesis. Since the NFA acceptance problem is a static (i.e., non-dynamic) problem, this provides a static reason for the hardness of many dynamic problems. Thus, a proof of the NFA Acceptance hypothesis would resolve several interesting barriers. Conversely, a refutation of the NFA Acceptance hypothesis may lead the way to attacking the current barriers observed for Context-Free Language Reachability, the Word Break problem and the growing list of dynamic problems proven hard under the OMv hypothesis.},
  citationcount = {3},
  venue = {Information Technology Convergence and Services},
  keywords = {dynamic,lower bound,static}
}

@article{bringmannTranslatingHausdorffIs2021,
  title = {Translating Hausdorff Is Hard: {{Fine-grained}} Lower Bounds for Hausdorff Distance under Translation},
  author = {Bringmann, K. and Nusser, Andr{\'e}},
  year = {2021},
  doi = {10.4230/LIPIcs.SoCG.2021.18},
  abstract = {Computing the similarity of two point sets is a ubiquitous task in medical imaging, geometric shape comparison, trajectory analysis, and many more settings. Arguably the most basic distance measure for this task is the Hausdorff distance, which assigns to each point from one set the closest point in the other set and then evaluates the maximum distance of any assigned pair. A drawback is that this distance measure is not translational invariant, that is, comparing two objects just according to their shape while disregarding their position in space is impossible. Fortunately, there is a canonical translational invariant version, the Hausdorff distance under translation, which minimizes the Hausdorff distance over all translations of one of the point sets. For point sets of size n and m, the Hausdorff distance under translation can be computed in time (nm) for the L{$_1$} and L{$_\infty$} norm [Chew, Kedem SWAT'92] and (nm(n+m)) for the L{$_2$} norm [Huttenlocher, Kedem, Sharir DCG'93]. As these bounds have not been improved for over 25 years, in this paper we approach the Hausdorff distance under translation from the perspective of fine-grained complexity theory. We show (i) a matching lower bound of (nm)\textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\} for L{$_1$} and L{$_\infty$} (and all other L\textsubscript{p} norms) assuming the Orthogonal Vectors Hypothesis and (ii) a matching lower bound of n\textsuperscript{\{\vphantom\}}2-o(1)\vphantom\{\} for L{$_2$} in the imbalanced case of m=O(1) assuming the 3SUM Hypothesis.},
  citationcount = {9},
  venue = {International Symposium on Computational Geometry},
  keywords = {lower bound}
}

@article{bringmannUnbalancedTriangleDetection2022,
  title = {Unbalanced Triangle Detection and Enumeration Hardness for Unions of Conjunctive Queries},
  author = {Bringmann, K. and Carmeli, Nofar},
  year = {2022},
  doi = {10.48550/arXiv.2210.11996},
  abstract = {We study the enumeration of answers to Unions of Conjunctive Queries (UCQs) with optimal time guarantees. More precisely, we wish to identify the queries that can be solved with linear preprocessing time and constant delay. Despite the basic nature of this problem, it was shown only recently that UCQs can be solved within these time bounds if they admit free-connex union extensions, even if all individual CQs in the union are intractable with respect to the same complexity measure. Our goal is to understand whether there exist additional tractable UCQs, not covered by the currently known algorithms. As a first step, we show that some previously unclassified UCQs are hard using the classic 3SUM hypothesis, via a known reduction from 3SUM to triangle listing in graphs. As a second step, we identify a question about a variant of this graph task that is unavoidable if we want to classify all self-join-free UCQs: is it possible to decide the existence of a triangle in a vertex-unbalanced tripartite graph in linear time? We prove that this task is equivalent in hardness to some family of UCQs. Finally, we show a dichotomy for unions of two self-join-free CQs if we assume the answer to this question is negative. In conclusion, this paper pinpoints a computational barrier in the form of a single decision problem that is key to advancing our understanding of the enumeration complexity of many UCQs. Without a breakthrough for unbalanced triangle detection, we have no hope of finding an efficient algorithm for additional unions of two self-join-free CQs. On the other hand, a sufficiently efficient unbalanced triangle detection algorithm can be turned into an efficient algorithm for a family of UCQs currently not known to be tractable.},
  citationcount = {7},
  venue = {arXiv.org},
  keywords = {query,reduction}
}

@article{brisaboaAggregated2dRange2016,
  title = {Aggregated {{2D}} Range Queries on Clustered Points},
  author = {Brisaboa, N. and {de Bernardo}, Guillermo and Konow, Roberto and Navarro, G. and Seco, Diego},
  year = {2016},
  doi = {10.1016/j.is.2016.03.004},
  abstract = {No abstract available},
  citationcount = {14},
  venue = {Information Systems},
  keywords = {query}
}

@article{brodalACommunicationComplexity1996,
  title = {A Communication Complexity Proof That Symmetric Functions Have Logarithmic Depth},
  author = {Brodal, G. and Husfeldt, T.},
  year = {1996},
  doi = {10.7146/BRICS.V3I1.19502},
  abstract = {We present a direct protocol with logarithmic communication that finds an element in the symmetric difference of two sets of different size. This yields a simple proof that symmetric functions have logarithmic circuit depth.},
  citationcount = {15},
  venue = {No venue available}
}

@article{brodalCacheObliviousDynamic2010,
  title = {Cache-Oblivious Dynamic Dictionaries with Update/Query Tradeoffs},
  author = {Brodal, G. and Demaine, E. and Fineman, Jeremy T. and Iacono, J. and Langerman, S. and Munro, J.},
  year = {2010},
  doi = {10.1137/1.9781611973075.117},
  abstract = {Several existing cache-oblivious dynamic dictionaries achieve {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown} {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown}) (or slightly better {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown} {\textexclamdown}i{\textquestiondown}N/M{\textexclamdown}/i{\textquestiondown})) memory transfers per operation, where {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown} is the number of items stored, {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown} is the memory size, and {\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown} is the block size, which matches the classic B-tree data structure. One recent structure achieves the same query bound and a sometimes-better amortized update bound of {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(1/{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}{$\Theta$}(1/log log {\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown})2){\textexclamdown}/sup{\textquestiondown} log{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown} {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown} + 1/{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown} log{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown}) memory trans-fers. This paper presents a new data structure, the {\textexclamdown}i{\textquestiondown}xDict{\textexclamdown}/i{\textquestiondown}, implementing predecessor queries in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(1/{$\varepsilon$} log {\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown} {\textexclamdown}i{\textquestiondown}N/M{\textexclamdown}/i{\textquestiondown}) worst-case memory transfers and insertions and deletions in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(1/{$\varepsilon$}{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}1-{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown} log{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown} {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown}/{\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown}) amortized memory transfers, for any constant {$\varepsilon$} with 0 {\textexclamdown} {$\varepsilon$} {\textexclamdown} 1. For example, the xDict achieves subconstant amortized update cost when {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown} = {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown} {\textexclamdown}i{\textquestiondown}B{\textexclamdown}sup{\textquestiondown}o{\textexclamdown}/sup{\textquestiondown}{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}1-{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown}), whereas the B-tree's {$\Theta$}(log{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown} {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown}/{\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown}) is subconstant only when {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown} = {\textexclamdown}i{\textquestiondown}o{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}MB{\textexclamdown}/i{\textquestiondown}), and the previously obtained {$\Theta$}(1/{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}{$\Theta$}(1/(log log {\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown})2){\textexclamdown}/sup{\textquestiondown} log{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown} {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown} + 1/{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown} log{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown}) is subconstant only when {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown} = {\textexclamdown}i{\textquestiondown}o{\textexclamdown}/i{\textquestiondown}(2{\textexclamdown}sup{\textquestiondown}{\textsurd}{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sup{\textquestiondown}). The xDict attains the optimal tradeoff between insertions and queries, even in the broader external-memory model, for the range where inserts cost between {\textohm}(1/{\textexclamdown}i{\textquestiondown}B{\textexclamdown}/i{\textquestiondown} lg{\textexclamdown}sup{\textquestiondown}1+{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown}) and {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(1/lg{\textexclamdown}sup{\textquestiondown}3{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown}) memory transfers.},
  citationcount = {32},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{brodalDataStructuresRange2009,
  title = {Data Structures for Range Median Queries},
  author = {Brodal, G. and J{\o}rgensen, A.},
  year = {2009},
  doi = {10.1007/978-3-642-10631-6_83},
  abstract = {No abstract available},
  citationcount = {32},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {data structure,query}
}

@article{brodalDynamicPlanarConvex2002,
  title = {Dynamic Planar Convex Hull},
  author = {Brodal, G. and Jacob, R.},
  year = {2002},
  doi = {10.1109/SFCS.2002.1181985},
  abstract = {In this paper we determine the computational complexity of the dynamic convex hull problem in the planar case. We present a data structure that maintains a finite set of n points in the plane under insertion and deletion of points in amortized O(log n) time per operation. The space usage of the data structure is O(n). The data structure supports extreme point queries in a given direction, tangent queries through a given point, and queries for the neighboring points on the convex hull in O(log n) time. The extreme point queries can be used to decide whether or not a given line intersects the convex hull, and the tangent queries to determine whether a given point is inside the convex hull. We give a lower bound on the amortized asymptotic time complexity that matches the performance of this data structure.},
  citationcount = {224},
  venue = {The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.},
  keywords = {data structure,dynamic,lower bound,query}
}

@article{brodalOptimalPlanarOrthogonal2013,
  title = {Optimal Planar Orthogonal Skyline Counting Queries},
  author = {Brodal, G. and Larsen, Kasper Green},
  year = {2013},
  doi = {10.1007/978-3-319-08404-6_10},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {query}
}

@article{brodalOptimalRangeMedians2011,
  title = {Towards Optimal Range Medians},
  author = {Brodal, Gerth St{\o}lting and Gfeller, Beat and J{\o}rgensen, Allan Gr{\o}nlund and Sanders, Peter},
  year = {2011},
  journal = {Theoretical Computer Science},
  volume = {412},
  number = {24},
  pages = {2588--2601},
  doi = {10.1016/j.tcs.2010.05.003},
  annotation = {Manuel Blum, Robert W. Floyd, Vaughan R. Pratt, Ronald L. Rivest, Robert Endre Tarjan, Linear time bounds for median computations, in: 4th Annual Symp. on Theory of Computing, STOC, 1972, pp.~119--124.\\
\\
\\
David R. Clark, Compact Pat Trees. Ph.D. Thesis, University of Waterloo, 1988.\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
Daisuke Okanohara, Kunihiko Sadakane, Practical entropy-compressed rank/select dictionary. The Computing Research Repository, CoRR, abs/cs/0610001, 2006.\\
\\
Mihai Pa{\textasciicaron}tra{\c s}cu, (Data) structures, in: Proc. 49th Annual IEEE Symposium on Foundations of Computer Science, 2008, pp.~434--443.\\
Holger Petersen, Improved bounds for range mode and range median queries, in: 34th International Conference on Current Trends in Theory and Practice of Computer Science, SOFSEM, 2008, pp.~418--423.\\
\\
\\
\\
\\
\\
Andrew Chi-Chih Yao, Space--time tradeoff for answering range queries (extended abstract), in: 14th Annual Symposium on Theory of Computing, STOC, 1982, pp.~128--136.},
  file = {/Users/tulasi/Zotero/storage/YEPZEBUR/Brodal et al. - 2011 - Towards optimal range medians.pdf}
}

@article{brodalPathMinimaQueries2011,
  title = {Path Minima Queries in Dynamic Weighted Trees},
  author = {Brodal, G. and Davoodi, P. and Rao, S.},
  year = {2011},
  doi = {10.1007/978-3-642-22300-6_25},
  abstract = {No abstract available},
  citationcount = {22},
  venue = {Workshop on Algorithms and Data Structures},
  keywords = {dynamic,query}
}

@article{brodalPredecessorQueriesIn1997,
  title = {Predecessor Queries in Dynamic Integer Sets},
  author = {Brodal, G.},
  year = {1997},
  doi = {10.1007/BFb0023445},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Symposium on Theoretical Aspects of Computer Science},
  keywords = {dynamic,query}
}

@article{brodalSpaceEfficientTwo2010,
  title = {On Space Efficient Two Dimensional Range Minimum Data Structures},
  author = {Brodal, G. and Davoodi, P. and Rao, S.},
  year = {2010},
  doi = {10.1007/s00453-011-9499-0},
  abstract = {No abstract available},
  citationcount = {54},
  venue = {Algorithmica},
  keywords = {data structure}
}

@article{brodnikAnO12006,
  title = {An {{O}}(1) Solution to the Prefix Sum Problem on a Specialized Memory Architecture},
  author = {Brodnik, A. and Karlsson, J. and Munro, J. and Nilsson, A.},
  year = {2006},
  doi = {10.1007/978-0-387-34735-6_12},
  abstract = {No abstract available},
  citationcount = {11},
  venue = {IFIP TCS}
}

@article{brodnikMembershipInConstant1994,
  title = {Membership in Constant Time and Minimum Space},
  author = {Brodnik, A. and Munro, J.},
  year = {1994},
  doi = {10.1007/BFb0049398},
  abstract = {No abstract available},
  citationcount = {26},
  venue = {Embedded Systems and Applications}
}

@article{brodnikMembershipInConstant1999,
  title = {Membership in Constant Time and Almost-Minimum Space},
  author = {Brodnik, A. and Munro, J.},
  year = {1999},
  doi = {10.1137/S0097539795294165},
  abstract = {This paper deals with the problem of storing a subset of elements from the bounded universe \{M\}= 0,{\dots},M-1  so that membership queries can be performed efficiently. In particular, we introduce a data structure to represent a subset of N elements of \{M\} in a number of bits close to the information-theoretic minimum, B={$\lceil$}\{MN\}{$\rceil$}, and use the structure to answer membership queries in constant time.},
  citationcount = {147},
  venue = {SIAM journal on computing (Print)}
}

@article{brodnikMultiprocessTimeQueue2001,
  title = {Multiprocess Time Queue},
  author = {Brodnik, A. and Karlsson, J.},
  year = {2001},
  doi = {10.1007/3-540-45678-3_51},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {International Symposium on Algorithms and Computation}
}

@article{brodnikTransDichotomousAlgorithms1997,
  title = {Trans-Dichotomous Algorithms without Multiplication - Some Upper and Lower Bounds},
  author = {Brodnik, A. and Miltersen, Peter Bro and Munro, J.},
  year = {1997},
  doi = {10.1007/3-540-63307-3_80},
  abstract = {No abstract available},
  citationcount = {39},
  venue = {Workshop on Algorithms and Data Structures},
  keywords = {lower bound}
}

@article{brodnikUnitTimePredecessor2010,
  title = {Unit-Time Predecessor Queries on Massive Data Sets},
  author = {Brodnik, A. and Iacono, J.},
  year = {2010},
  doi = {10.1007/978-3-642-17517-6_14},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {query}
}

@article{brodnikWorstCaseConstant2001,
  title = {Worst Case Constant Time Priority Queue},
  author = {Brodnik, A. and Carlsson, S. and Karlsson, J. and Munro, J.},
  year = {2001},
  doi = {10.5555/365411.365524},
  abstract = {We present a new data structure of size 3M + \&Ogr;(M) bits for solving the ``discrete priority queue'' problem. When this data structure is used in combination with a new memory topology it provides an O(1) worst case time solution. In doing so we demonstrate how an unconventional, but practically implementable, memory architecture can be employed to sidestep a lower bound (of lg lg M) and achieve constant time performance.},
  citationcount = {29},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,lower bound}
}

@article{brodnikWorstCaseConstant2005,
  title = {Worst Case Constant Time Priority Queue},
  author = {Brodnik, A. and Carlsson, S. and Fredman, M. and Karlsson, J. and Munro, J.},
  year = {2005},
  doi = {10.1016/j.jss.2004.09.002},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Journal of Systems and Software}
}

@article{brody_adapt_2015,
  title = {Adapt or Die: {{Polynomial}} Lower Bounds for Non-Adaptive Dynamic Data Structures},
  shorttitle = {Adapt or Die},
  author = {Brody, Joshua and Larsen, Kasper Green},
  year = {2015},
  month = dec,
  journal = {Theory of Computing},
  volume = {11},
  pages = {471--489},
  doi = {10.4086/toc.2015.v011a019},
  url = {https://theoryofcomputing.org/articles/v011a019/},
  urldate = {2024-09-01},
  abstract = {In this paper, we study the role non-adaptivity plays in maintaining dynamic data structures. Roughly speaking, a data structure is non-adaptive if the memory locations it reads and/or writes when processing a query or update depend only on the query or update and not on the contents of previously read cells. We study such non-adaptive data structures in the cell probe model. The cell probe model is one of the least restrictive lower bound models and in particular, cell probe lower bounds apply to data structures developed in the popular word-RAM model. Unfortunately, this generality comes at a high cost: the highest lower bound proved for any data structure problem is only polylogarithmic (if allowed adaptivity). Our main result is to demonstrate that one can in fact obtain polynomial cell probe lower bounds for non-adaptive data structures. To shed more light on the seemingly inherent polylogarithmic lower bound barrier, we study several different notions of non-adaptivity and identify key properties that must be dealt with if we are to prove polynomial lower bounds without restrictions on the data structures. Finally, our results also unveil an interesting connection between data structures and depth-2 circuits. This allows us to translate conjectured hard data structure problems into good candidates for high circuit lower bounds; in particular, in the area of linear circuits for linear operators. Building on lower bound proofs for data structures in slightly more restrictive models, we also present a number of properties of linear operators which we believe are worth investigating in the realm of circuit lower bounds.},
  keywords = {cell probe,Circuit Complexity,communication complexity,data structure,data structures,dynamic,dynamic data structures,encoding,lower bound,non-adaptive,query,update},
  file = {/Users/tulasi/Zotero/storage/QRK3JPLX/Brody and Larsen - 2015 - Adapt or Die Polynomial Lower Bounds for Non-Adaptive Dynamic Data Structures.pdf}
}

@article{brodyBetterGapHamming2009,
  title = {Better Gap-Hamming Lower Bounds via Better Round Elimination},
  author = {Brody, Joshua and Chakrabarti, Amit and Regev, O. and Vidick, Thomas and Wolf, R. D.},
  year = {2009},
  doi = {10.1007/978-3-642-15369-3_36},
  abstract = {Gap Hamming Distance is a well-studied problem in communication complexity, in which Alice and Bob have to decide whether the Hamming distance between their respective n-bit inputs is less than   or greater than  . We show that every k-round bounded-error communication protocol for this problem sends a message of at least {\textohm}(n/(k 2logk)) bits. This lower bound has an exponentially better dependence on the number of rounds than the previous best bound, due to Brody and Chakrabarti. Our communication lower bound implies strong space lower bounds on algorithms for a number of data stream computations, such as approximating the number of distinct elements in a stream.},
  citationcount = {22},
  venue = {International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques},
  keywords = {lower bound}
}

@article{brodyCertifyingEqualityWith2016,
  title = {Certifying Equality with Limited Interaction},
  author = {Brody, Joshua and Chakrabarti, Amit and Kondapally, Ranganath and Woodruff, David P. and Yaroslavtsev, G.},
  year = {2016},
  doi = {10.1007/s00453-016-0163-6},
  abstract = {No abstract available},
  citationcount = {18},
  venue = {Algorithmica}
}

@article{brodySpaceBoundedCommunication2013,
  title = {Space-Bounded Communication Complexity},
  author = {Brody, Joshua and Chen, Shiteng and Papakonstantinou, Periklis A. and Song, Hao and Sun, Xiaoming},
  year = {2013},
  doi = {10.1145/2422436.2422456},
  abstract = {In the past thirty years, Communication Complexity has emerged as a foundational tool to proving lower bounds in many areas of computer science. Its power comes from its generality, but this generality comes at a price---no superlinear communication lower bound is possible, since a player may communicate his entire input. However, what if the players are limited in their ability to recall parts of their interaction? We introduce memory models for 2-party communication complexity. Our general model is as follows: two computationally unrestricted players, Alice and Bob, each have s(n) bits of memory. When a player receives a bit of communication, he "compresses" his state. This compression may be an arbitrary function of his current memory contents, his input, and the bit of communication just received; the only restriction is that the compression must return at most s(n) bits. We obtain memory hierarchy theorems (also comparing this general model with its restricted variants), and show super-linear lower bounds for some explicit (non-boolean) functions. Our main conceptual and technical contribution concerns the following variant. The communication is one-way, from Alice to Bob, where Bob controls two types of memory: (i) a large, oblivious memory, where updates are only a function of the received bit and the current memory content, and (ii) a smaller, non-oblivious/general memory, where updates can be a function of the input given to Bob. We exhibit natural protocols where this semi-obliviousness shows up. For this model we also introduce new techniques through which certain limitations of space-bounded computation are revealed. One of the main motivations of this work is in understanding the difference in the use of space when computing the following functions: Equality (EQ), Inner Product (IP), and connectivity in a directed graph (REACH). When viewed as communication problems, EQ can be decided using 0 non-oblivious bits (and log2 n oblivious bits), IP requires exactly 1 non-oblivious bit, whereas for REACH we obtain the same lower bound as for IP and conjecture that the actual bound is Omega(log2 n). In fact, proving that 1 non-oblivious bit is required becomes technically sophisticated, and the question even for 2 non-oblivious bits for any explicit boolean function remains open.},
  citationcount = {13},
  venue = {Information Technology Convergence and Services},
  keywords = {communication,communication complexity,lower bound,update}
}

@article{brodyTheMaximumCommunication2009,
  title = {The Maximum Communication Complexity of Multi-Party Pointer Jumping},
  author = {Brody, Joshua},
  year = {2009},
  doi = {10.1109/CCC.2009.30},
  abstract = {We study the one-way number-on-the-forhead (NOF) communication complexity of the k-layer pointer jumping problem. Strong lower bounds for this problem would have important implications in circuit complexity. All of our results apply to myopic protocols (where players see only one layer ahead, but can still see arbitrarily far behind them.) Furthermore, our results apply to the maximum communication complexity, where a protocol is charged for the maximum communication sent by a single player rather than the total communication sent by all players. Our main result is a lower bound of n/2 bits for deterministic protocols, independent of the number of players. We also provide a matching upper bound, as well as an {\textohm}(n/kn) lower bound for randomized protocols, improving on the bounds of Chakrabarti. In the non-Boolean version of the problem, we give a lower bound of n(\textsuperscript{\{\vphantom\}}(k-1)\vphantom\{\}n)(1-o(1)) bits, essentially matching the upper bound from Damm et al.},
  citationcount = {8},
  venue = {2009 24th Annual IEEE Conference on Computational Complexity},
  keywords = {communication,communication complexity,lower bound}
}

@article{bromiltersenCellProbeComplexity1995,
  title = {On the Cell Probe Complexity of Polynomial Evaluation},
  author = {Bro Miltersen, Peter},
  year = {1995},
  month = jul,
  journal = {Theoretical Computer Science},
  volume = {143},
  number = {1},
  pages = {167--174},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(95)80032-5},
  url = {https://www.sciencedirect.com/science/article/pii/0304397595800325},
  urldate = {2024-09-03},
  abstract = {We consider the cell probe complexity of the polynomial evaluation problem with preprocessing of coefficients, for polynomials of degree at most n over a finite field K. We show that the trivial cell probe algorithm for the problem is optimal if K is sufficiently large compared to n. As an application, we give a new proof of the fact that P {$\neq$} incr-TIME(o(logn/log logn)).},
  keywords = {cell probe},
  file = {/Users/tulasi/Zotero/storage/LJLXJQDJ/1-s2.0-0304397595800325-main.pdf}
}

@article{brnnimannProductRangeSpaces1993,
  title = {Product Range Spaces, Sensitive Sampling, and Derandomization},
  author = {Br{\"o}nnimann, Herv{\'e} and Chazelle, B. and Matou{\v s}ek, J.},
  year = {1993},
  doi = {10.1109/SFCS.1993.366847},
  abstract = {We introduce the concept of a sensitive /spl epsi/-approximation, and use it to derive a more efficient algorithm for computing /spl epsi/-nets. We define and investigate product range spaces, for which we establish sampling theorems analogous to the standard finite VC-dimensional case. This generalizes and simplifies results from previous works. We derive a simpler optimal deterministic convex hull algorithm, and by extending the method to the intersection of a set of balls with the same radius, we obtain an O(nlog/sup 3/ n) deterministic algorithm for computing the diameter of an n-point set in 3-dimensional space.<<ETX>>},
  citationcount = {100},
  venue = {Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science}
}

@article{brustmannTheComplexityOf1987,
  title = {The Complexity of Symmetric Functions in Bounded-Depth Circuits},
  author = {Brustmann, B. and Wegener, I.},
  year = {1987},
  doi = {10.1016/0020-0190(87)90163-3},
  abstract = {No abstract available},
  citationcount = {16},
  venue = {Information Processing Letters}
}

@article{bshoutyALowerBound1988,
  title = {A Lower Bound for Matrix Multiplication},
  author = {Bshouty, N.},
  year = {1988},
  doi = {10.1109/SFCS.1988.21922},
  abstract = {It is proved that computing the product of two n*n matrices over the binary field requires at least 2.5n/sup 2/-O(n/sup 2/) multiplications.<<ETX>>},
  citationcount = {40},
  venue = {[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science}
}

@article{bucaloLifting2020,
  title = {Lifting},
  author = {Bucalo, A. and Rosolini, G.},
  year = {2020},
  doi = {10.1007/BFb0026994},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Category Theory and Computer Science}
}

@article{buchemApproximatingTheGeometric2024,
  title = {Approximating the Geometric Knapsack Problem in Near-Linear Time and Dynamically},
  author = {Buchem, Moritz and Deuker, Paul and Wiese, Andreas},
  year = {2024},
  doi = {10.48550/arXiv.2403.00536},
  abstract = {An important goal in algorithm design is determining the best running time for solving a problem (approximately). For some problems, we know the optimal running time, assuming certain conditional lower bounds. In this work, we study the d-dimensional geometric knapsack problem where we are far from this level of understanding. We are given a set of weighted d-dimensional geometric items like squares, rectangles, or hypercubes and a knapsack which is a square or a (hyper-)cube. We want to select a subset of items that fit non-overlappingly inside the knapsack, maximizing the total profit of the packed items. We make a significant step towards determining the best running time for solving these problems approximately by presenting approximation algorithms with near-linear running times for any constant dimension d and any constant parameter {$\epsilon$}. For (hyper)-cubes, we present a (1+{$\epsilon$})-approximation algorithm whose running time drastically improves upon the known (1+{$\epsilon$})-approximation algorithm which has a running time where the exponent of n depends exponentially on 1/{$\epsilon$} and d. Moreover, we present a (2+{$\epsilon$})-approximation algorithm for rectangles in the setting without rotations and a (17/9+{$\epsilon$})-approximation algorithm if we allow rotations by 90 degrees. The best known polynomial time algorithms for these settings have approximation ratios of 17/9+{$\epsilon$} and 1.5+{$\epsilon$}, respectively, and running times in which the exponent of n depends exponentially on 1/{$\epsilon$}. We also give dynamic algorithms with polylogarithmic query and update times and the same approximation guarantees as the algorithms above. Key to our results is a new family of structured packings which we call easily guessable packings. They are flexible enough to guarantee profitable solutions and structured enough so that we can compute these solutions quickly.},
  citationcount = {Unknown},
  venue = {International Symposium on Computational Geometry},
  keywords = {dynamic,lower bound,query,update,update time}
}

@article{buchinFourSovietsWalk2012,
  title = {Four Soviets Walk the Dog - with an Application to Alt's Conjecture},
  author = {Buchin, K. and Buchin, M. and Meulemans, Wouter and Mulzer, Wolfgang},
  year = {2012},
  doi = {10.1137/1.9781611973402.103},
  abstract = {Given two polygonal curves in the plane, there are many ways to define a notion of similarity between them. One measure that is extremely popular is the Frechet distance. Since it has been proposed by Alt and Godau in 1992, many variants and extensions have been studied. Nonetheless, even more than 20 years later, the original O(n2 log n) algorithm by Alt and Godau for computing the Frechet distance remains the state of the art (here n denotes the number of vertices on each curve). This has led Helmut Alt to conjecture that the associated decision problem is 3SUM-hard. In recent work, Agarwal et al. show how to break the quadratic barrier for the discrete version of the Frechet distance, where one considers sequences of points instead of polygonal curves. Building on their work, we give a randomized algorithm to compute the Frechet distance between two polygonal curves in time [EQUATION] on a pointer machine and in time O(n2(log log n)2) on a word RAM. Furthermore, we show that there exists an algebraic decision tree for the decision problem of depth O(n2-e), for some e {\textquestiondown} 0. This provides evidence that the decision problem may not be 3SUM-hard after all and reveals an intriguing new aspect of this well-studied problem.},
  citationcount = {61},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{buchinFourSovietsWalk2017,
  title = {Four Soviets Walk the Dog: {{Improved}} Bounds for Computing the Fr{\'e}chet Distance},
  author = {Buchin, K. and Buchin, M. and Meulemans, Wouter and Mulzer, Wolfgang},
  year = {2017},
  doi = {10.1007/s00454-017-9878-7},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Discrete \& Computational Geometry}
}

@article{buhrmanAreBitvectorsOptimal2000,
  title = {Are Bitvectors Optimal?},
  author = {Buhrman, H. and Miltersen, Peter Bro and Radhakrishnan, J. and Srinivasan, Venkatesh},
  year = {2000},
  doi = {10.1145/335305.335357},
  abstract = {We study the it static membership problem: Given a set S of at most n keys drawn from a universe U of size m, store it so that queries of the form "Is u in S?" can be answered by making few accesses to the memory. We study schemes for this problem that use space close to the information theoretic lower bound of {\textohm}(n(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{m}\vphantom\{\}\{n\})) bits and yet answer queries by reading a small number of bits of the memory. We show that, for {$\epsilon>$}0, there is a scheme that stores O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}\vphantom\{\}\{{$\epsilon^2$}\}m) bits and answers membership queries using a randomized algorithm that reads just one bit of memory and errs with probability at most {$\epsilon$}. We consider schemes that make no error for queries in S but are allowed to err with probability at most {$\epsilon$} for queries not in S. We show that there exist such schemes that store O((\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}\vphantom\{\}\{{$\epsilon$}\}){$^2$}m) bits and answer queries using just one bitprobe. If multiple probes are allowed, then the number of bits stored can be reduced to O(n\textsuperscript{\{\vphantom\}}1+{$\delta$}\vphantom\{\}m) for any {$\delta>$}0. The schemes mentioned above are based on probabilistic constructions of set systems with small intersections. We show lower bounds that come close to our upper bounds (for a large range of n and {$\epsilon$}): Schemes that answer queries with just one bitprobe and error probability {$\epsilon$} must use {\textohm}(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}\vphantom\{\}\{{$\epsilon$}(1/{$\epsilon$})\}m) bits of storage; if the error is restricted to queries not in S, then the scheme must use {\textohm}(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}{$^{2}$}\vphantom\{\}\{{$\epsilon^2$}(n/{$\epsilon$})\}m) bits of storage. We also consider deterministic schemes for the static membership problem and show tradeoffs between space and the number of probes.},
  citationcount = {128},
  venue = {Symposium on the Theory of Computing},
  keywords = {information theoretic,lower bound,query,static}
}

@article{buhrmanBoundsForSmall1999,
  title = {Bounds for Small-Error and Zero-Error Quantum Algorithms},
  author = {Buhrman, H. and Cleve, R. and Wolf, R. D. and Zalka, Christof},
  year = {1999},
  doi = {10.1109/SFFCS.1999.814607},
  abstract = {We present a number of results related to quantum algorithms with small error probability and quantum algorithms that are zero-error. First, we give a tight analysis of the trade-offs between the number of queries of quantum search algorithms, their error probability, the size of the search space, and the number of solutions in this space. Using this, we deduce new lower and upper bounds for quantum versions of amplification problems. Next, we establish nearly optimal quantum-classical separations for the query complexity of monotone functions in the zero-error model (where our quantum zero-error model is defined so as to be robust when the quantum gates are noisy). Also, we present a communication complexity problem related to a total function for which there is a quantum-classical communication complexity gap in the zero-error model. Finally, we prove separations for monotone graph properties in the zero-error and other error models which imply that the evasiveness conjecture for such properties does not hold for quantum computers.},
  citationcount = {159},
  venue = {40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039)}
}

@article{buhrmanComplexityMeasuresAnd2002,
  title = {Complexity Measures and Decision Tree Complexity: A Survey},
  author = {Buhrman, H. and Wolf, R. D.},
  year = {2002},
  doi = {10.1016/S0304-3975(01)00144-X},
  abstract = {No abstract available},
  citationcount = {633},
  venue = {Theoretical Computer Science}
}

@article{buhrmanLimitsOfQuantum2021,
  title = {Limits of Quantum Speed-Ups for Computational Geometry and Other Problems: {{Fine-grained}} Complexity via Quantum Walks},
  author = {Buhrman, H. and Loff, B. and Patro, Subhasree and Speelman, F.},
  year = {2021},
  doi = {10.4230/LIPIcs.ITCS.2022.31},
  abstract = {Many computational problems are subject to a quantum speed-up: one might find that a problem having an O(n{\textasciicircum}3)-time or O(n{\textasciicircum}2)-time classic algorithm can be solved by a known O(n{\textasciicircum}1.5)-time or O(n)-time quantum algorithm. The question naturally arises: how much quantum speed-up is possible? The area of fine-grained complexity allows us to prove optimal lower-bounds on the complexity of various computational problems, based on the conjectured hardness of certain natural, well-studied problems. This theory has recently been extended to the quantum setting, in two independent papers by Buhrman, Patro, and Speelman (arXiv:1911.05686), and by Aaronson, Chia, Lin, Wang, and Zhang (arXiv:1911.01973). In this paper, we further extend the theory of fine-grained complexity to the quantum setting. A fundamental conjecture in the classical setting states that the 3SUM problem cannot be solved by (classical) algorithms in time O(n{\textasciicircum}\{2-a\}), for any a{\textquestiondown}0. We formulate an analogous conjecture, the Quantum-3SUM-Conjecture, which states that there exist no sublinear O(n{\textasciicircum}\{1-b\})-time quantum algorithms for the 3SUM problem. Based on the Quantum-3SUM-Conjecture, we show new lower-bounds on the time complexity of quantum algorithms for several computational problems. Most of our lower-bounds are optimal, in that they match known upper-bounds, and hence they imply tight limits on the quantum speedup that is possible for these problems.},
  citationcount = {13},
  venue = {Information Technology Convergence and Services},
  keywords = {lower bound}
}

@article{bukovNearlyOptimalDynamic2023,
  title = {Nearly Optimal Dynamic Set Cover: {{Breaking}} the Quadratic-in-f Time Barrier},
  author = {Bukov, A. and Solomon, Shay and Zhang, Tianyi},
  year = {2023},
  doi = {10.48550/arXiv.2308.00793},
  abstract = {The dynamic set cover problem has been subject to extensive research since the pioneering works of [Bhattacharya et al, 2015] and [Gupta et al, 2017]. The input is a set system (U,S) on a fixed collection S of sets and a dynamic universe of elements, where each element appears in a most f sets and the cost of each set lies in the range [1/C,1], and the goal is to efficiently maintain an approximately-minimum set cover under insertions and deletions of elements. Most previous work considers the low-frequency regime, namely f=O(n), and this line of work has culminated with a deterministic (1+{$\epsilon$})f-approximation algorithm with amortized update time O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{f}{$^{2}$}\vphantom\{\}\{{$\epsilon^3$}\}+\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{f}\vphantom\{\}\{{$\epsilon^2$}\}C) [Bhattacharya et al, 2021]. In the high-frequency regime of f={\textohm}(n), an O(n)-approximation algorithm with amortized update time O(fn) was given by [Gupta et al, 2017]. Interestingly, at the intersection of the two regimes, i.e., f={$\Theta$}(n), the state-of-the-art results coincide: approximation {$\Theta$}(f)={$\Theta$}(n) with amortized update time O(f{$^2$})=O(fn)=O({$^2$}n). Up to this date, no previous work achieved update time of o(f{$^2$}). In this paper we break the {\textohm}(f{$^2$}) update time barrier via the following results: (1) (1+{$\epsilon$})f-approximation can be maintained in O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{f}\vphantom\{\}\{{$\epsilon^3$}\}\textsuperscript{*}f+\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{f}\vphantom\{\}\{{$\epsilon^3$}\}C)=O\textsubscript{\{\vphantom\}}{$\epsilon$},C\vphantom\{\}(f\textsuperscript{*}f) expected amortized update time; our algorithm works against an adaptive adversary. (2) (1+{$\epsilon$})f-approximation can be maintained deterministically in O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{{$\epsilon$}\}ff+\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{f}\vphantom\{\}\{{$\epsilon^3$}\}+\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{f}C\vphantom\{\}\{{$\epsilon^2$}\})=O\textsubscript{\{\vphantom\}}{$\epsilon$},C\vphantom\{\}(ff) amortized update time.},
  citationcount = {2},
  venue = {arXiv.org},
  keywords = {adaptive,dynamic,update,update time}
}

@article{bunemanSemistructuredData1997,
  title = {Semistructured Data},
  author = {Buneman, P.},
  year = {1997},
  doi = {10.1145/263661.263675},
  abstract = {In semistructured data, the information that is normally associated with a schema is contained within the data, which is sometimes called ``self-describing''. In some forms of semi-structured data there is no separate schema, in others it exists but only places loose constraints on the data. Semi-structured data has recently emerged as an important topic of study for a variety of reasons. First, there are data sources such as the Web, which we would like to treat as databases but which cannot be constrained by a schema. Second, it may be desirable to have an extremely flexible format for data exchange between disparate databases. Third, even when dealing with structured data, it may be helpful to view it. as semistructured for the purposes of browsing. This tutorial will cover a number of issues surrounding such data: finding a concise formulation, building a sufficiently expressive language for querying and transformation, and opti-mizat,ion problems.},
  citationcount = {789},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {query}
}

@article{brgisserAlgebraicComplexityTheory1997,
  title = {Algebraic Complexity Theory},
  author = {B{\"u}rgisser, Peter and Clausen, M. and Shokrollahi, A.},
  year = {1997},
  doi = {10.1007/978-3-662-03338-8},
  abstract = {No abstract available},
  citationcount = {813},
  venue = {Grundlehren der mathematischen Wissenschaften}
}

@article{brgisserLowerBoundsOn2002,
  title = {Lower Bounds on the Bounded Coefficient Complexity of Bilinear Maps},
  author = {B{\"u}rgisser, Peter and Lotz, Martin},
  year = {2002},
  doi = {10.1109/SFCS.2002.1181991},
  abstract = {We prove lower bounds of order n log n for both the problem to multiply polynomials of degree n, and to divide polynomials with remainder, in the model of bounded coefficient arithmetic circuits over the complex numbers. These lower bounds are optimal up to order of magnitude. The proof uses a recent idea of R. Raz [Proc. 34th STOC 2002] proposed for matrix multiplication. It reduces the linear problem to multiply a random circulant matrix with a vector to the bilinear problem of cyclic convolution. We treat the arising linear problem by extending J. Morgenstern's bound [J. ACM 20, pp. 305-306, 1973] in a unitarily invariant way. This establishes a new lower bound on the bounded coefficient complexity of linear forms in terms of the singular values of the corresponding matrix.},
  citationcount = {20},
  venue = {The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.}
}

@article{burkhardInherentComplexityTrade1981,
  title = {Inherent Complexity Trade-Offs for Range Query Problems},
  author = {Burkhard, W. and Fredman, M. and Kleitman, D.},
  year = {1981},
  doi = {10.1016/0304-3975(81)90099-2},
  abstract = {No abstract available},
  citationcount = {21},
  venue = {Theoretical Computer Science}
}

@article{buryPolynomialTimeApproximation2021,
  title = {Polynomial Time Approximation Schemes for All 1-Center Problems on Metric Rational Set Similarities},
  author = {Bury, Marc and Gentili, Michele and Schwiegelshohn, Chris and Sorella, Mara},
  year = {2021},
  doi = {10.1007/s00453-020-00787-3},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Algorithmica}
}

@article{c.fiorioMemoryManagementUnionFind1997,
  title = {Memory {{Management}} for {{Union-Find Algorithms}}},
  author = {C. Fiorio and J. Gustedt},
  year = {1997},
  journal = {Symposium on Theoretical Aspects of Computer Science},
  doi = {10.1007/BFb0023449},
  annotation = {Citation Count: 16}
}

@article{c.fiorioTwoLinearTime1996,
  title = {Two {{Linear Time Union-Find Strategies}} for {{Image Processing}}},
  author = {C. Fiorio and J. Gustedt},
  year = {1996},
  journal = {Theoretical Computer Science},
  doi = {10.1016/0304-3975(94)00262-2},
  annotation = {Citation Count: 235}
}

@article{c.gaibissoIntersectionReportingTwo1999,
  title = {Intersection {{Reporting}} on {{Two Collections}} of {{Disjoint Sets}}},
  author = {C. Gaibisso and E. Nardelli and Guido Proietti},
  year = {1999},
  journal = {Information Sciences},
  doi = {10.1016/S0020-0255(99)80001-2},
  annotation = {Citation Count: 1}
}

@article{c.gaibissoOutputSensitiveSolution1996,
  title = {An {{Output Sensitive Solution}} to the {{Set Union}} and {{Intersection Problem}}},
  author = {C. Gaibisso and E. Nardelli and Guido Proietti},
  year = {1996},
  journal = {Conference on Current Trends in Theory and Practice of Informatics},
  doi = {10.1007/BFb0037417},
  annotation = {Citation Count: 0}
}

@article{c.gaibissoPartiallyPersistentData1990,
  title = {A Partially Persistent Data Structure for the Set-Union Problem},
  author = {C. Gaibisso and G. Gambosi and M. Talamo},
  year = {1990},
  journal = {RAIRO - Theoretical Informatics and Applications},
  doi = {10.1051/ita/1990240201891},
  abstract = {Une extrusion du probleme de l'Union des ensembles est consideree, ou il est possible d'effectuer une recherche de l'histoire de la partition. Introduction d'une structure de donnees partiellement persistante qui maintient une partition d'un ensemble de n elements sans aucun cout additionnel, meme dans le cas de la complexite de la structure ephemere},
  keywords = {data structure},
  annotation = {Citation Count: 3}
}

@article{c.jonesSumofSquaresLowerBounds2023,
  title = {Sum-of-{{Squares Lower Bounds}} for {{Densest}} k-{{Subgraph}}},
  author = {C. Jones and Aaron Potechin and Goutham Rajendran and Jeff Xu},
  year = {2023},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/3564246.3585221},
  abstract = {Given a graph and an integer k, Densest k-Subgraph is the algorithmic task of finding the subgraph on k vertices with the maximum number of edges. This is a fundamental problem that has been subject to intense study for decades, with applications spanning a wide variety of fields. The state-of-the-art algorithm is an O(n1/4 + )-factor approximation (for any {$>$} 0) due to Bhaskara et al. [STOC '10]. Moreover, the so-called log-density framework predicts that this is optimal, i.e. it is impossible for an efficient algorithm to achieve an O(n1/4 - )-factor approximation. In the average case, Densest k-Subgraph is a prototypical noisy inference task which is conjectured to exhibit a statistical-computational gap. In this work, we provide the strongest evidence yet of hardness for Densest k-Subgraph by showing matching lower bounds against the powerful Sum-of-Squares (SoS) algorithm, a meta-algorithm based on convex programming that achieves state-of-art algorithmic guarantees for many optimization and inference problems. For k {$\leq$} n1/2, we obtain a degree n{$\delta$} SoS lower bound for the hard regime as predicted by the log-density framework. To show this, we utilize the modern framework for proving SoS lower bounds on average-case problems pioneered by Barak et al. [FOCS '16]. A key issue is that small denser-than-average subgraphs in the input will greatly affect the value of the candidate pseudoexpectation operator around the subgraph. To handle this challenge, we devise a novel matrix factorization scheme based on the positive minimum vertex separator. We then prove an intersection tradeoff lemma to show that the error terms when using this separator are indeed small.},
  keywords = {lower bound},
  annotation = {Citation Count: 8}
}

@article{cairoDecodingHiddenMarkov2015,
  title = {Decoding Hidden Markov Models Faster than Viterbi via Online Matrix-Vector (Max, +)-Multiplication},
  author = {Cairo, Massimo and Farina, Gabriele and Rizzi, Romeo},
  year = {2015},
  doi = {10.1609/aaai.v30i1.10263},
  abstract = {In this paper, we present a novel algorithm for the maximum a posteriori decoding (MAPD) of time-homogeneous Hidden Markov Models (HMM), improving the worst-case running time of the classical Viterbi algorithm by a logarithmic factor. In our approach, we interpret the Viterbi algorithm as a repeated computation of matrix-vector (max, +)-multiplications. On time-homogeneous HMMs, this computation is online: a matrix, known in advance, has to be multiplied with several vectors revealed one at a time. Our main contribution is an algorithm solving this version of matrix-vector (max,+)-multiplication in subquadratic time, by performing a polynomial preprocessing of the matrix. Employing this fast multiplication algorithm, we solve the MAPD problem in O(mn2/log n) time for any time-homogeneous HMM of size n and observation sequence of length m, with an extra polynomial preprocessing cost negligible for m {\textquestiondown} n. To the best of our knowledge, this is the first algorithm for the MAPD problem requiring subquadratic time per observation, under the assumption --- usually verified in practice --- that the transition probability matrix does not change with time.},
  citationcount = {5},
  venue = {AAAI Conference on Artificial Intelligence}
}

@article{cardinalSolvingKSum2015,
  title = {Solving K-{{SUM}} Using Few Linear Queries},
  author = {Cardinal, J. and Iacono, J. and Ooms, Aur{\'e}lien},
  year = {2015},
  doi = {10.4230/LIPIcs.ESA.2016.25},
  abstract = {The k-SUM problem is given n input real numbers to determine whether any k of them sum to zero. The problem is of tremendous importance in the emerging field of complexity theory within P, and it is in particular open whether it admits an algorithm of complexity O(n{\textasciicircum}c) with c{\textexclamdown}d where d is the ceiling of k/2. Inspired by an algorithm due to Meiser (1993), we show that there exist linear decision trees and algebraic computation trees of depth O(n{\textasciicircum}3 log{\textasciicircum}2 n) solving k-SUM. Furthermore, we show that there exists a randomized algorithm that runs in  O(n{\textasciicircum}\{d+8\}) time, and performs O(n{\textasciicircum}3 log{\textasciicircum}2 n) linear queries on the input. Thus, we show that it is possible to have an algorithm with a runtime almost identical (up to the +8) to the best known algorithm but for the first time also with the number of queries on the input a polynomial that is independent of k. The O(n{\textasciicircum}3 log{\textasciicircum}2 n) bound on the number of linear queries is also a tighter bound than any known algorithm solving k-SUM, even allowing unlimited total time outside of the queries. By simultaneously achieving few queries to the input without significantly sacrificing runtime vis-a-vis known algorithms, we deepen the understanding of this canonical problem which is a cornerstone of complexity-within-P. We also consider a range of tradeoffs between the number of terms involved in the queries and the depth of the decision tree. In particular, we prove that there exist o(n)-linear decision trees of depth  O(n{\textasciicircum}3) for the k-SUM problem.},
  citationcount = {23},
  venue = {Embedded Systems and Applications},
  keywords = {query}
}

@article{carlInequalitiesOfBernstein1985,
  title = {Inequalities of {{Bernstein-Jackson-type}} and the Degree of Compactness of Operators in {{Banach}} Spaces},
  author = {Carl, B.},
  year = {1985},
  doi = {10.5802/AIF.1020},
  abstract = {{\copyright} Annales de l'institut Fourier, 1985, tous droits r{\'e}serv{\'e}s. L'acc{\`e}s aux archives de la revue << Annales de l'institut Fourier >> (http://annalif.ujf-grenoble.fr/) implique l'accord avec les conditions g{\'e}n{\'e}rales d'utilisation (http://www.numdam.org/legal.php). Toute utilisation commerciale ou impression syst{\'e}matique est constitutive d'une infraction p{\'e}nale. Toute copie ou impression de ce fichier doit contenir la pr{\'e}sente mention de copyright.},
  citationcount = {134},
  venue = {No venue available}
}

@article{carlogaibissoPartiallyPersistentData1988,
  title = {A {{Partially Persistent Data Structure}} for the {{Set-Union Problem}} with {{Backtracking}}},
  author = {Carlo Gaibisso},
  year = {1988},
  journal = {International Meeting of Young Computer Scientists},
  doi = {10.1007/BFb0015933},
  keywords = {data structure},
  annotation = {Citation Count: 1}
}

@article{carmeliTractableOrdersFor2020,
  title = {Tractable Orders for Direct Access to Ranked Answers of Conjunctive Queries},
  author = {Carmeli, Nofar and Tziavelis, Nikolaos and Gatterbauer, Wolfgang and Kimelfeld, B. and Riedewald, Mirek},
  year = {2020},
  doi = {10.1145/3452021.3458331},
  abstract = {We study the question of when we can provide logarithmic-time direct access to the k-th answer to a Conjunctive Query (CQ) with a specified ordering over the answers, following a preprocessing step that constructs a data structure in time quasilinear in the size of the database. Specifically, we embark on the challenge of identifying the tractable answer orderings that allow for ranked direct access with such complexity guarantees. We begin with lexicographic orderings and give a decidable characterization (under conventional complexity assumptions) of the class of tractable lexicographic orderings for every CQ without self-joins. We then continue to the more general orderings by the sum of attribute weights and show for it that ranked direct access is tractable only in trivial cases. Hence, to better understand the computational challenge at hand, we consider the more modest task of providing access to only a single answer (i.e., finding the answer at a given position) - a task that we refer to as the selection problem. We indeed achieve a quasilinear-time algorithm for a subset of the class of full CQs without self-joins, by adopting a solution of Frederickson and Johnson to the classic problem of selection over sorted matrices. We further prove that none of the other queries in this class admit such an algorithm.},
  citationcount = {25},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {data structure,query}
}

@article{carterExactAndApproximate1978,
  title = {Exact and Approximate Membership Testers},
  author = {Carter, L. and Floyd, R. W. and Gill, John and Markowsky, G. and Wegman, M.},
  year = {1978},
  doi = {10.1145/800133.804332},
  abstract = {In this paper we consider the question of how much space is needed to represent a set. Given a finite universe U and some subset V (called the vocabulary), an exact membership tester is a procedure that for each element s in U determines if s is in V. An approximate membership tester is allowed to make mistakes: we require that the membership tester correctly accepts every element of V, but we allow it to also accept a small fraction of the elements of U - V.},
  citationcount = {135},
  venue = {Symposium on the Theory of Computing}
}

@article{carterUniversalClassesOf1979,
  title = {Universal Classes of Hash Functions},
  author = {Carter, L. and Wegman, M.},
  year = {1979},
  doi = {10.1016/0022-0000(79)90044-8},
  abstract = {No abstract available},
  citationcount = {2694},
  venue = {Journal of computer and system sciences (Print)}
}

@article{carusoANewFaster2017,
  title = {A New Faster Algorithm for Factoring Skew Polynomials over Finite Fields},
  author = {Caruso, X. and Borgne, J. L.},
  year = {2017},
  doi = {10.1016/j.jsc.2016.02.016},
  abstract = {No abstract available},
  citationcount = {33},
  venue = {Journal of symbolic computation}
}

@article{caryOptimalEpsilonApproximateNearest2001,
  title = {Toward Optimal Epsilon-{{Approximate}} Nearest Neighbor Algorithms},
  author = {Cary, Matthew},
  year = {2001},
  doi = {10.1006/jagm.2001.1193},
  abstract = {We combine the recent optimal predecessor algorithm with a recent randomized stratified tree algorithm for an ?-approximate nearest neighbor to give an algorithm for an ?-approximate nearest neighbor in a fixed-dimensional space that is optimal with respect to universe size. We also give a deterministic version of the stratified tree algorithm.},
  citationcount = {7},
  venue = {J. Algorithms}
}

@article{caselFineGrainedComplexity2021,
  title = {Fine-Grained Complexity of Regular Path Queries},
  author = {Casel, Katrin and Schmid, Markus L.},
  year = {2021},
  doi = {10.46298/lmcs-19(4:15)2023},
  abstract = {A regular path query (RPQ) is a regular expression q that returns all node pairs (u, v) from a graph database that are connected by an arbitrary path labelled with a word from L(q). The obvious algorithmic approach to RPQ-evaluation (called PG-approach), i.e., constructing the product graph between an NFA for q and the graph database, is appealing due to its simplicity and also leads to efficient algorithms. However, it is unclear whether the PG-approach is optimal. We address this question by thoroughly investigating which upper complexity bounds can be achieved by the PG-approach, and we complement these with conditional lower bounds (in the sense of the fine-grained complexity framework). A special focus is put on enumeration and delay bounds, as well as the data complexity perspective. A main insight is that we can achieve optimal (or near optimal) algorithms with the PG-approach, but the delay for enumeration is rather high (linear in the database). We explore three successful approaches towards enumeration with sub-linear delay: super-linear preprocessing, approximations of the solution sets, and restricted classes of RPQs.},
  citationcount = {16},
  venue = {International Conference on Database Theory},
  keywords = {lower bound,query}
}

@article{cashHighlyScalableSearchable2013,
  title = {Highly-Scalable Searchable Symmetric Encryption with Support for Boolean Queries},
  author = {Cash, David and Jarecki, Stanislaw and Jutla, C. and Krawczyk, H. and Rosu, Marcel-Catalin and Steiner, M.},
  year = {2013},
  doi = {10.1007/978-3-642-40041-4_20},
  abstract = {No abstract available},
  citationcount = {849},
  venue = {IACR Cryptology ePrint Archive},
  keywords = {query}
}

@article{castelliMultidimensionalIndexingStructures2001,
  title = {Multidimensional Indexing Structures for Content-Based Retrieval},
  author = {Castelli, Vittorio},
  year = {2001},
  doi = {10.1007/springerreference_64623},
  abstract = {No abstract available},
  citationcount = {31},
  venue = {No venue available}
}

@article{castelliMultidimensionalIndexingStructures2002,
  title = {Multidimensional Indexing Structures for Content-based Retrieval},
  author = {Castelli, Vittorio},
  year = {2002},
  doi = {10.1002/0471224634.CH14},
  abstract = {: This report has been submitted for publication outside of IBM and will probably be copyrighted if accepted for publication. It has been issued as a Research Reportfor early dissemination of its contents. In view of the transfer of copyright to the outside publisher, its distribution outside of IBM prior to publication should be limited to peer communications and specific requests.After outside publication, requests should be filled only by reprints or legally obtained copies of the article (e.g. , payment of royalties). Copies may be requested from IBM T. J. Watson Research Center , P. O. Box 218,Yorktown Heights, NY 10598 USA (email: reports@us.ibm.com). Some reports are available on the internet at http://domino.watson.ibm.com/library/CyberDig.nsf/home .},
  citationcount = {5},
  venue = {No venue available},
  keywords = {communication}
}

@article{catalnSurLesNombres1887,
  title = {Sur Les Nombres de Segner},
  author = {Catal{\'a}n, M.},
  year = {1887},
  doi = {10.1007/BF03020089},
  abstract = {No abstract available},
  citationcount = {47},
  venue = {No venue available}
}

@article{cazalsEffectiveNearestNeighbors1998,
  title = {Effective Nearest Neighbors Searching on the Hyper-Cube, with Applications to Molecular Clustering},
  author = {Cazals, F.},
  year = {1998},
  doi = {10.1145/276884.276910}
}

@article{censor-hillelFastApproximateCounting2024,
  title = {Fast Approximate Counting of Cycles},
  author = {{Censor-Hillel}, K. and Even, Tomer and Williams, Virginia Vassilevska},
  year = {2024},
  doi = {10.4230/LIPIcs.ICALP.2024.37},
  abstract = {We consider the problem of approximate counting of triangles and longer fixed length cycles in directed graphs. For triangles, T{\v \{}e\vphantom\{\}tek [ICALP'22] gave an algorithm that returns a (1{\textpm})-approximation in O\vphantom\{\}(n\textsuperscript{{$\omega$}}/t\textsuperscript{\{\vphantom\}}{$\omega$}-2\vphantom\{\}) time, where t is the unknown number of triangles in the given n node graph and {$\omega<$}2.372 is the matrix multiplication exponent. We obtain an improved algorithm whose running time is, within polylogarithmic factors the same as that for multiplying an n{\texttimes}n/t matrix by an n/t{\texttimes}n matrix. We then extend our framework to obtain the first nontrivial (1{\textpm})-approximation algorithms for the number of h-cycles in a graph, for any constant h{$\geq$}3. Our running time is  Finally, we show that under popular fine-grained hypotheses, this running time is optimal.},
  citationcount = {2},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{cerbuTheCycleStructure2016,
  title = {The Cycle Structure of a {{Markoff}} Automorphism over Finite Fields},
  author = {Cerbu, Alois and Gunther, E. and Magee, Michael and Peilen, Luke},
  year = {2016},
  doi = {10.1016/j.jnt.2019.09.022},
  abstract = {No abstract available},
  citationcount = {10},
  venue = {Journal of Number Theory}
}

@article{chakrabartiAnOptimalLower2010,
  title = {An Optimal Lower Bound on the Communication Complexity of Gap-Hamming-Distance},
  author = {Chakrabarti, Amit and Regev, O.},
  year = {2010},
  doi = {10.1145/1993636.1993644},
  abstract = {We prove an optimal {\textohm}(n) lower bound on the randomized communication complexity of the much-studied Gap-Hamming-Distance problem. As a consequence, we obtain essentially optimal multi-pass space lower bounds in the data stream model for a number of fundamental problems, including the estimation of frequency moments. The Gap-Hamming-Distance problem is a communication problem, wherein Alice and Bob receive n-bit strings x and y, respectively. They are promised that the Hamming distance between x and y is either at least n/2+{\textsurd}n or at most n/2-{\textsurd}n, and their goal is to decide which of these is the case. Since the formal presentation of the problem by Indyk and Woodruff (FOCS, 2003), it had been conjectured that the naive protocol, which uses n bits of communication, is asymptotically optimal. The conjecture was shown to be true in several special cases, e.g., when the communication is deterministic, or when the number of rounds of communication is limited. The proof of our aforementioned result, which settles this conjecture fully, is based on a new geometric statement regarding correlations in Gaussian space, related to a result of C. Borell (1985). To prove this geometric statement, we show that random projections of not-too-small sets in Gaussian space are close to a mixture of translated normal variables.},
  citationcount = {181},
  venue = {SIAM journal on computing (Print)}
}

@article{chakrabartiEverywhereTightInformation2011,
  title = {Everywhere-Tight Information Cost Tradeoffs for Augmented Index},
  author = {Chakrabarti, Amit and Kondapally, Ranganath},
  year = {2011},
  doi = {10.1007/978-3-642-22935-0_38},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques}
}

@article{chakrabartiInformationalComplexityAnd2001,
  title = {Informational Complexity and the Direct Sum Problem for Simultaneous Message Complexity},
  author = {Chakrabarti, Amit and Shi, Yaoyun and Wirth, Anthony and Yao, A.},
  year = {2001},
  doi = {10.1109/SFCS.2001.959901},
  abstract = {Given m copies of the same problem, does it take m times the amount of resources to solve these m problems? This is the direct sum problem, a fundamental question that has been studied in many computational models. We study this question in the simultaneous message (SM) model of communication introduced by A.C. Yao (1979). The equality problem for n-bit strings is well known to have SM complexity /spl Theta/(/spl radic/n). We prove that solving m copies of the problem has complexity /spl Omega/(m/spl radic/n); the best lower bound provable using previously known techniques is /spl Omega/(/spl radic/(mn)). We also prove similar lower bounds on certain Boolean combinations of multiple copies of the equality function. These results can be generalized to a broader class of functions. We introduce a new notion of informational complexity which is related to SM complexity and has nice direct sum properties. This notion is used as a tool to prove the above results; it appears to be quite powerful and may be of independent interest.},
  citationcount = {268},
  venue = {Proceedings IEEE International Conference on Cluster Computing}
}

@inproceedings{chakrabartiLowerBoundComplexity1999,
  title = {A Lower Bound on the Complexity of Approximate Nearest-Neighbor Searching on the Hamming Cube},
  booktitle = {Proc. 31st {{ACM Symposium}} on {{Theory}} of {{Computing}} ({{STOC}})},
  author = {Chakrabarti, Amit and Chazelle, Bernard and Gum, Benjamin and Lvov, Alexey},
  year = {1999},
  pages = {305--311},
  doi = {10.1145/301250.301325},
  keywords = {cell probe,lower bound,query,query time},
  annotation = {Arya , S. , Mount , D.M. Appro{\textasciitilde}mate nearest neighbor searching , Proc. 4th Annu. ACM-SIAM Symp. Disc. Alg. ( 1993 ), 271 - 280 . Arya, S., Mount, D.M. Appro{\textasciitilde}mate nearest neighbor searching, Proc. 4th Annu. ACM-SIAM Symp. Disc. Alg. (1993), 271-280.\\
\\
Arya , S. , Mount , D.M. , Netanyahu , N.S. , Silverman , R. , Wu , A. An optimal algorithm for approximate nearest neighbor searching , Proc. 5th Annu. ACM- SIAM Symp. Disc. AIg. ( 1994 ), 573 - 582 . Arya, S., Mount, D.M., Netanyahu, N.S., Silverman, R., Wu, A. An optimal algorithm for approximate nearest neighbor searching, Proc. 5th Annu. ACM- SIAM Symp. Disc. AIg. (1994), 573-582.\\
Beame P. Fich F. Optimal bounds for the predecessor problem manuscript \vphantom\{\}998.  Beame P. Fich F. Optimal bounds for the predecessor problem manuscript \vphantom\{\}998.\\
\\
\\
\\
\\
Cha{\textasciitilde}etle B. The Discrepancy Method: Randomness and Complexity manuscript to appear.  Cha{\textasciitilde}etle B. The Discrepancy Method: Randomness and Complexity manuscript to appear.\\
\\
\\
\\
Graham , R.L. , Knuth , D.E. , Patashnik , O . Concrete Mathematics : A Foundation for Computer Science, 2rid ed., Addison-Wesley , 1994 . Graham, R.L., Knuth, D.E., Patashnik, O. Concrete Mathematics: A Foundation for Computer Science, 2rid ed., Addison-Wesley, 1994.\\
Hoeffding W. Probability inequalities for sums oj{\textasciitilde} bounded random variables J. Amer. Stat. Assoc. 58 ({\textasciitilde}a) {\textasciitilde}a-ao.  Hoeffding W. Probability inequalities for sums oj{\textasciitilde} bounded random variables J. Amer. Stat. Assoc. 58 ({\textasciitilde}a) {\textasciitilde}a-ao.\\
\\
Karchmer , M. , Wigderson , A. Monotone circuits for connectivity require super-logarithmic depth , SIAM J. Disc. Math., 2 (t990). Karchmer, M., Wigderson, A. Monotone circuits for connectivity require super-logarithmic depth, SIAM J. Disc. Math., 2 (t990).\\
\\
Kushilevit{\textasciitilde}z , E. , Nisan , N. Communication Complexity , Cambridge University Press , 1997 Kushilevit{\textasciitilde}z, E., Nisan, N. Communication Complexity, Cambridge University Press, 1997\\
\\
Liniat , N. , London , E. , Rabinovitch , Y. The geometry of graphs and some of its algorithmic applications , Proc. 35th Annu. IEEE Symp. Found. Corn{\textasciitilde} put. Sci. ( 1994 ), 577 - 591 . Liniat, N., London, E., Rabinovitch, Y. The geometry of graphs and some of its algorithmic applications, Proc. 35th Annu. IEEE Symp. Found. Corn{\textasciitilde} put. Sci. (1994), 577-591.\\
\\
\\
Motwani , R. , Raghawn , P. Randomized Algorithms , Cambridge University Press , 1995 . Motwani, R., Raghawn, P. Randomized Algorithms, Cambridge University Press, 1995.\\
Ygnilos , P.N. Data structures and algorithms for nearest neighbor search in general metric spaces , Proc. 2nd Annual ACM-SIAM Symp. Disc. Alg. ({\textasciitilde}s), s{\textasciitilde}-a:{\textasciitilde}. Ygnilos, P.N. Data structures and algorithms for nearest neighbor search in general metric spaces, Proc. 2nd Annual ACM-SIAM Symp. Disc. Alg. ({\textasciitilde}s), s{\textasciitilde}-a:{\textasciitilde}.},
  file = {/Users/tulasi/Zotero/storage/3YWFZF3Z/301250.301325.pdf}
}

@article{chakrabartiOptimalRandomizedCell2010,
  title = {An {{Optimal Randomized Cell Probe Lower Bound}} for {{Approximate Nearest Neighbor Searching}}},
  author = {Chakrabarti, Amit and Regev, Oded},
  year = {2010},
  month = jan,
  journal = {SIAM J. Comput.},
  volume = {39},
  number = {5},
  pages = {1919--1940},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/080729955},
  url = {https://epubs.siam.org/doi/abs/10.1137/080729955},
  urldate = {2024-11-20},
  abstract = {Proving superpolylogarithmic lower bounds for dynamic data structures has remained an open problem despite years of research. P{\v a}tra{\c s}cu proposed an exciting approach for breaking this barrier via a two-player communication model in which one player gets private advice at the beginning of the protocol. He gave reductions from the problem of solving an asymmetric version of set-disjointness in his model to a diverse collection of natural dynamic data structure problems in the cell probe model. He also conjectured that, for any hard problem in the standard two-party communication model, the asymmetric version of the problem is hard in his model, provided not too much advice is given. In this paper, we prove several surprising results about his model. We show that there exist Boolean functions requiring linear randomized communication complexity in the two-party model, for which the asymmetric versions  in his model have deterministic protocols with exponentially smaller complexity. For set-disjointness, which also requires linear randomized communication complexity in the two-party model, we give a deterministic protocol for the asymmetric version in his model with a quadratic improvement in complexity. These results demonstrate that P{\v a}tra{\c s}cu's conjecture, as stated, is false. In addition, we show that the randomized and deterministic communication complexities of problems in his model differ by no more than a logarithmic multiplicative factor. We also prove lower bounds in some restricted versions of this model for natural functions such as set-disjointness and inner product. All of our upper bounds conform to these restrictions. Moreover, a special case of one of these lower bounds implies a new proof of a strong lower bound on the tradeoff between the query time and the amortized update time of dynamic data structures with nonadaptive query algorithms.},
  keywords = {adaptive,cell probe,communication,communication complexity,data structure,dynamic,lower bound,non-adaptive,query,query time,reduction,sorted,update,update time},
  file = {/Users/tulasi/Zotero/storage/KK4SQ4TE/Chakrabarti and Regev - 2010 - An Optimal Randomized Cell Probe Lower Bound for Approximate Nearest Neighbor Searching.pdf}
}

@inproceedings{chakrabortyTightCellProbe2018,
  title = {Tight Cell Probe Bounds for Succinct {{Boolean}} Matrix-Vector Multiplication},
  booktitle = {Proceedings of the 50th {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}},
  author = {Chakraborty, Diptarka and Kamma, Lior and Larsen, Kasper Green},
  year = {2018},
  month = jun,
  pages = {1297--1306},
  publisher = {ACM},
  address = {Los Angeles CA USA},
  doi = {10.1145/3188745.3188830},
  url = {https://dl.acm.org/doi/10.1145/3188745.3188830},
  urldate = {2025-04-15},
  isbn = {978-1-4503-5559-9},
  langid = {english},
  keywords = {cell probe,data structure,lower bound,query,query time},
  file = {/Users/tulasi/Zotero/storage/8NB392P8/Chakraborty et al. - 2018 - Tight cell probe bounds for succinct Boolean matrix-vector multiplication.pdf}
}

@article{chanAdaptiveApproximateOrthogonal2013,
  title = {Adaptive and Approximate Orthogonal Range Counting},
  author = {Chan, Timothy M. and Wilkinson, Bryan T.},
  year = {2013},
  doi = {10.1145/2830567},
  abstract = {We present three new results on one of the most basic problems in geometric data structures, 2-D orthogonal range counting. All the results are in the w-bit word RAM model. ---It is well known that there are linear-space data structures for 2-D orthogonal range counting with worst-case optimal query time O(log n/log log n). We give an O(nlog log n)-space adaptive data structure that improves the query time to O(log log n + log k/log log n), where k is the output count. When k = O(1), our bounds match the state of the art for the 2-D orthogonal range emptiness problem [Chan et al., 2011]. ---We give an O(nlog log n)-space data structure for approximate 2-D orthogonal range counting that can compute a (1 + {$\delta$})-factor approximation to the count in O(log log n) time for any fixed constant {$\delta$} {\textquestiondown} 0. Again, our bounds match the state of the art for the 2-D orthogonal range emptiness problem. ---Last, we consider the 1-D range selection problem, where a query in an array involves finding the kth least element in a given subarray. This problem is closely related to 2-D 3-sided orthogonal range counting. Recently, J{\o}rgensen and Larsen [2011] presented a linear-space adaptive data structure with query time O(log log n + log k/log log n). We give a new linear-space structure that improves the query time to O(1 + log k/log log n), exactly matching the lower bound proved by J{\o}rgensen and Larsen.},
  citationcount = {36},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {adaptive,data structure,lower bound,query,query time}
}

@article{chanApproximateNearestNeighbor1997,
  title = {Approximate Nearest Neighbor Queries Revisited},
  author = {Chan, Timothy M.},
  year = {1997},
  doi = {10.1145/262839.263001},
  abstract = {Abstract. This paper proposes new methods to answer approximate nearest neighbor queries on a set of n points in d -dimensional Euclidean space. For any fixed constant d , a data structure with O( {$\varepsilon$}(1-d)/2n log n) preprocessing time and O( {$\varepsilon$}(1-d)/2log n) query time achieves an approximation factor 1+ {$\varepsilon$} for any given 0 {\textexclamdown} {$\varepsilon$} {\textexclamdown} 1; a variant reduces the {$\varepsilon$} -dependence by a factor of {$\varepsilon$}-1/2 . For any arbitrary d , a data structure with O(d2n log n) preprocessing time and O(d2log n) query time achieves an approximation factor O(d3/2) . Applications to various proximity problems are discussed.},
  citationcount = {123},
  venue = {SCG '97},
  keywords = {data structure,query,query time}
}

@article{chanClusteredInteger3sum2015,
  title = {Clustered Integer {{3SUM}} via Additive Combinatorics},
  author = {Chan, Timothy M. and Lewenstein, Moshe},
  year = {2015},
  doi = {10.1145/2746539.2746568},
  abstract = {We present a collection of new results on problems related to 3SUM, including: The first truly subquadratic algorithm for computing the (min,+) convolution for monotone increasing sequences with integer values bounded by O(n), solving 3SUM for monotone sets in 2D with integer coordinates bounded by O(n), and preprocessing a binary string for histogram indexing (also called jumbled indexing). The running time is O(n(9+{\textsurd}177)/12, polylog,n)=O(n1.859) with randomization, or O(n1.864) deterministically. This greatly improves the previous n2/2{\textohm}({\textsurd}log n) time bound obtained from Williams' recent result on all-pairs shortest paths [STOC'14], and answers an open question raised by several researchers studying the histogram indexing problem. The first algorithm for histogram indexing for any constant alphabet size that achieves truly subquadratic preprocessing time and truly sublinear query time. A truly subquadratic algorithm for integer 3SUM in the case when the given set can be partitioned into n1-{$\delta$} clusters each covered by an interval of length n, for any constant {$\delta$}{\textquestiondown}0. An algorithm to preprocess any set of n integers so that subsequently 3SUM on any given subset can be solved in O(n13/7, polylog,n) time. All these results are obtained by a surprising new technique, based on the Balog--Szemeredi--Gowers Theorem from additive combinatorics.},
  citationcount = {134},
  venue = {Symposium on the Theory of Computing},
  keywords = {query,query time}
}

@article{chanCompressedIndexesFor2007,
  title = {Compressed Indexes for Dynamic Text Collections},
  author = {Chan, H. and Hon, W. and Lam, T. and Sadakane, K.},
  year = {2007},
  doi = {10.1145/1240233.1240244},
  abstract = {Let {\textexclamdown}i{\textquestiondown}T{\textexclamdown}/i{\textquestiondown} be a string with {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} characters over an alphabet of constant size. A recent breakthrough on compressed indexing allows us to build an index for {\textexclamdown}i{\textquestiondown}T{\textexclamdown}/i{\textquestiondown} in optimal space (i.e., {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) bits), while supporting very efficient pattern matching [Ferragina and Manzini 2000; Grossi and Vitter 2000]. Yet the compressed nature of such indexes also makes them difficult to update dynamically. This article extends the work on optimal-space indexing to a dynamic collection of texts. Our first result is a compressed solution to the {\textexclamdown}i{\textquestiondown}library management{\textexclamdown}/i{\textquestiondown} problem, where we show an index of {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) bits for a text collection {\textexclamdown}i{\textquestiondown}L{\textexclamdown}/i{\textquestiondown} of total length {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}, which can be updated in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textbar}{\textexclamdown}i{\textquestiondown}T{\textexclamdown}/i{\textquestiondown}{\textbar} log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) time when a text {\textexclamdown}i{\textquestiondown}T{\textexclamdown}/i{\textquestiondown} is inserted or deleted from {\textexclamdown}i{\textquestiondown}L{\textexclamdown}/i{\textquestiondown}; also, the index supports searching the occurrences of any pattern {\textexclamdown}i{\textquestiondown}P{\textexclamdown}/i{\textquestiondown} in all texts in {\textexclamdown}i{\textquestiondown}L{\textexclamdown}/i{\textquestiondown} in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textbar}{\textexclamdown}i{\textquestiondown}P{\textexclamdown}/i{\textquestiondown}{\textbar} log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {\textexclamdown}i{\textquestiondown}occ{\textexclamdown}/i{\textquestiondown} log{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) time, where {\textexclamdown}i{\textquestiondown}occ{\textexclamdown}/i{\textquestiondown} is the number of occurrences. Our second result is a compressed solution to the {\textexclamdown}i{\textquestiondown}dictionary matching{\textexclamdown}/i{\textquestiondown} problem, where we show an index of {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}) bits for a pattern collection {\textexclamdown}i{\textquestiondown}D{\textexclamdown}/i{\textquestiondown} of total length {\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}, which can be updated in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textbar}{\textexclamdown}i{\textquestiondown}P{\textexclamdown}/i{\textquestiondown}{\textbar} log{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}) time when a pattern {\textexclamdown}i{\textquestiondown}P{\textexclamdown}/i{\textquestiondown} is inserted or deleted from {\textexclamdown}i{\textquestiondown}D{\textexclamdown}/i{\textquestiondown}; also, the index supports searching the occurrences of all patterns of {\textexclamdown}i{\textquestiondown}D{\textexclamdown}/i{\textquestiondown} in any text {\textexclamdown}i{\textquestiondown}T{\textexclamdown}/i{\textquestiondown} in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(({\textbar}{\textexclamdown}i{\textquestiondown}T{\textexclamdown}/i{\textquestiondown}{\textbar} + {\textexclamdown}i{\textquestiondown}occ{\textexclamdown}/i{\textquestiondown})log{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}) time. When compared with the {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown} log {\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown})-bit suffix-tree-based solution of Amir et al. [1995], the compact solution increases the query time by roughly a factor of log {\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown} only. The solution to the dictionary matching problem is based on a new compressed representation of a suffix tree. Precisely, we give an {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown})-bit representation of a suffix tree for a dynamic collection of texts whose total length is {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}, which supports insertion and deletion of a text {\textexclamdown}i{\textquestiondown}T{\textexclamdown}/i{\textquestiondown} in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textbar}{\textexclamdown}i{\textquestiondown}T{\textexclamdown}/i{\textquestiondown}{\textbar} log{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) time, as well as all suffix tree traversal operations, including forward and backward suffix links. This work can be regarded as a generalization of the compressed representation of static texts. In the study of the aforementioned result, we also derive the first {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown})-bit representation for maintaining {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} pairs of balanced parentheses in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}/log log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) time per operation, matching the time complexity of the previous {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown})-bit solution.},
  citationcount = {98},
  venue = {TALG},
  keywords = {dynamic,query,query time,static,update}
}

@article{chanCountingInversionsOffline2010,
  title = {Counting Inversions, Offline Orthogonal Range Counting, and Related Problems},
  author = {Chan, Timothy M. and Patrascu, M.},
  year = {2010},
  doi = {10.1137/1.9781611973075.15},
  abstract = {We give an {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} {\textsurd}lg {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown})-time algorithm for counting the number of inversions in a permutation on {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} elements. This improves a long-standing previous bound of {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} lg {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}/ lg lg {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) that followed from Dietz's data structure [WADS'89], and answers a question of Andersson and Petersson [SODA'95]. As Dietz's result is known to be optimal for the related dynamic rank problem, our result demonstrates a significant improvement in the {\textexclamdown}i{\textquestiondown}offline{\textexclamdown}/i{\textquestiondown} setting. Our new technique is quite simple: we perform a "vertical partitioning" of a trie (akin to van Emde Boas trees), and use ideas from external memory. However, the technique finds numerous applications: for example, we obtain {$\bullet$} in {\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown} dimensions, an algorithm to answer {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} offline orthogonal range counting queries in time {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} lg{\textexclamdown}sup{\textquestiondown}{\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}-2+1/{\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}); {$\bullet$} an improved construction time for online data structures for orthogonal range counting; {$\bullet$} an improved update time for the partial sums problem; {$\bullet$} faster Word RAM algorithms for finding the maximum depth in an arrangement of axis-aligned rectangles, and for the slope selection problem. As a bonus, we also give a simple (1 + {$\varepsilon$})-approximation algorithm for counting inversions that runs in linear time, improving the previous {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown} ({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} lg lg {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) bound by Andersson and Petersson.},
  citationcount = {91},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,query,update,update time}
}

@article{chanDeterministicApspOrthogonal2016,
  title = {Deterministic {{APSP}}, Orthogonal Vectors, and More},
  author = {Chan, Timothy M. and Williams, Ryan},
  year = {2016},
  doi = {10.1145/3402926},
  abstract = {We show how to solve all-pairs shortest paths on n nodes in deterministic n3{\textquestiondown}/2{\textquestiondown}{\textohm} ( {\textsurd} log n) time, and how to count the pairs of orthogonal vectors among n 0-1 vectors in d = clog n dimensions in deterministic n2-1/O(log c) time. These running times essentially match the best known randomized algorithms of Williams [46] and Abboud, Williams, and Yu [8], respectively, and the ability to count was open even for randomized algorithms. By reductions, these two results yield faster deterministic algorithms for many other problems. Our techniques can also be used to deterministically count k-satisfiability (k-SAT) assignments on n variable formulas in 2n-n/O(k) time, roughly matching the best known running times for detecting satisfiability and resolving an open problem of Santhanam [24]. A key to our constructions is an efficient way to deterministically simulate certain probabilistic polynomials critical to the algorithms of prior work, carefully applying small-biased sets and modulus-amplifying polynomials.},
  citationcount = {139},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{chandraLowerBoundsFor1983,
  title = {Lower Bounds for Constant Depth Circuits for Prefix Problems},
  author = {Chandra, A. K. and Fortune, S. and Lipton, R.},
  year = {1983},
  doi = {10.1007/BFb0036901},
  abstract = {No abstract available},
  citationcount = {48},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{chandraUnboundedFanIn1983,
  title = {Unbounded Fan-in Circuits and Associative Functions},
  author = {Chandra, A. K. and Fortune, S. and Lipton, R.},
  year = {1983},
  doi = {10.1145/800061.808732},
  abstract = {We consider the computation of finite semigroups using unbounded fan-in circuits. There are constant-depth, polynomial size circuits for semigroup product iff the semigroup does not contain a nontrivial group as a subset. In the case that the semigroup in fact does not contain a group, then for any primitive recursive function f, circuits of size O(nf-1(n)) and constant depth exist for the semigroup product of n elements. The depth depends upon the choice of the primitive recursive function f. The circuits not only compute the semigroup product, but every prefix of the semigroup product. A consequence is that the same bounds apply for circuits computing the sum of two n-bit numbers.},
  citationcount = {118},
  venue = {Journal of computer and system sciences (Print)}
}

@article{chanDynamicConnectivityConnecting2008,
  title = {Dynamic Connectivity: {{Connecting}} to Networks and Geometry},
  author = {Chan, Timothy M. and Patrascu, M. and Roditty, L.},
  year = {2008},
  doi = {10.1137/090751670},
  abstract = {Dynamic connectivity is a well-studied problem, but so far the most compelling progress has been confined to the edge-update model: maintain an understanding of connectivity in an undirected graph, subject to edge insertions and deletions. In this paper, we study two more challenging, yet equally fundamental problems: subgraph connectivity asks to maintain an understanding of connectivity under vertex updates: updates can turn vertices on and off, and queries refer to the subgraph induced by "on" vertices. (For instance, this is closer to applications in networks of routers, where node faults may occur.)We describe a data structure supporting vertex updates in O (m{\textasciicircum}\{2/3\}) amortized time, where m denotes the number of edges in the graph. This greatly improves over the previous result [Chan, STOC'02], which required fast matrix multiplication and had an update time of O(m{\textasciicircum}\{0.94\}). The new data structure is also simpler. Geometric connectivity asks to maintain a dynamic set of n geometric objects, and query connectivity in their intersection graph. (For instance, the intersection graph of balls describes connectivity in a network of sensors with bounded transmission radius.) Previously, nontrivial fully dynamic results were known only for special cases like axis-parallel line segments and rectangles. We provide similarly improved update times, O (n{\textasciicircum}\{2/3\}), for these special cases. Moreover, we show how to obtain sublinear update bounds for virtually all families of geometric objects which allow sublinear-time range queries. In particular, we obtain the first sublinear update time for arbitrary 2D line segments: O*(n{\textasciicircum}\{9/10\}); for d-dimensional simplices: O*(n{\textasciicircum}\{1-1/d(2d+1)\}); and for d-dimensional balls: O*(n{\textasciicircum}\{1-1/(d+1)(2d+3)\}).},
  citationcount = {45},
  venue = {2008 49th Annual IEEE Symposium on Foundations of Computer Science},
  keywords = {data structure,dynamic,query,update,update time}
}

@inproceedings{chanDynamicConnectivityConnecting2008a,
  title = {Dynamic {{Connectivity}}: {{Connecting}} to {{Networks}} and {{Geometry}}},
  shorttitle = {Dynamic {{Connectivity}}},
  booktitle = {49th {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}}, {{FOCS}} 2008, {{October}} 25-28, 2008, {{Philadelphia}}, {{PA}}, {{USA}}},
  author = {Chan, Timothy M. and Puatracscu, Mihai and Roditty, Liam},
  year = {2008},
  pages = {95--104},
  publisher = {IEEE Computer Society},
  doi = {10.1109/FOCS.2008.29},
  keywords = {dynamic},
  file = {/Users/tulasi/Zotero/storage/23PHZF3D/Chan et al. - 2008 - Dynamic Connectivity Connecting to Networks and Geometry.pdf}
}

@article{chanDynamicPlanarOrthogonal2018,
  title = {Dynamic Planar Orthogonal Point Location in Sublogarithmic Time},
  author = {Chan, Timothy M. and Tsakalidis, Konstantinos},
  year = {2018},
  doi = {10.4230/LIPIcs.SoCG.2018.25},
  abstract = {We study a longstanding problem in computational geometry: dynamic 2-d orthogonal point location, i.e., vertical ray shooting among n horizontal line segments. We present a data structure achieving O(log n / log log n) optimal expected query time and O(log{\textasciicircum}\{1/2+epsilon\} n) update time (amortized) in the word-RAM model for any constant epsilon{\textquestiondown}0, under the assumption that the x-coordinates are integers bounded polynomially in n. This substantially improves previous results of Giyora and Kaplan [SODA 2007] and Blelloch [SODA 2008] with O(log n) query and update time, and of Nekrich (2010) with O(log n / log log n) query time and O(log{\textasciicircum}\{1+epsilon\} n) update time. Our result matches the best known upper bound for simpler problems such as dynamic 2-d dominance range searching. We also obtain similar bounds for orthogonal line segment intersection reporting queries, vertical ray stabbing, and vertical stabbing-max, improving previous bounds, respectively, of Blelloch [SODA 2008] and Mortensen [SODA 2003], of Tao (2014), and of Agarwal, Arge, and Yi [SODA 2005] and Nekrich [ISAAC 2011].},
  citationcount = {10},
  venue = {International Symposium on Computational Geometry},
  keywords = {data structure,dynamic,query,query time,update,update time}
}

@article{chanDynamicSubgraphConnectivity2002,
  title = {Dynamic Subgraph Connectivity with Geometric Applications},
  author = {Chan, Timothy M.},
  year = {2002},
  doi = {10.1145/509907.509911},
  abstract = {(MATH) Inspired by dynamic connectivity applications in computational geometry, we consider a problem we call dynamic subgraph connectivity: design a data structure for an undirected graph G=(V,E) and a subset of vertices SV, to support insertions and deletions in S and connectivity queries (are two vertices connected?) in the subgraph induced by S. We develop the first sublinear, fully dynamic method for this problem for general sparse graphs, using an elegant combination of several simple ideas. Our method requires linear space, ({\textbar}E{\textbar}\textsuperscript{\{\vphantom\}}4/(3+3)\vphantom\{\})=O({\textbar}E{\textbar}\textsuperscript{\{\vphantom\}}0.94\vphantom\{\}) amortized update time, and ({\textbar}E{\textbar}\textsuperscript{\{\vphantom\}}1/3\vphantom\{\}) query time, where  is the matrix multiplication exponent and  hides polylogarithmic factors.},
  citationcount = {44},
  venue = {Symposium on the Theory of Computing}
}

@article{chanFasterAlgorithmsFor2023,
  title = {Faster Algorithms for Text-to-Pattern Hamming Distances},
  author = {Chan, Timothy M. and Jin, Ce and Williams, Virginia Vassilevska and Xu, Yinzhan},
  year = {2023},
  doi = {10.1109/FOCS57990.2023.00136},
  abstract = {We study the classic Text-to-Pattern Hamming Distances problem: given a pattern P of length m and a text T of length n, both over a polynomial-size alphabet, compute the Hamming distance between P and T[i{\dots}i+m-1] for every shift i, under the standard Word-RAM model with {$\Theta$}(n)-bit words.{$\bullet$}We provide an O(n{\textsurd}\{m\}) time Las Vegas randomized algorithm for this problem, beating the decades-old O(n{\textsurd}\{mm\}) running time [Abrahamson, SICOMP 1987]. We also obtain a deterministic algorithm, with a slightly higher O(n{\textsurd}\{m\}(mm)\textsuperscript{\{\vphantom\}}1/4\vphantom\{\}) running time. Our randomized algorithm extends to the k-bounded setting, with running time O(n+\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}k\vphantom\{\}\{{\textsurd}\{m\}\}), removing all the extra logarithmic factors from earlier algorithms [Gawrychowski and Uznanski, ICALP 2018; Chan, Golan, Kociumaka, Kopelowitz and Porat, STOC 2020].{$\bullet$}For the (1+{$\varepsilon$})-approximate version of Text-to-Pattern Hamming Distances, we give an O\vphantom\{\}({$\varepsilon$}\textsuperscript{\{\vphantom\}}-0.93\vphantom\{\}n) time Monte Carlo randomized algorithm (where O\vphantom\{\} hides poly-logarithmic factors), beating the previous O\vphantom\{\}({$\varepsilon$}\textsuperscript{\{\vphantom\}}-1\vphantom\{\}n) running time [Kopelowitz and Porat, FOCS 2015; Kopelowitz and Porat, SOSA 2018].Our approximation algorithm exploits a connection with 3SUM, and uses a combination of Fredman's trick, equality matrix product, and random sampling; in particular, we obtain new results on approximate counting versions of 3 SUM and Exact Triangle, which may be of independent interest. Our exact algorithms use a novel combination of hashing, bit-packed FFT, and recursion; in particular, we obtain a faster algorithm for computing the sumset of two integer sets, in the regime when the universe size is close to quadratic in the number of elements. We also prove a fine-grained equivalence between the exact Text-to-Pattern Hamming Distances problem and a range-restricted, counting version of 3 SUM.},
  citationcount = {6},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{chanFastPreprocessingFor2020,
  title = {Fast Preprocessing for Optimal Orthogonal Range Reporting and Range Successor with Applications to Text Indexing},
  author = {Chan, Timothy M. and He, Qizheng},
  year = {2020},
  doi = {10.4230/LIPIcs.ESA.2020.54},
  abstract = {Under the word RAM model, we design three data structures that can be constructed in O(n{\textsurd}\{n\}) time over n points in an n{\texttimes}n grid. The first data structure is an O(n\textsuperscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}n)-word structure supporting orthogonal range reporting in O(n+k) time, where k denotes output size and {$\epsilon$} is an arbitrarily small constant. The second is an O(nn)-word structure supporting orthogonal range successor in O(n) time, while the third is an O(n\textsuperscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}n)-word structure supporting sorted range reporting in O(n+k) time. The query times of these data structures are optimal when the space costs must be within O(npolylogn) words. Their exact space bounds match those of the best known results achieving the same query times, and the O(n{\textsurd}\{n\}) construction time beats the previous bounds on preprocessing. Previously, among 2d range search structures, only the orthogonal range counting structure of Chan and P{\v a}trascu (SODA 2010) and the linear space, O(\textsuperscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}n) query time structure for orthogonal range successor by Belazzougui and Puglisi (SODA 2016) can be built in the same O(n{\textsurd}\{n\}) time. Hence our work is the first that achieve the same preprocessing time for optimal orthogonal range reporting and range successor. We also apply our results to improve the construction time of text indexes.},
  citationcount = {14},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,query,query time}
}

@article{chanFindingTrianglesAnd2022,
  title = {Finding Triangles and Other Small Subgraphs in Geometric Intersection Graphs},
  author = {Chan, Timothy M.},
  year = {2022},
  doi = {10.48550/arXiv.2211.05345},
  abstract = {We consider problems related to finding short cycles, small cliques, small independent sets, and small subgraphs in geometric intersection graphs. We obtain a plethora of new results. For example: * For the intersection graph of n line segments in the plane, we give algorithms to find a 3-cycle in O(n\textsuperscript{\{\vphantom\}}1.408\vphantom\{\}) time, a size-3 independent set in O(n\textsuperscript{\{\vphantom\}}1.652\vphantom\{\}) time, a 4-clique in near-O(n\textsuperscript{\{\vphantom\}}24/13\vphantom\{\}) time, and a k-clique (or any k-vertex induced subgraph) in O(n\textsuperscript{\{\vphantom\}}0.565k+O(1)\vphantom\{\}) time for any constant k; we can also compute the girth in near-O(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}) time. * For the intersection graph of n axis-aligned boxes in a constant dimension d, we give algorithms to find a 3-cycle in O(n\textsuperscript{\{\vphantom\}}1.408\vphantom\{\}) time for any d, a 4-clique (or any 4-vertex induced subgraph) in O(n\textsuperscript{\{\vphantom\}}1.715\vphantom\{\}) time for any d, a size-4 independent set in near-O(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}) time for any d, a size-5 independent set in near-O(n\textsuperscript{\{\vphantom\}}4/3\vphantom\{\}) time for d=2, and a k-clique (or any k-vertex induced subgraph) in O(n\textsuperscript{\{\vphantom\}}0.429k+O(1)\vphantom\{\}) time for any d and any constant k. * For the intersection graph of n fat objects in any constant dimension d, we give an algorithm to find any k-vertex (non-induced) subgraph in O(nn) time for any constant k, generalizing a result by Kaplan, Klost, Mulzer, Roddity, Seiferth, and Sharir (1999) for 3-cycles in 2D disk graphs. A variety of techniques is used, including geometric range searching, biclique covers,"high-low"tricks, graph degeneracy and separators, and shifted quadtrees. We also prove a near-{\textohm}(n\textsuperscript{\{\vphantom\}}4/3\vphantom\{\}) conditional lower bound for finding a size-4 independent set for boxes.},
  citationcount = {13},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {lower bound}
}

@article{chanFredmanSTrick2023,
  title = {Fredman's Trick Meets Dominance Product: {{Fine-grained}} Complexity of Unweighted {{APSP}}, {{3SUM}} Counting, and More},
  author = {Chan, Timothy M. and Williams, V. V. and Xu, Yinzhan},
  year = {2023},
  doi = {10.1145/3564246.3585237},
  abstract = {In this paper we carefully combine Fredman's trick [SICOMP'76] and Matou{\v s}ek's approach for dominance product [IPL'91] to obtain powerful results in fine-grained complexity. Under the hypothesis that APSP for undirected graphs with edge weights in \{1, 2, {\dots}, n\} requires n3-o(1) time (when {$\omega$}=2), we show a variety of conditional lower bounds, including an n7/3-o(1) lower bound for unweighted directed APSP and an n2.2-o(1) lower bound for computing the Minimum Witness Product between two n {\texttimes} n Boolean matrices, even if {$\omega$}=2, improving upon their trivial n2 lower bounds. Our techniques can also be used to reduce the unweighted directed APSP problem to other problems. In particular, we show that (when {$\omega$} = 2), if unweighted directed APSP requires n2.5-o(1) time, then Minimum Witness Product requires n7/3-o(1) time. We show that, surprisingly, many central problems in fine-grained complexity are equivalent to their natural counting versions. In particular, we show that Min-Plus Product and Exact Triangle are subcubically equivalent to their counting versions, and 3SUM is subquadratically equivalent to its counting version. We also obtain new algorithms using new variants of the Balog-Szemer{\'e}di-Gowers theorem from additive combinatorics. For example, we get an O(n3.83) time deterministic algorithm for exactly counting the number of shortest paths in an arbitrary weighted graph, improving the textbook O(n4) time algorithm. We also get faster algorithms for 3SUM in preprocessed universes, and deterministic algorithms for 3SUM on monotone sets in \{1, 2, {\dots}, n\}d.},
  citationcount = {9},
  venue = {Symposium on the Theory of Computing},
  keywords = {lower bound}
}

@article{chanFurtherResultsOn2020,
  title = {Further Results on Colored Range Searching},
  author = {Chan, Timothy M. and He, Qizheng and Nekrich, Yakov},
  year = {2020},
  doi = {10.4230/LIPIcs.SoCG.2020.28},
  abstract = {We present a number of new results about range searching for colored (or "categorical") data: 1. For a set of n colored points in three dimensions, we describe randomized data structures with O(n\{polylog\}n) space that can report the distinct colors in any query orthogonal range (axis-aligned box) in O(k\{polyloglog\}n) expected time, where k is the number of distinct colors in the range, assuming that coordinates are in  1,{\dots},n . Previous data structures require O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}n\vphantom\{\}\{n\}+k) query time. Our result also implies improvements in higher constant dimensions. 2. Our data structures can be adapted to halfspace ranges in three dimensions (or circular ranges in two dimensions), achieving O(kn) expected query time. Previous data structures require O(k{$^2$}n) query time. 3. For a set of n colored points in two dimensions, we describe a data structure with O(n\{polylog\}n) space that can answer colored "type-2" range counting queries: report the number of occurrences of every distinct color in a query orthogonal range. The query time is O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}n\vphantom\{\}\{n\}+kn), where k is the number of distinct colors in the range. Naively performing k uncolored range counting queries would require O(k\textsuperscript{\{\vphantom\}}{\textfractionsolidus}n\vphantom\{\}\{n\}) time. Our data structures are designed using a variety of techniques, including colored variants of randomized incremental construction (which may be of independent interest), colored variants of shallow cuttings, and bit-packing tricks.},
  citationcount = {10},
  venue = {International Symposium on Computational Geometry},
  keywords = {data structure,query,query time}
}

@article{chanHardnessForTriangle2022,
  title = {Hardness for Triangle Problems under Even More Believable Hypotheses: Reductions from Real {{APSP}}, Real {{3SUM}}, and {{OV}}},
  author = {Chan, Timothy M. and Williams, V. V. and Xu, Yinzhan},
  year = {2022},
  doi = {10.1145/3519935.3520032},
  abstract = {The 3SUM hypothesis, the All-Pairs Shortest Paths (APSP) hypothesis and the Strong Exponential Time Hypothesis are the three main hypotheses in the area of fine-grained complexity. So far, within the area, the first two hypotheses have mainly been about integer inputs in the Word RAM model of computation. The ``Real APSP'' and ``Real 3SUM'' hypotheses, which assert that the APSP and 3SUM hypotheses hold for real-valued inputs in a reasonable version of the Real RAM model, are even more believable than their integer counterparts. Under the very believable hypothesis that at least one of the Integer 3SUM hypothesis, Integer APSP hypothesis or SETH is true, Abboud, Vassilevska W. and Yu [STOC 2015] showed that a problem called Triangle Collection requires n3-o(1) time on an n-node graph. The main result of this paper is a nontrivial lower bound for a slight generalization of Triangle Collection, called All-Color-Pairs Triangle Collection, under the even more believable hypothesis that at least one of the Real 3SUM, the Real APSP, and the Orthogonal Vector (OV) hypotheses is true. Combined with slight modifications of prior reductions from Triangle Collection, we obtain polynomial conditional lower bounds for problems such as the (static) ST-Max Flow problem and dynamic versions of Max Flow, Single-Source Reachability Count, and Counting Strongly Connected Components, now under the new weaker hypothesis. Our main result is built on the following two lines of reductions. In the first line of reductions, we show Real APSP and Real 3SUM hardness for the All-Edges Sparse Triangle problem. Prior reductions only worked from the integer variants of these problems. In the second line of reductions, we show Real APSP and OV hardness for a variant of the Boolean Matrix Multiplication problem. Along the way we show that Triangle Collection is equivalent to a simpler restricted version of the problem, simplifying prior work. Our techniques also have other interesting implications, such as a super-linear lower bound of Integer All-Numbers 3SUM based on the Real 3SUM hypothesis, and a tight lower bound for a string matching problem based on the OV hypothesis.},
  citationcount = {11},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,lower bound,reduction,static}
}

@article{chanLinearSpaceData2012,
  title = {Linear-Space Data Structures for Range Minority Query in Arrays},
  author = {Chan, Timothy M. and Durocher, Stephane and Skala, M. and Wilkinson, Bryan T.},
  year = {2012},
  doi = {10.1007/s00453-014-9881-9},
  abstract = {No abstract available},
  citationcount = {26},
  venue = {Algorithmica},
  keywords = {data structure,query}
}

@article{chanLinearSpaceDataStructures2014,
  title = {Linear-{{Space Data Structures}} for {{Range Mode Query}} in {{Arrays}}},
  author = {Chan, Timothy M. and Durocher, Stephane and Larsen, Kasper Green and Morrison, Jason and Wilkinson, Bryan T.},
  year = {2014},
  month = nov,
  journal = {Theory Comput Syst},
  volume = {55},
  number = {4},
  pages = {719--741},
  issn = {1433-0490},
  doi = {10.1007/s00224-013-9455-2},
  url = {https://doi.org/10.1007/s00224-013-9455-2},
  urldate = {2024-11-19},
  abstract = {A mode of a multiset S is an element a{$\in$}S of maximum multiplicity; that is, a occurs at least as frequently as any other element in S. Given an array A[1:n] of n elements, we consider a basic problem: constructing a static data structure that efficiently answers range mode queries on A. Each query consists of an input pair of indices (i,j) for which a mode of A[i:j] must be returned. The best previous data structure with linear space, by Krizanc, Morin, and Smid (Proceedings of the International Symposium on Algorithms and Computation (ISAAC), pp.~517--526, 2003), requires \${\textbackslash}varTheta ({\textbackslash}sqrt\{n\}{\textbackslash}log{\textbackslash}log n)\$query time in the worst case. We improve their result and present an O(n)-space data structure that supports range mode queries in \$O({\textbackslash}sqrt\{n/{\textbackslash}log n\})\$worst-case time. In the external memory model, we give a linear-space data structure that requires \$O({\textbackslash}sqrt\{n/B\})\$I/Os per query, where B denotes the block size. Furthermore, we present strong evidence that a query time significantly below \${\textbackslash}sqrt\{n\}\$cannot be achieved by purely combinatorial techniques; we show that boolean matrix multiplication of two \${\textbackslash}sqrt\{n\} {\textbackslash}times {\textbackslash}sqrt\{n\}\$matrices reduces to n range mode queries in an array of size O(n).},
  langid = {english},
  keywords = {Array,data structure,Linear space,Mode,query,query time,Range query,static},
  annotation = {Agarwal, P.K.: Range searching. In: Goodman, J., O'Rourke, J. (eds.) Handbook of Discrete and Computational Geometry, 2nd edn., pp. 809--837. CRC Press, New York (2004)\\
Alon, N., Schieber, B.: Optimal preprocessing for answering on-line product queries. Technical report 71/87, Tel-Aviv University (1987)\\
Amir, A., Fischer, J., Lewenstein, M.: Two-dimensional range minimum queries. In: Proceedings of the Symposium on Combinatorial Pattern Matching (CPM). Lecture Notes in Computer Science, vol. 4580, pp. 286--294. Springer, Berlin (2007)\\
Bansal, N., Williams, R.: Regularity lemmas and combinatorial algorithms. In: Proceedings of the IEEE Symposium on Foundations of Computer Science (FOCS), pp. 745--754 (2009)\\
Bender, M.A., Farach-Colton, M.: The LCA problem revisited. In: Proceedings of the Latin American Theoretical Informatics Symposium (LATIN). Lecture Notes in Computer Science, vol. 1776, pp. 88--94. Springer, Berlin (2000)\\
Berkman, O., Breslauer, D., Galil, Z., Schieber, B., Vishkin, U.: Highly parallelizable problems. In: Proceedings of the ACM Symposium on the Theory of Computing (STOC), pp. 309--319 (1989)\\
Bose, P., Kranakis, E., Morin, P., Tang, Y.: Approximate range mode and range median queries. In: Proceedings of the International Symposium on Theoretical Aspects of Computer Science (STACS). Lecture Notes in Computer Science, vol. 3404, pp. 377--388. Springer, Berlin (2005)\\
Brodal, G.S., J{\o}rgensen, A.G.: Data structures for range median queries. In: Proceedings of the International Symposium on Algorithms and Computation (ISAAC). Lecture Notes in Computer Science, vol. 5878, pp. 822--831. Springer, Berlin (2009)\\
Brodal, G.S., Davoodi, P., Rao, S.S.: On space efficient two dimensional range minimum data structures. In: Proceedings of the European Symposium on Algorithms (ESA). Lecture Notes in Computer Science., vol. 6346/6347. Springer, Berlin (2010)\\
Brodal, G.S., Davoodi, P., Rao, S.S.: Path minima queries in dynamic weighted trees. In: Proceedings of the Workshop on Algorithms and Data Structures (WADS). Lecture Notes in Computer Science, vol. 6844, pp. 290--301. Springer, Berlin (2011)\\
Brodal, G.S., Gfeller, B., J{\o}rgensen, A.G., Sanders, P.: Towards optimal range medians. Theor. Comput. Sci. 412(24), 2588--2601 (2011)\\
Brodal, G.S., Davoodi, P., Lewenstein, M., Raman, R., Rao, S.S.: Two dimensional range minimum queries and Fibonacci lattices. In: Proceedings of the European Symposium on Algorithms (ESA). Lecture Notes in Computer Science, vol. 7501, pp. 217--228. Springer, Berlin (2012)\\
Chan, T.M.: Optimal partition trees. Discrete Comput. Geom. 47(4), 661--690 (2012)\\
Chan, T.M., P{\u a}tra{\c s}cu, M.: Counting inversions, offline orthogonal range counting, and related problems. In: Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 161--173 (2010)\\
Chan, T.M., Durocher, S., Larsen, K.G., Morrison, J., Wilkinson, B.T.: Linear-space data structures for range mode query in arrays. In: Proceedings of the International Symposium on Theoretical Aspects of Computer Science (STACS). Leibniz International Proceedings in Informatics, vol. 14, pp. 291--301 (2012)\\
Chan, T.M., Durocher, S., Skala, M., Wilkinson, B.T.: Linear-space data structures for range minority query in arrays. In: Proceedings of the Scandinavian Symposium and Workshops on Algorithm Theory (SWAT). Lecture Notes in Computer Science, vol. 7357, pp. 295--306. Springer, Berlin (2012)\\
Chazelle, B.: Cutting hyperplanes for divide-and-conquer. Discrete Comput. Geom. 9(2), 145--158 (1993)\\
Chazelle, B.: Cuttings. In: Handbook of Data Structures and Applications, pp. 25.1--25.10. CRC Press, Boca Raton (2005)\\
Chazelle, B., Rosenberg, B.: Computing partial sums in multidimensional arrays. In: Proceedings of the ACM Symposium on Computational Geometry (SoCG), pp. 131--139 (1989)\\
Davoodi, P.: Data structures: range queries and space efficiency. Ph.D. thesis, Aarhus University (2011)\\
Davoodi, P., Raman, R., Satti, S.R.: Succinct representations of binary trees for range minimum queries. In: Proceedings of the International Computing and Combinatorics Conference (COCOON). Lecture Notes in Computer Science, vol. 7434, pp. 396--407. Springer, Berlin (2012)\\
de Berg, M., Cheong, O., van Kreveld, M., Overmars, M.: Computational Geometry: Algorithms and Applications, 3rd edn. Springer, Berlin (2008)\\
Demaine, E.D., Landau, G.M., Weimann, O.: On Cartesian trees and range minimum queries. In: Proceedings of the International Colloquium on Automata, Languages, and Programming (ICALP). Lecture Notes in Computer Science, vol. 5555, pp. 341--353. Springer, Berlin (2009)\\
Dietz, P.F.: Optimal algorithms for list indexing and subset rank. In: Proceedings of the Workshop on Algorithms and Data Structures (WADS). Lecture Notes in Computer Science, vol. 382, pp. 39--46. Springer, Berlin (1989)\\
Dobkin, D., Munro, J.I.: Determining the mode. Theor. Comput. Sci. 12(3), 255--263 (1980)\\
Durocher, S.: A simple linear-space data structure for constant-time range minimum query. CoRR (2011). arXiv:1109.4460\\
Durocher, S., He, M., Munro, J.I., Nicholson, P.K., Skala, M.: Range majority in constant time and linear space. In: Proceedings of the International Colloquium on Automata, Languages, and Programming (ICALP). Lecture Notes in Computer Science, vol. 6755, pp. 244--255. Springer, Berlin (2011)\\
Durocher, S., He, M., Munro, J.I., Nicholson, P.K., Skala, M.: Range majority in constant time and linear space. Inf. Comput. 222, 169--179 (2013)\\
Elmasry, A., He, M., Munro, J.I., Nicholson, P.: Dynamic range majority data structures. In: Proceedings of the International Symposium on Algorithms and Computation (ISAAC). Lecture Notes in Computer Science, vol. 7074, pp. 150--159. Springer, Berlin (2011)\\
Fischer, J.: Optimal succinctness for range minimum queries. In: Proceedings of the Latin American Theoretical Informatics Symposium (LATIN). Lecture Notes in Computer Science, vol. 6034, pp. 158--169. Springer, Berlin (2010)\\
Fischer, J., Heun, V.: Theoretical and practical improvements on the RMQ-problem, with applications to LCA and LCE. In: Proceedings of the Symposium on Combinatorial Pattern Matching (CPM). Lecture Notes in Computer Science, vol. 4009, pp. 36--48. Springer, Berlin (2006)\\
Fischer, J., Heun, V.: A new succinct representation of RMQ-information and improvements in the enhanced suffix array. In: Proceedings of the International Symposium on Combinatorics, Algorithms, Probabilistic and Experimental Methodologies (ESCAPE). Lecture Notes in Computer Science, vol. 4614, pp. 459--470. Springer, Berlin (2007)\\
Fischer, J., Heun, V.: Finding range minima in the middle: approximations and applications. Math. Comput. Sci. 3(1), 17--30 (2010)\\
Gabow, H.N., Bentley, J.L., Tarjan, R.E.: Scaling and related techniques for geometry problems. In: Proceedings of the ACM Symposium on the Theory of Computing (STOC), pp. 135--143 (1984)\\
Gagie, T., Puglisi, S.J., Turpin, A.: Range quantile queries: another virtue of wavelet trees. In: Proceedings of the String Processing and Information Retrieval Symposium (SPIRE). Lecture Notes in Computer Science, vol. 5721, pp. 1--6. Springer, Berlin (2009)\\
Gagie, T., He, M., Munro, J.I., Nicholson, P.: Finding frequent elements in compressed 2D arrays and strings. In: Proceedings of the Symposium on String Processing and Information Retrieval (SPIRE). Lecture Notes in Computer Science, vol. 7024, pp. 295--300. Springer, Berlin (2011)\\
Gfeller, B., Sanders, P.: Towards optimal range medians. In: Proceedings of the International Colloquium on Automata, Languages, and Programming (ICALP). Lecture Notes in Computer Science, vol. 5555, pp. 475--486. Springer, Berlin (2009)\\
Golin, M.J., Iacono, J., Krizanc, D., Raman, R., Rao, S.S.: Encoding 2D range maximum queries. In: Proceedings of the International Symposium on Algorithms and Computation (ISAAC). Lecture Notes in Computer Science, vol. 7074, pp. 180--189. Springer, Berlin (2011)\\
Greve, M., J{\o}rgensen, A.G., Larsen, K.D., Truelsen, J.: Cell probe lower bounds and approximations for range mode. In: Proceedings of the International Colloquium on Automata, Languages, and Programming (ICALP). Lecture Notes in Computer Science, vol. 6198, pp. 605--616. Springer, Berlin (2010)\\
Har-Peled, S., Muthukrishnan, S.: Range medians. In: Proceedings of the European Symposium on Algorithms (ESA). Lecture Notes in Computer Science, vol. 5193, pp. 503--514. Springer, Berlin (2008)\\
He, M., Munro, J.I., Nicholson, P.: Dynamic range selection in linear space. In: Proceedings of the International Symposium on Algorithms and Computation (ISAAC). Lecture Notes in Computer Science, vol. 7074, pp. 160--169. Springer, Berlin (2011)\\
J{\'a}J{\'a}, J., Mortensen, C.W., Shi, Q.: Space-efficient and fast algorithms for multidimensional dominance reporting and counting. In: Proceedings of the International Symposium on Algorithms and Computation (ISAAC). Lecture Notes in Computer Science, vol. 3341, pp. 558--568. Springer, Berlin (2004)\\
J{\o}rgensen, A.G.: Data structures: sequence problems, range queries, and fault tolerance. Ph.D. thesis, Aarhus University (2010)\\
J{\o}rgensen, A.G., Larsen, K.D.: Range selection and median: tight cell probe lower bounds and adaptive data structures. In: Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 805--813 (2011)\\
Krizanc, D., Morin, P., Smid, M.: Range mode and range median queries on lists and trees. In: Proceedings of the International Symposium on Algorithms and Computation (ISAAC). Lecture Notes in Computer Science, vol. 2906, pp. 517--526. Springer, Berlin (2003)\\
Krizanc, D., Morin, P., Smid, M.: Range mode and range median queries on lists and trees. Nord. J. Comput. 12, 1--17 (2005)\\
Larsen, K.G.: The cell probe complexity of dynamic range counting. In: Proceedings of the ACM Symposium on the Theory of Computing (STOC), pp. 85--94 (2012)\\
Matou{\v s}ek, J.: Range searching with efficient hierarchical cuttings. Discrete Comput. Geom. 10(2), 157--182 (1993)\\
Munro, J.I.: Tables. In: Chandru, V., Vinay, V. (eds.) Foundations of Software Technology and Theoretical Computer Science. Lecture Notes in Computer Science, vol. 1180, pp. 37--42. Springer, Berlin (1996)\\
Munro, J.I., Spira, M.: Sorting and searching in multisets. SIAM J. Comput. 5(1), 1--8 (1976)\\
Nekrich, Y.: Orthogonal range searching in linear and almost-linear space. In: Proceedings of the Workshop on Algorithms and Data Structures (WADS). Lecture Notes in Computer Science, vol. 4619, pp. 15--26. Springer, Berlin (2007)\\
P{\v a}tra{\c s}cu, M.: Towards polynomial lower bounds for dynamic problems. In: Proceedings of the ACM Symposium on the Theory of Computing (STOC), pp. 603--610 (2010)\\
Petersen, H.: Improved bounds for range mode and range median queries. In: Proceedings of the Conference on Current Trends in Theory and Practice of Computer Science (SOFSEM). Lecture Notes in Computer Science, vol. 4910, pp. 418--423. Springer, Berlin (2008)\\
Petersen, H., Grabowski, S.: Range mode and range median queries in constant time and sub-quadratic space. Inf. Process. Lett. 109, 225--228 (2009)\\
Poon, C.K.: Optimal range max datacub for fixed dimensions. In: Proceedings of the International Conference on Database Theory (ICDT). Lecture Notes in Computer Science, vol. 2572, pp. 158--172. Springer, Berlin (2003)\\
Sadakane, K.: Succinct data structures for flexible text retrieval systems. J. Discrete Algorithms 5, 12--22 (2007)\\
Skiena, S.: The Algorithm Design Manual, 2nd edn. Springer, Berlin (2008)\\
van Emde Boas, P.: Preserving order in a forest in less than logarithmic time and linear space. Inf. Process. Lett. 6(3), 80--82 (1977)\\
Vassilevska Williams, V.: Multiplying matrices faster than Coppersmith-Winograd. In: Proceedings of the ACM Symposium on the Theory of Computing (STOC), pp. 887--898 (2012)\\
Yao, A.C.: Space-time tradeoff for answering range queries. In: Proceedings of the ACM Symposium on the Theory of Computing (STOC), pp. 128--136 (1982)\\
Yao, A.C.: On the complexity of maintaining partial sums. SIAM J. Comput. 14, 277--288 (1985)\\
Yuan, H., Atallah, M.J.: Data structures for range minimum queries. In: Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 150--160 (2010)},
  file = {/Users/tulasi/Zotero/storage/67E3U3HD/Chan et al. - 2014 - Linear-Space Data Structures for Range Mode Query in Arrays.pdf;/Users/tulasi/Zotero/storage/689EDQNU/Chan et al. - 2014 - Linear-Space Data Structures for Range Mode Query in Arrays.pdf}
}

@article{chanMoreLogarithmicFactor2018,
  title = {More Logarithmic-Factor Speedups for {{3SUM}}, (Median,+)-Convolution, and Some Geometric {{3SUM-hard}} Problems},
  author = {Chan, Timothy M.},
  year = {2018},
  doi = {10.1145/3363541},
  abstract = {This article presents an algorithm that solves the 3SUM problem for n real numbers in O((n2/ log2n)(log log n)O(1)) time, improving previous solutions by about a logarithmic factor. Our framework for shaving off two logarithmic factors can be applied to other problems, such as (median,+)-convolution/matrix multiplication and algebraic generalizations of 3SUM. This work also obtains the first subquadratic results on some 3SUM-hard problems in computational geometry, for example, deciding whether (the interiors of) a constant number of simple polygons have a common intersection.},
  citationcount = {49},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{chanOrthogonalPointLocation2018,
  title = {Orthogonal Point Location and Rectangle Stabbing Queries in 3-d},
  author = {Chan, Timothy M. and Nekrich, Yakov and Rahul, S. and Tsakalidis, Konstantinos},
  year = {2018},
  doi = {10.4230/LIPIcs.ICALP.2018.31},
  abstract = {In this work, we present a collection of new results on two fundamental problems in geometric data structures: orthogonal point location and rectangle stabbing. -We give the first linear-space data structure that supports 3-d point location queries on n disjoint axis-aligned boxes with optimal O(n) query time in the (arithmetic) pointer machine model. This improves the previous O(\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}n) bound of Rahul [SODA 2015]. We similarly obtain the first linear-space data structure in the I/O model with optimal query cost, and also the first linear-space data structure in the word RAM model with sub-logarithmic query time. -We give the first linear-space data structure that supports 3-d 4-sided and 5-sided rectangle stabbing queries in optimal O(\textsubscript{w}n+k) time in the word RAM model. We similarly obtain the first optimal data structure for the closely related problem of 2-d top-k rectangle stabbing in the word RAM model, and also improved results for 3-d 6-sided rectangle stabbing. For point location, our solution is simpler than previous methods, and is based on an interesting variant of the van Emde Boas recursion, applied in a round-robin fashion over the dimensions, combined with bit-packing techniques. For rectangle stabbing, our solution is a variant of Alstrup, Brodal, and Rauhe's grid-based recursive technique (FOCS 2000), combined with a number of new ideas.},
  citationcount = {20},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,query,query time}
}

@article{chanOrthogonalRangeReporting2019,
  title = {Orthogonal Range Reporting and Rectangle Stabbing for Fat Rectangles},
  author = {Chan, Timothy M. and Nekrich, Yakov and Smid, M.},
  year = {2019},
  doi = {10.1007/978-3-030-24766-9_21},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Workshop on Algorithms and Data Structures}
}

@article{chanOrthogonalRangeSearching2011,
  title = {Orthogonal Range Searching on the {{RAM}}, Revisited},
  author = {Chan, Timothy M. and Larsen, Kasper Green and Patrascu, M.},
  year = {2011},
  doi = {10.1145/1998196.1998198},
  abstract = {We present a number of new results on one of the most extensively studied topics in computational geometry, orthogonal range searching. All our results are in the standard word RAM model: We present two data structures for 2-d orthogonal range emptiness. The first achieves O(n lg lg n) space and O(lg lg n) query time, assuming that the n given points are in rank space. This improves the previous results by Alstrup, Brodal, and Rauhe (FOCS'00), with O(n lg{$\varepsilon$} n) space and O(lg lg n) query time, or with O(n lg lg n) space and O(lg2lg n) query time. Our second data structure uses O(n) space and answers queries in O(lg{$\varepsilon$} n) time. The best previous O(n)-space data structure, due to Nekrich (WADS'07), answers queries in O(lg n/lg lg n) time. We give a data structure for 3-d orthogonal range reporting with O(n lg1+{$\varepsilon$} n) space and O(lg lg n + k) query time for points in rank space, for any constant {$\varepsilon$}{\textquestiondown}0. This improves the previous results by Afshani (ESA'08), Karpinski and Nekrich (COCOON'09), and Chan (SODA'11), with O(n lg3 n) space and O(lg lg n + k) query time, or with O(n lg1+{$\varepsilon$}n) space and O(lg2lg n + k) query time. Consequently, we obtain improved upper bounds for orthogonal range reporting in all constant dimensions above 3. Our approach also leads to a new data structure for 2D orthogonal range minimum queries with O(n lg{$\varepsilon$} n) space and O(lg lg n) query time for points in rank space. We give a randomized algorithm for 4-d offline dominance range reporting/emptiness with running time O(n log n) plus the output size. This resolves two open problems (both appeared in Preparata and Shamos' seminal book): given a set of n axis-aligned rectangles in the plane, we can report all k enclosure pairs (i.e., pairs (r1,r2) where rectangle r1 completely encloses rectangle r2) in O(n lg n + k) expected time; given a set of n points in 4-d, we can find all maximal points (points not dominated by any other points) in O(n lg n) expected time. The most recent previous development on (a) was reported back in SoCG'95 by Gupta, Janardan, Smid, and Dasgupta, whose main result was an O([n lg n + k] lg lg n) algorithm. The best previous result on (b) was an O(n lg n lg lg n) algorithm due to Gabow, Bentley, and Tarjan---from STOC'84! As a consequence, we also obtain the current-record time bound for the maxima problem in all constant dimensions above 4.},
  citationcount = {204},
  venue = {International Symposium on Computational Geometry},
  keywords = {data structure,query,query time}
}

@article{chanOrthogonalRangeSearching2017,
  title = {Orthogonal Range Searching in Moderate Dimensions: K-d Trees and Range Trees Strike Back},
  author = {Chan, Timothy M.},
  year = {2017},
  doi = {10.1007/s00454-019-00062-5},
  abstract = {No abstract available},
  citationcount = {19},
  venue = {Discrete \& Computational Geometry}
}

@article{chanPersistentPredecessorSearch2011,
  title = {Persistent Predecessor Search and Orthogonal Point Location on the Word {{RAM}}},
  author = {Chan, Timothy M.},
  year = {2011},
  doi = {10.1145/2483699.2483702},
  abstract = {We answer a basic data structuring question (for example, raised by Dietz and Raman back in SODA 1991): can van Emde Boas trees be made persistent, without changing their asymptotic query/update time? We present a (partially) persistent data structure that supports predecessor search in a set of integers in \{1,...,U\} under an arbitrary sequence of n insertions and deletions, with O(log log U) expected query time and expected amortized update time, and O(n) space. The query bound is optimal in U for linear-space structures and improves previous near-O((log log U)2) methods. The same method solves a fundamental problem from computational geometry: point location in orthogonal planar subdivisions (where edges are vertical or horizontal). We obtain the first static data structure achieving O(log log U) worst-case query time and linear space. This result is again optimal in U for linear-space structures and improves the previous O((log log U)2) method by de Berg, Snoeyink, and van Kreveld (1992). The same result also holds for higher-dimensional subdivisions that are orthogonal binary space partitions, and for certain nonorthogonal planar subdivisions such as triangulations without small angles. Many geometric applications follow, including improved query times for orthogonal range reporting for dimensions {$\geq$} 3 on the RAM. Our key technique is an interesting new van-Emde-Boas-style recursion that alternates between two strategies, both quite simple.},
  citationcount = {55},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,query,query time,static,update,update time}
}

@article{chanReducing3sumTo2020,
  title = {Reducing {{3SUM}} to Convolution-{{3SUM}}},
  author = {Chan, Timothy M. and He, Qizheng},
  year = {2020},
  doi = {10.1137/1.9781611976014.1},
  abstract = {Given a set S of n numbers, the 3 SUM problem asks to determine whether there exist three elements a, b, c {$\in$} S such that a + b + c = 0. The related Convolution -3 SUM problem asks to determine whether there exist a pair of indices i, j such that A [ i ] + A [ j ] = A [ i + j ], where A is a given array of n numbers. When the numbers are integers, a randomized reduction from 3 SUM to Convolution -3 SUM was given in a seminal paper by P{\textasciicaron}atra{\c}scu [STOC 2010], which was later improved by Kopelowitz, Pettie, and Porat [SODA 2016] with an O (log n ) factor slowdown. In this paper, we present a simple deterministic reduction from 3 SUM to Convolution -3 SUM for integers bounded by U . We also describe additional ideas to obtaining further improved reductions, with only a (log log n ) O (1) factor slowdown in the randomized case, and a log O (1) U factor slowdown in the deterministic case.},
  citationcount = {21},
  venue = {SIAM Symposium on Simplicity in Algorithms},
  keywords = {reduction}
}

@article{chanSimplerReductionsFrom2023,
  title = {Simpler Reductions from Exact Triangle},
  author = {Chan, Timothy M. and Xu, Yinzhan},
  year = {2023},
  doi = {10.48550/arXiv.2310.11575},
  abstract = {In this paper, we provide simpler reductions from Exact Triangle to two important problems in fine-grained complexity: Exact Triangle with Few Zero-Weight 4-Cycles and All-Edges Sparse Triangle. Exact Triangle instances with few zero-weight 4-cycles was considered by Jin and Xu [STOC 2023], who used it as an intermediate problem to show 3SUM hardness of All-Edges Sparse Triangle with few 4-cycles (independently obtained by Abboud, Bringmann and Fischer [STOC 2023]), which is further used to show 3SUM hardness of a variety of problems, including 4-Cycle Enumeration, Offline Approximate Distance Oracle, Dynamic Approximate Shortest Paths and All-Nodes Shortest Cycles. We provide a simple reduction from Exact Triangle to Exact Triangle with few zero-weight 4-cycles. Our new reduction not only simplifies Jin and Xu's previous reduction, but also strengthens the conditional lower bounds from being under the 3SUM hypothesis to the even more believable Exact Triangle hypothesis. As a result, all conditional lower bounds shown by Jin and Xu [STOC 2023] and by Abboud, Bringmann and Fischer [STOC 2023] using All-Edges Sparse Triangle with few 4-cycles as an intermediate problem now also hold under the Exact Triangle hypothesis. We also provide two alternative proofs of the conditional lower bound of the All-Edges Sparse Triangle problem under the Exact Triangle hypothesis, which was originally proved by Vassilevska Williams and Xu [FOCS 2020]. Both of our new reductions are simpler, and one of them is also deterministic -- all previous reductions from Exact Triangle or 3SUM to All-Edges Sparse Triangle (including P{\u \{}a\vphantom\{\}tra{\c \{}s\vphantom\{\}cu's seminal work [STOC 2010]) were randomized.},
  citationcount = {2},
  venue = {SIAM Symposium on Simplicity in Algorithms},
  keywords = {dynamic,lower bound,reduction}
}

@article{chanSmallestKEnclosing2019,
  title = {Smallest K-{{Enclosing}} Rectangle Revisited},
  author = {Chan, Timothy M. and {Har-Peled}, Sariel},
  year = {2019},
  doi = {10.1007/s00454-020-00239-3},
  abstract = {No abstract available},
  citationcount = {13},
  venue = {Discrete \& Computational Geometry}
}

@article{chanTransdichotomousResultsIn2009,
  title = {Transdichotomous Results in Computational Geometry, {{I}}: {{Point}} Location in Sublogarithmic Time},
  author = {Chan, Timothy M. and Patrascu, M.},
  year = {2009},
  doi = {10.1137/07068669X},
  abstract = {Given a planar subdivision whose coordinates are integers bounded by U{$\leq$}2\textsuperscript{w}, we present a linear-space data structure that can answer point-location queries in O( n/n,{\textsurd}\{U/U\} ) time on the unit-cost random access machine (RAM) with word size w. This is the first result to beat the standard {$\Theta$}(n) bound for infinite precision models. As a consequence, we obtain the first o(nn) (randomized) algorithms for many fundamental problems in computational geometry for arbitrary integer input on the word RAM, including: constructing the convex hull of a three-dimensional (3D) point set, computing the Voronoi diagram or the Euclidean minimum spanning tree of a planar point set, triangulating a polygon with holes, and finding intersections among a set of line segments. Higher-dimensional extensions and applications are also discussed. Though computational geometry with bounded precision input has been investigated for a long time, improvements have been limited largely to problems of an orthogonal flavor. Our results surpass this long-standing limitation, answering, for example, a question of Willard (SODA'92).},
  citationcount = {46},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,query}
}

@article{chanVoronoiDiagramsIn2007,
  title = {Voronoi Diagrams in n {$\cdot$} 2o({\textsurd}lg Lg n) Time},
  author = {Chan, Timothy M. and Patrascu, M.},
  year = {2007},
  doi = {10.1145/1250790.1250796},
  abstract = {We reexamine fundamental problems from computational geometry in theallword RAM model, where input coordinates are integers that fit in a machine word. We develop a new algorithm for offline point location, a two-dimensional analog of sorting where one needs to order points with respect to segments. This result implies, for example, that the Voronoi diagram of n points in the plane can be constructed in (randomized) time n{$\cdot$} 2O({\textsurd} lg lg n). Similar bounds hold for numerous other geometric problems, such as three-dimensional convex hulls, planar Euclidean minimum spanning trees, line segment intersection, and triangulation of non-simple polygons. In FOCS'06, we developed a data structure for online point location, which implied a bound of O(n (lg n)/(lg lg n) for Voronoi diagrams and the other problems. Our current bounds are dramatically better, and a convincing improvement over the classic O(n lg n) algorithms. As in the field of integer sorting, the main challenge is to find ways to manipulate information, while avoiding the online problem (in that case, predecessor search).},
  citationcount = {17},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure}
}

@article{charalampopoulosAlmostOptimalExact2023,
  title = {Almost Optimal Exact Distance Oracles for Planar Graphs},
  author = {Charalampopoulos, P. and Gawrychowski, Pawe{\l} and Long, Yaowei and Mozes, S. and Pettie, Seth and Weimann, Oren and {Wulff-Nilsen}, Christian},
  year = {2023},
  doi = {10.1145/3580474},
  abstract = {We consider the problem of preprocessing a weighted directed planar graph in order to quickly answer exact distance queries. The main tension in this problem is between space S and query time Q, and since the mid-1990s all results had polynomial time-space tradeoffs, e.g., Q =   {$\Theta$}(n/{\textsurd} S) or Q =  {$\Theta$}(n5/2/S3/2). In this article we show that there is no polynomial tradeoff between time and space and that it is possible to simultaneously achieve almost optimal space n1+o(1) and almost optimal query time no(1). More precisely, we achieve the following space-time tradeoffs: n1+o(1) space and log2+o(1) n query time, n log2+o(1) n space and no(1) query time, n4/3+o(1) space and log1+o(1) n query time. We reduce a distance query to a variety of point location problems in additively weighted Voronoi diagrams and develop new algorithms for the point location problem itself using several partially persistent dynamic tree data structures.},
  citationcount = {5},
  venue = {Journal of the ACM},
  keywords = {data structure,dynamic,query,query time,time-space}
}

@article{charalampopoulosCircularPatternMatching2019,
  title = {Circular Pattern Matching with k Mismatches},
  author = {Charalampopoulos, P. and Kociumaka, Tomasz and Pissis, S. and Radoszewski, J. and Rytter, W. and Straszynski, Juliusz and Wale{\'n}, Tomasz and Zuba, Wiktor},
  year = {2019},
  doi = {10.1007/978-3-030-25027-0_15},
  abstract = {No abstract available},
  citationcount = {11},
  venue = {International Symposium on Fundamentals of Computation Theory}
}

@article{charalampopoulosCountingDistinctPatterns2020,
  title = {Counting Distinct Patterns in Internal Dictionary Matching},
  author = {Charalampopoulos, P. and Kociumaka, Tomasz and Mohamed, M. and Radoszewski, J. and Rytter, W. and Straszy'nski, Juliusz and Wale'n, Tomasz and Zuba, Wiktor},
  year = {2020},
  doi = {10.4230/LIPIcs.CPM.2020.8},
  abstract = {We consider the problem of preprocessing a text T of length n and a dictionary \{D\} in order to be able to efficiently answer queries CountDistinct(i,j), that is, given i and j return the number of patterns from \{D\} that occur in the fragment T[i\{.\,.\}j]. The dictionary is internal in the sense that each pattern in \{D\} is given as a fragment of T. This way, the dictionary takes space proportional to the number of patterns d={\textbar}\{D\}{\textbar} rather than their total length, which could be {$\Theta$}(n{$\cdot$}d). An \{O\}\vphantom\{\}(n+d)-size data structure that answers CountDistinct(i,j) queries \{O\}(n)-approximately in \{O\}\vphantom\{\}(1) time was recently proposed in a work that introduced internal dictionary matching [ISAAC 2019]. Here we present an \{O\}\vphantom\{\}(n+d)-size data structure that answers CountDistinct(i,j) queries 2-approximately in \{O\}\vphantom\{\}(1) time. Using range queries, for any m, we give an \{O\}\vphantom\{\}((nd/m,n{$^2$}/m{$^2$})+d)-size data structure that answers CountDistinct(i,j) queries exactly in \{O\}\vphantom\{\}(m) time. We also consider the special case when the dictionary consists of all square factors of the string. We design an \{O\}(n{$^2$}n)-size data structure that allows us to count distinct squares in a text fragment T[i\{.\,.\}j] in \{O\}(n) time.},
  citationcount = {11},
  venue = {Annual Symposium on Combinatorial Pattern Matching},
  keywords = {data structure,query}
}

@article{charalampopoulosDynamicLongestCommon2020,
  title = {Dynamic Longest Common Substring in Polylogarithmic Time},
  author = {Charalampopoulos, P. and Gawrychowski, Pawe{\l} and Pokorski, Karol},
  year = {2020},
  doi = {10.4230/LIPIcs.ICALP.2020.27},
  abstract = {The longest common substring problem consists in finding a longest string that appears as a (contiguous) substring of two input strings. We consider the dynamic variant of this problem, in which we are to maintain two dynamic strings S and T, each of length at most n, that undergo substitutions of letters, in order to be able to return a longest common substring after each substitution. Recently, Amir et al. [ESA 2019] presented a solution for this problem that needs only \{O\}\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}) time per update. This brought the challenge of determining whether there exists a faster solution with polylogarithmic update time, or (as is the case for other dynamic problems), we should expect a polynomial (conditional) lower bound. We answer this question by designing a significantly faster algorithm that processes each substitution in amortized \textsuperscript{\{\vphantom\}}\{O\}(1)\vphantom\{\}n time with high probability. Our solution relies on exploiting the local consistency of the parsing of a collection of dynamic strings due to Gawrychowski et al. [SODA 2018], and on maintaining two dynamic trees with labeled bicolored leaves, so that after each update we can report a pair of nodes, one from each tree, of maximum combined weight, which have at least one common leaf-descendant of each color. We complement this with a lower bound of {\textohm}(n/n) for the update time of any polynomial-size data structure that maintains the LCS of two dynamic strings, and the same lower bound for the update time of any data structure of size \{O\}\vphantom\{\}(n) that maintains the LCS of a static and a dynamic string. Both lower bounds hold even allowing amortization and randomization.},
  citationcount = {19},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,dynamic,lower bound,static,update,update time}
}

@article{charalampopoulosFasterApproximatePattern2020,
  title = {Faster Approximate Pattern Matching: A Unified Approach},
  author = {Charalampopoulos, P. and Kociumaka, Tomasz and Wellnitz, Philip},
  year = {2020},
  doi = {10.1109/FOCS46700.2020.00095},
  abstract = {In the approximate pattern matching problem, given a text {\textexclamdown}tex{\textquestiondown}T{\textexclamdown}/tex{\textquestiondown}, a pattern {\textexclamdown}tex{\textquestiondown}P{\textexclamdown}/tex{\textquestiondown}, and a threshold {\textexclamdown}tex{\textquestiondown}k{\textexclamdown}/tex{\textquestiondown}, the task is to find (the starting positions of) all substrings of {\textexclamdown}tex{\textquestiondown}T{\textexclamdown}/tex{\textquestiondown} that are at distance at most {\textexclamdown}tex{\textquestiondown}k{\textexclamdown}/tex{\textquestiondown} from {\textexclamdown}tex{\textquestiondown}P{\textexclamdown}/tex{\textquestiondown}. We consider the two most fundamental string metrics: Under the Hamming distance, we search for substrings of {\textexclamdown}tex{\textquestiondown}T{\textexclamdown}/tex{\textquestiondown} that have at most {\textexclamdown}tex{\textquestiondown}k{\textexclamdown}/tex{\textquestiondown} mismatches with {\textexclamdown}tex{\textquestiondown}P{\textexclamdown}/tex{\textquestiondown}, while under the edit distance, we search for substrings of {\textexclamdown}tex{\textquestiondown}T{\textexclamdown}/tex{\textquestiondown} that can be transformed to {\textexclamdown}tex{\textquestiondown}P{\textexclamdown}/tex{\textquestiondown} with at most {\textexclamdown}tex{\textquestiondown}k{\textexclamdown}/tex{\textquestiondown} edits. Exact occurrences of {\textexclamdown}tex{\textquestiondown}P{\textexclamdown}/tex{\textquestiondown} in {\textexclamdown}tex{\textquestiondown}T{\textexclamdown}/tex{\textquestiondown} have a very simple structure: If we assume for simplicity that {\textexclamdown}tex{\textquestiondown}{\textbar}P{\textbar}{$<\vert$}T{\textbar}{$\leq$}\{\{\}\vphantom\}\textsuperscript{\{\vphantom\}}3\vphantom\{\}\vphantom\{\}/\textsubscript{\{\vphantom\}}2\vphantom\{\}{\textbar}P{\textbar}{\textexclamdown}/tex{\textquestiondown} and that {\textexclamdown}tex{\textquestiondown}P{\textexclamdown}/tex{\textquestiondown} occurs both as a prefix and as a suffix of {\textexclamdown}tex{\textquestiondown}T{\textexclamdown}/tex{\textquestiondown}, then both {\textexclamdown}tex{\textquestiondown}P{\textexclamdown}/tex{\textquestiondown} and {\textexclamdown}tex{\textquestiondown}T{\textexclamdown}/tex{\textquestiondown} are periodic with a common period. However, an analogous characterization for occurrences with up to {\textexclamdown}tex{\textquestiondown}k{\textexclamdown}/tex{\textquestiondown} mismatches was proved only recently by Bringmann et al. [SODA'19]: Either there are {\textexclamdown}tex{\textquestiondown}\{O\}(k\textsuperscript{\{\vphantom\}}2\vphantom\{\})k{\textexclamdown}/tex{\textquestiondown}-mismatch occurrences of {\textexclamdown}tex{\textquestiondown}P{\textexclamdown}/tex{\textquestiondown} in {\textexclamdown}tex{\textquestiondown}T{\textexclamdown}/tex{\textquestiondown}, or both {\textexclamdown}tex{\textquestiondown}P{\textexclamdown}/tex{\textquestiondown} and {\textexclamdown}tex{\textquestiondown}T{\textexclamdown}/tex{\textquestiondown} are at Hamming distance {\textexclamdown}tex{\textquestiondown}\{O\}(k){\textexclamdown}/tex{\textquestiondown} from strings with a common string period of length {\textexclamdown}tex{\textquestiondown}\{O\}(m/k){\textexclamdown}/tex{\textquestiondown}. We tighten this characterization by showing that there are {\textexclamdown}tex{\textquestiondown}\{O\}(k)k{\textexclamdown}/tex{\textquestiondown}-mismatch occurrences in the non-periodic case, and we lift it to the edit distance setting, where we tightly bound the number of {\textexclamdown}tex{\textquestiondown}k{\textexclamdown}/tex{\textquestiondown}-edit occurrences by {\textexclamdown}tex{\textquestiondown}\{O\}(k\textsuperscript{\{\vphantom\}}2\vphantom\{\}){\textexclamdown}/tex{\textquestiondown} in the non-periodic case. Our proofs are constructive and let us obtain a unified framework for approximate pattern matching for both considered distances. In particular, we provide meta-algorithms that only rely on a small set of primitive operations. We showcase the generality of our meta-algorithms with results for the fully compressed setting, the dynamic setting, and the standard setting.},
  citationcount = {43},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic}
}

@article{charalampopoulosInternalDictionaryMatching2019,
  title = {Internal Dictionary Matching},
  author = {Charalampopoulos, P. and Kociumaka, Tomasz and Mohamed, M. and Radoszewski, J. and Rytter, W. and Wale{\'n}, Tomasz},
  year = {2019},
  doi = {10.1007/s00453-021-00821-y},
  abstract = {No abstract available},
  citationcount = {16},
  venue = {Algorithmica}
}

@article{charalampopoulosPatternMaskingFor2020,
  title = {Pattern Masking for Dictionary Matching},
  author = {Charalampopoulos, P. and Chen, Huiping and Christen, P. and Loukides, Grigorios and Pisanti, N. and Pissis, S. and Radoszewski, J.},
  year = {2020},
  doi = {10.4230/LIPIcs.ISAAC.2021.65},
  abstract = {In the Pattern Masking for Dictionary Matching (PMDM) problem, we are given a dictionary \{D\} of d strings, each of length {$\ell$}, a query string q of length {$\ell$}, and a positive integer z, and we are asked to compute a smallest set K{$\subseteq$} 1,{\dots},{$\ell$} , so that if q[i], for all i{$\in$}K, is replaced by a wildcard, then q matches at least z strings from \{D\}. The PMDM problem lies at the heart of two important applications featured in large-scale real-world systems: record linkage of databases that contain sensitive information, and query term dropping. In both applications, solving PMDM allows for providing data utility guarantees as opposed to existing approaches. We first show, through a reduction from the well-known k-Clique problem, that a decision version of the PMDM problem is NP-complete, even for strings over a binary alphabet. We present a data structure for PMDM that answers queries over \{D\} in time \{O\}(2\textsuperscript{\{\vphantom\}}{$\ell$}/2\vphantom\{\}(2\textsuperscript{\{\vphantom\}}{$\ell$}/2\vphantom\{\}+{$\tau$}){$\ell$}) and requires space \{O\}(2\textsuperscript{\{\vphantom\}}{$\ell$}\vphantom\{\}d{$^2$}/{$\tau^2$}+2\textsuperscript{\{\vphantom\}}{$\ell$}/2\vphantom\{\}d), for any parameter {$\tau\in$}[1,d]. We also approach the problem from a more practical perspective. We show an \{O\}((d{$\ell$})\textsuperscript{\{\vphantom\}}k/3\vphantom\{\}+d{$\ell$})-time and \{O\}(d{$\ell$})-space algorithm for PMDM if k={\textbar}K{\textbar}=\{O\}(1). We generalize our exact algorithm to mask multiple query strings simultaneously. We complement our results by showing a two-way polynomial-time reduction between PMDM and the Minimum Union problem [Chlamtac et al., SODA 2017]. This gives a polynomial-time \{O\}(d\textsuperscript{\{\vphantom\}}1/4+{$\epsilon$}\vphantom\{\})-approximation algorithm for PMDM, which is tight under plausible complexity conjectures.},
  citationcount = {3},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {data structure,query,reduction}
}

@article{charalampopoulosPatternMaskingFor2024,
  title = {Pattern Masking for Dictionary Matching: {{Theory}} and Practice},
  author = {Charalampopoulos, P. and Chen, Huiping and Christen, Peter and Loukides, Grigorios and Pisanti, N. and Pissis, S. and Radoszewski, J.},
  year = {2024},
  doi = {10.1007/s00453-024-01213-8},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Algorithmica}
}

@article{charikarNewAlgorithmsSubset2002,
  title = {New Algorithms for Subset Query, Partial Match, Orthogonal Range Searching, and Related Problems},
  author = {Charikar, M. and Indyk, P. and Panigrahy, R.},
  year = {2002},
  doi = {10.1007/3-540-45465-9_39},
  abstract = {No abstract available},
  citationcount = {66},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {query}
}

@article{charikarSimilarityEstimationTechniques2002,
  title = {Similarity Estimation Techniques from Rounding Algorithms},
  author = {Charikar, M.},
  year = {2002},
  doi = {10.1145/509907.509965},
  abstract = {(MATH) A locality sensitive hashing scheme is a distribution on a family  of hash functions operating on a collection of objects, such that for two objects {\textexclamdown}i{\textquestiondown}x,y{\textexclamdown}/i{\textquestiondown}, {\textexclamdown}b{\textquestiondown}Pr{\textexclamdown}/b{\textquestiondown}{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}h{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown}{$\varepsilon$}F[{\textexclamdown}i{\textquestiondown}h{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}) = {\textexclamdown}i{\textquestiondown}h{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}y{\textexclamdown}/i{\textquestiondown})] = sim({\textexclamdown}i{\textquestiondown}x,y{\textexclamdown}/i{\textquestiondown}), where {\textexclamdown}i{\textquestiondown}sim{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}x,y{\textexclamdown}/i{\textquestiondown}) {$\varepsilon$} [0,1] is some similarity function defined on the collection of objects. Such a scheme leads to a compact representation of objects so that similarity of objects can be estimated from their compact sketches, and also leads to efficient algorithms for approximate nearest neighbor search and clustering. Min-wise independent permutations provide an elegant construction of such a locality sensitive hashing scheme for a collection of subsets with the set similarity measure {\textexclamdown}i{\textquestiondown}sim{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}A,B{\textexclamdown}/i{\textquestiondown}) = \textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{{\textbar}}A \&Pgr; B{\textbar}\vphantom\{\}\{{\textbar}A \&Pgr B{\textbar}\}.(MATH) We show that rounding algorithms for LPs and SDPs used in the context of approximation algorithms can be viewed as locality sensitive hashing schemes for several interesting collections of objects. Based on this insight, we construct new locality sensitive hashing schemes for:{\textexclamdown}ol{\textquestiondown}{\textexclamdown}li{\textquestiondown}A collection of vectors with the distance between {$\rightarrow$} {\textexclamdown}i{\textquestiondown}u{\textexclamdown}/i{\textquestiondown} and {$\rightarrow$} {\textexclamdown}i{\textquestiondown}v{\textexclamdown}/i{\textquestiondown} measured by {\O}({$\rightarrow$} {\textexclamdown}i{\textquestiondown}u{\textexclamdown}/i{\textquestiondown}, {$\rightarrow$} {\textexclamdown}i{\textquestiondown}v{\textexclamdown}/i{\textquestiondown})/{$\pi$}, where {\O}({$\rightarrow$} {\textexclamdown}i{\textquestiondown}u{\textexclamdown}/i{\textquestiondown}, {$\rightarrow$} {\textexclamdown}i{\textquestiondown}v{\textexclamdown}/i{\textquestiondown}) is the angle between {$\rightarrow$} {\textexclamdown}i{\textquestiondown}u{\textexclamdown}/i{\textquestiondown}) and {$\rightarrow$} {\textexclamdown}i{\textquestiondown}v{\textexclamdown}/i{\textquestiondown}). This yields a sketching scheme for estimating the cosine similarity measure between two vectors, as well as a simple alternative to minwise independent permutations for estimating set similarity.{\textexclamdown}/li{\textquestiondown}{\textexclamdown}li{\textquestiondown}A collection of distributions on {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} points in a metric space, with distance between distributions measured by the Earth Mover Distance ({\textexclamdown}b{\textquestiondown}EMD{\textexclamdown}/b{\textquestiondown}), (a popular distance measure in graphics and vision). Our hash functions map distributions to points in the metric space such that, for distributions {\textexclamdown}i{\textquestiondown}P{\textexclamdown}/i{\textquestiondown} and {\textexclamdown}i{\textquestiondown}Q{\textexclamdown}/i{\textquestiondown}, {\textexclamdown}b{\textquestiondown}EMD{\textexclamdown}/b{\textquestiondown}({\textexclamdown}i{\textquestiondown}P,Q{\textexclamdown}/i{\textquestiondown}) \&xie; {\textexclamdown}b{\textquestiondown}E{\textexclamdown}/b{\textquestiondown}{\textexclamdown}sub{\textquestiondown}h{$\varepsilon$}{\textexclamdown}/sub{\textquestiondown} [{\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}h{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}P{\textexclamdown}/i{\textquestiondown}),{\textexclamdown}i{\textquestiondown}h{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}Q{\textexclamdown}/i{\textquestiondown}))] \&xie; {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} log log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}). {\textexclamdown}b{\textquestiondown}EMD{\textexclamdown}/b{\textquestiondown}({\textexclamdown}i{\textquestiondown}P, Q{\textexclamdown}/i{\textquestiondown}).{\textexclamdown}/li{\textquestiondown}{\textexclamdown}/ol{\textquestiondown}.},
  citationcount = {2551},
  venue = {Symposium on the Theory of Computing}
}

@article{chatterjeeAlgorithmsAndConditional2018,
  title = {Algorithms and Conditional Lower Bounds for Planning Problems},
  author = {Chatterjee, K. and Dvo{\v r}{\'a}k, Wolfgang and Henzinger, Monika and Svozil, Alexander},
  year = {2018},
  doi = {10.1016/J.ARTINT.2021.103499},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {International Conference on Automated Planning and Scheduling},
  keywords = {lower bound}
}

@article{chatterjeeAlgorithmsForAlgebraic2015,
  title = {Algorithms for Algebraic Path Properties in Concurrent Systems of Constant Treewidth Components},
  author = {Chatterjee, K. and Goharshady, A. and {Ibsen-Jensen}, Rasmus and Pavlogiannis, Andreas},
  year = {2015},
  doi = {10.1145/3210257},
  abstract = {We study algorithmic questions wrt algebraic path properties in concurrent systems, where the transitions of the system are labeled from a complete, closed semiring. The algebraic path properties can model dataflow analysis problems, the shortest path problem, and many other natural problems that arise in program analysis. We consider that each component of the concurrent system is a graph with constant treewidth, a property satisfied by the controlflow graphs of most programs. We allow for multiple possible queries, which arise naturally in demand driven dataflow analysis. The study of multiple queries allows us to consider the tradeoff between the resource usage of the one-time preprocessing and for each individual query. The traditional approach constructs the product graph of all components and applies the best-known graph algorithm on the product. In this approach, even the answer to a single query requires the transitive closure (i.e., the results of all possible queries), which provides no room for tradeoff between preprocessing and query time. Our main contributions are algorithms that significantly improve the worst-case running time of the traditional approach, and provide various tradeoffs depending on the number of queries. For example, in a concurrent system of two components, the traditional approach requires hexic time in the worst case for answering one query as well as computing the transitive closure, whereas we show that with one-time preprocessing in almost cubic time, each subsequent query can be answered in at most linear time, and even the transitive closure can be computed in almost quartic time. Furthermore, we establish conditional optimality results showing that the worst-case running time of our algorithms cannot be improved without achieving major breakthroughs in graph algorithms (i.e., improving the worst-case bound for the shortest path problem in general graphs). Preliminary experimental results show that our algorithms perform favorably on several benchmarks.},
  citationcount = {24},
  venue = {ACM Transactions on Programming Languages and Systems},
  keywords = {query,query time}
}

@article{chatterjeeAlgorithmsForAlgebraic2016,
  title = {Algorithms for Algebraic Path Properties in Concurrent Systems of Constant Treewidth Components},
  author = {Chatterjee, K. and Goharshady, A. and {Ibsen-Jensen}, Rasmus and Pavlogiannis, Andreas},
  year = {2016},
  doi = {10.1145/2837614.2837624},
  abstract = {We study algorithmic questions for concurrent systems where the transitions are labeled from a complete, closed semiring, and path properties are algebraic with semiring operations. The algebraic path properties can model dataflow analysis problems, the shortest path problem, and many other natural problems that arise in program analysis. We consider that each component of the concurrent system is a graph with constant treewidth, a property satisfied by the controlflow graphs of most programs. We allow for multiple possible queries, which arise naturally in demand driven dataflow analysis. The study of multiple queries allows us to consider the tradeoff between the resource usage of the one-time preprocessing and for each individual query. The traditional approach constructs the product graph of all components and applies the best-known graph algorithm on the product. In this approach, even the answer to a single query requires the transitive closure (i.e., the results of all possible queries), which provides no room for tradeoff between preprocessing and query time. Our main contributions are algorithms that significantly improve the worst-case running time of the traditional approach, and provide various tradeoffs depending on the number of queries. For example, in a concurrent system of two components, the traditional approach requires hexic time in the worst case for answering one query as well as computing the transitive closure, whereas we show that with one-time preprocessing in almost cubic time, each subsequent query can be answered in at most linear time, and even the transitive closure can be computed in almost quartic time. Furthermore, we establish conditional optimality results showing that the worst-case running time of our algorithms cannot be improved without achieving major breakthroughs in graph algorithms (i.e., improving the worst-case bound for the shortest path problem in general graphs). Preliminary experimental results show that our algorithms perform favorably on several benchmarks.},
  citationcount = {8},
  venue = {ACM-SIGACT Symposium on Principles of Programming Languages},
  keywords = {query,query time}
}

@article{chatterjeeConditionallyOptimalAlgorithms2016,
  title = {Conditionally Optimal Algorithms for Generalized B{\"u}chi Games},
  author = {Chatterjee, K. and Dvo{\v r}{\'a}k, Wolfgang and Henzinger, Monika and Loitzenbauer, Veronika},
  year = {2016},
  doi = {10.4230/LIPIcs.MFCS.2016.25},
  abstract = {Games on graphs provide the appropriate framework to study several central problems in computer science, such as verification and synthesis of reactive systems. One of the most basic objectives for games on graphs is the liveness (or Buchi) objective that given a target set of vertices requires that some vertex in the target set is visited infinitely often. We study generalized Buchi objectives (i.e., conjunction of liveness objectives), and implications between two generalized Buchi objectives (known as GR(1) objectives), that arise in numerous applications in computer-aided verification. We present improved algorithms and conditional super-linear lower bounds based on widely believed assumptions about the complexity of (A1) combinatorial Boolean matrix multiplication and (A2) CNF-SAT. We consider graph games with n vertices, m edges, and generalized Buchi objectives with k conjunctions. First, we present an algorithm with running time O(k*n{\textasciicircum}2), improving the previously known O(k*n*m) and O(k{\textasciicircum}2*n{\textasciicircum}2) worst-case bounds. Our algorithm is optimal for dense graphs under (A1). Second, we show that the basic algorithm for the problem is optimal for sparse graphs when the target sets have constant size under (A2). Finally, we consider GR(1) objectives, with k\_1 conjunctions in the antecedent and k\_2 conjunctions in the consequent, and present an O(k\_1 k\_2 n{\textasciicircum}\{2.5\})-time algorithm, improving the previously known O(k\_1*k\_2*n*m)-time algorithm for m {\textquestiondown} n{\textasciicircum}\{1.5\}.},
  citationcount = {27},
  venue = {International Symposium on Mathematical Foundations of Computer Science},
  keywords = {lower bound}
}

@article{chatterjeeModelAndObjective2016,
  title = {Model and Objective Separation with Conditional Lower Bounds: {{Disjunction}} Is Harder than Conjunction *},
  author = {Chatterjee, K. and Dvo{\v r}{\'a}k, Wolfgang and Henzinger, Monika and Loitzenbauer, Veronika},
  year = {2016},
  doi = {10.1145/2933575.2935304},
  abstract = {Given a model of a system and an objective, the model-checking question asks whether the model satisfies the objective. We study polynomial-time problems in two classical models, graphs and Markov Decision Processes (MDPs), with respect to several fundamental {$\omega$}-regular objectives, e.g., Rabin and Streett objectives. For many of these problems the best-known upper bounds are quadratic or cubic, yet no super-linear lower bounds are known. In this work our contributions are two-fold: First, we present several improved algorithms, and second, we present the first conditional super-linear lower bounds based on widely believed assumptions about the complexity of CNF-SAT and combinatorial Boolean matrix multiplication. A separation result for two models with respect to an objective means a conditional lower bound for one model that is strictly higher than the existing upper bound for the other model, and similarly for two objectives with respect to a model. Our results establish the following separation results: (1) A separation of models (graphs and MDPs) for disjunctive queries of reachability and B{\"u}chi objectives. (2) Two kinds of separations of objectives, both for graphs and MDPs, namely, (2a) the separation of dual objectives such as Streett/Rabin objectives, and (2b) the separation of conjunction and disjunction of multiple objectives of the same type such as safety, B{\"u}chi, and coB{\"u}chi. In summary, our results establish the first model and objective separation results for graphs and MDPs for various classical {$\omega$}-regular objectives. Quite strikingly, we establish conditional lower bounds for the disjunction of objectives that are strictly higher than the existing upper bounds for the conjunction of the same objectives.},
  citationcount = {17},
  venue = {Logic in Computer Science},
  keywords = {lower bound,query}
}

@article{chattopadhyayDiscrepancyAndThe2007,
  title = {Discrepancy and the Power of Bottom Fan-in in Depth-Three Circuits},
  author = {Chattopadhyay, A.},
  year = {2007},
  doi = {10.1109/FOCS.2007.30},
  abstract = {We develop a new technique of proving lower bounds for the randomized communication complexity of boolean functions in the multiparty 'number on the forehead' model. Our method is based on the notion of voting polynomial degree of functions and extends the degree-discrepancy lemma in the recent work of Sherstov (2007). Using this we prove that depth three circuits consisting of a MAJORITY gate at the output, gates computing arbitrary symmetric function at the second layer and arbitrary gates of bounded fan-in at the base layer i.e. circuits of type MAJ o SYMM o ANYO(1) cannot simulate the circuit class AC0 in sub-exponential size. Further, even if the fan-in of the bottom ANY gates are increased to o(log log n), such circuits cannot simulate AC0 in quasi-polynomial size. This is in contrast to the classical result of Yao and Beigel-Tarui that shows that such circuits, having only MAJORITY gales, can simulate the class ACC0 in quasi-polynomial size when the bottom fan-in is increased to poly-logarithmic size. In the second part, we simplify the arguments in the breakthrough work of Bourgain (2005) for obtaining exponentially small upper bounds on the correlation between the boolean function MODq and functions represented bv polynomials of small degree over Zm, when m,q ges 2 are co-prime integers. Our calculation also shows similarity with techniques used to estimate discrepancy of functions in the multiparty communication setting. This results in a slight improvement of the estimates of Bourgain et al. (2005). It is known that such estimates imply that circuits of type MAJ o MODm o ANDisin log n cannot compute the MODq function in sub-exponential size. It remains a major open question to determine if such circuits can simulate ACC0 in polynomial size when the bottom fan-in is increased to poly-logarithmic size.},
  citationcount = {69},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{chattopadhyayLiftingToParity2022,
  title = {Lifting to Parity Decision Trees via Stifling},
  author = {Chattopadhyay, A. and Mande, Nikhil S. and Sanyal, Swagato and Sherif, Suhail},
  year = {2022},
  doi = {10.4230/LIPIcs.ITCS.2023.33},
  abstract = {We show that the deterministic decision tree complexity of a (partial) function or relation f lifts to the deterministic parity decision tree (PDT) size complexity of the composed function/relation f{$\circ$}g as long as the gadget g satisfies a property that we call stifling. We observe that several simple gadgets of constant size, like Indexing on 3 input bits, Inner Product on 4 input bits, Majority on 3 input bits and random functions, satisfy this property. It can be shown that existing randomized communication lifting theorems ([G{\"\{}o\vphantom\{\}{\"\{}o\vphantom\{\}s, Pitassi, Watson. SICOMP'20], [Chattopadhyay et al. SICOMP'21]) imply PDT-size lifting. However there are two shortcomings of this approach: first they lift randomized decision tree complexity of f, which could be exponentially smaller than its deterministic counterpart when either f is a partial function or even a total search problem. Second, the size of the gadgets in such lifting theorems are as large as logarithmic in the size of the input to f. Reducing the gadget size to a constant is an important open problem at the frontier of current research. Our result shows that even a random constant-size gadget does enable lifting to PDT size. Further, it also yields the first systematic way of turning lower bounds on the width of tree-like resolution proofs of the unsatisfiability of constant-width CNF formulas to lower bounds on the size of tree-like proofs in the resolution with parity system, i.e., \emph{\{\vphantom\}}Res\vphantom\{\}({$\oplus$}), of the unsatisfiability of closely related constant-width CNF formulas.},
  citationcount = {10},
  venue = {Information Technology Convergence and Services},
  keywords = {communication,lower bound}
}

@inproceedings{chattopadhyayLittleAdviceCan2012,
  title = {A Little Advice Can Be Very Helpful},
  author = {Chattopadhyay, A. and Edmonds, J. and Ellen, Faith and Pitassi, T.},
  year = {2012},
  doi = {10.1137/1.9781611973099.52},
  abstract = {Proving superpolylogarithmic lower bounds for dynamic data structures has remained an open problem despite years of research. Recently, Patrascu proposed an exciting new approach for breaking this barrier via a two player communication model in which one player gets private advice at the beginning of the protocol. He gave reductions from the problem of solving an asymmetric version of set-disjointness in his model to a diverse collection of natural dynamic data structure problems in the cell probe model. He also conjectured that, for any hard problem in the standard two-party communication model, the asymmetric version of the problem is hard in his model, provided not too much advice is given. In this paper, we prove several surprising results about his model. We show that there exist Boolean functions requiring linear randomized communication complexity in the two-party model, for which the asymmetric versions in his model have deterministic protocols with exponentially smaller complexity. For set-disjointness, which also requires linear randomized communication complexity in the two-party model, we give a deterministic protocol for the asymmetric version in his model with a quadratic improvement in complexity. These results demonstrate that Patrascu's conjecture, as stated, is false. In addition, we show that the randomized and deterministic communication complexities of problems in his model differ by no more than a logarithmic multiplicative factor. We also prove lower bounds in some restricted versions of this model for natural functions such as set-disjointness and inner product. All of our upper bounds conform to these restrictions.},
  citationcount = {14},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,communication,communication complexity,data structure,dynamic,lower bound,not a DSLB,reduction,sorted},
  file = {/Users/tulasi/Zotero/storage/I9FCX724/Chattopadhyay et al. - 2012 - A little advice can be very helpful.pdf}
}

@article{chattopadhyayQueryToCommunication2019,
  title = {Query-to-Communication Lifting for {{BPP}} Using Inner Product},
  author = {Chattopadhyay, A. and Filmus, Yuval and Koroth, Sajin and Meir, Or and Pitassi, T.},
  year = {2019},
  doi = {10.4230/LIPIcs.ICALP.2019.35},
  abstract = {We prove a new query-to-communication lifting for randomized protocols, with inner product as gadget. This allows us to use a much smaller gadget, leading to a more efficient lifting. Prior to this work, such a theorem was known only for deterministic protocols, due to Chattopadhyay et al. and Wu et al. The only query-to-communication lifting result for randomized protocols, due to G{\"o}{\"o}s, Pitassi and Watson, used the much larger indexing gadget. Our proof also provides a unified treatment of randomized and deterministic lifting. Most existing proofs of deterministic lifting theorems use a measure of information known as thickness. In contrast, G{\"o}{\"o}s, Pitassi and Watson used blockwise min-entropy as a measure of information. Our proof uses the blockwise min-entropy framework to prove lifting theorems in both settings in a unified way.},
  citationcount = {20},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {communication,query}
}

@article{chattopadhyayQueryToCommunication2019,
  title = {Query-to-Communication Lifting Using Low-Discrepancy Gadgets},
  author = {Chattopadhyay, A. and Filmus, Yuval and Koroth, Sajin and Meir, Or and Pitassi, T.},
  year = {2019},
  doi = {10.1137/19M1310153},
  abstract = {Lifting theorems are theorems that relate the query complexity of a function f: 0,1 \textsuperscript{\{\vphantom\}}n\vphantom\{\}{$\rightarrow$} 0,1  to the communication complexity of the composed function f{$\circ$}g\textsuperscript{\{\vphantom\}}n\vphantom\{\}, for some "gadget" g: 0,1 \textsuperscript{\{\vphantom\}}b\vphantom\{\}{\texttimes} 0,1 \textsuperscript{\{\vphantom\}}b\vphantom\{\}{$\rightarrow$} 0,1 . Such theorems allow transferring lower bounds from query complexity to the communication complexity, and have seen numerous applications in the recent years. In addition, such theorems can be viewed as a strong generalization of a direct-sum theorem for the gadget g. We prove a new lifting theorem that works for all gadgets g that have logarithmic length and exponentially-small discrepancy, for both deterministic and randomized communication complexity. Thus, we significantly increase the range of gadgets for which such lifting theorems hold. Our result has two main motivations: First, allowing a larger variety of gadgets may support more applications. In particular, our work is the first to prove a randomized lifting theorem for logarithmic-size gadgets, thus improving some applications of the theorem. Second, our result can be seen as a strong generalization of a direct-sum theorem for functions with low discrepancy.},
  citationcount = {26},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {communication,communication complexity,lower bound,query,query complexity}
}

@article{chattopadhyaySeparationOfUnbounded2018,
  title = {Separation of Unbounded-Error Models in Multi-Party Communication Complexity},
  author = {Chattopadhyay, A. and Mande, Nikhil S.},
  year = {2018},
  doi = {10.4086/toc.2018.v014a021},
  abstract = {We construct a simple function that has small unbounded-error communication complexity in the k-party number-on-forehead (NOF) model but every probabilistic protocol that solves it with subexponential advantage over random guessing has cost essentially {\textohm}( {\textsurd} n/4k) bits. This separates these classes up to k {$\leq$} {$\delta$} logn players for any constant {$\delta$} {\textexclamdown} 1/4, and gives the largest known separation by an explicit function in this regime of k. Our analysis is elementary and self-contained, inspired by the methods of Goldmann, H{\aa}stad, and Razborov (Computational Complexity, 1992). After initial publication of our work as a preprint (ECCC, 2016), Sherstov pointed out to us that an alternative proof of an {\textohm}((n/4k)1/7) separation is implicit in his prior work (SICOMP, 2016). Furthermore, based on his prior work (SICOMP, 2013 and SICOMP, 2016), Sherstov gave an alternative proof of our constructive {\textohm}( {\textsurd} n/4k) separation and also produced a stronger non-constructive {\textohm}(n/4k) separation. These results are explained in Sherstov's preprint (ECCC, 2016) and in article 14/22 in Theory of Computing. A preliminary version of this paper appeared as ECCC technical report TR 16-095. {$\ast$}This work was done while the author was partially supported by a Ramanujan fellowship of the Department of Science and Technology, India. {\dag}This work was done while the author was supported by a fellowship of the Department of Atomic Energy, India. ACM Classification: F.1.3, F.2.3 AMS Classification: 68Q05, 68Q10, 68Q15, 68Q17},
  citationcount = {4},
  venue = {Theory of Computing},
  keywords = {communication,communication complexity}
}

@article{chattopadhyaySimulationBeatsRichness2018,
  title = {Simulation Beats Richness: New Data-Structure Lower Bounds},
  author = {Chattopadhyay, A. and Kouck{\'y}, M. and Loff, B. and Mukhopadhyay, Sagnik},
  year = {2018},
  doi = {10.1145/3188745.3188874},
  abstract = {We develop a new technique for proving lower bounds in the setting of asymmetric communication, a model that was introduced in the famous works of Miltersen (STOC'94) and Miltersen, Nisan, Safra and Wigderson (STOC'95). At the core of our technique is the first simulation theorem in the asymmetric setting, where Alice gets a p {\texttimes} n matrix x over F2 and Bob gets a vector y {$\in$} F2n. Alice and Bob need to evaluate f(x{$\cdot$} y) for a Boolean function f: \{0,1\}p {$\rightarrow$} \{0,1\}. Our simulation theorems show that a deterministic/randomized communication protocol exists for this problem, with cost C{$\cdot$} n for Alice and C for Bob, if and only if there exists a deterministic/randomized *parity decision tree* of cost {$\Theta$}(C) for evaluating f. As applications of this technique, we obtain the following results: 1. The first strong lower-bounds against randomized data-structure schemes for the Vector-Matrix-Vector product problem over F2. Moreover, our method yields strong lower bounds even when the data-structure scheme has tiny advantage over random guessing. 2. The first lower bounds against randomized data-structures schemes for two natural Boolean variants of Orthogonal Vector Counting. 3. We construct an asymmetric communication problem and obtain a deterministic lower-bound for it which is provably better than any lower-bound that may be obtained by the classical Richness Method of Miltersen et al. (STOC '95). This seems to be the first known limitation of the Richness Method in the context of proving deterministic lower bounds.},
  citationcount = {24},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {communication,data structure,lower bound}
}

@inproceedings{chattopadhyaySimulationBeatsRichness2018a,
  title = {Simulation Beats Richness: New Data-Structure Lower Bounds},
  shorttitle = {Simulation Beats Richness},
  booktitle = {Proceedings of the 50th {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}},
  author = {Chattopadhyay, Arkadev and Kouck{\'y}, Michal and Loff, Bruno and Mukhopadhyay, Sagnik},
  year = {2018},
  month = jun,
  series = {{{STOC}} 2018},
  pages = {1013--1020},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3188745.3188874},
  url = {https://dl.acm.org/doi/10.1145/3188745.3188874},
  urldate = {2024-09-05},
  abstract = {We develop a new technique for proving lower bounds in the setting of asymmetric communication, a model that was introduced in the famous works of Miltersen (STOC'94) and Miltersen, Nisan, Safra and Wigderson (STOC'95). At the core of our technique is the first simulation theorem in the asymmetric setting, where Alice gets a p {\texttimes} n matrix x over F2 and Bob gets a vector y {$\in$} F2n. Alice and Bob need to evaluate f(x{$\cdot$} y) for a Boolean function f: \{0,1\}p {$\rightarrow$} \{0,1\}. Our simulation theorems show that a deterministic/randomized communication protocol exists for this problem, with cost C{$\cdot$} n for Alice and C for Bob, if and only if there exists a deterministic/randomized *parity decision tree* of cost {$\Theta$}(C) for evaluating f. As applications of this technique, we obtain the following results: 1. The first strong lower-bounds against randomized data-structure schemes for the Vector-Matrix-Vector product problem over F2. Moreover, our method yields strong lower bounds even when the data-structure scheme has tiny advantage over random guessing. 2. The first lower bounds against randomized data-structures schemes for two natural Boolean variants of Orthogonal Vector Counting. 3. We construct an asymmetric communication problem and obtain a deterministic lower-bound for it which is provably better than any lower-bound that may be obtained by the classical Richness Method of Miltersen et al. (STOC '95). This seems to be the first known limitation of the Richness Method in the context of proving deterministic lower bounds.},
  isbn = {978-1-4503-5559-9},
  keywords = {communication,data structure,lower bound},
  annotation = {Anurag Anshu Naresh B Goud Rahul Jain Srijita Kundu and Priyanka Mukhopadhyay. 2017.  Anurag Anshu Naresh B Goud Rahul Jain Srijita Kundu and Priyanka Mukhopadhyay. 2017.\\
Lifting randomized query complexity to randomized communication complexity. arXiv:1703.07521 ( 2017 ). Lifting randomized query complexity to randomized communication complexity. arXiv:1703.07521 (2017).\\
V. Z. Arlazarov E. A. Dinic M. A. Kronrod and I. A. Faradzev. 1970. On economical construction of the transitive closure of a directed graph. Soviet Mathematics Doklady (1970) 11(5):1209--1210.  V. Z. Arlazarov E. A. Dinic M. A. Kronrod and I. A. Faradzev. 1970. On economical construction of the transitive closure of a directed graph. Soviet Mathematics Doklady (1970) 11(5):1209--1210.\\
Khanh Do Ba , Piotr Indyk , Eric Price , and David P . Woodruff . 2010 . Khanh Do Ba, Piotr Indyk, Eric Price, and David P. Woodruff. 2010.\\
Lower Bounds for Sparse Recovery. In Proceedings of the 21st SODA. 1190--1197 . Lower Bounds for Sparse Recovery. In Proceedings of the 21st SODA. 1190--1197.\\
\\
Mark Braverman. 2011.  Mark Braverman. 2011.\\
\\
\\
Timothy M. Chan and Ryan Williams . 2016 . Timothy M. Chan and Ryan Williams. 2016.\\
Deterministic APSP , orthogonal vectors, and more : Quickly derandomizing razborov-smolensky . In Proceedings of the 27th SODA. 1246--1255 . Deterministic APSP, orthogonal vectors, and more: Quickly derandomizing razborov-smolensky. In Proceedings of the 27th SODA. 1246--1255.\\
STOC'18 June 25--29 2018 Los Angeles CA USA Arkadev Chattopadhyay Michal Kouck{\'y} Bruno Loff and Sagnik Mukhopadhyay  STOC'18 June 25--29 2018 Los Angeles CA USA Arkadev Chattopadhyay Michal Kouck{\'y} Bruno Loff and Sagnik Mukhopadhyay\\
Arkadev Chattopadhyay. 2007.  Arkadev Chattopadhyay. 2007.\\
\\
Arkadev Chattopadhyay. 2008.  Arkadev Chattopadhyay. 2008.\\
Arkadev Chattopadhyay Michal Kouck{\'y} Bruno Loff and Sagnik Mukhopadhyay. 2017.  Arkadev Chattopadhyay Michal Kouck{\'y} Bruno Loff and Sagnik Mukhopadhyay. 2017.\\
Simulation Theorems via Pseudorandom Properties . arXiv:1704.06807 ( 2017 ). Simulation Theorems via Pseudorandom Properties. arXiv:1704.06807 (2017).\\
Benny Chor and Oded Goldreich. 1988.  Benny Chor and Oded Goldreich. 1988.\\
\\
\\
\\
Holger Dell and John Lapinskas . 2017. Fine-grained reductions from approximate counting to decision. CoRR abs/1707.04609 ( 2017 ). arXiv: 1707.04609 http://arxiv. org/abs/1707.04609 Holger Dell and John Lapinskas. 2017. Fine-grained reductions from approximate counting to decision. CoRR abs/1707.04609 (2017). arXiv: 1707.04609 http://arxiv. org/abs/1707.04609\\
Gudmund Skovbjerg Frandsen Johan P Hansen and Peter Bro Miltersen. 2001.  Gudmund Skovbjerg Frandsen Johan P Hansen and Peter Bro Miltersen. 2001.\\
\\
\\
\\
\\
\\
\\
\\
\\
Trinh Huynh and Jakob Nordstrom. 2012.  Trinh Huynh and Jakob Nordstrom. 2012.\\
\\
T. S. Jayram Subhash Khot Ravi Kumar and Yuval Rabani. 2004.  T. S. Jayram Subhash Khot Ravi Kumar and Yuval Rabani. 2004.\\
\\
Jan Johannsen. 2001.  Jan Johannsen. 2001.\\
Depth Lower Bounds for Monotone Semi-Unbounded Fan-in Circuits. ITA 35, 3 ( 2001 ), 277--286 . Depth Lower Bounds for Monotone Semi-Unbounded Fan-in Circuits. ITA 35, 3 (2001), 277--286.\\
\\
\\
Eyal Kushilevitz and Noam Nisan. 1997.  Eyal Kushilevitz and Noam Nisan. 1997.\\
Communication Complexity . Cambridge University Press , New York, NY, USA . 0. Communication Complexity. Cambridge University Press, New York, NY, USA. 0.\\
Kasper Green Larsen and Ryan Williams. 2017.  Kasper Green Larsen and Ryan Williams. 2017.\\
Faster online matrix-vector multiplication . In Proceedings of the 28th SODA. 2182--2189 . Faster online matrix-vector multiplication. In Proceedings of the 28th SODA. 2182--2189.\\
Raghu Meka and Toniann Pitassi (Eds.). 2017.  Raghu Meka and Toniann Pitassi (Eds.). 2017.\\
Hardness Escalation in Communication Complexity and Query Complexity , Workshop at 58th FOCS. https: //raghumeka.github.io/workshop.html Hardness Escalation in Communication Complexity and Query Complexity, Workshop at 58th FOCS. https: //raghumeka.github.io/workshop.html\\
\\
\\
\\
\\
Mihai P{\u a}tra{\c s}cu and Mikkel Thorup. 2009.  Mihai P{\u a}tra{\c s}cu and Mikkel Thorup. 2009.\\
\\
\\
\\
\\
Robert Robere Toniann Pitassi Benjamin Rossman and Stephen A Cook. 2016.  Robert Robere Toniann Pitassi Benjamin Rossman and Stephen A Cook. 2016.\\
Exponential lower bounds for monotone span programs . In Proceedings of the 57th FOCS. 406--415 . Exponential lower bounds for monotone span programs. In Proceedings of the 57th FOCS. 406--415.\\
Pranab Sen and Srinivasan Venkatesh. 2008.  Pranab Sen and Srinivasan Venkatesh. 2008.\\
\\
\\
\\
\\
Umesh Vazirani. 1986.  Umesh Vazirani. 1986.\\
\\
Thomas Watson . 2017. A ZPP NP Lifting Theorem. Unpublished preprint ( 2017 ). Thomas Watson. 2017. A ZPP NP Lifting Theorem. Unpublished preprint (2017).\\
Ryan Williams . 2007 . Matrix-vector multiplication in sub-quadratic time (some preprocessing required) . In Proceedings of the SODA , Vol. 7. 995 -- 1001 . Ryan Williams. 2007. Matrix-vector multiplication in sub-quadratic time (some preprocessing required). In Proceedings of the SODA, Vol. 7. 995--1001.\\
\\
David P. Woodruff. 2014.  David P. Woodruff. 2014.\\
\\
Xiaodi Wu , Penghui Yao , and Henry S. Yuen . 2017 . Raz-McKenzie simulation with the inner product gadget . Electronic Colloquium on Computational Complexity (ECCC) 24 (2017), 10 . https://eccc.weizmann.ac.il/report/2017/010 Xiaodi Wu, Penghui Yao, and Henry S. Yuen. 2017. Raz-McKenzie simulation with the inner product gadget. Electronic Colloquium on Computational Complexity (ECCC) 24 (2017), 10. https://eccc.weizmann.ac.il/report/2017/010\\
B. Xiao. 1992.  B. Xiao. 1992.\\
New bounds in cell probe model. Ph.D. Dissertation. UC San Diego.  New bounds in cell probe model. Ph.D. Dissertation. UC San Diego.\\
\\
Penghui Yao . 2015. Parity decision tree complexity and 4-party communication complexity of XOR-functions are polynomially equivalent. arXiv:1506.02936 ( 2015 ). Penghui Yao. 2015. Parity decision tree complexity and 4-party communication complexity of XOR-functions are polynomially equivalent. arXiv:1506.02936 (2015).},
  file = {/Users/tulasi/Zotero/storage/ULNYHRHL/Chattopadhyay et al. - 2018 - Simulation beats richness new data-structure lower bounds.pdf}
}

@article{chattopadhyaySimulationTheoremsVia2017,
  title = {Simulation Theorems via Pseudo-Random Properties},
  author = {Chattopadhyay, A. and Kouck{\'y}, M. and Loff, B. and Mukhopadhyay, Sagnik},
  year = {2017},
  doi = {10.1007/s00037-019-00190-7},
  abstract = {No abstract available},
  citationcount = {44},
  venue = {Computational Complexity}
}

@article{chattopadhyayTowardsStrongerCounterexamples2020,
  title = {Towards Stronger Counterexamples to the Log-Approximate-Rank Conjecture},
  author = {Chattopadhyay, A. and Garg, A. and Sherif, Suhail},
  year = {2020},
  doi = {10.4230/LIPIcs.FSTTCS.2021.13},
  abstract = {We give improved separations for the query complexity analogue of the log-approximate-rank conjecture i.e. we show that there are a plethora of total Boolean functions on n input bits, each of which has approximate Fourier sparsity at most O(n{$^3$}) and randomized parity decision tree complexity {$\Theta$}(n). This improves upon the recent work of Chattopadhyay, Mande and Sherif (JACM '20) both qualitatively (in terms of designing a large number of examples) and quantitatively (improving the gap from quartic to cubic). We leave open the problem of proving a randomized communication complexity lower bound for XOR compositions of our examples. A linear lower bound would lead to new and improved refutations of the log-approximate-rank conjecture. Moreover, if any of these compositions had even a sub-linear cost randomized communication protocol, it would demonstrate that randomized parity decision tree complexity does not lift to randomized communication complexity in general (with the XOR gadget).},
  citationcount = {9},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {communication,communication complexity,lower bound,query,query complexity}
}

@article{chattopadhyayUpperAndLower2016,
  title = {Upper and Lower Bounds on the Power of Advice},
  author = {Chattopadhyay, A. and Edmonds, J. and Ellen, Faith and Pitassi, T.},
  year = {2016},
  doi = {10.1137/15M1031862},
  abstract = {Proving superpolylogarithmic lower bounds for dynamic data structures has remained an open problem despite years of research. P{\v a}trascu proposed an exciting approach for breaking this barrier via a two-player communication model in which one player gets private advice at the beginning of the protocol. He gave reductions from the problem of solving an asymmetric version of set-disjointness in his model to a diverse collection of natural dynamic data structure problems in the cell probe model. He also conjectured that, for any hard problem in the standard two-party communication model, the asymmetric version of the problem is hard in his model, provided not too much advice is given. In this paper, we prove several surprising results about his model. We show that there exist Boolean functions requiring linear randomized communication complexity in the two-party model, for which the asymmetric versions in his model have deterministic protocols with exponentially smaller complexity. For set-disjointness, which ...},
  citationcount = {3},
  venue = {SIAM journal on computing (Print)},
  keywords = {cell probe,communication,communication complexity,data structure,dynamic,lower bound,reduction,sorted}
}

@article{chaudhuriHeavyTailedDistributions2007,
  title = {Heavy-Tailed Distributions and Multi-Keyword Queries},
  author = {Chaudhuri, S. and Church, Kenneth Ward and K{\"o}nig, A. and Sui, L.},
  year = {2007},
  doi = {10.1145/1277741.1277855},
  abstract = {Intersecting inverted indexes is a fundamental operation for many applications in information retrieval and databases. Efficient indexing for this operation is known to be a hard problem for arbitrary data distributions. However, text corpora used in Information Retrieval applications often have convenient power-law constraints (also known as Zipf's Law and long tails) that allow us to materialize carefully chosen combinations of multi-keyword indexes, which significantly improve worst-case performance without requiring excessive storage. These multi-keyword indexes limit the number of postings accessed when computing arbitrary index intersections. Our evaluation on an e-commerce collection of 20 million products shows that the indexes of up to four arbitrary keywords can be intersected while accessing less than 20},
  citationcount = {42},
  venue = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  keywords = {query}
}

@article{chauhanParameterizedAnaloguesOf2014,
  title = {Parameterized Analogues of Probabilistic Computation},
  author = {Chauhan, Ankit and Rao, B. V. Raghavendra},
  year = {2014},
  doi = {10.1007/978-3-319-14974-5_18},
  abstract = {No abstract available},
  citationcount = {10},
  venue = {International Conference on Algorithms and Discrete Applied Mathematics}
}

@article{Chazelle2004,
  title = {Lower Bounds for Intersection Searching and Fractional Cascading in Higher Dimension},
  author = {Chazelle, Bernard and Liu, Ding},
  year = {2004},
  journal = {Journal of Computer and System Sciences},
  volume = {68},
  number = {2},
  pages = {269--284},
  doi = {10.1016/j.jcss.2003.07.003},
  keywords = {lower bound},
  annotation = {P.K. Agarwal, J. Erickson, Geometric range searching and its relatives, in: B. Chazelle, J.E. Goodman, R. Pollack (Eds.), Advances in Discrete and Computational Geometry, Contemporary Mathematics, 223, Amer. Math. Soc., 1999, pp. 1--56.\\
\\
L. Arge, D.E. Vengroff, J.S. Vitter, External-memory algorithms for processing line segments in geographic information systems, Proceedings of the 3rd Annual European Symposium on Algorithm 1995, pp. 295--310.\\
\\
\\
E. Bertino, B. Catania, B. Shidlovsky, Towards optimal indexing for segment databases, Technical report, University of Milano, Italy, 1998.\\
M.M. Buddhikot, S. Suri, M. Waldvogel, Fast layer-4 packet classification using space decomposition techniques, Proceedings of the Protocols for High Speed Networks, Salem, MA, 1999.\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
M.T. Goodrich, Efficient parallel techniques for computational geometry, Ph.D. Thesis, Department of Comput. Science, Purdue University, West Lafayette, IN, 1987.\\
\\
T.V. Lakshman, D. Stiliadis, High-speed policy-based packet forwarding using efficient multi-dimensional range matching, Proc. ACM SIGCOMM (1998), 191--202.},
  file = {/Users/tulasi/Zotero/storage/RR2LSVLQ/Chazelle and Liu - 2004 - Lower bounds for intersection searching and fractional cascading in higher dimension.pdf}
}

@article{chazelleAdvancesInDiscrete1999,
  title = {Advances in Discrete and Computational Geometry},
  author = {Chazelle, B. and Goodman, J. and Pollack, R.},
  year = {1999},
  doi = {10.1090/CONM/223},
  abstract = {Geometric range searching and its relatives by P. K. Agarwal and J. Erickson Deformed products and maximal shadows of polytopes by N. Amenta and G. M. Ziegler Flag complexes, labelled rooted trees, and star shellings by L. J. Billera, C. S. Chan, and N. Liu Discrepancy bounds for geometric set systems with square incidence matrices by B. Chazelle Computational topology by T. K. Dey, H. Edelsbrunner, and S. Guha Recent progress on packing and covering by G. Fejes Toth Acoptic polyhedra by B. Grunbaum A proof of the strict monotone 4-step conjecture by F. Holt and V. Klee Interactions between real algebraic geometry and discrete and computational geometry by I. Itenberg and M.-F. Roy Open problems in the combinatorics of visibility and illumination by J. O'Rourke Halving lines and perfect cross-matchings by J. Pach and J. Solymosi Three-dimensional grid drawings of graphs by J. Pach, T. Thiele, and G. Toth On polygonal covers by M. Pocchiola and G. Vegter The universality theorems for oriented matroids and polytopes by J. Richter-Gebert Periodic and aperiodic tilings of E{$^n$} by M. Senechal The early years of computational geometry-A personal memoir by M. I. Shamos Arrangements of surfaces in higher dimensions by M. Sharir Geometric discrepancy theory by J. Spencer Proof of Reay's conjecture on certain positive-dimensional intersections by H. Tverberg Progress in geometric transversal theory by R. Wenger Recent progress on polytopes by G. M. Ziegler Application challenges to computational geometry (CG impact task force report).},
  citationcount = {191},
  venue = {No venue available}
}

@article{chazelleASpectralApproach1998,
  title = {A Spectral Approach to Lower Bounds with Applications to Geometric Searching},
  author = {Chazelle, B.},
  year = {1998},
  doi = {10.1137/S0097539794275665},
  abstract = {We establish a nonlinear lower bound for halfplane range searching over a group. Specifically, we show that summing up the weights of n (weighted) points within n halfplanes requires {\textohm}(nn) additions and subtractions. This is the first nontrivial lower bound for range searching over a group. By contrast, range searching over a semigroup (which forbids subtractions) is almost completely understood. Our proof has two parts. First, we develop a general, entropy-based method for relating the linear circuit complexity of a linear map A to the spectrum of A\textsuperscript{{$\top$}}A. In the second part of the proof, we design a "high-spectrum" geometric set system for halfplane range searching and, using techniques from discrepancy theory, we estimate the median eigenvalue of its associated map. Interestingly, the method also shows that using up to a linear number of help gates cannot help; these are gates that can compute any bivariate function.},
  citationcount = {30},
  venue = {SIAM journal on computing (Print)}
}

@article{chazelleATraceBound2000,
  title = {A Trace Bound for the Hereditary Discrepancy},
  author = {Chazelle, B. and Lvov, A.},
  year = {2000},
  doi = {10.1145/336154.336179},
  abstract = {AbstractLet A be the incidence matrix of a set system with m sets and n points, m {$\leq$} n , and let t= tr M , where M= ATA . Finally, let {$\sigma$} = tr M2 be the sum of squares of the elements of M . We prove that the hereditary discrepancy of the set system is at least \{{$\frac{1}{4}$}\}\,c\textsuperscript{\{\vphantom\}}n{$\sigma$}/t{$^{2}$}\vphantom\{\}{\textsurd}\{t/n\}, with c=\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{324\} . This general trace bound allows us to resolve discrepancy-type questions for which spectral methods had previously failed. Also, by using this result in conjunction with the spectral lemma for linear circuits, we derive new complexity bounds for range searching.We show that the (red---blue) discrepancy of the set system formed by n points and n lines in the plane is {\textohm}(n1/6) in the worst case and always1{\~O}(n1/6) .We give a simple explicit construction of n points and n halfplanes with hereditary discrepancy {\~{\textohm}}(n1/4) .We show that in any dimension d= {\textohm}( log  n/-1ptlog log  n ) , there is a set system of n points and n axis-parallel boxes in \textbf{R d with discrepancy n{\textohm}(1/-1ptlog log n) .Applying these discrepancy results together with a new variation of the spectral lemma, we derive a lower bound of {\textohm}(nlog  n) on the arithmetic complexity of off-line range searching for points and lines (for nonmonotone circuits). We also prove a lower bound of {\textohm}(nlog  n/-1ptlog log  n) on the complexity of orthogonal range searching in any dimension {\textohm}(log  n/-1ptlog log  n) . 1 We use the notation }{\~O}\textbf{(m) and }{\~{\textohm}}\textbf{(m) as shorthand for O(mlog c m) and {\textohm}(m/-1ptlog c m) , respectively, for some constant c{\textquestiondown}0 .}},
  citationcount = {32},
  venue = {SCG '00}
}

@article{chazelleFilteringSearchA1983,
  title = {Filtering Search: {{A}} New Approach to Query-Answering},
  author = {Chazelle, B.},
  year = {1983},
  doi = {10.1137/0215051},
  abstract = {We introduce a new technique for solving problems of the following form: preprocess a set of objects so that those satisfying a given property with respect to a query object can be listed very effectively. Among well-known problems to fall into this category we find range query, point enclosure, intersection, near-neighbor problems, etc. The approach which we take is very general and rests on a new concept called fitering search. We show on a number of examples how it can be used to improve the complexity of known algorithms and simplify their implementations as well. In particular, filtering search allows us to improve on the worst-case complexity of the best algorithms known so far for solving the problems mentioned above.},
  citationcount = {368},
  venue = {24th Annual Symposium on Foundations of Computer Science (sfcs 1983)}
}

@article{chazelleFractionalCascadingI1986,
  title = {Fractional Cascading: {{I}}. {{A}} Data Structuring Technique},
  author = {Chazelle, B. and Guibas, L.},
  year = {1986},
  doi = {10.1007/BF01840440},
  abstract = {No abstract available},
  citationcount = {308},
  venue = {Algorithmica}
}

@article{chazelleGeometricDiscrepancyRevisited1993,
  title = {Geometric Discrepancy Revisited},
  author = {Chazelle, B.},
  year = {1993},
  doi = {10.1109/SFCS.1993.366848},
  abstract = {Discrepancy theory addresses the general issue of approximating one measure by another one. Originally an offshoot of diophantine approximation theory, the area has expanded into applied mathematics, and now, computer science. Besides providing the theoretical foundation for sampling, it holds some of the keys to understanding the computational power of randomization. A few applications of discrepancy theory are listed. We give elementary algorithms for estimating the discrepancy between various measures arising in practice. We also present a general technique for proving discrepancy lower bounds.<<ETX>>},
  citationcount = {16},
  venue = {Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science}
}

@article{chazelleGeometricSearchingRationals1999,
  title = {Geometric Searching over the Rationals},
  author = {Chazelle, B.},
  year = {1999},
  doi = {10.1007/3-540-48481-7_31},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Embedded Systems and Applications}
}

@article{chazelleLowerBoundsComplexity1989,
  title = {{Lower bounds on the complexity of polytope range searching}},
  author = {Chazelle, Bernard},
  year = {1989},
  month = oct,
  journal = {Journal of the American Mathematical Society},
  volume = {2},
  number = {4},
  pages = {637--666},
  publisher = {American Mathematical Society},
  issn = {0894-0347},
  doi = {10.1090/S0894-0347-1989-1001852-0},
  url = {https://collaborate.princeton.edu/en/publications/lower-bounds-on-the-complexity-of-polytope-range-searching},
  urldate = {2024-11-19},
  langid = {English (US)},
  keywords = {lower bound,query,query time},
  annotation = {M. Abramowitz and I. A. Stegun, Handbook of mathematical functions, Dover, New York, 1970.\\
\\
\\
B. Chazelle, Lower bounds on the complexity of multidimensional searching, Proc. 27th Annual IEEE Sympos. on Foundations of Computer Science, IEEE, New York, 1986, pp. 87-96.\\
\\
\\
H. Edelsbrunner and E. Welzl, Halfplanar range search in linear space and {$O$}({$n$}{\textasciicircum}\{0.695\}) query time, Inform. Process. Lett. 23 (1986), 289-293.\\
\\
\\
\\
\\
J. Koml{\'o}s, E. Szemer{\'e}di, and J. Pintz, On Heilbronn's triangle problem, J. London Math. Soc. (2) 24 (1981), 385-396.\\
\\
\\
\\
\\
\\
\\
\\
F. F. Yao, A 3-space partition and its applications, Proc. 15th Annual ACM Sympos. on Theory of Comput., ACM, New York, 1983, pp. 258-263.\\
A. C. Yao and F. F. Yao, On computing the rank function for a set of vectors, Report No. UIUCDCS-R-75-699, Univ. of Illinois at Urbana-Champaign, 1975.\\
{\textbackslash}bysame, A general approach to {$d$}-dimensional geometric queries, Proc. 17th Annual ACM Sympos. on Theory of Comput., ACM, New York, 1985, pp. 163-168.},
  file = {/Users/tulasi/Zotero/storage/5YG8N37T/Chazelle - 1989 - Lower bounds on the complexity of polytope range searching.pdf}
}

@article{chazelleLowerBoundsFor1990,
  title = {Lower Bounds for Orthogonal Range Searching: Part {{II}}. {{The}} Arithmetic Model},
  author = {Chazelle, B.},
  year = {1990},
  doi = {10.1145/79147.79149},
  abstract = {Lower bounds on the complexity of orthogonal range searching in thestatic case are established. Specifically, we consider the followingdominance search problem: Given a collection of{\textexclamdown}?Pub Fmt italic{\textquestiondown}n{\textexclamdown}?Pub Fmt /italic{\textquestiondown} weighted points in{\textexclamdown}?Pub Fmt italic{\textquestiondown}d{\textexclamdown}?Pub Fmt /italic{\textquestiondown}-space and a query point{\textexclamdown}?Pub Fmt italic{\textquestiondown}q{\textexclamdown}?Pub Fmt /italic{\textquestiondown}, compute the cumulative weight ofthe points dominated (in all coordinates) by{\textexclamdown}?Pub Fmt italic{\textquestiondown}q{\textexclamdown}?Pub Fmt /italic{\textquestiondown}. It is assumed that the weights arechosen in a commutative semigroup and that the query time measures onlythe number of arithmetic operations needed to compute the answer. It isproved that if {\textexclamdown}?Pub Fmt italic{\textquestiondown}m{\textexclamdown}?Pub Fmt /italic{\textquestiondown} units of storage areavailable, then the query time is at least proportional to (log{\textexclamdown}?Pub Fmt italic{\textquestiondown}n{\textexclamdown}?Pub Fmt /italic{\textquestiondown}/log(2{\textexclamdown}?Pub Fmt italic{\textquestiondown}m{\textexclamdown}?Pub Fmt /italic{\textquestiondown}/{\textexclamdown}?Pub Fmt italic{\textquestiondown}n{\textexclamdown}?Pub Fmt /italic{\textquestiondown})){\textexclamdown}supscrpt{\textquestiondown}{\textexclamdown}?Pub Fmt italic{\textquestiondown}d{\textexclamdown}?Pub Fmt /italic{\textquestiondown}--{\textexclamdown}?Pub Caret1{\textquestiondown}1{\textexclamdown}/supscrpt{\textquestiondown}in both the worst and average cases. This lower bound is provably tightfor {\textexclamdown}?Pub Fmt italic{\textquestiondown}m{\textexclamdown}?Pub Fmt /italic{\textquestiondown} =\&OHgr;({\textexclamdown}?Pub Fmt italic{\textquestiondown}n{\textexclamdown}?Pub Fmt /italic{\textquestiondown}(log{\textexclamdown}?Pub Fmt italic{\textquestiondown}n{\textexclamdown}?Pub Fmt /italic{\textquestiondown}){\textexclamdown}supscrpt{\textquestiondown}{\textexclamdown}?Pub Fmt italic{\textquestiondown}d{\textexclamdown}?Pub Fmt /italic{\textquestiondown}--1+{$\varepsilon$}){\textexclamdown}/supscrpt{\textquestiondown} and any fixed {$\varepsilon$}{\textquestiondown} 0. A lower bound of \&OHgr;({\textexclamdown}?Pub Fmt italic{\textquestiondown}n{\textexclamdown}?Pub Fmt /italic{\textquestiondown}/loglog{\textexclamdown}?Pub Fmt italic{\textquestiondown}n{\textexclamdown}?Pub Fmt /italic{\textquestiondown}){\textexclamdown}supscrpt{\textquestiondown}{\textexclamdown}?Pub Fmt italic{\textquestiondown}d{\textexclamdown}?Pub Fmt /italic{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown})on the time required for executing {\textexclamdown}?Pub Fmt italic{\textquestiondown}n{\textexclamdown}?Pub Fmt /italic{\textquestiondown}inserts and queries is also established. {\textexclamdown}abstrbyl{\textquestiondown}{\textexclamdown}italic{\textquestiondown}---Author's Abstract{\textexclamdown}/italic{\textquestiondown}},
  citationcount = {128},
  venue = {JACM}
}

@article{chazelleLowerBoundsFor1995,
  title = {Lower Bounds for Off-Line Range Searching},
  author = {Chazelle, B.},
  year = {1995},
  doi = {10.1145/225058.225291},
  abstract = {This paper proves three lower bounds for variants of the following rangesearching problem: Given n weighted points inRd andn axis-parallel boxes, compute the sum of the weights within each box: (1) if both additions and subtractions are allowed, we prove that {\textohm}(n log logn) is a lower bound on the number of arithmetic operations; (2) if subtractions are disallowed the lower bound becomes {\textohm}(n(logn/loglogn)d-1), which is nearly optimal; (3) finally, for the case where boxes are replaced by simplices, we establish a quasi-optimal lower bound of {\textohm}(n2-2/(d+1))/polylog(n).},
  citationcount = {53},
  venue = {Symposium on the Theory of Computing}
}

@article{chazelleQuasiOptimalRange1989,
  title = {Quasi-Optimal Range Searching in Spaces of Finite {{VC-dimension}}},
  author = {Chazelle, B. and Welzl, E.},
  year = {1989},
  doi = {10.1007/BF02187743},
  abstract = {No abstract available},
  citationcount = {198},
  venue = {Discrete \& Computational Geometry}
}

@article{chazelleQuasiOptimalUpper1990,
  title = {Quasi-Optimal Upper Bounds for Simplex Range Searching and New Zone Theorems},
  author = {Chazelle, B. and Sharir, M. and Welzl, E.},
  year = {1990},
  doi = {10.1007/BF01758854},
  abstract = {No abstract available},
  citationcount = {150},
  venue = {SCG '90}
}

@article{chazelleSimplexRangeReporting1996,
  title = {Simplex Range Reporting on a Pointer Machine},
  author = {Chazelle, B. and Rosenberg, B.},
  year = {1996},
  journal = {Computational Geometry: Theory and Applications},
  volume = {5},
  pages = {237--247},
  doi = {10.1016/0925-7721(95)00002-X},
  file = {/Users/tulasi/Zotero/storage/8NETZRWI/Chazelle and Rosenberg - 1996 - Simplex range reporting on a pointer machine.pdf}
}

@article{chazelleTheDiscrepancyMethod1998,
  title = {The Discrepancy Method},
  author = {Chazelle, B.},
  year = {1998},
  doi = {10.1007/3-540-49381-6_1},
  abstract = {No abstract available},
  citationcount = {472},
  venue = {International Symposium on Algorithms and Computation}
}

@article{chechikImprovedDistanceOracles2011,
  title = {Improved Distance Oracles and Spanners for Vertex-Labeled Graphs},
  author = {Chechik, S.},
  year = {2011},
  doi = {10.1007/978-3-642-33090-2_29},
  abstract = {No abstract available},
  citationcount = {19},
  venue = {Embedded Systems and Applications}
}

@article{chechikNearOptimalAlgorithms2019,
  title = {Near Optimal Algorithms for the Single Source Replacement Paths Problem},
  author = {Chechik, S. and Cohen, S.},
  year = {2019},
  doi = {10.1137/1.9781611975482.126},
  abstract = {The Single Source Replacement Paths (SSRP) problem is as follows; Given a graph G = ( V, E ), a source vertex s and a shortest paths tree T s rooted in s , output for every vertex t {$\in$} V and for every edge e in T s the length of the shortest path from s to t avoiding e . We present near optimal upper bounds, by providing {\texttildelow} O ( m {\textsurd} n + n 2 ) time randomized combinatorial algorithm 1 for unweighted undirected graphs, and matching conditional lower bounds for the SSRP problem.},
  citationcount = {16},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {lower bound}
}

@article{chechikNearOptimalApproximate2018,
  title = {Near-Optimal Approximate Decremental All Pairs Shortest Paths},
  author = {Chechik, S.},
  year = {2018},
  doi = {10.1109/FOCS.2018.00025},
  abstract = {In this paper we consider the decremental approximate all-pairs shortest paths (APSP) problem, where given a graph G the goal is to maintain approximate shortest paths between all pairs of nodes in G under a sequence of online adversarial edge deletions. We present a decremental APSP algorithm for undirected weighted graphs with (2+{$\varepsilon$})k-1 stretch, O(m n{\textasciicircum}1/k +o(1) log(n W)) total update time and O(loglog(n W)) query time for a fixed constant {$\varepsilon$}, where W is the maximum edge weight (assuming the minimum edge weight is 1) and k is any integer parameter. This is an exponential improvement both in the stretch and in the query time over previous works.},
  citationcount = {40},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {query,query time,update,update time}
}

@article{chenAlmostLinearTime2023,
  title = {Almost-Linear Time Algorithms for Incremental Graphs: {{Cycle}} Detection, Sccs, s-t Shortest Path, and Minimum-Cost Flow},
  author = {Chen, Li and Kyng, Rasmus and Liu, Yang P. and Meierhans, Simon and Gutenberg, Maximilian Probst},
  year = {2023},
  doi = {10.1145/3618260.3649745},
  abstract = {We give the first almost-linear time algorithms for several problems in incremental graphs including cycle detection, strongly connected component maintenance, s-t shortest path, maximum flow, and minimum-cost flow. To solve these problems, we give a deterministic data structure that returns a mo(1)-approximate minimum-ratio cycle in fully dynamic graphs in amortized mo(1) time per update. Combining this with the interior point method framework of Brand-Liu-Sidford (STOC 2023) gives the first almost-linear time algorithm for deciding the first update in an incremental graph after which the cost of the minimum-cost flow attains value at most some given threshold F. By rather direct reductions to minimum-cost flow, we are then able to solve the problems in incremental graphs mentioned above. Our new data structure also leads to a modular and deterministic almost-linear time algorithm for minimum-cost flow by removing the need for complicated modeling of a restricted adversary, in contrast to the recent randomized and deterministic algorithms for minimum-cost flow in Chen-Kyng-Liu-Peng-Probst Gutenberg-Sachdeva (FOCS 2022)Brand-Chen-Kyng-Liu-Peng-Probst Gutenberg-Sachdeva-Sidford (FOCS 2023). At a high level, our algorithm dynamizes the {$\ell$}1 oblivious routing of Rozho{\v n}-Grunau-Haeupler-Zuzic-Li (STOC 2022), and develops a method to extract an approximate minimum ratio cycle from the structure of the oblivious routing. To maintain the oblivious routing, we use tools from concurrent work of Kyng-Meierhans-Probst Gutenberg (STOC 2024) which designed vertex sparsifiers for shortest paths, in order to maintain a sparse neighborhood cover in fully dynamic graphs. To find a cycle, we first show that an approximate minimum ratio cycle can be represented as a fundamental cycle on a small set of trees resulting from the oblivious routing. Then, we find a cycle whose quality is comparable to the best tree cycle. This final cycle query step involves vertex and edge sparsification procedures reminiscent of the techniques introduced in Chen-Kyng-Liu-Peng-Probst Gutenberg-Sachdeva (FOCS 2022), but crucially requires a more powerful dynamic spanner, which can handle far more edge insertions than prior work. We build such a spanner via a construction that hearkens back to the classic greedy spanner algorithm of Alth{\"o}fer-Das-Dobkin-Joseph-Soares (DiscreteComputational Geometry 1993).},
  citationcount = {14},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,dynamic,query,reduction,update}
}

@article{chenAnEquivalenceClass2018,
  title = {An Equivalence Class for Orthogonal Vectors},
  author = {Chen, Lijie and Williams, Ryan},
  year = {2018},
  doi = {10.1137/1.9781611975482.2},
  abstract = {The Orthogonal Vectors problem (\{OV\}) asks: given n vectors in  0,1 \textsuperscript{\{\vphantom\}}O(n)\vphantom\{\}, are two of them orthogonal? \{OV\} is easily solved in O(n{$^2$}n) time, and it is a central problem in fine-grained complexity: dozens of conditional lower bounds are based on the popular hypothesis that \{OV\} cannot be solved in (say) n\textsuperscript{\{\vphantom\}}1.99\vphantom\{\} time. However, unlike the APSP problem, few other problems are known to be non-trivially equivalent to \{OV\}. We show \{OV\} is truly-subquadratic equivalent to several fundamental problems, all of which (a priori) look harder than \{OV\}. A partial list is given below: (\{Min-IP\}/\{Max-IP\}) Find a red-blue pair of vectors with minimum (respectively, maximum) inner product, among n vectors in  0,1 \textsuperscript{\{\vphantom\}}O(n)\vphantom\{\}. (\{Exact-IP\}) Find a red-blue pair of vectors with inner product equal to a given target integer, among n vectors in  0,1 \textsuperscript{\{\vphantom\}}O(n)\vphantom\{\}. (\{Apx-Min-IP\}/\{Apx-Max-IP\}) Find a red-blue pair of vectors that is a 100-approximation to the minimum (resp. maximum) inner product, among n vectors in  0,1 \textsuperscript{\{\vphantom\}}O(n)\vphantom\{\}. (Approx. \{Bichrom.-{$\ell\_$}p-Closest-Pair\}) Compute a (1+{\textohm}(1))-approximation to the {$\ell_p$}-closest red-blue pair (for a constant p{$\in$}[1,2]), among n points in \{R\}\textsuperscript{d}, d{$\leq$}n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\}. (Approx. \{{$\ell\_$}p-Furthest-Pair\}) Compute a (1+{\textohm}(1))-approximation to the {$\ell_p$}-furthest pair (for a constant p{$\in$}[1,2]), among n points in \{R\}\textsuperscript{d}, d{$\leq$}n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\}. We also show that there is a \{poly\}(n) space, n\textsuperscript{\{\vphantom\}}1-{$\epsilon$}\vphantom\{\} query time data structure for Partial Match with vectors from  0,1 \textsuperscript{\{\vphantom\}}O(n)\vphantom\{\} if and only if such a data structure exists for 1+{\textohm}(1) Approximate Nearest Neighbor Search in Euclidean space.},
  citationcount = {44},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,lower bound,query,query time}
}

@article{chenDistanceQueriesOver2024,
  title = {Distance Queries over Dynamic Interval Graphs},
  author = {Chen, Jingbang and He, Meng and Munro, J. I. and Peng, Richard and Wu, Kaiyu and Zhang, Daniel J.},
  year = {2024},
  doi = {10.4230/LIPIcs.ISAAC.2023.18},
  abstract = {We design the first dynamic distance oracles for interval graphs, which are intersection graphs of a set of intervals on the real line, and for proper interval graphs, which are intersection graphs of a set of intervals in which no interval is properly contained in another. For proper interval graphs, we design a linear space data structure which supports distance queries (computing the distance between two query vertices) and vertex insertion or deletion in O (lg n ) worst-case time, where n is the number of vertices currently in G . Under incremental (insertion only) or decremental (deletion only) settings, we design linear space data structures that support distance queries in O (lg n ) worst-case time and vertex insertion or deletion in O (lg n ) amortized time, where n is the maximum number of vertices in the graph. Under fully dynamic settings, we design a data structure that represents an interval graph G in O ( n ) words of space to support distance queries in O ( n lg n/S ( n )) worst-case time and vertex insertion or deletion in O ( S ( n ) + lg n ) worst-case time, where n is the number of vertices currently in G and S ( n ) is an arbitrary function that satisfies S ( n ) = {\textohm}(1) and S ( n ) = O ( n ). This implies an O ( n )-word solution with O ( {\textsurd} n lg n )-time support for both distance queries and updates. All four data structures can answer shortest path queries by reporting the vertices in the shortest path between two query vertices in O (lg n ) worst-case time per vertex. We also study the hardness of supporting distance queries under updates over an intersection graph of 3D axis-aligned line segments, which generalizes our problem to 3D. Finally, we solve the problem of computing the diameter of a dynamic connected interval graph.},
  citationcount = {1},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {data structure,dynamic,query,update}
}

@article{chenEfficientFullyDynamic2020,
  title = {Efficient Fully Dynamic Elimination Forests with Applications to Detecting Long Paths and Cycles},
  author = {Chen, Jiehua and Czerwi'nski, Wojciech and Disser, Y. and Feldmann, A. and Hermelin, D. and Nadara, Wojciech and Pilipczuk, Michal and Pilipczuk, Marcin and Sorge, Manuel and Wr'oblewski, Bartlomiej and {Zych-Pawlewicz}, Anna},
  year = {2020},
  doi = {10.1137/1.9781611976465.50},
  abstract = {We present a data structure that in a dynamic graph of treedepth at most d, which is modified over time by edge insertions and deletions, maintains an optimum-height elimination forest. The data structure achieves worst-case update time 2\textsuperscript{\{\vphantom\}}\{O\}(d{$^2$})\vphantom\{\}, which matches the best known parameter dependency in the running time of a static fpt algorithm for computing the treedepth of a graph. This improves a result of Dvo{\v r}ak et al. [ESA 2014], who for the same problem achieved update time f(d) for some non-elementary (i.e. tower-exponential) function f. As a by-product, we improve known upper bounds on the sizes of minimal obstructions for having treedepth d from doubly-exponential in d to d\textsuperscript{\{\vphantom\}}\{O\}(d)\vphantom\{\}. As applications, we design new fully dynamic parameterized data structures for detecting long paths and cycles in general graphs. More precisely, for a fixed parameter k and a dynamic graph G, modified over time by edge insertions and deletions, our data structures maintain answers to the following queries: - Does G contain a simple path on k vertices? - Does G contain a simple cycle on at least k vertices? In the first case, the data structure achieves amortized update time 2\textsuperscript{\{\vphantom\}}\{O\}(k{$^2$})\vphantom\{\}. In the second case, the amortized update time is 2\textsuperscript{\{\vphantom\}}\{O\}(k{$^4$})\vphantom\{\}+\{O\}(kn). In both cases we assume access to a dictionary on the edges of G.},
  citationcount = {16},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,query,static,update,update time}
}

@article{chenErrorCorrectingData2013,
  title = {Error-Correcting Data Structures},
  author = {Chen, Victor and Grigorescu, Elena and {de Wolf}, Ronald},
  year = {2013},
  doi = {10.1137/110834949},
  abstract = {We study data structures in the presence of adversarial noise. We want to encode a given object in a succinct data structure that enables us to efficiently answer specific queries about the object, even if the data structure has been corrupted by a constant fraction of errors. We measure the efficiency of a data structure in terms of its length (the number of bits in its representation) and query-answering time, measured by the number of bit-probes to the (possibly corrupted) representation. The main issue is the trade-off between these two. This new model is the common generalization of (static) data structures and locally decodable error-correcting codes (LDCs). We prove a number of upper and lower bounds on various natural error-correcting data structure problems. In particular, we show that the optimal length of t-probe error-correcting data structures for the Membership problem (where we want to store subsets of size s from a universe of size n such that membership queries can be answered effic...},
  citationcount = {22},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,lower bound,query,static}
}

@article{chenFastDynamicCuts2020,
  title = {Fast Dynamic Cuts, Distances and Effective Resistances via Vertex Sparsifiers},
  author = {Chen, L. and Goranci, Gramoz and Henzinger, Monika and Peng, Richard and Saranurak, Thatchaphol},
  year = {2020},
  doi = {10.1109/FOCS46700.2020.00109},
  abstract = {We present a general framework of designing efficient dynamic approximate algorithms for optimization problems on undirected graphs. In particular, we develop a technique that, given any problem that admits a certain notion of vertex sparsifiers, gives data structures that maintain approximate solutions in sub-linear update and query time. We illustrate the applicability of our paradigm to the following problems. (1)A fully-dynamic algorithm that approximates all-pair maximum-flows/minimum-cuts up to a nearly logarithmic factor in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}) 11The O\vphantom\{\}({$\cdot$}) notation is used in this paper to hide poly-logarithmic factors. amortized time against an oblivious adversary, and O\vphantom\{\}(m\textsuperscript{\{\vphantom\}}3/4\vphantom\{\}) time against an adaptive adversary. (2)An incremental data structure that maintains O(1) - approximate shortest path in n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} time per operation, as well as fully dynamic approximate all-pair shortest path and transshipment in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2/3+o(1)\vphantom\{\}) amortized time per operation. (3)A fully-dynamic algorithm that approximates all-pair effective resistance up to an (1+{$\epsilon$}) factor in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2/3+o(1)\vphantom\{\}{$\epsilon$}\textsuperscript{\{\vphantom\}}-O(1)\vphantom\{\}) amortized update time per operation. The key tool behind result (1) is the dynamic maintenance of an algorithmic construction due to Madry [FOCS' 10], which partitions a graph into a collection of simpler graph structures (known as j-trees) and approximately captures the cut-flow and metric structure of the graph. The O(1)-approximation guarantee of (2) is by adapting the distance oracles by [Thorup-Zwick JACM '05]. Result (3) is obtained by invoking the random-walk based spectral vertex sparsifier by [Durfee et al. STOC '19] in a hierarchical manner, while carefully keeping track of the recourse among levels in the hierarchy. See https://arxiv.org/pdf/2005.02368.pdf for the full version of this paper.},
  citationcount = {39},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {adaptive,data structure,dynamic,query,query time,update,update time}
}

@article{chenFineGrainedComplexity2018,
  title = {Fine-Grained Complexity Meets {{IP}} = {{PSPACE}}},
  author = {Chen, Lijie and Goldwasser, S. and Lyu, Kaifeng and Rothblum, G. and Rubinstein, A.},
  year = {2018},
  doi = {10.1137/1.9781611975482.1},
  abstract = {In this paper we study the fine-grained complexity of finding exact and approximate solutions to problems in P. Our main contribution is showing reductions from exact to approximate solution for a host of such problems. As one (notable) example, we show that the Closest-LCS-Pair problem (Given two sets of strings A and B, compute exactly the maximum \{LCS\}(a,b) with (a,b){$\in$}A{\texttimes}B) is equivalent to its approximation version (under near-linear time reductions, and with a constant approximation factor). More generally, we identify a class of problems, which we call BP-Pair-Class, comprising both exact and approximate solutions, and show that they are all equivalent under near-linear time reductions. Exploring this class and its properties, we also show: {$\bullet$} Under the NC-SETH assumption (a significantly more relaxed assumption than SETH), solving any of the problems in this class requires essentially quadratic time. {$\bullet$} Modest improvements on the running time of known algorithms (shaving log factors) would imply that NEXP is not in non-uniform \{NC\}{$^1$}. {$\bullet$} Finally, we leverage our techniques to show new barriers for deterministic approximation algorithms for LCS. At the heart of these new results is a deep connection between interactive proof systems for bounded-space computations and the fine-grained complexity of exact and approximate solutions to problems in P. In particular, our results build on the proof techniques from the classical IP = PSPACE result.},
  citationcount = {36},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {reduction}
}

@article{chengOpenMapsBehavioural1996,
  title = {Open Maps, Behavioural Equivalences, and Congruences},
  author = {Cheng, A. and Nielsen, M.},
  year = {1996},
  doi = {10.1016/S0304-3975(97)00085-6},
  abstract = {No abstract available},
  citationcount = {18},
  venue = {Theoretical Computer Science}
}

@article{chenLempelZivFactorization2008,
  title = {Lempel--Ziv Factorization Using Less Time \& Space},
  author = {Chen, Gang and Puglisi, S. and Smyth, W. F.},
  year = {2008},
  doi = {10.1007/S11786-007-0024-4},
  abstract = {No abstract available},
  citationcount = {57},
  venue = {Mathematics and Computer Science}
}

@article{chenLinearFptReductions2004,
  title = {Linear {{FPT}} Reductions and Computational Lower Bounds},
  author = {Chen, Jianer and Huang, Xiuzhen and Kanj, Iyad A. and Xia, Ge},
  year = {2004},
  doi = {10.1145/1007352.1007391},
  abstract = {We develop new techniques for deriving very strong computational lower bounds for a class of well-known NP-hard problems, including weighted satisfiability, dominating set, hitting set, set cover, clique, and independent set. For example, although a trivial enumeration can easily test in time O(nk) if a given graph of n vertices has a clique of size k, we prove that unless an unlikely collapse occurs in parameterized complexity theory, the problem is not solvable in time f(k) no(k) for any function f, even if we restrict the parameter value k to be bounded by an arbitrarily small function of n. Under the same assumption, we prove that even if we restrict the parameter values k to be {$\Theta$}({$\mu$}(n)) for any reasonable function {$\mu$}, no algorithm of running time no(k) can test if a graph of n vertices has a clique of size k. Similar strong lower bounds are also derived for other problems in the above class. Our techniques can be extended to derive computational lower bounds on approximation algorithms for NP-hard optimization problems. For example, we prove that the NP-hard distinguishing substring selection problem, for which a polynomial time approximation scheme has been recently developed, has no polynomial time approximation schemes of running time f(1/{$\varepsilon$})no(1/{$\varepsilon$}) for any function f unless an unlikely collapse occurs in parameterized complexity theory.},
  citationcount = {122},
  venue = {Symposium on the Theory of Computing}
}

@article{chenMaximumFlowAnd2022,
  title = {Maximum Flow and Minimum-Cost Flow in Almost-Linear Time},
  author = {Chen, L. and Kyng, Rasmus and Liu, Yang P. and Peng, Richard and Gutenberg, Maximilian Probst and Sachdeva, Sushant},
  year = {2022},
  doi = {10.1109/FOCS54457.2022.00064},
  abstract = {We give an algorithm that computes exact maximum flows and minimum-cost flows on directed graphs with m edges and polynomially bounded integral demands, costs, and capacities in m\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\} time. Our algorithm builds the flow through a sequence of m\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\} approximate undirected minimum-ratio cycles, each of which is computed and processed in amortized m\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} time using a new dynamic graph data structure. Our framework extends to algorithms running in m\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\} time for computing flows that minimize general edge-separable convex functions to high accuracy. This gives almost-linear time algorithms for several problems including entropy-regularized optimal transport, matrix scaling, p-norm flows, and p-norm isotonic regression on arbitrary directed acyclic graphs.},
  citationcount = {222},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{chenNearlyOptimalSeparation2018,
  title = {Nearly Optimal Separation between Partially and Fully Retroactive Data Structures},
  author = {Chen, Lijie and Demaine, E. and Gu, Yuzhou and Williams, V. V. and Xu, Yinzhan and Yu, Yuancheng},
  year = {2018},
  doi = {10.4230/LIPIcs.SWAT.2018.33},
  abstract = {Since the introduction of retroactive data structures at SODA 2004, a major unsolved problem has been to bound the gap between the best partially retroactive data structure (where changes can be made to the past, but only the present can be queried) and the best fully retroactive data structure (where the past can also be queried) for any problem. It was proved in 2004 that any partially retroactive data structure with operation time T(n,m) can be transformed into a fully retroactive data structure with operation time O({\textsurd}\{m\}{$\cdot$}T(n,m)), where n is the size of the data structure and m is the number of operations in the timeline [Demaine 2004], but it has been open for 14 years whether such a gap is necessary. In this paper, we prove nearly matching upper and lower bounds on this gap for all n and m. We improve the upper bound for n{$\ll$}{\textsurd}m by showing a new transformation with multiplicative overhead nm. We then prove a lower bound of  nm,{\textsurd}m \textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\} assuming any of the following conjectures: - Conjecture I: Circuit SAT requires 2\textsuperscript{\{\vphantom\}}n-o(n)\vphantom\{\} time on n-input circuits of size 2\textsuperscript{\{\vphantom\}}o(n)\vphantom\{\}. (Far weaker than the well-believed SETH conjecture, which asserts that CNF SAT with n variables and O(n) clauses already requires 2\textsuperscript{\{\vphantom\}}n-o(n)\vphantom\{\} time.) - Conjecture II: Online (,+) product between an integer n{\texttimes}n matrix and n vectors requires n\textsuperscript{\{\vphantom\}}3-o(1)\vphantom\{\} time. - Conjecture III (3-SUM Conjecture): Given three sets A,B,C of integers, each of size n, deciding whether there exist a{$\in$}A,b{$\in$}B,c{$\in$}C such that a+b+c=0 requires n\textsuperscript{\{\vphantom\}}2-o(1)\vphantom\{\} time. Our lower bound construction illustrates an interesting power of fully retroactive queries: they can be used to quickly solve batched pair evaluation. We believe this technique can prove useful for other data structure lower bounds, especially dynamic ones.},
  citationcount = {8},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {data structure,dynamic,lower bound,query}
}

@article{chenOnIrregularitiesOf1980,
  title = {On Irregularities of Distribution.},
  author = {Chen, William W. L.},
  year = {1980},
  doi = {10.1112/S0025579300010044},
  abstract = {{\S}},
  citationcount = {546},
  venue = {No venue available}
}

@article{chenOnTheHardness2018,
  title = {On the Hardness of Approximate and Exact (Bichromatic) Maximum Inner Product},
  author = {Chen, Lijie},
  year = {2018},
  doi = {10.4230/LIPIcs.CCC.2018.14},
  abstract = {In this paper we study the (Bichromatic) Maximum Inner Product Problem (Max-IP), in which we are given sets A and B of vectors, and the goal is to find a{$\in$}A and b{$\in$}B maximizing inner product a{$\cdot$}b. Max-IP is very basic and serves as the base problem in the recent breakthrough of [Abboud et al., FOCS 2017] on hardness of approximation for polynomial-time problems. It is also used (implicitly) in the argument for hardness of exact {$\ell_2$}-Furthest Pair (and other important problems in computational geometry) in poly-log-log dimensions in [Williams, SODA 2018]. We have three main results regarding this problem. First, we study the best multiplicative approximation ratio for Boolean Max-IP in sub-quadratic time. We show that, for Max-IP with two sets of n vectors from  0,1 \textsuperscript{\{\vphantom\}}d\vphantom\{\}, there is an n\textsuperscript{\{\vphantom\}}2-{\textohm}(1)\vphantom\{\} time (d/n)\textsuperscript{\{\vphantom\}}{\textohm}(1)\vphantom\{\}-multiplicative-approximating algorithm, and we show this is conditionally optimal, as such a (d/n)\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\}-approximating algorithm would refute SETH. Second, we achieve a similar characterization for the best additive approximation error to Boolean Max-IP. We show that, for Max-IP with two sets of n vectors from  0,1 \textsuperscript{\{\vphantom\}}d\vphantom\{\}, there is an n\textsuperscript{\{\vphantom\}}2-{\textohm}(1)\vphantom\{\} time {\textohm}(d)-additive-approximating algorithm, and this is conditionally optimal, as such an o(d)-approximating algorithm would refute SETH [Rubinstein, STOC 2018]. Last, we revisit the hardness of solving Max-IP exactly for vectors with integer entries. We show that, under SETH, for Max-IP with sets of n vectors from \{Z\}\textsuperscript{\{\vphantom\}}d\vphantom\{\} for some d=2\textsuperscript{\{\vphantom\}}O(\textsuperscript{\{\vphantom\}}*\vphantom\{\}n)\vphantom\{\}, every exact algorithm requires n\textsuperscript{\{\vphantom\}}2-o(1)\vphantom\{\} time. With the reduction from [Williams, SODA 2018], it follows that {$\ell_2$}-Furthest Pair and Bichromatic {$\ell_2$}-Closest Pair in 2\textsuperscript{\{\vphantom\}}O(\textsuperscript{\{\vphantom\}}*\vphantom\{\}n)\vphantom\{\} dimensions require n\textsuperscript{\{\vphantom\}}2-o(1)\vphantom\{\} time.},
  citationcount = {57},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {reduction}
}

@article{chenOnTheRange2004,
  title = {On the Range Maximum-Sum Segment Query Problem},
  author = {Chen, Kuan-Yu and Chao, K.},
  year = {2004},
  doi = {10.1016/j.dam.2007.05.018},
  abstract = {No abstract available},
  citationcount = {42},
  venue = {Discrete Applied Mathematics}
}

@article{chenSettlingTheComplexity2007,
  title = {Settling the Complexity of Computing Two-Player {{Nash}} Equilibria},
  author = {Chen, Xi and Deng, Xiaotie and Teng, S.},
  year = {2007},
  doi = {10.1145/1516512.1516516},
  abstract = {We prove that Bimatrix, the problem of finding a Nash equilibrium in a two-player game, is complete for the complexity class PPAD (Polynomial Parity Argument, Directed version) introduced by Papadimitriou in 1991. Our result, building upon the work of Daskalakis et al. [2006a] on the complexity of four-player Nash equilibria, settles a long standing open problem in algorithmic game theory. It also serves as a starting point for a series of results concerning the complexity of two-player Nash equilibria. In particular, we prove the following theorems: ---Bimatrix does not have a fully polynomial-time approximation scheme unless every problem in PPAD is solvable in polynomial time. ---The smoothed complexity of the classic Lemke-Howson algorithm and, in fact, of any algorithm for Bimatrix is not polynomial unless every problem in PPAD is solvable in randomized polynomial time. Our results also have a complexity implication in mathematical economics: ---Arrow-Debreu market equilibria are PPAD-hard to compute.},
  citationcount = {608},
  venue = {JACM}
}

@article{chenSparseGamesAre2006,
  title = {Sparse Games Are Hard},
  author = {Chen, Xi and Deng, Xiaotie and Teng, S.},
  year = {2006},
  doi = {10.1007/11944874_24},
  abstract = {No abstract available},
  citationcount = {48},
  venue = {Workshop on Internet and Network Economics}
}

@article{chenSymmetricSparseBoolean2021,
  title = {Symmetric Sparse Boolean Matrix Factorization and Applications},
  author = {Chen, Sitan and Song, Zhao and Tao, Runzhou and Zhang, Ruizhe},
  year = {2021},
  doi = {10.4230/LIPIcs.ITCS.2022.46},
  abstract = {In this work, we study a variant of nonnegative matrix factorization where we wish to find a symmetric factorization of a given input matrix into a sparse, Boolean matrix. Formally speaking, given \{M\}{$\in$}\{Z\}\textsuperscript{\{\vphantom\}}m{\texttimes}m\vphantom\{\}, we want to find \{W\}{$\in$} 0,1 \textsuperscript{\{\vphantom\}}m{\texttimes}r\vphantom\{\} such that \{M\}-\{W\}\{W\}\textsuperscript{{$\top$}}{$_0$} is minimized among all \{W\} for which each row is k-sparse. This question turns out to be closely related to a number of questions like recovering a hypergraph from its line graph, as well as reconstruction attacks for private neural network training. As this problem is hard in the worst-case, we study a natural average-case variant that arises in the context of these reconstruction attacks: \{M\}=\{W\}\{W\}\textsuperscript{\{\vphantom\}}{$\top$}\vphantom\{\} for \{W\} a random Boolean matrix with k-sparse rows, and the goal is to recover \{W\} up to column permutation. Equivalently, this can be thought of as recovering a uniformly random k-uniform hypergraph from its line graph. Our main result is a polynomial-time algorithm for this problem based on bootstrapping higher-order information about \{W\} and then decomposing an appropriate tensor. The key ingredient in our analysis, which may be of independent interest, is to show that such a matrix \{W\} has full column rank with high probability as soon as m={\textohm}\vphantom\{\}(r), which we do using tools from Littlewood-Offord theory and estimates for binary Krawtchouk polynomials.},
  citationcount = {5},
  venue = {Information Technology Convergence and Services}
}

@article{chernoffAMeasureOf1952,
  title = {A Measure of Asymptotic Efficiency for Tests of a Hypothesis Based on the Sum of Observations},
  author = {Chernoff, H.},
  year = {1952},
  doi = {10.1214/AOMS/1177729330},
  abstract = {No abstract available},
  citationcount = {3751},
  venue = {No venue available}
}

@article{chiangDynamicAlgorithmsComputational1992,
  title = {Dynamic Algorithms in Computational Geometry},
  author = {Chiang, Yi-Jen and Tamassia, R.},
  year = {1992},
  doi = {10.1109/5.163409},
  abstract = {Dynamic algorithms and data structures in the area of computational geometry are surveyed. The work has a twofold purpose: it introduces the area to the nonspecialist and reviews the state of the art for the specialist. Fundamental data structures, such as balanced search trees and general techniques for dynamization, are reviewed. Range searching, intersections, point location, convex hull, and proximity are discussed. Problems that do not fall into these categories are also discussed. Open problems are given. {\textquestiondown}},
  citationcount = {167},
  venue = {Proceedings of the IEEE},
  keywords = {data structure,dynamic}
}

@article{chillaraANearOptimal2018,
  title = {A Near-Optimal Depth-Hierarchy Theorem for Small-Depth Multilinear Circuits},
  author = {Chillara, Suryajith and Engels, C. and Limaye, N. and Srinivasan, S.},
  year = {2018},
  doi = {10.1109/FOCS.2018.00092},
  abstract = {We study the size blow-up that is necessary to convert an algebraic circuit of product-depth {$\delta$}+1 to one of product-depth {$\delta$} in the multilinear setting. We show that for every positive {$\delta$} = {$\delta$}(n) = o(log n/log log n), there is an explicit multilinear polynomial P{\textasciicircum}({$\delta$}) on n variables that can be computed by a multilinear formula of product-depth {$\delta$}+1 and size O(n), but not by any multilinear circuit of product-depth {$\delta$} and size less than exp(n{\textasciicircum}{\textohm}(1/{$\delta$})). This result is tight up to the constant implicit in the double exponent for all {$\delta$} = o(log n/log log n). This strengthens a result of Raz and Yehudayoff (Computational Complexity 2009) who prove a quasipolynomial separation for constant-depth multilinear circuits, and a result of Kayal, Nair and Saha (STACS 2016) who give an exponential separation in the case {$\delta$} = 1. Our separating examples may be viewed as algebraic analogues of variants of the Graph Reachability problem studied by Chen, Oliveira, Servedio and Tan (STOC 2016), who used them to prove lower bounds for constant-depth Boolean circuits.},
  citationcount = {20},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{chimaniACloserLook2011,
  title = {A Closer Look at the Closest String and Closest Substring Problem},
  author = {Chimani, Markus and Woste, Matthias and B{\"o}cker, Sebastian},
  year = {2011},
  doi = {10.1137/1.9781611972917.2},
  abstract = {Let S be a set of k strings over an alphabet {$\Sigma$} each string has a length between e and n. The Closest Substring Problem (CSSP) is to find a minimal integer d (and a corresponding string t of length e) such that each string s {$\in$} S has a substring of length e with Hamming distance at most d to t. We say t is the closest substring to S. For e = n, this problem is known as the Closest String Problem (CSP). Particularly in computational biology, the CSP and CSSP have found numerous practical applications such as identifying regulatory motifs and approximate gene clusters, and in degenerate primer design. We study ILP formulations for both problems. Our experiments show that a position-based formulation for the CSP performs very well on real-world instances emerging from biology. Even on randomly generated instances that are hard to solve to optimality, solving the root relaxation leads to solutions very close to the optimum. For the CSSP we give a new formulation that is polytope-wise stronger than a straightforward extension of the CSP formulation. Furthermore we propose a strengthening constraint class that speeds up the running time.},
  citationcount = {34},
  venue = {Workshop on Algorithm Engineering and Experimentation}
}

@article{chorPrivateInformationRetrieval1995,
  title = {Private Information Retrieval},
  author = {Chor, B. and Kushilevitz, E. and Goldreich, Oded and Sudan, M.},
  year = {1995},
  doi = {10.1109/SFCS.1995.492461},
  abstract = {We describe schemes that enable a user to access k replicated copies of a database (k/spl ges/2) and privately retrieve information stored in the database. This means that each individual database gets no information on the identity of the item retrieved by the user. For a single database, achieving this type of privacy requires communicating the whole database, or n bits (where n is the number of bits in the database). Our schemes use the replication to gain substantial saving. In particular, we have: A two database scheme with communication complexity of O(n/sup 1/3/). A scheme for a constant number, k, of databases with communication complexity O(n/sup 1/k/). A scheme for 1/3 log/sub 2/ n databases with polylogarithmic (in n) communication complexity.},
  citationcount = {2689},
  venue = {Proceedings of IEEE 36th Annual Foundations of Computer Science}
}

@article{chorTheBitExtraction1985,
  title = {The Bit Extraction Problem or T-Resilient Functions},
  author = {Chor, B. and Goldreich, Oded and H{\aa}stad, J. and Friedman, J. and Rudich, S. and Smolensky, R.},
  year = {1985},
  doi = {10.1109/SFCS.1985.55},
  abstract = {We consider the following adversarial situation. Let n, m and t be arbitrary integers, and let f : \{0, 1\}n {$\rightarrow$} \{0, 1\}m be a function. An adversary, knowing the function f, sets t of the n input bits, while the rest (n-t input, bits) are chosen at random (independently and with uniform probability distribution) The adversary tries to prevent the outcome of f from being uniformly distributed in \{0, 1\}m. The question addressed is for what values of n, m and t does the adversary necessarily fail in biasing the outcome of f : \{0,1\}n {$\rightarrow$} \{0, 1\}m, when being restricted to set t of the input bits of f. We present various lower and upper bounds on m's allowing an affirmative answer. These bounds are relatively close for t {$\leq$} n/3 and for t {$\geq$} 2n/3. Our results have applications in the fields of faulttolerance and cryptography.},
  citationcount = {392},
  venue = {26th Annual Symposium on Foundations of Computer Science (sfcs 1985)}
}

@article{chorUnbiasedBitsFrom1985,
  title = {Unbiased Bits from Sources of Weak Randomness and Probabilistic Communication Complexity},
  author = {Chor, B. and Goldreich, Oded},
  year = {1985},
  doi = {10.1109/SFCS.1985.62},
  abstract = {We introduce a general model for physical sources or weak randomness. Loosely speaking, we view physical sources as devices which output strings according to probability distributions in which no single string is too probable. The main question addressed is whether it is possible to extract alrnost unbiased random bits from such "probability bounded" sources. We show that most or the functions can be used to extract almost unbiased and independent bits from the output of any two independent "probability-bounded" sources. The number of extractable bits is within a constant factor of the information theoretic bound. We conclude this paper by establishing further connections between communication complexity and the problem discussed above. This allows us to show that most Boolean functions have linear communication complexity in a very strong sense.},
  citationcount = {660},
  venue = {26th Annual Symposium on Foundations of Computer Science (sfcs 1985)}
}

@article{choudharyBudgetedDominatingSets2021,
  title = {Budgeted Dominating Sets in Uncertain Graphs},
  author = {Choudhary, Keerti and Cohen, Avi and Narayanaswamy, N. and Peleg, D. and Vijayaragunathan, R.},
  year = {2021},
  doi = {10.4230/LIPIcs.MFCS.2021.32},
  abstract = {We study the \{\vphantom\}\emph{Budgeted Dominating Set}\vphantom\{\}\emph{ (BDS) problem on uncertain graphs, namely, graphs with a probability distribution p associated with the edges, such that an edge e exists in the graph with probability p(e). The input to the problem consists of a vertex-weighted uncertain graph =(V,E,p,{$\omega$}) and an integer }\{\vphantom\}\emph{budget}\vphantom\{\}\emph{ (or }\{\vphantom\}\emph{solution size}\vphantom\{\}\emph{) k, and the objective is to compute a vertex set S of size k that maximizes the expected total domination (or total weight) of vertices in the closed neighborhood of S. We refer to the problem as the }\{\vphantom\}\emph{Probabilistic Budgeted Dominating Set}\vphantom\{\}\emph{ (PBDS) problem and present the following results. }\{\vphantom\}\emph{enumerate}\vphantom\{\}\emph{ We show that the PBDS problem is NP-complete even when restricted to uncertain }\{\vphantom\}\emph{trees}\vphantom\{\}\emph{ of diameter at most four. This is in sharp contrast with the well-known fact that the BDS problem is solvable in polynomial time in trees. We further show that PBDS is -hard for the budget parameter k, and under the }\{\vphantom\}\emph{Exponential time hypothesis}\vphantom\{\}\emph{ it cannot be solved in n\textsuperscript{\{\vphantom\}}o(k)\vphantom\{\} time.  We show that if one is willing to settle for (1-{$\epsilon$}) approximation, then there exists a PTAS for PBDS on trees. Moreover, for the scenario of uniform edge-probabilities, the problem can be solved optimally in polynomial time.  We consider the parameterized complexity of the PBDS problem, and show that Uni-PBDS (where all edge probabilities are identical) is -hard for the parameter pathwidth. On the other hand, we show that it is FPT in the combined parameters of the budget k and the treewidth.  Finally, we extend some of our parameterized results to planar and apex-minor-free graphs. }\{\vphantom\}\emph{enumerate}\vphantom\{\}},
  citationcount = {Unknown},
  venue = {International Symposium on Mathematical Foundations of Computer Science}
}

@article{christianiAFrameworkFor2016,
  title = {A Framework for Similarity Search with Space-Time Tradeoffs Using Locality-Sensitive Filtering},
  author = {Christiani, Tobias},
  year = {2016},
  doi = {10.1137/1.9781611974782.3},
  abstract = {We present a framework for similarity search based on Locality-Sensitive Filtering (LSF), generalizing the Indyk-Motwani (STOC 1998) Locality-Sensitive Hashing (LSH) framework to support space-time tradeoffs. Given a family of filters, defined as a distribution over pairs of subsets of space with certain locality-sensitivity properties, we can solve the approximate near neighbor problem in d-dimensional space for an n-point data set with query time dn\textsuperscript{\{\vphantom\}}{$\rho$}\textsubscript{q}+o(1)\vphantom\{\}, update time dn\textsuperscript{\{\vphantom\}}{$\rho$}\textsubscript{u}+o(1)\vphantom\{\}, and space usage dn+n\textsuperscript{\{\vphantom\}}1+{$\rho$}\textsubscript{u}+o(1)\vphantom\{\}. The space-time tradeoff is tied to the tradeoff between query time and update time, controlled by the exponents {$\rho$}\textsubscript{q},{$\rho$}\textsubscript{u} that are determined by the filter family. Locality-sensitive filtering was introduced by Becker et al. (SODA 2016) together with a framework yielding a single, balanced, tradeoff between query time and space, further relying on the assumption of an efficient oracle for the filter evaluation algorithm. We extend the LSF framework to support space-time tradeoffs and through a combination of existing techniques we remove the oracle assumption. Building on a filter family for the unit sphere by Laarhoven (arXiv 2015) we use a kernel embedding technique by Rahimi \& Recht (NIPS 2007) to show a solution to the (r,cr)-near neighbor problem in {$\ell_{s}^d$}-space for 0q=\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{c}\textsuperscript{s}(1+{$\lambda$}){$^{2}$}\vphantom\{\}\{(c\textsuperscript{s}+{$\lambda$}){$^2$}\} and {$\rho$}\textsubscript{u}=\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{c}\textsuperscript{s}(1-{$\lambda$}){$^{2}$}\vphantom\{\}\{(c\textsuperscript{s}+{$\lambda$}){$^2$}\} where {$\lambda\in$}[-1,1] is a tradeoff parameter. This result improves upon the space-time tradeoff of Kapralov (PODS 2015) and is shown to be optimal in the case of a balanced tradeoff. Finally, we show a lower bound for the space-time tradeoff on the unit sphere that matches Laarhoven's and our own upper bound in the case of random data.},
  citationcount = {51},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {lower bound,query,query time,update,update time}
}

@article{christianiFromIndependenceTo2015,
  title = {From Independence to Expansion and Back Again},
  author = {Christiani, Tobias and Pagh, R. and Thorup, M.},
  year = {2015},
  doi = {10.1145/2746539.2746620},
  abstract = {We consider the following fundamental problems: Constructing k-independent hash functions with a space-time tradeoff close to Siegel's lower bound. Constructing representations of unbalanced expander graphs having small size and allowing fast computation of the neighbor function. It is not hard to show that these problems are intimately connected in the sense that a good solution to one of them leads to a good solution to the other one. In this paper we exploit this connection to present efficient, recursive constructions of k-independent hash functions (and hence expanders with a small representation). While the previously most efficient construction (Thorup, FOCS 2013) needed time quasipolynomial in Siegel's lower bound, our time bound is just a logarithmic factor from the lower bound.},
  citationcount = {17},
  venue = {Symposium on the Theory of Computing},
  keywords = {lower bound}
}

@article{christianiGeneratingKIndependent2014,
  title = {Generating K-{{Independent}} Variables in Constant Time},
  author = {Christiani, Tobias and Pagh, R.},
  year = {2014},
  doi = {10.1109/FOCS.2014.29},
  abstract = {The generation of pseudorandom elements over finite fields is fundamental to the time, space and randomness complexity of randomized algorithms and data structures. We consider the problem of generating k-independent random values over a finite field F in a word RAM model equipped with constant time addition and multiplication in F, and present the first nontrivial construction of a generator that outputs each value in constant time, not dependent on k. Our generator has period length {\textbar}F{\textbar} poly log k and uses k poly (log k) log {\textbar}F{\textbar} bits of space, which is optimal up to a poly log k factor. We are able to bypass Siegel's lower bound on the time-space tradeoff for k-independent functions by a restriction to sequential evaluation.},
  citationcount = {14},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,lower bound,time-space}
}

@article{christiansenAdaptiveOutOrientations2022,
  title = {Adaptive Out-Orientations with Applications},
  author = {Christiansen, Aleksander B. G. and Holm, J. and Hoog, I. and Rotenberg, E. and Schwiegelshohn, Chris},
  year = {2022},
  doi = {10.48550/arXiv.2209.14087},
  abstract = {We give improved algorithms for maintaining edge-orientations of a fully-dynamic graph, such that the out-degree of each vertex is bounded. On one hand, we show how to orient the edges such that the out-degree of each vertex is proportional to the arboricity {$\alpha$} of the graph, in, either, an amortised update time of O({$^2$}n{$\alpha$}), or a worst-case update time of O({$^3$}n{$\alpha$}). On the other hand, motivated by applications including dynamic maximal matching, we obtain a different trade-off, namely either O(n{$\alpha$}), amortised, or O({$^2$}n{$\alpha$}), worst-case time, for the problem of maintaining an edge-orientation with at most O({$\alpha$}+n) out-edges per vertex. Since our algorithms have update times with worst-case guarantees, the number of changes to the solution (i.e. the recourse) is naturally limited. Our algorithms adapt to the current arboricity of the graph, and yield improvements over previous work: Firstly, we obtain an O({$\varepsilon$}\textsuperscript{\{\vphantom\}}-6\vphantom\{\}{$^3$}n{$\rho$}) worst-case update time algorithm for maintaining a (1+{$\varepsilon$}) approximation of the maximum subgraph density, {$\rho$}. Secondly, we obtain an O({$\varepsilon$}\textsuperscript{\{\vphantom\}}-6\vphantom\{\}{$^3$}n{$\alpha$}) worst-case update time algorithm for maintaining a (1+{$\varepsilon$}){$\cdot$}OPT+2 approximation of the optimal out-orientation of a graph with adaptive arboricity {$\alpha$}. This yields the first worst-case polylogarithmic dynamic algorithm for decomposing into O({$\alpha$}) forests.Thirdly, we obtain arboricity-adaptive fully-dynamic deterministic algorithms for a variety, of problems including maximal matching, {$\Delta$}+1 coloring, and matrix vector multiplication. All update times are worst-case O({$\alpha$}+{$^2$}n{$\alpha$}), where {$\alpha$} is the current arboricity of the graph.},
  citationcount = {10},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {adaptive,dynamic,update,update time}
}

@article{christopherhojnyPolytopesAssociatedSymmetry2018,
  title = {Polytopes Associated with Symmetry Handling},
  author = {Christopher Hojny and M. Pfetsch},
  year = {2018},
  journal = {Mathematical programming},
  doi = {10.1007/s10107-018-1239-7},
  annotation = {Citation Count: 49}
}

@article{chungLowerBoundsOn2022,
  title = {Lower Bounds on Retroactive Data Structures},
  author = {Chung, Lily and Demaine, E. and Hendrickson, D. and Lynch, J.},
  year = {2022},
  doi = {10.48550/arXiv.2211.14664},
  abstract = {We prove essentially optimal fine-grained lower bounds on the gap between a data structure and a partially retroactive version of the same data structure. Precisely, assuming any one of three standard conjectures, we describe a problem that has a data structure where operations run in O(T(n,m)) time per operation, but any partially retroactive version of that data structure requires T(n,m){$\cdot$}m\textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\} worst-case time per operation, where n is the size of the data structure at any time and m is the number of operations. Any data structure with operations running in O(T(n,m)) time per operation can be converted (via the"rollback method") into a partially retroactive data structure running in O(T(n,m){$\cdot$}m) time per operation, so our lower bound is tight up to an m\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} factor common in fine-grained complexity.},
  citationcount = {1},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {data structure,lower bound}
}

@article{chungOnTheHardness2020,
  title = {On the Hardness of Massively Parallel Computation},
  author = {Chung, Kai-Min and Ho, Kuan-Yi and Sun, Xiaorui},
  year = {2020},
  doi = {10.1145/3350755.3400223},
  abstract = {We investigate whether there are inherent limits of parallelization in the (randomized) massively parallel computation (MPC) model by comparing it with the (sequential) RAM model. As our main result, we show the existence of hard functions that are essentially not parallelizable in the MPC model. Based on the widely-used random oracle methodology in cryptography with a cryptographic hash function h:\{0,1\}n {$\rightarrow$} \{0,1\}n computable in time th, we show that there exists a function that can be computed in time O(T {$\cdot$} th) and space S by a RAM algorithm, but any MPC algorithm with local memory size s {\textexclamdown} S/c for some c{\textquestiondown}1 requires at least  {\textohm}(T) rounds to compute the function, even in the average case, for a wide range of parameters n {$\leq$} S {$\leq$} T {$\leq$} 2n1/4. Our result is almost optimal in the sense that by taking T to be much larger than th,e.g., T to be sub-exponential in th, to compute the function, the round complexity of any MPC algorithm with small local memory size is asymptotically the same (up to a polylogarithmic factor) as the time complexity of the RAM algorithm. Our result is obtained by adapting the so-called compression argument from the data structure lower bounds and cryptography literature to the context of massively parallel computation.},
  citationcount = {3},
  venue = {ACM Symposium on Parallelism in Algorithms and Architectures},
  keywords = {data structure,lower bound}
}

@article{chungStronger3sumIndexing2022,
  title = {Stronger {{3SUM-indexing}} Lower Bounds},
  author = {Chung, E. and Larsen, Kasper Green},
  year = {2022},
  doi = {10.1137/1.9781611977554.ch19},
  abstract = {The 3SUM-Indexing problem was introduced as a data structure version of the 3SUM problem, with the goal of proving strong conditional lower bounds for static data structures via reductions. Ideally, the conjectured hardness of 3SUM-Indexing should be replaced by an unconditional lower bound. Unfortunately, we are far from proving this, with the strongest current lower bound being a logarithmic query time lower bound by Golovnev et al. from STOC'20. Moreover, their lower bound holds only for non-adaptive data structures and they explicitly asked for a lower bound for adaptive data structures. Our main contribution is precisely such a lower bound against adaptive data structures. As a secondary result, we also strengthen the non-adaptive lower bound of Golovnev et al. and prove strong lower bounds for 2-bit-probe non-adaptive 3SUM-Indexing data structures via a completely new approach that we find interesting in its own right.},
  citationcount = {1},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,lower bound,non-adaptive,query,query time,reduction,static}
}

@article{chuzhoyADeterministicAlgorithm2019,
  title = {A Deterministic Algorithm for Balanced Cut with Applications to Dynamic Connectivity, Flows, and Beyond},
  author = {Chuzhoy, Julia and Gao, Yu and Li, Jason and Nanongkai, Danupon and Peng, Richard and Saranurak, Thatchaphol},
  year = {2019},
  doi = {10.1109/FOCS46700.2020.00111},
  abstract = {We consider the classical Minimum Balanced Cut problem: given a graph G, compute a partition of its vertices into two subsets of roughly equal volume, while minimizing the number of edges connecting the subsets. We present the first deterministic, almost-linear time approximation algorithm for this problem. Specifically, our algorithm, given an n-vertex m-edge graph G and any parameter 1{$\leq$}r{$\leq$}O(n), computes a (m)\textsuperscript{\{\vphantom\}}r\textsuperscript{\{\vphantom\}}2\vphantom\{\}\vphantom\{\}-approximation for Minimum Balanced Cut in G, in time O(m\textsuperscript{\{\vphantom\}}1+O(1/r)+o(1)\vphantom\{\}{$\cdot$}(m)\textsuperscript{\{\vphantom\}}O(r\textsuperscript{\{\vphantom\}}2\vphantom\{\})\vphantom\{\}). In particular, we obtain a (m)\textsuperscript{\{\vphantom\}}1/{$\epsilon$}\vphantom\{\}-approximation in time m\textsuperscript{\{\vphantom\}}1+O({\textsurd}\{{$\epsilon$}\})\vphantom\{\} for any constant {$\epsilon>$}0, and a (m)\textsuperscript{\{\vphantom\}}f(m)\vphantom\{\}-approximation in time m\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}, for any slowly growing function f(m). We obtain deterministic algorithms with similar guarantees for the Sparsest Cut and the Lowest-Conductance Cut problems. Our algorithm for the Minimum Balanced Cut problem in fact provides a stronger guarantee: it either returns a balanced cut whose value is close to a given target value, or it certifies that such a cut does not exist by exhibiting a large subgraph of G that has high conductance. We use this algorithm to obtain deterministic algorithms for dynamic connectivity and minimum spanning forest, whose worst-case update time on an n-vertex graph is n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\}, thus resolving a major open problem in the area of dynamic graph algorithms. Our work also implies deterministic algorithms for a host of additional problems, whose time complexities match, up to subpolynomial in n factors, those of known randomized algorithms. The implications include almost-linear time deterministic algorithms for solving Laplacian systems and for approximating maximum flows in undirected graphs.},
  citationcount = {98},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,update,update time}
}

@article{chuzhoyANewAlgorithm2019,
  title = {A New Algorithm for Decremental Single-Source Shortest Paths with Applications to Vertex-Capacitated Flow and Cut Problems},
  author = {Chuzhoy, Julia and Khanna, S.},
  year = {2019},
  doi = {10.1145/3313276.3316320},
  abstract = {We study the vertex-decremental Single-Source Shortest Paths (SSSP) problem: given an undirected graph G=(V,E) with lengths {$\ell$}(e){$\geq$} 1 on its edges that undergoes vertex deletions, and a source vertex s, we need to support (approximate) shortest-path queries in G: given a vertex v, return a path connecting s to v, whose length is at most (1+{\cyrchar\cyrie}) times the length of the shortest such path, where {\cyrchar\cyrie} is a given accuracy parameter. The problem has many applications, for example to flow and cut problems in vertex-capacitated graphs. Decremental SSSP is a fundamental problem in dynamic algorithms that has been studied extensively, especially in the more standard edge-decremental setting, where the input graph G undergoes edge deletions. The classical algorithm of Even and Shiloach supports exact shortest-path queries in O(mn) total update time. A series of recent results have improved this bound to O(m1+o(1)logL), where L is the largest length of any edge. However, these improved results are randomized algorithms that assume an oblivious adversary. To go beyond the oblivious adversary restriction, recently, Bernstein, and Bernstein and Chechik designed deterministic algorithms for the problem, with total update time {\~O}(n2logL), that by definition work against an adaptive adversary. Unfortunately, their algorithms introduce a new limitation, namely, they can only return the approximate length of a shortest path, and not the path itself. Many applications of the decremental SSSP problem, including the ones considered in this paper, crucially require both that the algorithm returns the approximate shortest paths themselves and not just their lengths, and that it works against an adaptive adversary. Our main result is a randomized algorithm for vertex-decremental SSSP with total expected update time O(n2+o(1)logL), that responds to each shortest-path query in {\~O}(nlogL) time in expectation, returning a (1+{\cyrchar\cyrie})-approximate shortest path. The algorithm works against an adaptive adversary. The main technical ingredient of our algorithm is an {\~O}({\textbar}E(G){\textbar}+ n1+o(1))-time algorithm to compute a core decomposition of a given dense graph G, which allows us to compute short paths between pairs of query vertices in G efficiently. We use our result for vertex-decremental SSSP to obtain (1+{\cyrchar\cyrie})-approximation algorithms for maximum s-t flow and minimum s-t cut in vertex-capacitated graphs, in expected time n2+o(1), and an O(log4n)-approximation algorithm for the vertex version of the sparsest cut problem with expected running time n2+o(1). These results improve upon the previous best known algorithms for these problems in the regime where m= {$\omega$}(n1.5 + o(1)).},
  citationcount = {46},
  venue = {Symposium on the Theory of Computing},
  keywords = {adaptive,dynamic,query,update,update time}
}

@article{chuzhoyANewDeterministic2023,
  title = {A New Deterministic Algorithm for Fully Dynamic All-Pairs Shortest Paths},
  author = {Chuzhoy, Julia and Zhang, Ruimin},
  year = {2023},
  doi = {10.1145/3564246.3585196},
  abstract = {We study the fully dynamic All-Pairs Shortest Paths (APSP) problem in undirected edge-weighted graphs. Given an n-vertex graph G with non-negative edge lengths, that undergoes an online sequence of edge insertions and deletions, the goal is to support approximate distance queries and shortest-path queries. We provide a deterministic algorithm for this problem, that, for a given precision parameter {\cyrchar\cyrie}, achieves approximation factor (loglogn)2O(1/{\cyrchar\cyrie}3), and has amortized update time O(n{\cyrchar\cyrie}logL) per operation, where L is the ratio of longest to shortest edge length. Query time for distance-query is O(2O(1/{\cyrchar\cyrie}){$\cdot$} logn{$\cdot$} loglogL), and query time for shortest-path query is O({\textbar}E(P){\textbar}+2O(1/{\cyrchar\cyrie}){$\cdot$} logn{$\cdot$} loglogL), where P is the path that the algorithm returns. To the best of our knowledge, even allowing any o(n)-approximation factor, no adaptive-update algorithms with better than {$\Theta$}(m) amortized update time and better than {$\Theta$}(n) query time were known prior to this work. We also note that our guarantees are stronger than the best current guarantees for APSP in decremental graphs in the adaptive-adversary setting. In order to obtain these results, we consider an intermediate problem, called Recursive Dynamic Neighborhood Cover (RecDynNC), that was formally introduced in [Chuzhoy, STOC '21]. At a high level, given an undirected edge-weighted graph G undergoing an online sequence of edge deletions, together with a distance parameter D, the goal is to maintain a sparse D-neighborhood cover of G, with some additional technical requirements. Our main technical contribution is twofolds. First, we provide a black-box reduction from APSP in fully dynamic graphs to the RecDynNC problem. Second, we provide a new deterministic algorithm for the RecDynNC problem, that, for a given precision parameter {\cyrchar\cyrie}, achieves approximation factor (loglogm)2O(1/{\cyrchar\cyrie}2), with total update time O(m1+{\cyrchar\cyrie}), where m is the total number of edges ever present in G. This improves the previous algorithm of [Chuzhoy, STOC '21], that achieved approximation factor (logm)2O(1/{\cyrchar\cyrie}) with similar total update time. Combining these two results immediately leads to the deterministic algorithm for fully-dynamic APSP with the guarantees stated above.},
  citationcount = {10},
  venue = {Symposium on the Theory of Computing},
  keywords = {adaptive,dynamic,query,query time,reduction,update,update time}
}

@article{chuzhoyDecrementalAllPairs2021,
  title = {Decremental All-Pairs Shortest Paths in Deterministic near-Linear Time},
  author = {Chuzhoy, Julia},
  year = {2021},
  doi = {10.1145/3406325.3451025},
  abstract = {We study the decremental All-Pairs Shortest Paths (APSP) problem in undirected edge-weighted graphs. The input to the problem is an undirected n-vertex m-edge graph G with non-negative lengths on edges, that undergoes an online sequence of edge deletions. The goal is to support approximate shortest-paths queries: given a pair x,y of vertices of G, return a path P connecting x to y, whose length is within factor {$\alpha$} of the length of the shortest x-y path, in time {\~O}({\textbar}E(P){\textbar}), where {$\alpha$} is the approximation factor of the algorithm. APSP is one of the most basic and extensively studied dynamic graph problems. A long line of work culminated in the algorithm of [Chechik, FOCS 2018] with near optimal guarantees: for any constant 0{\textexclamdown}{\cyrchar\cyrie}{$\leq$} 1 and parameter k{$\geq$} 1, the algorithm achieves approximation factor (2+{\cyrchar\cyrie})k-1, and total update time O(mn1/k+o(1)log(nL)), where L is the ratio of longest to shortest edge lengths. Unfortunately, as much of prior work, the algorithm is randomized and needs to assume an oblivious adversary; that is, the input edge-deletion sequence is fixed in advance and may not depend on the algorithm's behavior. In many real-world scenarios, and in applications of APSP to static graph problems, it is crucial that the algorithm works against an adaptive adversary, where the edge deletion sequence may depend on the algorithm's past behavior arbitrarily; ideally, such an algorithm should be deterministic. Unfortunately, unlike the oblivious-adversary setting, its adaptive-adversary counterpart is still poorly understood. For unweighted graphs, the algorithm of [Henzinger, Krinninger and Nanongkai, FOCS '13, SICOMP '16] achieves a (1+{\cyrchar\cyrie})-approximation with total update time {\~O}(mn/{\cyrchar\cyrie}); the best current total update time guarantee of n2.5+O({\cyrchar\cyrie}) is achieved by the recent deterministic algorithm of [Chuzhoy, Saranurak, SODA'21], with 2O(1/{\cyrchar\cyrie})-multiplicative and 2O(log3/4n/{\cyrchar\cyrie})-additive approximation. To the best of our knowledge, for arbitrary non-negative edge weights, the fastest current adaptive-update algorithm has total update time O(n3logL/{\cyrchar\cyrie}), achieving a (1+{\cyrchar\cyrie})-approximation. Even if we are willing to settle for any o(n)-approximation factor, no currently known algorithm has a better than {$\Theta$}(n3) total update time in weighted graphs and better than {$\Theta$}(n2.5) total update time in unweighted graphs. Several conditional lower bounds suggest that no algorithm with a sufficiently small approximation factor can achieve an o(n3) total update time. Our main result is a deterministic algorithm for decremental APSP in undirected edge-weighted graphs, that, for any {\textohm}(1/loglogm){$\leq$} {\cyrchar\cyrie}{\textexclamdown} 1, achieves approximation factor (logm)2O(1/{\cyrchar\cyrie}), with total update time O(m1+O({\cyrchar\cyrie}){$\cdot$} (logm)O(1/{\cyrchar\cyrie}2){$\cdot$} logL). In particular, we obtain a (polylogm)-approximation in time {\~O}(m1+{\cyrchar\cyrie}) for any constant {\cyrchar\cyrie}, and, for any slowly growing function f(m), we obtain (logm)f(m)-approximation in time m1+o(1). We also provide an algorithm with similar guarantees for decremental Sparse Neighborhood Covers.},
  citationcount = {26},
  venue = {Symposium on the Theory of Computing},
  keywords = {adaptive,dynamic,lower bound,query,static,update,update time}
}

@article{chuzhoyDeterministicAlgorithmsFor2020,
  title = {Deterministic Algorithms for Decremental Shortest Paths via Layered Core Decomposition},
  author = {Chuzhoy, Julia and Saranurak, Thatchaphol},
  year = {2020},
  doi = {10.1137/1.9781611976465.147},
  abstract = {In the decremental single-source shortest paths (SSSP) problem, the input is an undirected graph G=(V,E) with n vertices and m edges undergoing edge deletions, together with a fixed source vertex s{$\in$}V. The goal is to maintain a data structure that supports shortest-path queries: given a vertex v{$\in$}V, quickly return an (approximate) shortest path from s to v. The decremental all-pairs shortest paths (APSP) problem is defined similarly, but now the shortest-path queries are allowed between any pair of vertices of V. Both problems have been studied extensively since the 80's, and algorithms with near-optimal total update time and query time have been discovered for them. Unfortunately, all these algorithms are randomized and, more importantly, they need to assume an oblivious adversary. Our first result is a deterministic algorithm for the decremental SSSP problem on weighted graphs with O(n\textsuperscript{\{\vphantom\}}2+o(1)\vphantom\{\}) total update time, that supports (1+{$\epsilon$})-approximate shortest-path queries, with query time O({\textbar}P{\textbar}{$\cdot$}n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\}), where P is the returned path. This is the first (1+{$\epsilon$})-approximation algorithm against an adaptive adversary that supports shortest-path queries in time below O(n), that breaks the O(mn) total update time bound of the classical algorithm of Even and Shiloah from 1981. Our second result is a deterministic algorithm for the decremental APSP problem on unweighted graphs that achieves total update time O(n\textsuperscript{\{\vphantom\}}2.5+{$\delta$}\vphantom\{\}), for any constant {$\delta>$}0, supports approximate distance queries in O(n) time; the algorithm achieves an O(1)-multiplicative and n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\}-additive approximation on the path length. All previous algorithms for APSP either assume an oblivious adversary or have an {\textohm}(n\textsuperscript{\{\vphantom\}}3\vphantom\{\}) total update time when m={\textohm}(n\textsuperscript{\{\vphantom\}}2\vphantom\{\}).},
  citationcount = {29},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {adaptive,data structure,query,query time,update,update time}
}

@article{chuzhoyLargeTreewidthGraph2016,
  title = {Large-Treewidth Graph Decompositions},
  author = {Chuzhoy, Julia},
  year = {2016},
  doi = {10.1007/978-1-4939-2864-4_691},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Encyclopedia of Algorithms}
}

@article{chvtalProbabilisticMethodsIn1984,
  title = {Probabilistic Methods in Graph Theory},
  author = {Chv{\'a}tal, V.},
  year = {1984},
  doi = {10.1007/BF01874387},
  abstract = {No abstract available},
  citationcount = {19},
  venue = {Annals of Operations Research}
}

@article{clarksonAnAlgorithmFor1994,
  title = {An Algorithm for Approximate Closest-Point Queries},
  author = {Clarkson, K.},
  year = {1994},
  doi = {10.1145/177424.177609},
  abstract = {This paper gives an algorithm for approximately solving the {\textexclamdown}italic{\textquestiondown}post office problem:{\textexclamdown}/italic{\textquestiondown} given {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} points (called sites) in {\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown} dimensions, build a data structure so that, given a query point {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown}, a closest site to {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown} can be found quickly. The algorithm is also given a relative error bound {$\varepsilon$}, and depends on a ratio \&rgr;, which is no more than the ratio of the distance between the farthest pair of sites to the distance between the closest pair of sites. The algorithm builds a data structure of size {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}\&eegr;){\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(1/{$\varepsilon$}){\textexclamdown}supscrpt{\textquestiondown}({\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}-1)/2{\textexclamdown}/supscrpt{\textquestiondown} in time {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}2{\textexclamdown}/supscrpt{\textquestiondown}\&eegr;){\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(1/{$\varepsilon$}){\textexclamdown}supscrpt{\textquestiondown}({\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}-1){\textexclamdown}/supscrpt{\textquestiondown}. Here \&eegr;=log(\&rgr;/{$\varepsilon$}). With this data structure, a site is returned whose distance to a query point {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown} is within 1+{$\varepsilon$} of the distance of the closest site. A query needs {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(log{\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}){\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(1/{$\varepsilon$}){\textexclamdown}supscrpt{\textquestiondown}({\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}-1)/2{\textexclamdown}/supscrpt{\textquestiondown} time, with high probability.},
  citationcount = {153},
  venue = {SCG '94},
  keywords = {data structure,query}
}

@article{clarksonAProbabilisticAlgorithm1985,
  title = {A Probabilistic Algorithm for the Post Office Problem},
  author = {Clarkson, K.},
  year = {1985},
  doi = {10.1145/22145.22165},
  abstract = {The {\textexclamdown}italic{\textquestiondown}post office problem{\textexclamdown}/italic{\textquestiondown} is the following: points in {\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}-dimensional space, so that given an arbitrary point {\textexclamdown}italic{\textquestiondown}p{\textexclamdown}/italic{\textquestiondown}, the closest points in {\textexclamdown}italic{\textquestiondown}S{\textexclamdown}/italic{\textquestiondown} to {\textexclamdown}italic{\textquestiondown}p{\textexclamdown}/italic{\textquestiondown} can be found quickly. We consider the case of this problem where the Euclidean norm is the measure of distance. The previous best algorithm for this problem for {\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic>>2 requires \&Ogr;({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}2{\textexclamdown}supscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}+1{\textexclamdown}/supscrpt{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown}) preprocessing time to build a data structure allowing an \&Ogr;(log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} query time. We will show that a data structure can be built in expected \&Ogr;({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}({\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}-1)(1+{\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}){\textexclamdown}/supscrpt{\textquestiondown}) time, for any fixed {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown};{\textquestiondown}\&Ogr;, so that closest-point queries can be answered in \&Ogr;(log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) worstcase time. (The constant factors depend on {\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown} and {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}.) The algorithm employs random sampling, so the expected time holds for any set of points. A variant of this algorithm (for the variant problem where only one closest point of {\textexclamdown}italic{\textquestiondown}S{\textexclamdown}/italic{\textquestiondown} to the query point is desired) requires \&Ogr;({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{$\lceil$}{\textexclamdown}supscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}/2{\textexclamdown}/supscrpt{\textquestiondown}{$\rceil$}) \&ogr;({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}{$\lceil$}{\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}/2{$\rceil$}{\textexclamdown}/supscrpt{\textquestiondown}) preprocessing time for \&ogr;(n{\textexclamdown}supscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}t{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown}) worst-case query time, for any fixed {$\varepsilon$}{\textquestiondown}0. These results approach the \&OHgr;({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}{$\lceil$}{\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}/2{$\rceil$}{\textexclamdown}/supscrpt{\textquestiondown}) preprocessing time required for any algorithm constructing the Voronoi diagram of the input points. Implementation of these algorithms requires not too much more than a random sampling procedure and a procedure for constructing the Voronoi diagram of that random sample.},
  citationcount = {47},
  venue = {Symposium on the Theory of Computing}
}

@article{clarksonARandomizedAlgorithm1988,
  title = {A Randomized Algorithm for Closest-Point Queries},
  author = {Clarkson, K.},
  year = {1988},
  doi = {10.1137/0217052},
  abstract = {An algorithm for closest-point queries is given. The problem is this: given a set S of n points in d-dimensional space, build a data structure so that given an arbitrary query point p, a closest point in S to p can be found quickly. The measure of distance is the Euclidean norm. This is sometimes called the post-office problem. The new data structure will be termed an RPO tree, from Randomized Post Office. The expected time required to build an RPO tree is O(n\textsuperscript{\{\vphantom\}}{$\lceil$}\{\{d/2\}\}{$\rceil$}(1+{$\epsilon$})\vphantom\{\}), for any fixed {$\epsilon>$}0, and a query can be answered in O(n) worst-case time. An RPO tree requires O(n\textsuperscript{\{\vphantom\}}{$\lceil$}\{\{d/2\}\}{$\rceil$}(1+{$\epsilon$})\vphantom\{\}) space in the worst case. The constant factors in these bounds depend on d and {$\epsilon$}. The bounds are average-case due to the randomization employed by the algorithm, and hold for any set of input points. This result approaches the {\textohm}(n\textsuperscript{\{\vphantom\}}{$\lceil$}\{\{d/2\}\}{$\rceil$}\vphantom\{\}) worst-case time required for any algorithm that constructs the Voronoi...},
  citationcount = {253},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,query}
}

@article{cliffordCellprobeBoundsOnline2014,
  title = {Cell-Probe Bounds for Online Edit Distance and Other Pattern Matching Problems},
  author = {Clifford, R. and Jalsenius, Markus and Sach, Benjamin},
  year = {2014},
  doi = {10.1137/1.9781611973730.37},
  abstract = {We give cell-probe bounds for the computation of edit distance, Hamming distance, convolution and longest common subsequence in a stream. In this model, a fixed string of n symbols is given and one {$\delta$}-bit symbol arrives at a time in a stream. After each symbol arrives, the distance between the fixed string and a suffix of most recent symbols of the stream is reported. The cell-probe model is perhaps the strongest model of computation for showing data structure lower bounds, subsuming in particular the popular word-RAM model. {$\bullet$} We first give an {\textohm}(({$\delta$} log n)/(w+log log n)) lower bound for the time to give each output for both online Hamming distance and convolution, where w is the word size. This bound relies on a new encoding scheme and for the first time holds even when w is as small as a single bit. {$\bullet$} We then consider the online edit distance and longest common subsequence problems in the bit-probe model (w = 1) with a constant sized input alphabet. We give a lower bound of {\textohm}([EQUATION]log n/(log log n)3/2) which applies for both problems. This second set of results relies both on our new encoding scheme as well as a carefully constructed hard input distribution. {$\bullet$} Finally, for the online edit distance problem we show that there is an O((log2n)/w) upper bound in the cell-probe model. This bound gives a contrast to our new lower bound and also establishes an exponential gap between the known cell-probe and RAM model complexities.},
  citationcount = {10},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,data structure,lower bound,sorted}
}

@article{cliffordCellProbeLower2016,
  title = {Cell-Probe Lower Bounds for Bit Stream Computation},
  author = {Clifford, R. and Jalsenius, Markus and Sach, Benjamin},
  year = {2016},
  doi = {10.4230/LIPIcs.ESA.2016.31},
  abstract = {We revisit the complexity of online computation in the cell probe model. We consider a class of problems where we are first given a fixed pattern F of n symbols and then one symbol arrives at a time in a stream. After each symbol has arrived we must output some function of F and the n-length suffix of the arriving stream. Cell probe bounds of Omega(delta lg n/w) have previously been shown for both convolution and Hamming distance in this setting, where delta is the size of a symbol in bits and w in Omega(lg n) is the cell size in bits. However, when delta is a constant, as it is in many natural situations, the existing approaches no longer give us non-trivial bounds. We introduce a lop-sided information transfer proof technique which enables us to prove meaningful lower bounds even for constant size input alphabets. Our new framework is capable of proving amortised cell probe lower bounds of Omega(lg{\textasciicircum}2 n/(w lg lg n)) time per arriving bit. We demonstrate this technique by showing a new lower bound for a problem known as pattern matching with address errors or the L\_2-rearrangement distance problem. This gives the first non-trivial cell probe lower bound for any online problem on bit streams that still holds when the cell size is large.},
  citationcount = {1},
  venue = {Embedded Systems and Applications},
  keywords = {cell probe,lower bound,sorted}
}

@inproceedings{cliffordLowerBoundsOnline2011,
  title = {Lower {{Bounds}} for {{Online Integer Multiplication}} and {{Convolution}} in the {{Cell-Probe Model}}},
  booktitle = {Automata, {{Languages}} and {{Programming}}},
  author = {Clifford, Rapha{\"e}l and Jalsenius, Markus},
  editor = {Aceto, Luca and Henzinger, Monika and Sgall, Ji{\v r}{\'i}},
  year = {2011},
  pages = {593--604},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-22006-7_50},
  abstract = {We show time lower bounds for both online integer multiplication and convolution in the cell-probe model with word size w. For the multiplication problem, one pair of digits, each from one of two n digit numbers that are to be multiplied, is given as input at step i. The online algorithm outputs a single new digit from the product of the numbers before step i\,+\,1. We give a lower bound of \${\textbackslash}Omega({\textbackslash}frac\{{\textbackslash}delta\}\{w\} {\textbackslash}log n)\$time on average per output digit for this problem where 2{$\delta$}is the maximum value of a digit. In the convolution problem, we are given a fixed vector V of length n and we consider a stream in which numbers arrive one at a time. We output the inner product of V and the vector that consists of the last n numbers of the stream. We show an \${\textbackslash}Omega({\textbackslash}frac\{{\textbackslash}delta\}\{w\}{\textbackslash}log n)\$lower bound for the time required per new number in the stream. All the bounds presented hold under randomisation and amortisation. Multiplication and convolution are central problems in the study of algorithms which also have the widest range of practical applications.},
  isbn = {978-3-642-22006-7},
  langid = {english},
  keywords = {cell probe,lower bound,sorted,streaming}
}

@article{cliffordNewUnconditionalHardness2015,
  title = {New Unconditional Hardness Results for Dynamic and Online Problems},
  author = {Clifford, R. and J{\o}rgensen, A. and Larsen, Kasper Green},
  year = {2015},
  doi = {10.1109/FOCS.2015.71},
  abstract = {There has been a resurgence of interest in lower bounds whose truth rests on the conjectured hardness of well known computational problems. These conditional lower bounds have become important and popular due to the painfully slow progress on proving strong unconditional lower bounds. Nevertheless, the long term goal is to replace these conditional bounds with unconditional ones. In this paper we make progress in this direction by studying the cell probe complexity of two conjectured to be hard problems of particular importance: matrix-vector multiplication and a version of dynamic set disjointness known as Patrascu's Multiphase Problem. We give improved unconditional lower bounds for these problems as well as introducing new proof techniques of independent interest. These include a technique capable of proving strong threshold lower bounds of the following form: If we insist on having a very fast query time, then the update time has to be slow enough to compute a lookup table with the answer to every possible query. This is the first time a lower bound of this type has been proven.},
  citationcount = {27},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {cell probe,dynamic,lower bound,query,query time,update,update time}
}

@misc{cliffordNewUnconditionalHardness2015a,
  title = {New {{Unconditional Hardness Results}} for {{Dynamic}} and {{Online Problems}}},
  author = {Clifford, Raphael and Gr{\o}nlund, Allan and Larsen, Kasper Green},
  year = {2015},
  month = apr,
  number = {arXiv:1504.01836},
  eprint = {1504.01836},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1504.01836},
  url = {http://arxiv.org/abs/1504.01836},
  urldate = {2024-09-03},
  abstract = {There has been a resurgence of interest in lower bounds whose truth rests on the conjectured hardness of well known computational problems. These conditional lower bounds have become important and popular due to the painfully slow progress on proving strong unconditional lower bounds. Nevertheless, the long term goal is to replace these conditional bounds with unconditional ones. In this paper we make progress in this direction by studying the cell probe complexity of two conjectured to be hard problems of particular importance: matrix-vector multiplication and a version of dynamic set disjointness known as Patrascu's Multiphase Problem. We give improved unconditional lower bounds for these problems as well as introducing new proof techniques of independent interest. These include a technique capable of proving strong threshold lower bounds of the following form: If we insist on having a very fast query time, then the update time has to be slow enough to compute a lookup table with the answer to every possible query. This is the first time a lower bound of this type has been proven.},
  archiveprefix = {arXiv},
  keywords = {cell probe,Computer Science - Computational Complexity,Computer Science - Data Structures and Algorithms,Computer Science - Information Theory,dynamic,lower bound,query,query time,update,update time},
  file = {/Users/tulasi/Zotero/storage/92M299PZ/Clifford et al. - 2015 - New Unconditional Hardness Results for Dynamic and Online Problems.pdf}
}

@article{cliffordTheDynamicK2021,
  title = {The Dynamic K-{{Mismatch}} Problem},
  author = {Clifford, R. and Gawrychowski, Pawe{\l} and Kociumaka, Tomasz and Martin, Daniel P. and Uzna'nski, Przemyslaw},
  year = {2021},
  doi = {10.4230/LIPIcs.CPM.2022.18},
  abstract = {The text-to-pattern Hamming distances problem asks to compute the Hamming distances between a given pattern of length m and all length-m substrings of a given text of length n{$\geq$}m. We focus on the k-mismatch version of the problem, where a distance needs to be returned only if it does not exceed a threshold k. We assume n{$\leq$}2m (in general, one can partition the text into overlapping blocks). In this work, we show data structures for the dynamic version of this problem supporting two operations: An update performs a single-letter substitution in the pattern or the text, and a query, given an index i, returns the Hamming distance between the pattern and the text substring starting at position i, or reports that it exceeds k. First, we show a data structure with O\vphantom\{\}(1) update and O\vphantom\{\}(k) query time. Then we show that O\vphantom\{\}(k) update and O\vphantom\{\}(1) query time is also possible. These two provide an optimal trade-off for the dynamic k-mismatch problem with k{$\leq$}{\textsurd}\{n\}: we prove that, conditioned on the strong 3SUM conjecture, one cannot simultaneously achieve k\textsuperscript{\{\vphantom\}}1-{\textohm}(1)\vphantom\{\} time for all operations. For k{$\geq$}{\textsurd}\{n\}, we give another lower bound, conditioned on the Online Matrix-Vector conjecture, that excludes algorithms taking n\textsuperscript{\{\vphantom\}}1/2-{\textohm}(1)\vphantom\{\} time per operation. This is tight for constant-sized alphabets: Clifford et al. (STACS 2018) achieved O\vphantom\{\}({\textsurd}\{n\}) time per operation in that case, but with O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}3/4\vphantom\{\}) time per operation for large alphabets. We improve and extend this result with an algorithm that, given 1{$\leq$}x{$\leq$}k, achieves update time O\vphantom\{\}(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}\vphantom\{\}\{k\}+{\textsurd}\{\vphantom\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}k\vphantom\{\}\{x\}\vphantom\{\}) and query time O\vphantom\{\}(x). In particular, for k{$\geq$}{\textsurd}\{n\}, an appropriate choice of x yields O\vphantom\{\}({\textsurd}[3]\{nk\}) time per operation, which is O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}) when no threshold k is provided.},
  citationcount = {7},
  venue = {Annual Symposium on Combinatorial Pattern Matching},
  keywords = {data structure,dynamic,lower bound,query,query time,update,update time}
}

@article{cliffordTightCellProbe2012,
  title = {Tight Cell-Probe Bounds for Online Hamming Distance Computation},
  author = {Clifford, R. and Jalsenius, Markus and Sach, Benjamin},
  year = {2012},
  doi = {10.1137/1.9781611973105.48},
  abstract = {We show tight bounds for online Hamming distance computation in the cell-probe model with word size w. The task is to output the Hamming distance between a fixed string of length n and the last n symbols of a stream. We give a lower bound of {\textohm}({$\delta$}/w log n) time on average per output, where {$\delta$} is the number of bits needed to represent an input symbol. We argue that this bound is tight within the model. The lower bound holds under randomisation and amortisation.},
  citationcount = {8},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,lower bound,sorted,streaming}
}

@article{cliffordTimeBoundsFor2014,
  title = {Time Bounds for Streaming Problems},
  author = {Clifford, R. and Jalsenius, Markus and Sach, Benjamin},
  year = {2014},
  doi = {10.4086/toc.2019.v015a002},
  abstract = {We give tight cell-probe bounds for the time to compute convolution, multiplication and Hamming distance in a stream. The cell probe model is a particularly strong computational model and subsumes, for example, the popular word RAM model. We first consider online convolution where the task is to output the inner product between a fixed n-dimensional vector and a vector of the n most recent values from a stream. One symbol of the stream arrives at a time and the each output must be computed before the next symbols arrives. Next we show bounds for online multiplication where the stream consists of pairs of digits, one from each of two n digit numbers that are to be multiplied. One pair arrives at a time and the task is to output a single new digit from the product before the next pair of digits arrives. Finally we look at the online Hamming distance problem where the Hamming distance is outputted instead of the inner product. For each of these three problems, we give a lower bound of {\textohm}(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{{$\delta$}}\vphantom\{\}\{w\}n) time on average per output, where {$\delta$} is the number of bits needed to represent an input symbol and w is the cell or word size. We argue that these bound are in fact tight within the cell probe model.},
  citationcount = {Unknown},
  venue = {Theory of Computing},
  keywords = {cell probe,lower bound,sorted}
}

@article{cliffordUpperAndLower2018,
  title = {Upper and Lower Bounds for Dynamic Data Structures on Strings},
  author = {Clifford, R. and J{\o}rgensen, A. and Larsen, Kasper Green and Starikovskaya, Tatiana},
  year = {2018},
  doi = {10.4230/LIPIcs.STACS.2018.22},
  abstract = {We consider a range of simply stated dynamic data structure problems on strings. An update changes one symbol in the input and a query asks us to compute some function of the pattern of length m and a substring of a longer text. We give both conditional and unconditional lower bounds for variants of exact matching with wildcards, inner product, and Hamming distance computation via a sequence of reductions. As an example, we show that there does not exist an O(m\textsuperscript{\{\vphantom\}}1/2-{$\varepsilon$}\vphantom\{\}) time algorithm for a large range of these problems unless the online Boolean matrix-vector multiplication conjecture is false. We also provide nearly matching upper bounds for most of the problems we consider.},
  citationcount = {17},
  venue = {Symposium on Theoretical Aspects of Computer Science},
  keywords = {data structure,dynamic,lower bound,query,reduction,update}
}

@article{clineFast8Bit2007,
  title = {Fast 8-Bit Median Filtering Based on Separability},
  author = {Cline, David and White, K. and Egbert, P.},
  year = {2007},
  doi = {10.1109/ICIP.2007.4379820},
  abstract = {We present a fast 8-bit median filter implementation based on a separability argument. Our strategy is to start with a naive separable implementation of the median filter which displays O(1) time complexity per pixel, and then optimize this implementation to reduce the time constant. The optimizations that we employ include (a) lowering the histogram memory requirements by using unsigned shorts or bytes as histogram elements, (b) consolidating operations by performing multiple short additions with a single integer addition, (c) employing SSE instructions to further speed up the histogram updates, and (d) updating only the part of the histogram that contains the median. We show that by employing these rather straightforward optimizations, we can achieve filter speeds that approach the performance of the fastest proprietary median filters currently in use.},
  citationcount = {25},
  venue = {2007 IEEE International Conference on Image Processing},
  keywords = {update}
}

@article{cohen-addadLowerBoundsFor2018,
  title = {Lower Bounds for Text Indexing with Mismatches and Differences},
  author = {{Cohen-Addad}, Vincent and Feuilloley, Laurent and Starikovskaya, Tatiana},
  year = {2018},
  doi = {10.1137/1.9781611975482.70},
  abstract = {In this paper we study lower bounds for the fundamental problem of text indexing with mis-matches and differences. In this problem we are given a long string of length n, the "text", and the task is to preprocess it into a data structure such that given a query string Q, one can quickly identify substrings that are within Hamming or edit distance at most k from Q. This problem is at the core of various problems arising in biology and text processing. While exact text indexing allows linear-size data structures with linear query time, text indexing with k mismatches (or k differences) seems to be much harder: All known data structures have exponential dependency on k either in the space, or in the time bound. We provide conditional and pointer-machine lower bounds that make a step toward explaining this phenomenon. We start by demonstrating lower bounds for k={$\Theta$}(logn). We show that assuming the Strong Exponential Time Hypothesis, any data structure for text indexing that can be constructed in polynomial time cannot have strongly sublinear query time. This bound also extends to the setting where we only ask for (1+e)-approximate solutions for text indexing. However, in many applications the value of k is rather small, and one might hope that for small k we can develop more efficient solutions. We show that this would require a radically new approach as using the current methods one cannot avoid exponential dependency on k either in the space, or in the time bound for all even (8/{\textsurd}3){\textsurd}logn{$\leq$}k=o(logn). Our lower bounds also apply to the dictionary look-up problem, where instead of a text one is given a set of strings.},
  citationcount = {10},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,lower bound,query,query time}
}

@article{cohenMinimalIndicesFor2013,
  title = {Minimal Indices for Successor Search - (Extended Abstract)},
  author = {Cohen, S. and Fiat, A. and Hershcovitch, Moshik and Kaplan, Haim},
  year = {2013},
  doi = {10.1007/978-3-642-40313-2_26},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {International Symposium on Mathematical Foundations of Computer Science}
}

@article{cohenMinimalIndicesFor2015,
  title = {Minimal Indices for Predecessor Search},
  author = {Cohen, S. and Fiat, A. and Hershcovitch, Moshik and Kaplan, Haim},
  year = {2015},
  doi = {10.1016/j.ic.2014.09.005},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Information and Computation}
}

@article{cohenSolvingLinearPrograms2018,
  title = {Solving Linear Programs in the Current Matrix Multiplication Time},
  author = {Cohen, Michael B. and Lee, Y. and Song, Zhao},
  year = {2018},
  doi = {10.1145/3313276.3316303},
  abstract = {This paper shows how to solve linear programs of the form minAx=b,x{$\geq$}0 c{$\top$}x with n variables in time O*((n{$\omega$}+n2.5-{$\alpha$}/2+n2+1/6) log(n/{$\delta$})) where {$\omega$} is the exponent of matrix multiplication, {$\alpha$} is the dual exponent of matrix multiplication, and {$\delta$} is the relative accuracy. For the current value of {$\omega\sim$}2.37 and {$\alpha\sim$}0.31, our algorithm takes O*(n{$\omega$} log(n/{$\delta$})) time. When {$\omega$} = 2, our algorithm takes O*(n2+1/6 log(n/{$\delta$})) time. Our algorithm utilizes several new concepts that we believe may be of independent interest: (1) We define a stochastic central path method. (2) We show how to maintain a projection matrix {\textsurd}W A{$\top$}(AWA{$\top$})-1A {\textsurd}W in sub-quadratic time under {$\ell$}2 multiplicative changes in the diagonal matrix W.},
  citationcount = {312},
  venue = {Symposium on the Theory of Computing}
}

@article{cohenSolvingLinearPrograms2021,
  title = {Solving Linear Programs in the Current Matrix Multiplication Time},
  author = {Cohen, Michael B. and Lee, Y. and Song, Zhao},
  year = {2021},
  doi = {10.1145/3424305},
  abstract = {This article shows how to solve linear programs of the form minAx=b,x{$\geq$} 0 c{$\top$} x with n variables in time O*((n{$\omega$}+n2.5-{$\alpha$}/2+n2+1/6) log (n/{$\delta$})), where {$\omega$} is the exponent of matrix multiplication, {$\alpha$} is the dual exponent of matrix multiplication, and {$\delta$} is the relative accuracy. For the current value of {$\omega$} {$\delta$} 2.37 and {$\alpha$} {$\delta$} 0.31, our algorithm takes O*(n{$\omega$} log (n/{$\delta$})) time. When {$\omega$} = 2, our algorithm takes O*(n2+1/6 log (n/{$\delta$})) time. Our algorithm utilizes several new concepts that we believe may be of independent interest: {$\bullet$} We define a stochastic central path method. {$\bullet$} We show how to maintain a projection matrix {\textsurd} WA{$\top$} (AWA{$\top$})-1A{\textsurd} W in sub-quadratic time under {$\ell$}2 multiplicative changes in the diagonal matrix W.},
  citationcount = {54},
  venue = {Journal of the ACM}
}

@article{cohenStreamSamplingFor2008,
  title = {Stream Sampling for Variance-Optimal Estimation of Subset Sums},
  author = {Cohen, E. and Duffield, N. and Kaplan, Haim and Lund, C. and Thorup, M.},
  year = {2008},
  doi = {10.1137/1.9781611973068.136},
  abstract = {From a high volume stream of weighted items, we want to maintain a generic sample of a certain limited size k that we can later use to estimate the total weight of arbitrary subsets. This is the classic context of on-line reservoir sampling, thinking of the generic sample as a reservoir. We present an efficient reservoir sampling scheme, VarOptk, that dominates all previous schemes in terms of estimation quality. VarOptk provides variance optimal unbiased estimation of subset sums. More precisely, if we have seen n items of the stream, then for any subset size m, our scheme based on k samples minimizes the average variance over all subsets of size m. In fact, the optimality is against any off-line scheme with k samples tailored for the concrete set of items seen. In addition to optimal average variance, our scheme provides tighter worst-case bounds on the variance of particular subsets than previously possible. It is efficient, handling each new item of the stream in O(log k) time, which is optimal even on the word RAM. Finally, it is particularly well suited for combination of samples from different streams in a distributed setting.},
  citationcount = {63},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{coleGeometricRetrievalProblems1986,
  title = {Geometric Retrieval Problems},
  author = {Cole, R. and Yap, C.},
  year = {1986},
  doi = {10.1016/S0019-9958(84)80040-6},
  abstract = {No abstract available},
  citationcount = {68},
  venue = {24th Annual Symposium on Foundations of Computer Science (sfcs 1983)}
}

@article{comerUbiquitousBTree1979,
  title = {Ubiquitous B-Tree},
  author = {Comer, Douglas},
  year = {1979},
  doi = {10.1145/356770.356776},
  abstract = {B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree, contrasting the relatwe merits and costs of each implementatmn. It illustrates a general purpose access method whmh uses a B-tree.},
  citationcount = {2141},
  venue = {CSUR}
}

@article{coppersmithMatrixMultiplicationVia1987,
  title = {Matrix Multiplication via Arithmetic Progressions},
  author = {Coppersmith, D. and Winograd, S.},
  year = {1987},
  doi = {10.1145/28395.28396},
  abstract = {We present a new method for accelerating matrix multiplication asymptotically. This work builds on recent ideas of Volker Strassen, by using a basic trilinear form which is not a matrix product. We make novel use of the Salem-Spencer Theorem, which gives a fairly dense set of integers with no three-term arithmetic progression. Our resulting matrix exponent is 2.376.},
  citationcount = {2970},
  venue = {Symposium on the Theory of Computing}
}

@article{cordovaPracticalDynamicEntropy2016,
  title = {Practical Dynamic Entropy-Compressed Bitvectors with Applications},
  author = {Cordova, Joshimar and Navarro, G.},
  year = {2016},
  doi = {10.1007/978-3-319-38851-9_8},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {The Sea},
  keywords = {dynamic}
}

@article{cormodeAnImprovedData2004,
  title = {An Improved Data Stream Summary: The Count-Min Sketch and Its Applications},
  author = {Cormode, Graham and Muthukrishnan, S.},
  year = {2004},
  doi = {10.1007/978-3-540-24698-5_7},
  abstract = {No abstract available},
  citationcount = {2329},
  venue = {J. Algorithms}
}

@article{corrigan-gibbsTheFunctionInversion2019,
  title = {The Function-Inversion Problem: {{Barriers}} and Opportunities},
  author = {{Corrigan-Gibbs}, Henry and Kogan, Dmitry},
  year = {2019},
  doi = {10.1007/978-3-030-36030-6_16},
  abstract = {No abstract available},
  citationcount = {27},
  venue = {Electron. Colloquium Comput. Complex.}
}

@article{costAWeightedNearest2004,
  title = {A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features},
  author = {Cost, Scott and Salzberg, S.},
  year = {2004},
  doi = {10.1007/BF00993481},
  abstract = {No abstract available},
  citationcount = {161},
  venue = {Machine-mediated learning}
}

@article{cotumaccioEnhancedGraphPattern2024,
  title = {Enhanced Graph Pattern Matching},
  author = {Cotumaccio, Nicola},
  year = {2024},
  doi = {10.48550/arXiv.2402.16205},
  abstract = {Pattern matching queries on strings can be solved in linear time by Knuth-Morris-Pratt (KMP) algorithm. In 1973, Weiner introduced the suffix tree of a string [FOCS 1973] and showed that the seemingly more difficult problem of computing matching statistics can also be solved in liner time. Pattern matching queries on graphs are inherently more difficult: under the Orthogonal Vector hypothesis, the graph pattern matching problem cannot be solved in subquadratic time [TALG 2023]. The complexity of graph pattern matching can be parameterized by the topological complexity of the considered graph, which is captured by a parameter p [JACM 2023]. In this paper, we show that, as in the string setting, computing matching statistics on graph is as difficult as solving standard pattern matching queries. To this end, we introduce a notion of longest common prefix (LCP) array for arbitrary graphs.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {query}
}

@article{couveignesAFasterPseudo2012,
  title = {A Faster Pseudo-Primality Test},
  author = {Couveignes, J. and Ezome, Tony},
  year = {2012},
  doi = {10.1007/s12215-012-0088-0},
  abstract = {No abstract available},
  citationcount = {23},
  venue = {Rendiconti del circolo matematico di Palermo}
}

@article{couveignesEllipticPeriodsAnd2008,
  title = {Elliptic Periods and Primality Proving},
  author = {Couveignes, J. and Ezome, Tony and Lercier, R.},
  year = {2008},
  doi = {10.1016/J.JNT.2012.07.007},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {No venue available}
}

@article{couveignesFastConstructionOf2009,
  title = {Fast Construction of Irreducible Polynomials over Finite Fields},
  author = {Couveignes, J. and Lercier, R.},
  year = {2009},
  doi = {10.1007/s11856-012-0070-8},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {Israel Journal of Mathematics}
}

@article{coverElementsOfInformation2005,
  title = {Elements of Information Theory},
  author = {Cover, T. and Thomas, Joy A.},
  year = {2005},
  doi = {10.1002/047174882X},
  abstract = {Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.},
  citationcount = {46983},
  venue = {No venue available}
}

@article{coverNearestNeighborPattern1967,
  title = {Nearest Neighbor Pattern Classification},
  author = {Cover, T. and Hart, P.},
  year = {1967},
  doi = {10.1109/TIT.1967.1053964},
  abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error R of such a rule must be at least as great as the Bayes probability of error R{\textasciicircum}\{{$\ast$}\} --the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the M -category case that R{\textasciicircum}\{{$\ast$}\} {$\leq$}R {$\leq$}R{\textasciicircum}\{{$\ast$}\}(2 --MR{\textasciicircum}\{{$\ast$}\}/(M-1)) , where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
  citationcount = {13450},
  venue = {IEEE Transactions on Information Theory}
}

@article{crochemoreImprovedAlgorithmsFor2012,
  title = {Improved Algorithms for the Range next Value Problem and Applications},
  author = {Crochemore, M. and Iliopoulos, C. and Kubica, M. and Rahman, Mohammad Sohel and Wale{\'n}, Tomasz},
  year = {2012},
  doi = {10.4230/LIPIcs.STACS.2008.1359},
  abstract = {The Range Next Value problem (Problem RNV) is a recent interesting variant of the range search problems, where the query is for the immediate next (or equal) value of a given number within a given interval of an array. Problem RNV was introduced and studied very recently by Crochemore et. al [Finding Patterns In Given Intervals, MFCS 2007]. In this paper, we present improved algorithms for Problem RNV. We also show how this problem can be used to achieve optimal query time for a number of interesting variants of the classic pattern matching problems.},
  citationcount = {41},
  venue = {Theoretical Computer Science}
}

@article{cushmanScaledFenwickTrees2023,
  title = {Scaled Fenwick Trees},
  author = {Cushman, Matthew},
  year = {2023},
  doi = {10.1109/ACCESS.2023.3299352},
  abstract = {A novel data structure that enables the storage and retrieval of linear array numeric data with logarithmic time complexity updates, range sums, and rescaling is introduced and studied. Computing sums of ranges of arrays of numbers is a common computational problem encountered in data compression, coding, machine learning, computational vision, and finance, among other fields. Efficient data structures enabling {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}n {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} updates of the underlying data (including range updates), queries of sums over ranges, and searches for ranges with a given sum have been extensively studied ({\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}n {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} being the length of the array). Two solutions to this problem are well-known: Fenwick trees (also known as Binary Indexed Trees) and Segment Trees. The new data structure extends the capabilities for the first time to further enable multiplying (rescaling) ranges of the underlying data by a scalar as well in {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}n {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}. Scaling by 0 can be enabled, with the effect that subsequent updates may take {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}(n)\textsuperscript{\{\vphantom\}}2\vphantom\{\} {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} time. The new data structure introduced here consists of a pair of interacting Fenwick tree-like structures, one of which holds the unscaled values and the other of which holds the scalars. Experimental results demonstrating performance improvements for the multiplication operation on arrays from a few dozen to over 30 million data points are discussed. This research was done as part of Ajna Labs in the course of developing a decentralized finance protocol. It enables an efficient on-chain encoding and processing of an order book-like data structure used to manage lending, interest, and collateral.},
  citationcount = {Unknown},
  venue = {IEEE Access},
  keywords = {data structure,query,update}
}

@article{cyganApproximationAndParameterized2016,
  title = {Approximation and Parameterized Complexity of Minimax Approval Voting},
  author = {Cygan, Marek and Kowalik, Lukasz and Socala, Arkadiusz and Sornat, Krzysztof},
  year = {2016},
  doi = {10.1609/aaai.v31i1.10575},
  abstract = {We present three results on the complexity of Minimax Approval Voting. First, we study Minimax Approval Voting parameterized by the Hamming distance d from the solution to the votes. We show Minimax Approval Voting admits no algorithm running in time O*(2o(d log d)), unless the Exponential Time Hypothesis (ETH) fails. This means that the O*(d2d) algorithm of Misra, Nabeel and Singh is essentially optimal. Motivated by this, we then show a parameterized approximation scheme, running in time O*((3/{$\varepsilon$})2d), which is essentially tight assuming ETH. Finally, we get a new polynomial-time randomized approximation scheme for Minimax Approval Voting, which runs in time nO(1/{$\varepsilon$}2{$\cdot$}log(1/{$\varepsilon$})){$\cdot$}poly(m), where n is a number of voters and m is a number of alternatives. It almost matches the running time of the fastest known PTAS for Closest String due to Ma and Sun.},
  citationcount = {15},
  venue = {AAAI Conference on Artificial Intelligence}
}

@article{cyganLowerBoundsFor2015,
  title = {Lower Bounds for Approximation Schemes for {{Closest String}}},
  author = {Cygan, Marek and Lokshtanov, D. and Pilipczuk, Marcin and Pilipczuk, Michal and Saurabh, Saket},
  year = {2015},
  doi = {10.4230/LIPIcs.SWAT.2016.12},
  abstract = {In the Closest String problem one is given a family S of equal-length strings over some fixed alphabet, and the task is to find a string y that minimizes the maximum Hamming distance between y and a string from S. While polynomial-time approximation schemes (PTASes) for this problem are known for a long time [Li et al., J. ACM'02], no efficient polynomial-time approximation scheme (EPTAS) has been proposed so far. In this paper, we prove that the existence of an EPTAS for Closest String is in fact unlikely, as it would imply that \{FPT\}=\{W\}[1], a highly unexpected collapse in the hierarchy of parameterized complexity classes. Our proof also shows that the existence of a PTAS for Closest String with running time f({$\varepsilon$}){$\cdot$}n\textsuperscript{\{\vphantom\}}o(1/{$\varepsilon$})\vphantom\{\}, for any computable function f, would contradict the Exponential Time Hypothesis.},
  citationcount = {35},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {lower bound}
}

@article{cyganOnProblemsEquivalent2017,
  title = {On Problems Equivalent to (Min,+)-Convolution},
  author = {Cygan, Marek and Mucha, M. and Wegrzycki, Karol and W{\l}odarczyk, Micha{\l}},
  year = {2017},
  doi = {10.1145/3293465},
  abstract = {In recent years, significant progress has been made in explaining the apparent hardness of improving upon the naive solutions for many fundamental polynomially solvable problems. This progress has come in the form of conditional lower bounds---reductions from a problem assumed to be hard. The hard problems include 3SUM, All-Pairs Shortest Path, SAT, Orthogonal Vectors, and others. In the (min ,+)-convolution problem, the goal is to compute a sequence (c[i])n-1i=0, where c[k] = mini=0,{\dots}; ,k \{ a[i] + b[k-i]\}, given sequences (a[i])n-1i=0 and (b[i])n-1i=0. This can easily be done in O(n2) time, but no O(n2-{$\varepsilon$}) algorithm is known for {$\varepsilon$} {\textquestiondown} 0. In this article, we undertake a systematic study of the (min ,+)-convolution problem as a hardness assumption. First, we establish the equivalence of this problem to a group of other problems, including variants of the classic knapsack problem and problems related to subadditive sequences. The (min ,+)-convolution problem has been used as a building block in algorithms for many problems, notably problems in stringology. It has also appeared as an ad hoc hardness assumption. Second, we investigate some of these connections and provide new reductions and other results. We also explain why replacing this assumption with the Strong Exponential Time Hypothesis might not be possible for some problems.},
  citationcount = {94},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {lower bound,reduction}
}

@article{cyrillew.combettesRevisitingApproximateCaratheodory2019,
  title = {Revisiting the Approximate {{Carath{\'e}odory}} Problem via the {{Frank-Wolfe}} Algorithm},
  author = {Cyrille W. Combettes and S. Pokutta},
  year = {2019},
  journal = {Mathematical programming},
  doi = {10.1007/s10107-021-01735-x},
  annotation = {Citation Count: 23}
}

@article{czumajHaystackHuntingHints2020,
  title = {Haystack Hunting Hints and Locker Room Communication},
  author = {Czumaj, A. and Kontogeorgiou, George and Paterson, M.},
  year = {2020},
  doi = {10.1002/rsa.21114},
  abstract = {We want to efficiently find a specific object in a large unstructured set, which we model by a random n-permutation, and we have to do it by revealing just a single element. Clearly, without any help this task is hopeless and the best one can do is to select the element at random, and achieve the success probability 1n . Can we do better with some small amount of advice about the permutation, even without knowing the target object? We show that by providing advice of just one integer in \{0,1,{\dots},n-1\} , one can improve the success probability considerably, by a {$\Theta$}(lognloglogn) factor. We study this and related problems, and show asymptotically matching upper and lower bounds for their optimal probability of success. Our analysis relies on a close relationship of such problems to some intrinsic properties of random permutations related to the rencontres number.},
  citationcount = {1},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {communication,lower bound}
}

@article{d.dadushApproximateExactInteger2022,
  title = {From Approximate to Exact Integer Programming},
  author = {D. Dadush and F. Eisenbrand and T. Rothvoss},
  year = {2022},
  journal = {Conference on Integer Programming and Combinatorial Optimization},
  doi = {10.1007/s10107-024-02084-1},
  annotation = {Citation Count: 2}
}

@article{d.goyalFormalReconstructionSpeedup1997,
  title = {The Formal Reconstruction and Speedup of the Linear Time Fragment of {{Willard}}'s Relational Calculus Subset},
  author = {D. Goyal and R. Paige},
  year = {1997},
  journal = {Algorithmic Languages and Calculi},
  doi = {10.1007/978-0-387-35264-0_15},
  annotation = {Citation Count: 15}
}

@article{d.luginbuhlHierarchiesSpaceMeasures1993,
  title = {Hierarchies and {{Space Measures}} for {{Pointer Machines}}},
  author = {D. Luginbuhl and M. Loui},
  year = {1993},
  journal = {Information and Computation},
  doi = {10.1006/inco.1993.1032},
  abstract = {Abstract We present two definitions of space complexity for Schonhage{$\prime$}s pointer machine (PM): a uniform measure, mass, and a logarithmic measure, capacity. We consider how each space measure affects the time and space relationships between pointer machines and the more classical models of computation. For example, we show that a Turing machine of space complexity s can be simulated by a pointer machine of mass complexity O(s/log s) in real time. This is an improvement of a result of van Emde Boas. We show that space compression is possible for pointer machines, and we show that the time and space hierarchies for pointer machines are tight. We also present a simulation of an alternating pointer machine of time complexity t by a deterministic pointer machine of mass complexity O(t/log t).},
  annotation = {Citation Count: 12}
}

@article{d.ranjanComplexityOrparallelism1999,
  title = {On the Complexity of Or-Parallelism},
  author = {D. Ranjan and Enrico Pontelli and G. Gupta},
  year = {1999},
  journal = {New generation computing},
  doi = {10.1007/BF03037223},
  annotation = {Citation Count: 6}
}

@article{d.ranjanDataStructuresOrdersensitive2000,
  title = {Data Structures for Order-Sensitive Predicates in Parallel Nondeterministic Systems},
  author = {D. Ranjan and Enrico Pontelli and G. Gupta},
  year = {2000},
  journal = {Acta Informatica},
  doi = {10.1007/PL00013301},
  keywords = {data structure},
  annotation = {Citation Count: 7}
}

@article{d.ranjanEfficientAlgorithmsTemporal1998,
  title = {Efficient {{Algorithms}} for the {{Temporal Precedence Problem}}},
  author = {D. Ranjan and Enrico Pontelli and G. Gupta},
  year = {1998},
  journal = {Information Processing Letters},
  doi = {10.1016/S0020-0190(98)00141-0},
  annotation = {Citation Count: 5}
}

@article{d.ranjanLevelAncestorProblemPure2003,
  title = {The {{Level-Ancestor}} Problem on {{Pure Pointer Machines}}},
  author = {D. Ranjan and Enrico Pontelli},
  year = {2003},
  journal = {Information Processing Letters},
  doi = {10.1016/S0020-0190(02)00428-3},
  annotation = {Citation Count: 1}
}

@article{d.ranjanTemporalPrecedenceProblem2000,
  title = {The {{Temporal Precedence Problem}}},
  author = {D. Ranjan and Enrico Pontelli and G. Gupta and L. Longpr{\'e}},
  year = {2000},
  journal = {Algorithmica},
  doi = {10.1007/s004530010036},
  annotation = {Citation Count: 20}
}

@article{d.sleatorSelfAdjustingHeaps1986,
  title = {Self-{{Adjusting Heaps}}},
  author = {D. Sleator and R. Tarjan},
  year = {1986},
  journal = {SIAM journal on computing (Print)},
  doi = {10.1137/0215004},
  abstract = {In this paper we explore two themes in data structure design: amortized computational complexity and self adjustment. We are motivated by the following observations. In most applications of data structures, we wish to perform not just a single operation but a sequence of operations, possibly having correlated behavior. By averaging the running time per operation over a worst-case sequence of operations, we can sometimes obtain an overall time bound much smaller than the worst-case time per operation multiplied by the number of operations. We call this kind of averaging amortization.Standard kinds of data structures, such as the many varieties of balanced trees, are specifically designed so that the worst-case time per operation is small. Such efficiency is achieved by imposing an explicit structural constraint that must be maintained during updates, at a cost of both running time and storage space. However, if amortized running time is the complexity measure of interest, we can guarantee efficiency withou...},
  keywords = {data structure,update},
  annotation = {Citation Count: 141}
}

@article{d.wagnerLineartimeAlgorithmEdgedisjoint1993,
  title = {A Linear-Time Algorithm for Edge-Disjoint Paths in Planar Graphs},
  author = {D. Wagner and K. Weihe},
  year = {1993},
  journal = {Comb.},
  doi = {10.1007/BF01294465},
  annotation = {Citation Count: 61}
}

@article{dahlgaardOnTheHardness2016,
  title = {On the Hardness of Partially Dynamic Graph Problems and Connections to Diameter},
  author = {Dahlgaard, S{\o}ren},
  year = {2016},
  doi = {10.4230/LIPIcs.ICALP.2016.48},
  abstract = {Conditional lower bounds for dynamic graph problems has received a great deal of attention in recent years. While many results are now known for the fully-dynamic case and such bounds often imply worst-case bounds for the partially dynamic setting, it seems much more difficult to prove amortized bounds for incremental and decremental algorithms. In this paper we consider partially dynamic versions of three classic problems in graph theory. Based on popular conjectures we show that: -- No algorithm with amortized update time O(n\textsuperscript{\{\vphantom\}}1-{$\varepsilon$}\vphantom\{\}) exists for incremental or decremental maximum cardinality bipartite matching. This significantly improves on the O(m\textsuperscript{\{\vphantom\}}1/2-{$\varepsilon$}\vphantom\{\}) bound for sparse graphs of Henzinger et al. [STOC'15] and O(n\textsuperscript{\{\vphantom\}}1/3-{$\varepsilon$}\vphantom\{\}) bound of Kopelowitz, Pettie and Porat. Our linear bound also appears more natural. In addition, the result we present separates the node-addition model from the edge insertion model, as an algorithm with total update time O(m{\textsurd}\{n\}) exists for the former by Bosek et al. [FOCS'14]. -- No algorithm with amortized update time O(m\textsuperscript{\{\vphantom\}}1-{$\varepsilon$}\vphantom\{\}) exists for incremental or decremental maximum flow in directed and weighted sparse graphs. No such lower bound was known for partially dynamic maximum flow previously. Furthermore no algorithm with amortized update time O(n\textsuperscript{\{\vphantom\}}1-{$\varepsilon$}\vphantom\{\}) exists for directed and unweighted graphs or undirected and weighted graphs. -- No algorithm with amortized update time O(n\textsuperscript{\{\vphantom\}}1/2-{$\varepsilon$}\vphantom\{\}) exists for incremental or decremental (4/3-{$\varepsilon$}')-approximating the diameter of an unweighted graph. We also show a slightly stronger bound if node additions are allowed. [...]},
  citationcount = {53},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {dynamic,lower bound,update,update time}
}

@article{dalirooyfardApproximationAlgorithmsAnd2022,
  title = {Approximation Algorithms and Hardness for N-{{Pairs}} Shortest Paths and All-Nodes Shortest Cycles},
  author = {Dalirooyfard, Mina and Jin, Ce and Williams, V. V. and Wein, Nicole},
  year = {2022},
  doi = {10.1109/FOCS54457.2022.00034},
  abstract = {We study the approximability of two related problems on graphs with n nodes and m edges: n-Pairs Shortest Paths (n-PSP), where the goal is to find a shortest path between O(n) prespecified pairs, and All Node Shortest Cycles (ANSC), where the goal is to find the shortest cycle passing through each node. Approximate n-PSP has been previously studied, mostly in the context of distance oracles. We ask the question of whether approximate n-PSP can be solved faster than by using distance oracles or All Pair Shortest Paths (APSP). ANSC has also been studied previously, but only in terms of exact algorithms, rather than approximation.We provide a thorough study of the approximability of n PSP and ANSC, providing a wide array of algorithms and conditional lower bounds that trade off between running time and approximation ratio.A highlight of our conditional lower bounds results is that for any integer k{$\geq$}1, under the combinatorial 4k-clique hypothesis, there is no combinatorial algorithm for unweighted undirected n-PSP with approximation ratio better than 1+1/k that runs in O(m\textsuperscript{\{\vphantom\}}2-2/(k+1)\vphantom\{\}n\textsuperscript{\{\vphantom\}}1/(k+1)-{$\varepsilon$}\vphantom\{\}) time. This nearly matches an upper bound implied by the result of Agarwal (2014).Our algorithms use a surprisingly wide range of techniques, including techniques from the girth problem, distance oracles, approximate APSP, spanners, fault-tolerant spanners, and link-cut trees.A highlight of our algorithmic results is that one can solve both n-PSP and ANSC in O(m+n\textsuperscript{\{\vphantom\}}3/2+{$\in$}\vphantom\{\}) time1 with approximation factor 2+{$\varepsilon$} (and additive error that is function of {$\varepsilon$}), for any constant {$\varepsilon$}0. For n-PSP, our conditional lower bounds imply that this approximation ratio is nearly optimal for any subquadratic-time combinatorial algorithm. We further extend these algorithms for n-PSP and ANSC to obtain a time/accuracy trade-off that includes near-linear time algorithms. 1O\vphantom\{\} hides sub-polynomial factors.Additionally, for ANSC, for all integers k{$\geq$}1, we extend the very recent almost k-approximation algorithm for the girth problem that works in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}1+1/k\vphantom\{\}) time [Kadria et al. SODA'22], and obtain an almost k-approximation algorithm for ANSC in O\vphantom\{\}(mn\textsuperscript{\{\vphantom\}}1/k\vphantom\{\}) time.},
  citationcount = {11},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {lower bound}
}

@article{dalirrooyfardNewTechniquesFor2020,
  title = {New Techniques for Proving Fine-Grained Average-Case Hardness},
  author = {Dalirrooyfard, Mina and Lincoln, Andrea and Williams, V. V.},
  year = {2020},
  doi = {10.1109/FOCS46700.2020.00077},
  abstract = {The recent emergence of fine-grained cryptography strongly motivates developing an average-case analogue of Fine-Grained Complexity (FGC). Prior work [Goldreich-Rothblum 2018, Boix-Adser{\`a} et al. 2019, Ball et al. 2017] developed worst-case to average-case fine-grained reductions (WCtoACFG) for certain algebraic and counting problems over natural distributions and used them to obtain a limited set of cryptographic primitives. To obtain stronger cryptographic primitives based on standard FGC assumptions, ideally, one would like to develop WCtoACFG reductions from the core hard problems of FGC, Orthogonal Vectors (OV), CNF-SAT, 3SUM, All-Pairs Shortest Paths (APSP) and zero-k clique. Unfortunately, it is unclear whether these problems actually are hard for any natural distribution. It is known, that e.g. OV can be solved quickly for very natural distributions [Kane-Williams 2019], and in this paper we show that even counting the number of OV pairs on average has a fast algorithm. This paper defines new versions of OV, k\{SUM\} and zero-k-clique that are both worst-case and average-case fine-grained hard assuming the core hypotheses of FGC. We then use these as a basis for fine-grained hardness and average-case hardness of other problems. The new problems represent their inputs in a certain ``factored'' form. We call them ``factored''-OV, ``factored''-zero-k-clique and ``factored''-3SUM. We show that factored-k-OV and factored k\{SUM\} are equivalent and are complete for a class of problems defined over Boolean functions. Factored zero-k-clique is also complete, for a different class of problems. Our hard factored problems are also simple enough that we can reduce them to many other problems, e.g. to edit distance, k-LCS and versions of Max-Flow. We further consider counting variants of the factored problems and give WCtoACFG reductions for them for a natural distribution. Through FGC reductions we then get average-case hardness for well-studied problems like regular expression matching from standard worst-case FGC assumptions. To obtain our WCtoACFG reductions, we formalize the framework of [Boix-Adser{\`a} et al. 2019] that was used to give a WCtoACFG reduction for counting k-cliques. We define an explicit property of problems such that if a problem has that property one can use the framework on the problem to get a WCtoACFG self reduction. We then use the framework to slightly extend Bolx-Adser{\`a} et al.'s average-case counting k-cliques result to average-case hardness for counting arbitrary subgraph patterns of constant size in -partite graphs. The fine-grained public-key encryption scheme of [LaVigne et al.'20] is based on an average-case hardness hypothesis for the decision problem, zero-k-clique, and the known techniques for building such schemes break down for algebraic/counting problems. Meanwhile, the WCtoACFG reductions so far have only been for counting problems. To bridge this gap, we show that for a natural distribution, an algorithm that detects a zero-k-clique with high enough probability also implies an algorithm that can count zero-k-cliques with high probability. This gives hope that the FGC cryptoscheme of [LaVigne et al.'20] can be based on standard FGC assumptions.},
  citationcount = {21},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {reduction}
}

@article{dalirrooyfardTowardsOptimalOutput2023,
  title = {Towards Optimal Output-Sensitive Clique Listing or: {{Listing}} Cliques from Smaller Cliques},
  author = {Dalirrooyfard, Mina and Mathialagan, Surya and Williams, V. V. and Xu, Yinzhan},
  year = {2023},
  doi = {10.1145/3618260.3649663},
  abstract = {We study the problem of finding and listing k-cliques in an m-edge, n-vertex graph, for constant k{$\geq$} 3. This is a fundamental problem of both theoretical and practical importance. Our first contribution is an algorithmic framework for finding k-cliques that gives the first improvement in 19 years over the old runtimes for 4 and 5-clique finding, as a function of m [Eisenbrand and Grandoni, TCS'04]. With the current bounds on matrix multiplication, our algorithms run in O(m1.66) and O(m2.06) time, respectively, for 4-clique and 5-clique finding. Our main contribution is an output-sensitive algorithm for listing k-cliques, for any constant k{$\geq$} 3. We complement the algorithm with tight lower bounds based on standard fine-grained assumptions. Previously, the only known conditionally optimal output-sensitive algorithms were for the case of 3-cliques given by Bj'orklund, Pagh, Vassilevska W. and Zwick [ICALP'14]. If the matrix multiplication exponent {$\omega$} is 2, and if the number of k-cliques t is large enough, the running time of our algorithms is {\~O}(min\{m1/k-2t1 - 2/k(k-2),n2/k-1t1-2/k(k-1)\}), and this is tight under the Exact-k-Clique Hypothesis. This running time naturally extends the running time obtained by Bj'orklund, Pagh, Vassilevska W. and Zwick for k=3. Our framework is very general in that it gives k-clique listing algorithms whose running times can be measured in terms of the number of {$\ell$}-cliques {$\Delta\ell$} in the graph for any 1{$\leq$} {$\ell$}{\textexclamdown}k. This generalizes the typical parameterization in terms of n (the number of 1-cliques) and m (the number of 2-cliques). If {$\omega$} is 2, and if the size of the output, {$\Delta$}k, is sufficiently large, then for every {$\ell$}{\textexclamdown}k, the running time of our algorithm for listing k-cliques is {\~O}({$\Delta\ell$}2/{$\ell$} (k - {$\ell$}){$\Delta$}k1-2/k(k-{$\ell$})). We also show that this runtime is optimal for all 1 {$\leq$} {$\ell$} {\textexclamdown} k under the Exact k-Clique hypothesis.},
  citationcount = {5},
  venue = {Symposium on the Theory of Computing},
  keywords = {lower bound}
}

@article{dallantConditionalLowerBounds2021,
  title = {Conditional Lower Bounds for Dynamic Geometric Measure Problems},
  author = {Dallant, Justin and Iacono, J.},
  year = {2021},
  doi = {10.4230/LIPIcs.ESA.2022.39},
  abstract = {We give new polynomial lower bounds for a number of dynamic measure problems in computational geometry. These lower bounds hold in the Word-RAM model, conditioned on the hardness of either 3SUM, APSP, or the Online Matrix-Vector Multiplication problem [Henzinger et al., STOC 2015]. In particular we get lower bounds in the incremental and fully-dynamic settings for counting maximal or extremal points in R 3 , different variants of Klee's Measure Problem, problems related to finding the largest empty disk in a set of points, and querying the size of the i 'th convex layer in a planar set of points. We also answer a question of Chan et al. [SODA 2022] by giving a conditional lower bound for dynamic approximate square set cover. While many conditional lower bounds for dynamic data structures have been proven since the seminal work of P{\u a}tra{\c s}cu [STOC 2010], few of them relate to computational geometry problems. This is the first paper focusing on this topic. Most problems we consider can be solved in O ( n log n ) time in the static case and their dynamic versions have only been approached from the perspective of improving known upper bounds. One exception to this is Klee's measure problem in R 2 , for which Chan [CGTA 2010] gave an unconditional {\textohm}( {\textsurd} n ) lower bound on the worst-case update time. By a similar approach, we show that such a lower bound also holds for an important special case of Klee's measure problem in R 3 known as the Hypervolume Indicator problem, even for amortized runtime in the incremental setting. discussions about the topic of this paper.},
  citationcount = {4},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,dynamic,lower bound,query,static,update,update time}
}

@article{dallantHowFastCan2022,
  title = {How Fast Can We Play Tetris Greedily with Rectangular Pieces?},
  author = {Dallant, Justin and Iacono, J.},
  year = {2022},
  doi = {10.4230/LIPIcs.FUN.2022.13},
  abstract = {Consider a variant of Tetris played on a board of width w and infinite height, where the pieces are axis-aligned rectangles of arbitrary integer dimensions, the pieces can only be moved before letting them drop, and a row does not disappear once it is full. Suppose we want to follow a greedy strategy: let each rectangle fall where it will end up the lowest given the current state of the board. To do so, we want a data structure which can always suggest a greedy move. In other words, we want a data structure which maintains a set of O(n) rectangles, supports queries which return where to drop the rectangle, and updates which insert a rectangle dropped at a certain position and return the height of the highest point in the updated set of rectangles. We show via a reduction to the Multiphase problem [P{\u \{}a\vphantom\{\}tra{\c \{}s\vphantom\{\}cu, 2010] that on a board of width w={$\Theta$}(n), if the OMv conjecture [Henzinger et al., 2015] is true, then both operations cannot be supported in time O(n\textsuperscript{\{\vphantom\}}1/2-{$\epsilon$}\vphantom\{\}) simultaneously. The reduction also implies polynomial bounds from the 3-SUM conjecture and the APSP conjecture. On the other hand, we show that there is a data structure supporting both operations in O(n\textsuperscript{\{\vphantom\}}1/2\vphantom\{\}\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}n) time on boards of width n\textsuperscript{\{\vphantom\}}O(1)\vphantom\{\}, matching the lower bound up to a n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} factor.},
  citationcount = {2},
  venue = {Fun with Algorithms},
  keywords = {data structure,lower bound,query,reduction,update}
}

@article{damgrdLinearZeroKnowledgde1996,
  title = {Linear Zero-Knowledgde. {{A}} Note on Efficient Zero-Knowledge Proofs and Arguments},
  author = {Damg{\aa}rd, I. and Cramer, R.},
  year = {1996},
  doi = {10.7146/BRICS.V3I7.19970},
  abstract = {We present a zero-knowledge proof system [19] for any NP language L, which allows showing that x in L with error probability less than 2{\textasciicircum}-k using communication corresponding to O({\textbar}x{\textbar}{\textasciicircum}c) + k bit commitments, where c is a constant depending only on L. The proof can be based on any bit commitment scheme with a particular set of properties. We suggest an efficient implementation based on factoring. We also present a 4-move perfect zero-knowledge interactive argument for any NP-language L. On input x in L, the communication complexity is O({\textbar}x{\textbar}{\textasciicircum}c) max(k; l) bits, where l is the security parameter for the prover. Again, the protocol can be based on any bit commitment scheme with a particular set of properties. We suggest efficient implementations based on discrete logarithms or factoring. We present an application of our techniques to multiparty computations, allowing for example t committed oblivious transfers with error probability 2{\textasciicircum}-k to be done simultaneously using O(t+k) commitments. Results for general computations follow from this. As a function of the security parameters, our protocols have the smallest known asymptotic communication complexity among general proofs or arguments for NP. Moreover, the constants involved are small enough for the protocols to be practical in a realistic situation: both protocols are based on a Boolean formula Phi containing and- , or- and not-operators which verifies an NP-witness of membership in L. Let n be the number of times this formula reads an input variable. Then the communication complexity of the protocols when using our concrete commitment schemes can be more precisely stated as at most 4n + k + 1 commitments for the interactive proof and at most 5nl +5l bits for the argument (assuming k the number of commitments required for the proof is linear in n. Both protocols are also proofs of knowledge of an NP-witness of membership in the language involved.},
  citationcount = {114},
  venue = {No venue available}
}

@article{danalistarhSearchFastestConcurrent2019,
  title = {In {{Search}} of the {{Fastest Concurrent Union-Find Algorithm}}},
  author = {Dan Alistarh and Alexander Fedorov and N. Koval},
  year = {2019},
  journal = {International Conference on Principles of Distributed Systems},
  doi = {10.4230/LIPIcs.OPODIS.2019.15},
  abstract = {Union-Find (or Disjoint-Set Union) is one of the fundamental problems in computer science; it has been well-studied from both theoretical and practical perspectives in the sequential case. Recently, there has been mounting interest in analyzing this problem in the concurrent scenario, and several asymptotically-efficient algorithms have been proposed. Yet, to date, there is very little known about the practical performance of concurrent Union-Find.  This work addresses this gap. We evaluate and analyze the performance of several concurrent Union-Find algorithms and optimization strategies across a wide range of platforms (Intel, AMD, and ARM) and workloads (social, random, and road networks, as well as integrations into more complex algorithms). We first observe that, due to the limited computational cost, the number of induced cache misses is the critical determining factor for the performance of existing algorithms. We introduce new techniques to reduce this cost by storing node priorities implicitly and by using plain reads and writes in a way that does not affect the correctness of the algorithms. Finally, we show that Union-Find implementations are an interesting application for Transactional Memory (TM): one of the fastest algorithm variants we discovered is a sequential one that uses coarse-grained locking with the lock elision optimization to reduce synchronization cost and increase scalability.},
  annotation = {Citation Count: 6}
}

@article{dasANearOptimal2022,
  title = {A Near-Optimal Offline Algorithm for Dynamic All-Pairs Shortest Paths in Planar Digraphs},
  author = {Das, Debarati and Gutenberg, Maximilian Probst and {Wulff-Nilsen}, Christian},
  year = {2022},
  doi = {10.1137/1.9781611977073.138},
  abstract = {In the planar, dynamic All-Pairs Shortest Paths (APSP) problem, a planar, weighted digraph G undergoes a sequence of edge weight updates and the goal is to maintain a data structure on G , that can quickly answer distance queries between any two vertices x, y {$\in$} V ( G ). The currently best algorithms [FOCS'01, SODA'05] for this problem require {\texttildelow} O ( n 2 / 3 ) worst-case update and query time, while conditional lower bounds [FOCS'16] show that either update or query time {\texttildelow}{\textohm}( {\textsurd} n ) is needed 1 . In this article, we present the first algorithm with near-optimal {\texttildelow} O ( {\textsurd} n ) worst-case update and query time for the offline setting, where the update sequence is given initially. This result is obtained by giving the first offline dynamic algorithm for maintaining dense distance graphs (DDGs) faster than recomputing from scratch after each update. Further, we also present an online algorithm for the incremental APSP problem with {\texttildelow} O ( {\textsurd} n ) worst-case update/ query time. This allows us to reduce the online dynamic APSP problem to the online decremental APSP problem, which constitutes partial progress even for the online version of this notorious problem.},
  citationcount = {6},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,lower bound,query,query time,update}
}

@article{dasDynamicBooleanFormula2021,
  title = {Dynamic Boolean Formula Evaluation},
  author = {Das, Rathish and Lincoln, Andrea and Lynch, J. and Munro, J.},
  year = {2021},
  doi = {10.4230/LIPIcs.ISAAC.2021.61},
  abstract = {We present a linear space data structure for Dynamic Evaluation of k -CNF Boolean Formulas which achieves O ( m 1 - 1 /k ) query and variable update time where m is the number of clauses in the formula and clauses are of size at most a constant k . Our algorithm is additionally able to count the total number of satisfied clauses. We then show how this data structure can be parallelized in the PRAM model to achieve O (log m ) span (i.e. parallel time) and still O ( m 1 - 1 /k ) work. This parallel algorithm works in the stronger Binary Fork model. We then give a series of lower bounds on the problem including an average-case result showing the lower bounds hold even when the updates to the variables are chosen at random. Specifically, a reduction from k - Clique shows that dynamically counting the number of satisfied clauses takes time at least n 2 {$\omega$} - 3 6 {\textsurd} 2 k - 1 - o ( {\textsurd} k ) , where 2 {$\leq$} {$\omega$} {\textexclamdown} 2 . 38 is the matrix multiplication constant. We show the Combinatorial k -Clique Hypothesis implies a lower bound of m (1 - k - 1 / 2 )(1 - o (1)) which suggests our algorithm is close to optimal without involving Matrix Multiplication or new techniques. We next give an average-case reduction to k -clique showing the prior lower bounds hold even when the updates are chosen at random. We use our conditional lower bound to show any Binary Fork algorithm solving these problems requires at least {\textohm}(log m ) span, which is tight against our algorithm in this model. Finally, we give an unconditional linear space lower bound for Dynamic k -CNF Boolean Formula Evaluation.},
  citationcount = {Unknown},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {data structure,dynamic,lower bound,query,reduction,update,update time}
}

@article{dasguptaLearningMixturesOf1999,
  title = {Learning Mixtures of Gaussians},
  author = {Dasgupta, S.},
  year = {1999},
  doi = {10.1109/SFFCS.1999.814639},
  abstract = {Mixtures of Gaussians are among the most fundamental and widely used statistical models. Current techniques for learning such mixtures from data are local search heuristics with weak performance guarantees. We present the first provably correct algorithm for learning a mixture of Gaussians. This algorithm is very simple and returns the true centers of the Gaussians to within the precision specified by the user with high probability. It runs in time only linear in the dimension of the data and polynomial in the number of Gaussians.},
  citationcount = {723},
  venue = {40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039)}
}

@article{dasguptaOnComputingDiscretized2022,
  title = {On Computing Discretized {{Ricci}} Curvatures of Graphs: {{Local}} Algorithms and (Localized) Fine-Grained Reductions},
  author = {Dasgupta, B. and Grigorescu, Elena and Mukherjee, Tamalika},
  year = {2022},
  doi = {10.1016/j.tcs.2023.114127},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Theoretical Computer Science},
  keywords = {reduction}
}

@article{dasguptaOnComputingOllivier2022,
  title = {On Computing {{Ollivier-Ricci}} Curvatures of Graphs: Fine-Grained Reductions and Local Algorithms},
  author = {Dasgupta, B. and Grigorescu, Elena and Mukherjee, Tamalika},
  year = {2022},
  doi = {10.48550/arXiv.2208.09535},
  abstract = {Characterizing shapes of high-dimensional objects via Ricci curvatures plays a critical role in many research areas in mathematics and physics. However, even though several discretizations of Ricci curvatures for discrete combinatorial objects such as networks have been proposed and studied by mathe-maticians, the computational complexity aspects of these discretizations have escaped the attention of theoretical computer scientists to a large extent. In this paper, we study one such discretization, namely the Ollivier-Ricci curvature, from the perspective of efficient computation by fine-grained reductions and local query-based algorithms. Our main contributions are the following.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {query,reduction}
}

@article{dasguptaSparseAndLopsided2012,
  title = {Sparse and Lopsided Set Disjointness via Information Theory},
  author = {Dasgupta, Anirban and Kumar, Ravi and Sivakumar, D.},
  year = {2012},
  doi = {10.1007/978-3-642-32512-0_44},
  abstract = {No abstract available},
  citationcount = {31},
  venue = {International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques}
}

@article{daskalakisANoteOn2006,
  title = {A Note on Approximate {{Nash}} Equilibria},
  author = {Daskalakis, C. and Mehta, Aranyak and Papadimitriou, C.},
  year = {2006},
  doi = {10.1016/j.tcs.2008.12.031},
  abstract = {No abstract available},
  citationcount = {199},
  venue = {Theoretical Computer Science}
}

@article{daskalakisOnObliviousPtas2009,
  title = {On Oblivious {{PTAS}}'s for Nash Equilibrium},
  author = {Daskalakis, C. and Papadimitriou, C.},
  year = {2009},
  doi = {10.1145/1536414.1536427},
  abstract = {If a class of games is known to have a Nash equilibrium with probability values that are either zero or {\textohm}(1) -- and thus with support of bounded size -- then obviously this equilibrium can be found exhaustively in polynomial time. Somewhat surprisingly, we show that there is a PTAS for the class of games whose equilibria are guaranteed to have small --- O(1/n) -- values, and therefore large -- {\textohm}(n) -- supports. We also point out that there is a PTAS for games with sparse payoff matrices, a family for which the exact problem is known to be PPAD-complete [Chen, Deng, Teng 2006]. Both algorithms are of a special kind that we call oblivious: The algorithm just samples a fixed distribution on pairs of mixed strategies, and the game is only used to determine whether the sampled strategies comprise an {$\varepsilon$}-Nash equilibrium; the answer is "yes" with inverse polynomial probability (in the second case, the algorithm is actually deterministic). These results bring about the question: Is there an oblivious PTAS for finding a Nash equilibrium in general games? We answer this question in the negative; our lower bound comes close to the quasi-polynomial upper bound of [Lipton, Markakis, Mehta 2003]. Another recent PTAS for anonymous games [Daskalakis, Papadimitriou 2007 and 2008, Daskalakis 2008] is also oblivious in a weaker sense appropriate for this class of games (it samples from a fixed distribution on unordered collections of mixed strategies), but its running time is exponential in 1/{$\varepsilon$}. We prove that any oblivious PTAS for anonymous games with two strategies and three player types must have 1/{$\varepsilon\alpha$} in the exponent of the running time for some {$\alpha$} {$\geq$} 1/3, rendering the algorithm in [Daskalakis 2008] (which works with any bounded number of player types) essentially optimal within oblivious algorithms. In contrast, we devise a poly n {$\bullet$} (1/{$\varepsilon$})O(2(1/{$\varepsilon$})) non-oblivious PTAS for anonymous games with two strategies and any bounded number of player types. The key idea of our algorithm is to search not over unordered sets of mixed strategies, but over a carefully crafted set of collections of the first O(log 1/{$\varepsilon$}) moments of the distribution of the number of players playing strategy 1 at equilibrium. The algorithm works because of a probabilistic result of more general interest that we prove: the total variation distance between two sums of independent indicator random variables decreases exponentially with the number of moments of the two sums that are equal, independent of the number of indicators.},
  citationcount = {77},
  venue = {Symposium on the Theory of Computing}
}

@article{daskalakisOnTheComplexity2011,
  title = {On the Complexity of Approximating a {{Nash}} Equilibrium},
  author = {Daskalakis, C.},
  year = {2011},
  doi = {10.1145/2483699.2483703},
  abstract = {We show that computing a relative---that is, multiplicative as opposed to additive---approximate Nash equilibrium in two-player games is PPAD-complete, even for constant values of the approximation. Our result is the first constant inapproximability result for the problem, since the appearance of the original results on the complexity of the Nash equilibrium [8, 5, 7]. Moreover, it provides an apparent---assuming that PPAD {$\not\subseteq$} TIME(nO(log n))---dichotomy between the complexities of additive and relative notions of approximation, since for constant values of additive approximation a quasi-polynomial-time algorithm is known [22]. Such a dichotomy does not arise for values of the approximation that scale with the size of the game, as both relative and additive approximations are PPAD-complete [7]. As a byproduct, our proof shows that the Lipton-Markakis-Mehta sampling lemma is not applicable to relative notions of constant approximation, answering in the negative direction a question posed to us by Shang-Hua Teng [26].},
  citationcount = {96},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{daskalakisProgressInApproximate2007,
  title = {Progress in Approximate Nash Equilibria},
  author = {Daskalakis, C. and Mehta, Aranyak and Papadimitriou, C.},
  year = {2007},
  doi = {10.1145/1250910.1250962},
  abstract = {It is known [5] that an additively {$\varepsilon$}-approximate Nash equilibrium (with supports of size at most two) can be computed in polynomial time in any 2-player game with {$\varepsilon$}=.5. It is also known that no approximation better than .5 is possible unless equilibria with support larger than logn are considered, where n is the number of strategies per player. We give a polynomial algorithm for computing an {$\varepsilon$}-approximate Nash equilibrium in 2-player games with {$\varepsilon$} {$\approx$} .38; our algorithm computes equilibria with arbitrarily large supports.},
  citationcount = {139},
  venue = {ACM Conference on Economics and Computation}
}

@article{daskalakisTheComplexityOf2006,
  title = {The Complexity of Computing a {{Nash}} Equilibrium},
  author = {Daskalakis, C. and Goldberg, P. and Papadimitriou, C.},
  year = {2006},
  doi = {10.1145/1132516.1132527},
  abstract = {We resolve the question of the complexity of Nash equilibrium by showing that the problem of computing a Nash equilibrium in a game with 4 or more players is complete for the complexity class PPAD. Our proof uses ideas from the recently-established equivalence between polynomial time solvability of normal form games and graphical games, establishing that these kinds of games can simulate a PPAD-complete class of Brouwer functions.},
  citationcount = {1388},
  venue = {Symposium on the Theory of Computing}
}

@article{dasOnTheHardness2022,
  title = {On the Hardness of the Finite Field Isomorphism Problem},
  author = {Das, Dipayan and Joux, A.},
  year = {2022},
  doi = {10.1007/978-3-031-30589-4_12},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {IACR Cryptology ePrint Archive}
}

@article{datarLocalitySensitiveHashing2004,
  title = {Locality-Sensitive Hashing Scheme Based on p-Stable Distributions},
  author = {Datar, Mayur and Immorlica, Nicole and Indyk, P. and Mirrokni, V.},
  year = {2004},
  doi = {10.1145/997817.997857},
  abstract = {We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on p-stable distributions.Our scheme improves the running time of the earlier algorithm for the case of the lp norm. It also yields the first known provably efficient approximate NN algorithm for the case p{\textexclamdown}1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain "bounded growth" condition.Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.},
  citationcount = {3198},
  venue = {SCG '04}
}

@article{davidImprovedSeparationsBetween2008,
  title = {Improved Separations between Nondeterministic and Randomized Multiparty Communication},
  author = {David, Matei and Pitassi, T. and Viola, Emanuele},
  year = {2008},
  doi = {10.1145/1595391.1595392},
  abstract = {We exhibit an explicit function {\textexclamdown}i{\textquestiondown}f{\textexclamdown}/i{\textquestiondown} : \{0, 1\}{\textexclamdown}sup{\textquestiondown}n{\textexclamdown}/sup{\textquestiondown} {$\rightarrow$}\{0, 1\} that can be computed by a nondeterministic number-on-forehead protocol communicating {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) bits, but that requires {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}{\textohm}(1){\textexclamdown}/sup{\textquestiondown} bits of communication for randomized number-on-forehead protocols with {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}\,=\,{\textexclamdown}i{\textquestiondown}{$\Delta$}{\textexclamdown}/i{\textquestiondown}{$\cdot$}log{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} players, for any fixed {\textexclamdown}i{\textquestiondown}{$\Delta$}{\textexclamdown}/i{\textquestiondown}\,{\textexclamdown}\,1. Recent breakthrough results for the Set-Disjointness function [Lee and Shraibman 2008; Chattopadhyay and Ada 2008] based on the work of Sherstov [2009; 2008a] imply such a separation but only when the number of players is {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}\,{\textexclamdown}\,loglog{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}. We also show that for any {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}\,=\,{\textexclamdown}i{\textquestiondown}A{\textexclamdown}/i{\textquestiondown} {$\cdot$}loglog{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} the above function {\textexclamdown}i{\textquestiondown}f{\textexclamdown}/i{\textquestiondown} is computable by a small circuit whose depth is constant whenever {\textexclamdown}i{\textquestiondown}A{\textexclamdown}/i{\textquestiondown} is a (possibly large) constant. Recent results again give such functions but only when the number of players is {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}\,{\textexclamdown}\,loglog{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}.},
  citationcount = {30},
  venue = {TOCT}
}

@article{davoodiForestdshAUniversal2021,
  title = {{{ForestDSH}}: A Universal Hash Design for Discrete Probability Distributions},
  author = {Davoodi, Arash Gholami and Chang, Sean and Yoo, Clay (Hyungon) and Baweja, Anubhav and Mongia, Mihir and Mohimani, H.},
  year = {2021},
  doi = {10.1007/s10618-020-00732-6},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Data mining and knowledge discovery}
}

@article{davoodiOnSuccinctRepresentations2014,
  title = {On Succinct Representations of Binary Trees},
  author = {Davoodi, P. and Raman, R. and Satti, S. R.},
  year = {2014},
  doi = {10.1007/s11786-017-0294-4},
  abstract = {No abstract available},
  citationcount = {14},
  venue = {Mathematics and Computer Science}
}

@article{deeparnabchakrabartySubquadraticSubmodularFunction2016,
  title = {Subquadratic Submodular Function Minimization},
  author = {Deeparnab Chakrabarty and Y. Lee and Aaron Sidford and {Sam Chiu-wai Wong}},
  year = {2016},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/3055399.3055419},
  abstract = {Submodular function minimization (SFM) is a fundamental discrete optimization problem which generalizes many well known problems, has applications in various fields, and can be solved in polynomial time. Owing to applications in computer vision and machine learning, fast SFM algorithms are highly desirable. The current fastest algorithms [Lee, Sidford, Wong, 2015] run in O(n2lognM{$\cdot$} EO + n3logO(1)nM) time and O(n3log2n{$\cdot$} EO +n4logO(1)n)time respectively, where M is the largest absolute value of the function (assuming the range is integers) and is the time taken to evaluate the function on any set. Although the best known lower bound on the query complexity is only {\textohm}(n) [Harvey, 2008], the current shortest non-deterministic proof [Cunningham, 1985] certifying the optimum value of a function requires {\textohm}(n2) function evaluations. The main contribution of this paper are subquadratic SFM algorithms. For integer-valued submodular functions, we give an SFM algorithm which runs in O(nM3logn{$\cdot$} EO) time giving the first nearly linear time algorithm in any known regime. For real-valued submodular functions with range in [-1,1], we give an algorithm which in {\~O}(n5/3{$\cdot$} EO/{$\varepsilon$}2) time returns an {$\varepsilon$}-additive approximate solution. At the heart of it, our algorithms are projected stochastic subgradient descent methods on the Lovasz extension of submodular functions where we crucially exploit submodularity and data structures to obtain fast, i.e. sublinear time, subgradient updates. The latter is crucial for beating the n2 bound - we show that algorithms which access only subgradients of the Lovasz extension, and these include the empirically fast Fujishige-Wolfe heuristic [Fujishige, 1980; Wolfe, 1976]},
  keywords = {data structure,lower bound,query,query complexity,update},
  annotation = {Citation Count: 40}
}

@article{degermarkSmallForwardingTables1997,
  title = {Small Forwarding Tables for Fast Routing Lookups},
  author = {Degermark, M. and Brodnik, A. and Carlsson, S. and Pink, S.},
  year = {1997},
  doi = {10.1145/263105.263133},
  abstract = {For some time, the networking community has assumed that it is impossible to do IP routing lookups in software fast enough to support gigabit speeds. IP routing lookups must find the routing entry with the longest matching prefix, a task that has been thought to require hardware support at lookup frequencies of millions per second.We present a forwarding table data structure designed for quick routing lookups. Forwarding tables are small enough to fit in the cache of a conventional general purpose processor. With the table in cache, a 200 MHz Pentium Pro or a 333 MHz Alpha 21164 can perform a few million lookups per second. This means that it is feasible to do a full routing lookup for each IP packet at gigabit speeds without special hardware.The forwarding tables are very small, a large routing table with 40,000 routing entries can be compacted to a forwarding table of 150-160 Kbytes. A lookup typically requires less than 100 instructions on an Alpha, using eight memory references accessing a total of 14 bytes.},
  citationcount = {639},
  venue = {Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication}
}

@article{deGrammarBoostingA2023,
  title = {Grammar Boosting: A New Technique for Proving Lower Bounds for Computation over Compressed Data},
  author = {De, Rajat and Kempa, Dominik},
  year = {2023},
  doi = {10.1137/1.9781611977912.121},
  abstract = {Grammar compression is a general compression framework in which a string T of length N is represented as a context-free grammar of size n whose language contains only T. In this paper, we focus on studying the limitations of algorithms and data structures operating on strings in grammar-compressed form. Previous work focused on proving lower bounds for grammars constructed using algorithms that achieve the approximation ratio {$\rho$}=\{O\}(\{polylog\}N). Unfortunately, for the majority of grammar compressors, {$\rho$} is either unknown or satisfies {$\rho$}={$\omega$}(\{polylog\}N). In their seminal paper, Charikar et al. [IEEE Trans. Inf. Theory 2005] studied seven popular grammar compression algorithms: RePair, Greedy, LongestMatch, Sequential, Bisection, LZ78, and {$\alpha$}-Balanced. Only one of them ({$\alpha$}-Balanced) is known to achieve {$\rho$}=\{O\}(\{polylog\}N). We develop the first technique for proving lower bounds for data structures and algorithms on grammars that is fully general and does not depend on the approximation ratio {$\rho$} of the used grammar compressor. Using this technique, we first prove that {\textohm}(N/N) time is required for random access on RePair, Greedy, LongestMatch, Sequential, and Bisection, while {\textohm}(N) time is required for random access to LZ78. All these lower bounds hold within space \{O\}(n\{polylog\}N) and match the existing upper bounds. We also generalize this technique to prove several conditional lower bounds for compressed computation. For example, we prove that unless the Combinatorial k-Clique Conjecture fails, there is no combinatorial algorithm for CFG parsing on Bisection (for which it holds {$\rho$}={$\Theta$}\vphantom\{\}(N\textsuperscript{\{\vphantom\}}1/2\vphantom\{\})) that runs in \{O\}(n\textsuperscript{c}{$\cdot$}N\textsuperscript{\{\vphantom\}}3-{$\epsilon$}\vphantom\{\}) time for all constants c{$>$}0 and {$\epsilon>$}0. Previously, this was known only for c{$<$}2{$\epsilon$}.},
  citationcount = {1},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,lower bound}
}

@article{dellFinegrainedReductionsApproximate2017,
  title = {Fine-Grained Reductions from Approximate Counting to Decision},
  author = {Dell, Holger and Lapinskas, John},
  year = {2017},
  doi = {10.1145/3188745.3188920},
  abstract = {In this paper, we introduce a general framework for fine-grained reductions of approximate counting problems to their decision versions. (Thus we use an oracle that decides whether any witness exists to multiplicatively approximate the number of witnesses with minimal overhead.) This mirrors a foundational result of Sipser (STOC 1983) and Stockmeyer (SICOMP 1985) in the polynomial-time setting, and a similar result of M{\"u}ller (IWPEC 2006) in the FPT setting. Using our framework, we obtain such reductions for some of the most important problems in fine-grained complexity: the Orthogonal Vectors problem, 3SUM, and the Negative-Weight Triangle problem (which is closely related to All-Pairs Shortest Path). While all these problems have simple algorithms over which it is conjectured that no polynomial improvement is possible, our reductions would remain interesting even if these conjectures were proved; they have only polylogarithmic overhead, and can therefore be applied to subpolynomial improvements such as the n3/exp({$\Theta$}({\textsurd}logn))-time algorithm for the Negative-Weight Triangle problem due to Williams (STOC 2014). Our framework is also general enough to apply to versions of the problems for which more efficient algorithms are known. For example, the Orthogonal Vectors problem over GF(m)d for constant m can be solved in time n{$\cdot$}poly(d) by a result of Williams and Yu (SODA 2014); our result implies that we can approximately count the number of orthogonal pairs with essentially the same running time. We also provide a fine-grained reduction from approximate \#SAT to SAT. Suppose the Strong Exponential Time Hypothesis (SETH) is false, so that for some 1{\textexclamdown}c{\textexclamdown}2 and all k there is an O(cn)-time algorithm for \#k-SAT. Then we prove that for all k, there is an O((c+o(1))n)-time algorithm for approximate \#k-SAT. In particular, our result implies that the Exponential Time Hypothesis (ETH) is equivalent to the seemingly-weaker statement that there is no algorithm to approximate \#3-SAT to within a factor of 1+{$\varepsilon$} in time 2o(n)/{$\varepsilon$}2 (taking {$\varepsilon$} {\textquestiondown} 0 as part of the input). A full version of this paper containing detailed proofs is available at https://arxiv.org/abs/1707.04609.},
  citationcount = {30},
  venue = {Symposium on the Theory of Computing},
  keywords = {reduction}
}

@article{demaineALinearLower2001,
  title = {A Linear Lower Bound on Index Size for Text Retrieval},
  author = {Demaine, E. and {L{\'o}pez-Ortiz}, A.},
  year = {2001},
  doi = {10.1016/S0196-6774(03)00043-9},
  abstract = {No abstract available},
  citationcount = {47},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{demaineDynamicOptimalityAlmost2004,
  title = {Dynamic Optimality - Almost},
  author = {Demaine, E. and Harmon, Dion and Iacono, J. and Patrascu, M.},
  year = {2004},
  doi = {10.1109/FOCS.2004.23},
  abstract = {We present an O(lg lg n)-competitive online binary search tree, improving upon the best previous (trivial) competitive ratio of O(lg n). This is the first major progress on Sleator and Tarjan's dynamic optimality conjecture of 1985 that O(1)-competitive binary search trees exist.},
  citationcount = {112},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic}
}

@article{demaineDynamicOptimalityAlmost2007,
  title = {Dynamic Optimality - Almost [Competitive Online Binary Search Tree]},
  author = {Demaine, E. and Harmon, Dion and Iacono, J. and Patrascu, M.},
  year = {2007},
  doi = {10.1137/S0097539705447347},
  abstract = {We present an O(lg lg n)-competitive online binary search tree, improving upon the best previous (trivial) competitive ratio of O(lg n). This is the first major progress on Sleator and Tarjan's dynamic optimality conjecture of 1985 that O(1)-competitive binary search trees exist.},
  citationcount = {27},
  venue = {45th Annual IEEE Symposium on Foundations of Computer Science},
  keywords = {dynamic}
}

@article{demaineFineGrainedI2017,
  title = {Fine-Grained {{I}}/{{O}} Complexity via Reductions: {{New}} Lower Bounds, Faster Algorithms, and a Time Hierarchy},
  author = {Demaine, E. and Lincoln, Andrea and Liu, Quanquan C. and Lynch, J. and Williams, V. V.},
  year = {2017},
  doi = {10.4230/LIPIcs.ITCS.2018.34},
  abstract = {This paper initiates the study of I/O algorithms (minimizing cache misses) from the perspective of fine-grained complexity (conditional polynomial lower bounds). Specifically, we aim to answer why sparse graph problems are so hard, and why the Longest Common Subsequence problem gets a savings of a factor of the size of cache times the length of a cache line, but no more. We take the reductions and techniques from complexity and fine-grained complexity and apply them to the I/O model to generate new (conditional) lower bounds as well as faster algorithms. We also prove the existence of a time hierarchy for the I/O model, which motivates the fine-grained reductions. Using fine-grained reductions, we give an algorithm for distinguishing 2 vs. 3 diameter and radius that runs in O({\textbar}E{\textbar}{$^2$}/(MB)) cache misses, which for sparse graphs improves over the previous O({\textbar}V{\textbar}{$^2$}/B) running time. We give new reductions from radius and diameter to Wiener index and median. We show meaningful reductions between problems that have linear-time solutions in the RAM model. The reductions use low I/O complexity (typically O(n/B)), and thus help to finely capture the relationship between "I/O linear time" {$\Theta$}(n/B) and RAM linear time {$\Theta$}(n). We generate new I/O assumptions based on the difficulty of improving sparse graph problem running times in the I/O model. We create conjectures that the current best known algorithms for Single Source Shortest Paths (SSSP), diameter, and radius are optimal. From these I/O-model assumptions, we show that many of the known reductions in the word-RAM model can naturally extend to hold in the I/O model as well (e.g., a lower bound on the I/O complexity of Longest Common Subsequence that matches the best known running time). Finally, we prove an analog of the Time Hierarchy Theorem in the I/O model.},
  citationcount = {10},
  venue = {Information Technology Convergence and Services},
  keywords = {lower bound,reduction}
}

@article{demetrescuANewApproach2003,
  title = {A New Approach to Dynamic All Pairs Shortest Paths},
  author = {Demetrescu, C. and Italiano, G.},
  year = {2003},
  doi = {10.1145/780542.780567},
  abstract = {We study novel combinatorial properties of graphs that allow us to devise a completely new approach to dynamic all pairs shortest paths problems. Our approach yields a fully dynamic algorithm for general directed graphs with non-negative real-valued edge weights that supports any sequence of operations in {\~O}(n2) amortized time per update and unit worst-case time per distance query, where n is the number of vertices. We can also report shortest paths in optimal worst-case time. These bounds improve substantially over previous results and solve a long-standing open problem. Our algorithm is deterministic and uses simple data structures.},
  citationcount = {238},
  venue = {Symposium on the Theory of Computing}
}

@article{demetrescuANewApproach2004,
  title = {A New Approach to Dynamic All Pairs Shortest Paths},
  author = {Demetrescu, C. and Italiano, G.},
  year = {2004},
  doi = {10.1145/1039488.1039492},
  abstract = {We study novel combinatorial properties of graphs that allow us to devise a completely new approach to dynamic all pairs shortest paths problems. Our approach yields a fully dynamic algorithm for general directed graphs with non-negative real-valued edge weights that supports any sequence of operations in O(n2log3n) amortized time per update and unit worst-case time per distance query, where n is the number of vertices. We can also report shortest paths in optimal worst-case time. These bounds improve substantially over previous results and solve a long-standing open problem. Our algorithm is deterministic, uses simple data structures, and appears to be very fast in practice.},
  citationcount = {171},
  venue = {JACM}
}

@article{demetrescuTradeOffsFor2005,
  title = {Trade-Offs for Fully Dynamic Transitive Closure on {{DAGs}}: Breaking through the {{O}}(N2 Barrier},
  author = {Demetrescu, C. and Italiano, G.},
  year = {2005},
  doi = {10.1145/1059513.1059514},
  abstract = {We present an algorithm for directed acyclic graphs that breaks through the {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}) barrier on the single-operation complexity of fully dynamic transitive closure, where {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} is the number of edges in the graph. We can answer queries in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown}) worst-case time and perform updates in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}{$\omega$}(1,{$\varepsilon$},1)-{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown}+{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}1+{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown}) worst-case time, for any {$\varepsilon\in$}[0,1], where {$\omega$}(1,{$\varepsilon$},1) is the exponent of the multiplication of an {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} {\texttimes} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown} matrix by an {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown} {\texttimes} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} matrix. The current best bounds on {$\omega$}(1,{$\varepsilon$},1) imply an {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}0.575{\textexclamdown}/sup{\textquestiondown}) query time and an {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}1.575{\textexclamdown}/sup{\textquestiondown}) update time in the worst case. Our subquadratic algorithm is randomized, and has one-sided error. As an application of this result, we show how to solve single-source reachability in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}1.575{\textexclamdown}/sup{\textquestiondown}) time per update and constant time per query.},
  citationcount = {39},
  venue = {JACM}
}

@article{demovicMovieRecommendationBased2013,
  title = {Movie Recommendation Based on Graph Traversal Algorithms},
  author = {Demovic, L. and Fritscher, Eduard and Kriz, Jakub and Kuzmik, Ondrej and Proksa, Ondrej and Vandlikova, Diana and Zelen{\'i}k, Dusan and Bielikov{\'a}, M.},
  year = {2013},
  doi = {10.1109/DEXA.2013.24},
  abstract = {Media content recommendation is nowadays a common problem. Traditional algorithms based on collaborative filtering require an up-to-date dataset of users and their preferences, which is difficult to gather for huge database of items. Content-based approach suffers from the complex computation of similarity among items. In this paper we propose an approach to recommendation with a focus on the natural change of user's interests in movies. We make use of a graph representation and experimented with modified graph algorithms. We design a representation of the data about movies in a graph structure and a method which uses our data model for recommendation. We propose four recommendation algorithms which are capable to find recommendations based on initial nodes, which selection is based on the user's current interests. We implemented these algorithms and experimentally evaluated them with real users.},
  citationcount = {15},
  venue = {2013 24th International Workshop on Database and Expert Systems Applications}
}

@article{denardoFlowsInNetworks2011,
  title = {Flows in Networks},
  author = {Denardo, E.},
  year = {2011},
  doi = {10.1007/978-1-4419-6491-5_9},
  abstract = {No abstract available},
  citationcount = {1517},
  venue = {No venue available}
}

@article{dengDiscrepancyMinimizationIn2022,
  title = {Discrepancy Minimization in Input-Sparsity Time},
  author = {Deng, Yichuan and Song, Zhao and Weinstein, Omri},
  year = {2022},
  doi = {10.48550/arXiv.2210.12468},
  abstract = {A recent work of Larsen [Lar23] gave a faster combinatorial alternative to Bansal's SDP algorithm for finding a coloring x {$\in$} \{-1, 1\}n that approximately minimizes the discrepancy disc(A, x) := {\textbardbl}Ax{\textbardbl}{$\infty$} of a general real-valued m {\texttimes} n matrix A. Larsen's algorithm runs in {\~O}(mn) time compared to Bansal's {\~O}(mn)-time algorithm, at the price of a slightly weaker logarithmic approximation ratio in terms of the hereditary discrepancy of A [Ban10]. In this work we present a combinatorial {\~O}(nnz(A) + n) time algorithm with the same approximation guarantee as Larsen, which is optimal for tall matrices m = poly(n). Using a more intricate analysis and fast matrix-multiplication, we achieve {\~O}(nnz(A) + n) time, which breaks cubic runtime for square matrices, and bypasses the barrier of linear-programming approaches [ES14] for which input-sparsity time is currently out of reach. Our algorithm relies on two main ideas: (i) A new sketching technique for finding a projection matrix with short `2-basis using implicit leverage-score sampling; (ii) A data structure for faster implementation of the iterative Edge-Walk partial-coloring algorithm of Lovett-Meka, using an alternative analysis that enables ``lazy'' batch-updates with low-rank corrections. Our result nearly closes the computational gap between real-valued and binary matrices (set-systems), for which input-sparsity time coloring was very recently obtained [JSS23]. ethandeng02@gmail.com. University of Science and Technology of China. zsong@adobe.com. Adobe Research. omri@cs.columbia.edu. Hebrew University and Columbia University. ar X iv :2 21 0. 12 46 8v 1 [ cs .D S] 2 2 O ct 2 02 2},
  citationcount = {23},
  venue = {arXiv.org},
  keywords = {data structure,update}
}

@article{dengFastDistanceOracles2022,
  title = {Fast Distance Oracles for Any Symmetric Norm},
  author = {Deng, Yichuan and Song, Zhao and Weinstein, Omri and Zhang, Ruizhe},
  year = {2022},
  doi = {10.48550/arXiv.2205.14816},
  abstract = {In the Distance Oracle problem, the goal is to preprocess n vectors x{$_1$},x{$_2$},{$\cdots$},x\textsubscript{n} in a d-dimensional metric space (\{X\}\textsuperscript{d},{$\cdot_l$}) into a cheap data structure, so that given a query vector q{$\in$}\{X\}\textsuperscript{d} and a subset S{$\subseteq$}[n] of the input data points, all distances q-x\textsubscript{i}\textsubscript{l} for x\textsubscript{i}{$\in$}S can be quickly approximated (faster than the trivial {$\sim$}d{\textbar}S{\textbar} query time). This primitive is a basic subroutine in machine learning, data mining and similarity search applications. In the case of {$\ell_p$} norms, the problem is well understood, and optimal data structures are known for most values of p. Our main contribution is a fast (1+{$\varepsilon$}) distance oracle for any symmetric norm {$\cdot_l$}. This class includes {$\ell_p$} norms and Orlicz norms as special cases, as well as other norms used in practice, e.g. top-k norms, max-mixture and sum-mixture of {$\ell_p$} norms, small-support norms and the box-norm. We propose a novel data structure with O\vphantom\{\}(n(d+\{mmc\}(l){$^2$})) preprocessing time and space, and t\textsubscript{q}=O\vphantom\{\}(d+{\textbar}S{\textbar}{$\cdot$}\{mmc\}(l){$^2$}) query time, for computing distances to a subset S of data points, where \{mmc\}(l) is a complexity-measure (concentration modulus) of the symmetric norm. When l={$\ell$}\textsubscript{\{\vphantom\}}p\vphantom\{\} , this runtime matches the aforementioned state-of-art oracles.},
  citationcount = {7},
  venue = {Neural Information Processing Systems},
  keywords = {data structure,query,query time}
}

@article{dengRandomizedAndDeterministic2023,
  title = {Randomized and Deterministic Attention Sparsification Algorithms for Over-Parameterized Feature Dimension},
  author = {Deng, Yichuan and Mahadevan, S. and Song, Zhao},
  year = {2023},
  doi = {10.48550/arXiv.2304.04397},
  abstract = {Large language models (LLMs) have shown their power in different areas. Attention computation, as an important subroutine of LLMs, has also attracted interests in theory. Recently the static computation and dynamic maintenance of attention matrix has been studied by [Alman and Song 2023] and [Brand, Song and Zhou 2023] from both algorithmic perspective and hardness perspective. In this work, we consider the sparsification of the attention problem. We make one simplification which is the logit matrix is symmetric. Let n denote the length of sentence, let d denote the embedding dimension. Given a matrix X{$\in$}\{R\}\textsuperscript{\{\vphantom\}}n{\texttimes}d\vphantom\{\}, suppose d{$\gg$}n and XX\textsuperscript{{$\top$}}\textsubscript{\{\vphantom\}}{$\infty$}\vphantom\{\}\{n{\texttimes}m\} (where m{$\ll$}d) such that \{align*\}  D(Y){\textasciicircum}\{-1\} ( Y Y{\textasciicircum}{$\top$}) - D(X){\textasciicircum}\{-1\} ( X X{\textasciicircum}{$\top$}) \_\{{$\infty$}\} {$\leq$}O(r) \{align*\} We provide two results for this problem. {$\bullet$} Our first result is a randomized algorithm. It runs in O\vphantom\{\}(\{nnz\}(X)+n\textsuperscript{\{\vphantom\}}{$\omega$}\vphantom\{\}) time, has 1-{$\delta$} succeed probability, and chooses m=O(n(n/{$\delta$})). Here \{nnz\}(X) denotes the number of non-zero entries in X. We use {$\omega$} to denote the exponent of matrix multiplication. Currently {$\omega\approx$}2.373. {$\bullet$} Our second result is a deterministic algorithm. It runs in O\vphantom\{\}( {$\sum\_$}\{i{$\in$}[d]\}\{nnz\}(X\_i){\textasciicircum}2,dn{\textasciicircum}\{{$\omega$}-1\} +n\textsuperscript{\{\vphantom\}}{$\omega$}+1\vphantom\{\}) time and chooses m=O(n). Here X\textsubscript{i} denote the i-th column of matrix X. Our main findings have the following implication for applied LLMs task: for any super large feature dimension, we can reduce it down to the size nearly linear in length of sentence.},
  citationcount = {33},
  venue = {arXiv.org},
  keywords = {dynamic,static}
}

@article{denzumiSequenceBinaryDecision2016,
  title = {Sequence Binary Decision Diagram: {{Minimization}}, Relationship to Acyclic Automata, and Complexities of {{Boolean}} Set Operations},
  author = {Denzumi, Shuhei and Yoshinaka, Ryo and Arimura, Hiroki and Minato, S.},
  year = {2016},
  doi = {10.1016/j.dam.2014.11.022},
  abstract = {No abstract available},
  citationcount = {5},
  venue = {Discrete Applied Mathematics}
}

@article{deorowiczOnTwoVariants2009,
  title = {On Two Variants of the Longest Increasing Subsequence Problem},
  author = {Deorowicz, Sebastian and Grabowski, S.},
  year = {2009},
  doi = {10.1007/978-3-642-00563-3_57},
  abstract = {No abstract available},
  citationcount = {6},
  venue = {International Conference on Man-Machine Interactions}
}

@article{deqingfuTopologicalRegularizationDense2021,
  title = {Topological {{Regularization}} for {{Dense Prediction}}},
  author = {Deqing Fu and Bradley J. Nelson},
  year = {2021},
  journal = {International Conference on Machine Learning and Applications},
  doi = {10.1109/ICMLA55696.2022.00014},
  abstract = {Dense prediction tasks such as depth perception and semantic segmentation are important applications in computer vision that have a concrete topological description in terms of partitioning an image into connected components or estimating a function with a small number of local extrema corresponding to objects in the image. We develop a form of topological regularization based on persistent homology that can be used in dense prediction tasks with these topological descriptions. Experimental results show that the output topology can also appear in the internal activations of trained neural networks which allows for a novel use of topological regularization to the internal states of neural networks during training, reducing the computational cost of the regularization. We demonstrate that this topological regularization of internal activations leads to improved convergence and test benchmarks on several problems and architectures.},
  annotation = {Citation Count: 0}
}

@article{devosAQuadraticLower2006,
  title = {A Quadratic Lower Bound for Subset Sums},
  author = {DeVos, Matt and Goddyn, Luis A. and Mohar, B. and {\v S}{\'a}mal, Robert},
  year = {2006},
  doi = {10.4064/aa129-2-4},
  abstract = {Let A be a finite nonempty subset of an additive abelian group G, and let {$\Sigma$}(A) denote the set of all group elements representable as a sum of some subset of A. We prove that {\textbar}{$\Sigma$}(A){\textbar} {\textquestiondown}= {\textbar}H{\textbar} + 1/64 {\textbar}A H{\textbar}{\textasciicircum}2 where H is the stabilizer of {$\Sigma$}(A). Our result implies that {$\Sigma$}(A) = Z/nZ for every set A of units of Z/nZ with {\textbar}A{\textbar} {\textquestiondown}= 8 {\textsurd}\{n\}. This consequence was first proved by Erd{\H \{}o\vphantom\{\}s and Heilbronn for n prime, and by Vu (with a weaker constant) for general n.},
  citationcount = {8},
  venue = {No venue available}
}

@article{devroye8NearestNeighbor1982,
  title = {8 {{Nearest}} Neighbor Methods in Discrimination},
  author = {Devroye, L. and Wagner, T.},
  year = {1982},
  doi = {10.1016/S0169-7161(82)02011-2},
  citationcount = {77},
  venue = {Classification, Pattern Recognition and Reduction of Dimensionality}
}

@article{devroyeNonUniformRandom1986,
  title = {Non-Uniform Random Variate Generation},
  author = {Devroye, L.},
  year = {1986},
  doi = {10.2307/2531615},
  abstract = {This is a survey of the main methods in non-uniform random variate generation, and highlights recent research on the subject. Classical paradigms such as inversion, rejection, guide tables, and transformations are reviewed. We provide information on the expected time complexity of various algorithms, before addressing modern topics such as indirectly specified distributions, random processes, and Markov chain methods. Authors' address: School of Computer Science, McGill University, 3480 University Street, Montreal, Canada H3A 2K6. The authors' research was sponsored by NSERC Grant A3456 and FCAR Grant 90-ER-0291. 1. The main paradigms The purpose of this chapter is to review the main methods for generating random variables, vectors and processes. Classical workhorses such as the inversion method, the rejection method and table methods are reviewed in section 1. In section 2, we discuss the expected time complexity of various algorithms, and give a few examples of the design of generators that are uniformly fast over entire families of distributions. In section 3, we develop a few universal generators, such as generators for all log concave distributions on the real line. Section 4 deals with random variate generation when distributions are indirectly specified, e.g, via Fourier coefficients, characteristic functions, the moments, the moment generating function, distributional identities, infinite series or Kolmogorov measures. Random processes are briefly touched upon in section 5. Finally, the latest developments in Markov chain methods are discussed in section 6. Some of this work grew from Devroye (1986a), and we are carefully documenting work that was done since 1986. More recent references can be found in the book by H{\"o}rmann, Leydold and Derflinger (2004). Non-uniform random variate generation is concerned with the generation of random variables with certain distributions. Such random variables are often discrete, taking values in a countable set, or absolutely continuous, and thus described by a density. The methods used for generating them depend upon the computational model one is working with, and upon the demands on the part of the output. For example, in a ram (random access memory) model, one accepts that real numbers can be stored and operated upon (compared, added, multiplied, and so forth) in one time unit. Furthermore, this model assumes that a source capable of producing an i.i.d. (independent identically distributed) sequence of uniform [0, 1] random variables is available. This model is of course unrealistic, but designing random variate generators based on it has several advantages: first of all, it allows one to disconnect the theory of non-uniform random variate generation from that of uniform random variate generation, and secondly, it permits one to plan for the future, as more powerful computers will be developed that permit ever better approximations of the model. Algorithms designed under finite approximation limitations will have to be redesigned when the next generation of computers arrives. For the generation of discrete or integer-valued random variables, which includes the vast area of the generation of random combinatorial structures, one can adhere to a clean model, the pure bit model, in which each bit operation takes one time unit, and storage can be reported in terms of bits. Typically, one now assumes that an i.i.d. sequence of independent perfect bits is available. In this model, an elegant information-theoretic theory can be derived. For example, Knuth and Yao (1976) showed that to generate a random integer X described by the probability distribution \{X = n\} = pn, n {$\geq$} 1, any method must use an expected number of bits greater than the binary entropy of the distribution, {$\sum$}},
  citationcount = {4225},
  venue = {No venue available}
}

@article{dhulipalaParallelBatchDynamic2020,
  title = {Parallel Batch-Dynamic k-{{Clique}} Counting},
  author = {Dhulipala, Laxman and Liu, Quanquan C. and Shun, Julian},
  year = {2020},
  doi = {10.1137/1.9781611976489.10},
  abstract = {In this paper, we study new batch-dynamic algorithms for k-clique counting, which are dynamic algorithms where the updates are batches of edge insertions and deletions. We study this problem in the parallel setting, where the goal is to obtain algorithms with low (poly-logarithmic) depth. Our first result is a new parallel batch-dynamic triangle counting algorithm with O({$\Delta$}{\textsurd}\{{$\Delta$}+m\}) amortized work and O(\textsuperscript{*}({$\Delta$}+m)) depth with high probability (w.h.p.), and O({$\Delta$}+m) space for a batch of {$\Delta$} edge insertions or deletions. Our second result is a simple parallel batch-dynamic k-clique counting algorithm that uses a newly developed parallel k-clique counting algorithm to bootstrap itself, by enumerating smaller cliques, and intersecting them with the batch. Instantiating this idea gives a simple batch-dynamic algorithm running in O({$\Delta$}(m+{$\Delta$}){$\alpha$}\textsuperscript{\{\vphantom\}}k-4\vphantom\{\}) expected work and O(\textsuperscript{\{\vphantom\}}k-2\vphantom\{\}n) depth w.h.p., all in O(m+{$\Delta$}) space. Our third result is an algebraic algorithm based on parallel fast matrix multiplication. Assuming that a parallel fast matrix multiplication algorithm exists with parallel matrix multiplication constant {$\omega_p$}, the same algorithm solves dynamic k-clique counting with O(({$\Delta$}m\textsuperscript{\{\{\vphantom{\}\}}}{\textfractionsolidus}{$_($}2k-1){$\omega_{p}$}\vphantom\{\}\{3({$\omega_p$}+1)\}\vphantom\{\},({$\Delta$}+m)\textsuperscript{\{\{\vphantom{\}\}}}{\textfractionsolidus}{$_2$}(k+1){$\omega_{p}$}\vphantom\{\}\{3({$\omega_p$}+1)\}\vphantom\{\})) amortized work, O(({$\Delta$}+m)) depth, and O(({$\Delta$}+m)\textsuperscript{\{\{\vphantom{\}\}}}{\textfractionsolidus}{$_2$}(k+1){$\omega_{p}$}\vphantom\{\}\{3({$\omega_p$}+1)\}\vphantom\{\}) space.},
  citationcount = {23},
  venue = {SIAM Symposium on Algorithmic Principles of Computer Systems},
  keywords = {dynamic,update}
}

@article{dietzfelbingerAlgorithmenUndDatenstrukturen1990,
  title = {Algorithmen Und Datenstrukturen - Die Grundwerkzeuge},
  author = {Dietzfelbinger, Martin and Mehlhorn, Kurt and Sanders, P.},
  year = {1990},
  doi = {10.1007/978-3-642-05472-3},
  abstract = {No abstract available},
  citationcount = {78},
  venue = {eXamen.press}
}

@article{dietzfelbingerANewUniversal1990,
  title = {A New Universal Class of Hash Functions and Dynamic Hashing in Real Time},
  author = {Dietzfelbinger, Martin and Heide, F.},
  year = {1990},
  doi = {10.1007/BFb0032018},
  abstract = {No abstract available},
  citationcount = {168},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{dietzfelbingerAReliableRandomized1997,
  title = {A Reliable Randomized Algorithm for the Closest-Pair Problem},
  author = {Dietzfelbinger, Martin and Hagerup, T. and Katajainen, J. and Penttonen, M.},
  year = {1997},
  doi = {10.1006/jagm.1997.0873},
  abstract = {The following two computational problems are studied:Duplicate grouping:Assume thatnitems are given, each of which is labeled by an integer key from the set \{0,?,U?1\}. Store the items in an array of sizensuch that items with the same key occupy a contiguous segment of the array.Closest pair:Assume that a multiset ofnpoints in thed-dimensional Euclidean space is given, whered?1 is a fixed integer. Each point is represented as ad-tuple of integers in the range \{0,?,U?1\} (or of arbitrary real numbers). Find a closest pair, i.e., a pair of points whose distance is minimal over all such pairs.In 1976, Rabin described a randomized algorithm for the closest-pair problem that takes linear expected time. As a subroutine, he used a hashing procedure whose implementation was left open. Only years later randomized hashing schemes suitable for filling this gap were developed.In this paper, we return to Rabin's classic algorithm to provide a fully detailed description and analysis, thereby also extending and strengthening his result. As a preliminary step, we study randomized algorithms for the duplicate-grouping problem. In the course of solving the duplicate-grouping problem, we describe a new universal class of hash functions of independent interest.It is shown that both of the foregoing problems can be solved by randomized algorithms that useO(n) space and finish inO(n) time with probability tending to 1 asngrows to infinity. The model of computation is a unit-cost RAM capable of generating random numbers and of performing arithmetic operations from the set \{+,?,?,div,log2,exp2\}, wheredivdenotes integer division andlog2andexp2are the mappings from N to N?\{0\} withlog2(m)=?log2m? andexp2(m)=2mfor allm?N. If the operationslog2andexp2are not available, the running time of the algorithms increases by an additive term ofO(loglogU). All numbers manipulated by the algorithms consist ofO(logn+logU) bits.The algorithms for both of the problems exceed the time boundO(n) orO(n+loglogU) with probability 2?n?(1). Variants of the algorithms are also given that use onlyO(logn+logU) random bits and have probabilityO(n??) of exceeding the time bounds, where ??1 is a constant that can be chosen arbitrarily.The algorithms for the closest-pair problem also works if the coordinates of the points are arbitrary real numbers, provided that the RAM is able to perform arithmetic operations from \{+,?,?,div\} on real numbers, whereadivbnow means ?a/b?. In this case, the running time isO(n) withlog2andexp2andO(n+loglog(?max/?max)) without them, where ?maxis the maximum and ?minis the minimum distance between any two distinct input points.},
  citationcount = {244},
  venue = {J. Algorithms}
}

@article{dietzfelbingerASubquadraticAlgorithm2018,
  title = {A Subquadratic Algorithm for {{3XOR}}},
  author = {Dietzfelbinger, Martin and Schlag, Philipp and Walzer, Stefan},
  year = {2018},
  doi = {10.4230/LIPIcs.MFCS.2018.59},
  abstract = {Given a set X of n binary words of equal length w, the 3XOR problem asks for three elements a,b,c{$\in$}X such that a{$\oplus$}b=c, where {$\oplus$} denotes the bitwise XOR operation. The problem can be easily solved on a word RAM with word length w in time O(n{$^2$}\{n\}). Using Han's fast integer sorting algorithm (2002/2004) this can be reduced to O(n{$^2$}\{\{n\}\}). With randomization or a sophisticated deterministic dictionary construction, creating a hash table for X with constant lookup time leads to an algorithm with (expected) running time O(n{$^2$}). At present, seemingly no faster algorithms are known. We present a surprisingly simple deterministic, quadratic time algorithm for 3XOR. Its core is a version of the Patricia trie for X, which makes it possible to traverse the set a{$\oplus$}X in ascending order for arbitrary a{$\in$} 0,1 \textsuperscript{\{\vphantom\}}w\vphantom\{\} in linear time. Furthermore, we describe a randomized algorithm for 3XOR with expected running time O(n{$^{2}\cdot$} {\textasciicircum}3\{w\}/w,(\{n\}){\textasciicircum}2/{\textasciicircum}2n ). The algorithm transfers techniques to our setting that were used by Baran, Demaine, and P{\u \{}a\vphantom\{\}tra{\c \{}s\vphantom\{\}cu (2005/2008) for solving the related int3SUM problem (the same problem with integer addition in place of binary XOR) in expected time o(n{$^2$}). As suggested by Jafargholi and Viola (2016), linear hash functions are employed. The latter authors also showed that assuming 3XOR needs expected running time n\textsuperscript{\{\vphantom\}}2-o(1)\vphantom\{\} one can prove conditional lower bounds for triangle enumeration just as with 3SUM. We demonstrate that 3XOR can be reduced to other problems as well, treating the examples offline SetDisjointness and offline SetIntersection, which were studied for 3SUM by Kopelowitz, Pettie, and Porat (2016).},
  citationcount = {3},
  venue = {International Symposium on Mathematical Foundations of Computer Science},
  keywords = {lower bound}
}

@article{dietzfelbingerDynamicPerfectHashing1994,
  title = {Dynamic Perfect Hashing: {{Upper}} and Lower Bounds},
  author = {Dietzfelbinger, Martin and Karlin, Anna R. and Mehlhorn, K. and Heide, F. and Rohnert, H. and Tarjan, R.},
  year = {1994},
  doi = {10.1137/S0097539791194094},
  abstract = {The dynamic dictionary problem is considered: provide an algorithm for storing a dynamic set, allowing the operations insert, delete, and lookup. A dynamic perfect hashing strategy is given: a randomized algorithm for the dynamic dictionary problem that takes O(1) worst-case time for lookups and O(1) amortized expected time for insertions and deletions; it uses space proportional to the size of the set stored. Furthermore, lower bounds for the time complexity of a class of deterministic algorithms for the dictionary problem are proved. This class encompasses realistic hashing-based schemes that use linear space. Such algorithms have amortized worst-case time complexity {\textohm}(n) for a sequence of n insertions and lookups; if the worst-case lookup time is restricted to k, then the lower bound becomes {\textohm}(k{$\cdot$}n\textsuperscript{\{\vphantom\}}1/k\vphantom\{\}).},
  citationcount = {452},
  venue = {SIAM journal on computing (Print)}
}

@article{dietzfelbingerPolynomialHashFunctions1992,
  title = {Polynomial Hash Functions Are Reliable (Extended Abstract)},
  author = {Dietzfelbinger, Martin and Gil, Joseph and Matias, Yossi and Pippenger, N.},
  year = {1992},
  doi = {10.1007/3-540-55719-9_77},
  abstract = {No abstract available},
  citationcount = {104},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{dietzfelbingerSuccinctDataStructures2008,
  title = {Succinct Data Structures for Retrieval and Approximate Membership},
  author = {Dietzfelbinger, Martin and Pagh, R.},
  year = {2008},
  doi = {10.1007/978-3-540-70575-8_32},
  abstract = {No abstract available},
  citationcount = {88},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{dietzfelbingerUniversalHashingAnd1996,
  title = {Universal Hashing and K-{{Wise}} Independent Random Variables via Integer Arithmetic without Primes},
  author = {Dietzfelbinger, Martin},
  year = {1996},
  doi = {10.1007/3-540-60922-9_46},
  abstract = {No abstract available},
  citationcount = {128},
  venue = {Symposium on Theoretical Aspects of Computer Science}
}

@article{dietzfelbingerUniversalHashingVia2018,
  title = {Universal Hashing via Integer Arithmetic without Primes, Revisited},
  author = {Dietzfelbinger, Martin},
  year = {2018},
  doi = {10.1007/978-3-319-98355-4_15},
  abstract = {No abstract available},
  citationcount = {10},
  venue = {Adventures Between Lower Bounds and Higher Altitudes}
}

@article{dietzOptimalAlgorithmsList1989,
  title = {Optimal Algorithms for List Indexing and Subset Rank},
  author = {Dietz, Paul F.},
  year = {1989},
  doi = {10.1007/3-540-51542-9_5},
  abstract = {No abstract available},
  citationcount = {100},
  venue = {Workshop on Algorithms and Data Structures}
}

@article{dietzPersistenceAmortizationAnd1991,
  title = {Persistence, Amortization and Randomization},
  author = {Dietz, Paul F. and Raman, R.},
  year = {1991},
  doi = {10.5555/127787.127809},
  abstract = {We explore several problems associated with persistent data structures, deriving our moti\- vation from problems left open by Driscoll, Sarnak, Sleator and Tarjan in [15]. We exhibit simple methods to completely eliminate amortization from one of the data structures of Driscoll et. at.. We show new methods for making some data structures, including disjoint\- set union-find, partially persistent in optimal time and space. We discuss some motivations for eliminating amortization from data structures in general, and explore a family of "pebble" games associated with eliminating amortization from data structures in general and from persistent data structures in particular. One relevant version of this pebble game shows that randomization may be a useful tool for elimination of amortization. The University of Rochester Computer Science Department supported this work. {$\bullet$}A preliminary version ofthis paper was presentedat the 2nd Annual ACM.SIAM Symposium on Discrete Algorithms (SODA), San Francisco, USA, Jan. 1991. tDept. of CS, University of Rochester, Rochester, NY 14627. This work is supported by NSF grant CCR-8909667. lDept. of CS, University of Rochester, Rochester, NY 14627.},
  citationcount = {63},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure}
}

@article{dimitrisfotakisSelectiveTourCongestion2015,
  title = {A {{Selective Tour Through Congestion Games}}},
  author = {Dimitris Fotakis},
  year = {2015},
  journal = {Algorithms, Probability, Networks, and Games},
  doi = {10.1007/978-3-319-24024-4_14},
  annotation = {Citation Count: 7}
}

@article{dingScalableSubspaceClustering2015,
  title = {Scalable Subspace Clustering with Application to Motion Segmentation},
  author = {Ding, Liangjing and Barbu, Adrian},
  year = {2015},
  doi = {10.1201/B18502-14},
  abstract = {1.},
  citationcount = {1},
  venue = {No venue available}
}

@article{dinklageEngineeringPredecessorData2021,
  title = {Engineering Predecessor Data Structures for Dynamic Integer Sets},
  author = {Dinklage, P. and Fischer, J. and Herlez, Alexander},
  year = {2021},
  doi = {10.4230/LIPIcs.SEA.2021.7},
  abstract = {We present highly optimized data structures for the dynamic predecessor problem, where the task is to maintain a set S of w-bit numbers under insertions, deletions, and predecessor queries (return the largest element in S no larger than a given key). The problem of finding predecessors can be viewed as a generalized form of the membership problem, or as a simple version of the nearest neighbour problem. It lies at the core of various real-world problems such as internet routing. In this work, we engineer (1) a simple implementation of the idea of universe reduction, similar to van-Emde-Boas trees (2) variants of y-fast tries [Willard, IPL'83], and (3) B-trees with different strategies for organizing the keys contained in the nodes, including an implementation of dynamic fusion nodes [P{\v a}trascu and Thorup, FOCS'14]. We implement our data structures for w=32,40,64, which covers most typical scenarios. Our data structures finish workloads faster than previous approaches while being significantly more space-efficient, e.g., they clearly outperform standard implementations of the STL by finishing up to four times as fast using less than a third of the memory. Our tests also provide more general insights on data structure design, such as how small sets should be stored and handled and if and when new CPU instructions such as advanced vector extensions pay off.},
  citationcount = {4},
  venue = {The Sea},
  keywords = {data structure,dynamic,query,reduction}
}

@article{dinurDirectProductTesting2014,
  title = {Direct Product Testing},
  author = {Dinur, Irit and Steurer, David},
  year = {2014},
  doi = {10.1109/CCC.2014.27},
  abstract = {A direct product function is a function of the form g(x{\textexclamdown}sub{\textquestiondown}1{\textexclamdown}/sub{\textquestiondown}, {$\cdots$}, x{\textexclamdown}sub{\textquestiondown}k{\textexclamdown}/sub{\textquestiondown})=(g{\textexclamdown}sub{\textquestiondown}1{\textexclamdown}/sub{\textquestiondown}(x{\textexclamdown}sub{\textquestiondown}1{\textexclamdown}/sub{\textquestiondown}), {$\cdots$}, g(x{\textexclamdown}sub{\textquestiondown}k{\textexclamdown}/sub{\textquestiondown})). We show that the direct product property is locally testable with two queries, that is, a canonical two-query test distinguishes between direct product functions and functions that are far from direct products with constant probability. This local testing question comes up naturally in the context of PCPs, where direct products play a prominent role for gap amplification. We consider the following natural two query test for a given function f:[N]{\textexclamdown}sup{\textquestiondown}k{\textexclamdown}/sup{\textquestiondown}{$\rightarrow$}[M]{\textexclamdown}sup{\textquestiondown}k{\textexclamdown}/sup{\textquestiondown} Two query direct product test: Choose x, y that agree on a random set A of t coordinates and accept if f(x){\textexclamdown}sub{\textquestiondown}A{\textexclamdown}/sub{\textquestiondown}=f(y){\textexclamdown}sub{\textquestiondown}A{\textexclamdown}/sub{\textquestiondown}. We provide a comprehensive analysis of this test for all parameters N, M, k, t{$\leq$}O(k) and success probability {$\delta$}{\textquestiondown}0. Our main result is that if a given function f:[N]{\textexclamdown}sup{\textquestiondown}k{\textexclamdown}/sup{\textquestiondown}{$\rightarrow$}[M]{\textexclamdown}sup{\textquestiondown}k{\textexclamdown}/sup{\textquestiondown} passes the test with probability {$\delta\geq$}1-{$\varepsilon$} then there is a direct product function g such that P[f(x)=g(x)]{$\geq$}1-O({$\varepsilon$}). This is the first result relating success in the above (or any) test to the fraction of the domain on which f is equal to a direct product function. This test has been analyzed in previous works for the case t{$\ll$}k{$\ll$}N, and results show closeness of f to a direct product under a less natural measure of "approximate agreement". In the small soundness regime, we prove that if the test above passes with probability {$\delta$} {$\geq$} exp(-k), then the function agrees with a direct product function on local parts of the domain. This extends the previous range of parameters of {$\delta\geq$}exp(-3{\textsurd}k) to the entire meaningful range of {$\delta$}{\textquestiondown}exp(-k).},
  citationcount = {36},
  venue = {Cybersecurity and Cyberforensics Conference}
}

@article{dinurFineGrainedCryptanalysis2021,
  title = {Fine-Grained Cryptanalysis: {{Tight}} Conditional Bounds for Dense k-{{SUM}} and k-{{XOR}}},
  author = {Dinur, Itai and Keller, Nathan and Klein, Ohad},
  year = {2021},
  doi = {10.1145/3653014},
  abstract = {An average-case variant of the k-SUM conjecture asserts that finding k numbers that sum to 0 in a list of r random numbers, each of the order r\textsuperscript{\{\vphantom\}}k\vphantom\{\}, cannot be done in much less than r\textsuperscript{\{\vphantom\}}{$\lceil$}k/2{$\rceil$}\vphantom\{\} time. On the other hand, in the dense regime of parameters, where the list contains more numbers and many solutions exist, the complexity of finding one of them can be significantly improved by Wagner's k-tree algorithm. Such algorithms for k-SUM in the dense regime have many applications, notably in cryptanalysis. In this paper, assuming the average-case k-SUM conjecture, we prove that known algorithms are essentially optimal for k=3,4,5. For k{$>$}5, we prove the optimality of the k-tree algorithm for a limited range of parameters. We also prove similar results for k-XOR, where the sum is replaced with exclusive or. Our results are obtained by a self-reduction that, given an instance of k-SUM which has a few solutions, produces from it many instances in the dense regime. We solve each of these instances using the dense k-SUM oracle, and hope that a solution to a dense instance also solves the original problem. We deal with potentially malicious oracles (that repeatedly output correlated useless solutions) by an obfuscation process that adds noise to the dense instances. Using discrete Fourier analysis, we show that the obfuscation eliminates correlations among the oracle's solutions, even though its inputs are highly correlated.},
  citationcount = {1},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {reduction}
}

@article{dinurOnDifferentialPrivacy2023,
  title = {On Differential Privacy and Adaptive Data Analysis with Bounded Space},
  author = {Dinur, Itai and Stemmer, Uri and Woodruff, David P. and Zhou, Samson},
  year = {2023},
  doi = {10.48550/arXiv.2302.05707},
  abstract = {We study the space complexity of the two related fields of differential privacy and adaptive data analysis. Specifically, (1) Under standard cryptographic assumptions, we show that there exists a problem P that requires exponentially more space to be solved efficiently with differential privacy, compared to the space needed without privacy. To the best of our knowledge, this is the first separation between the space complexity of private and non-private algorithms. (2) The line of work on adaptive data analysis focuses on understanding the number of samples needed for answering a sequence of adaptive queries. We revisit previous lower bounds at a foundational level, and show that they are a consequence of a space bottleneck rather than a sampling bottleneck. To obtain our results, we define and construct an encryption scheme with multiple keys that is built to withstand a limited amount of key leakage in a very particular way.},
  citationcount = {10},
  venue = {IACR Cryptology ePrint Archive},
  keywords = {adaptive,lower bound,query}
}

@article{dinurThePcpTheorem2006,
  title = {The {{PCP}} Theorem by Gap Amplification},
  author = {Dinur, Irit},
  year = {2006},
  doi = {10.1145/1132516.1132553},
  abstract = {We present a new proof of the PCP theorem that is based on a combinatorial amplification lemma. The unsat value of a set of constraints C = (c1,...,cn), denoted UNSAT(C), is the smallest fraction of unsatisfied constraints, ranging over all possible assignments for the underlying variables.We describe a new combinatorial amplification transformation that doubles the unsat-value of a constraint-system, with only a linear blowup in the size of the system. The amplification step causes an increase in alphabet-size that is corrected by a PCP composition step. Iterative application of these two steps yields a proof for the PCP theorem.The amplification lemma relies on a new notion of "graph powering" that can be applied to systems of constraints. This powering amplifies the unsat-value of a constraint system provided that the underlying graph structure is an expander.We also apply the amplification lemma to construct PCPs and locally-testable codes whose length is linear up to a polylog factor, and whose correctness can be probabilistically verified by making a constant number of queries. Namely, we prove SAT {$\in$} PCP1/2,1[log2(n {$\bullet$} poly log n),O(1)].},
  citationcount = {402},
  venue = {Symposium on the Theory of Computing}
}

@article{dixonConcurrencyInAn1994,
  title = {Concurrency in an {{O}}(Log Log {{N}}) Priority Queue},
  author = {Dixon, B.},
  year = {1994},
  doi = {10.1007/3-540-58078-6_6},
  abstract = {No abstract available},
  citationcount = {5},
  venue = {Canada-France Conference on Parallel and Distributed Computing}
}

@article{dobkinMultidimensionalSearchingProblems1976,
  title = {Multidimensional Searching Problems},
  author = {Dobkin, D. and Lipton, R.},
  year = {1976},
  doi = {10.1137/0205015},
  abstract = {Classic binary search is extended to multidimensional search problems. This extension yields efficient algorithms for a number of tasks such as a secondary searching problem of Knuth, region location in planar graphs, and speech recognition.},
  citationcount = {252},
  venue = {SIAM journal on computing (Print)}
}

@article{dobrevImprovedAnalysisOf2017,
  title = {Improved Analysis of the Online Set Cover Problem with Advice},
  author = {Dobrev, S. and Edmonds, J. and Komm, D. and Kralovic, Rastislav and Kr{\'a}lovic, Richard and Krug, S. and M{\"o}mke, Tobias},
  year = {2017},
  doi = {10.1016/j.tcs.2017.05.029},
  abstract = {No abstract available},
  citationcount = {9},
  venue = {Theoretical Computer Science}
}

@article{dodisChangingBaseLosing2010,
  title = {Changing Base without Losing Space},
  author = {Dodis, Y. and Patrascu, M. and Thorup, M.},
  year = {2010},
  doi = {10.1145/1806689.1806771},
  abstract = {We describe a simple, but powerful local encoding technique, implying two surprising results: 1. We show how to represent a vector of n values from some alphabet S using ceiling(n * log2 {\textbar}S{\textbar}) bits, such that reading or writing any entry takes O(1) time. This demonstrates, for instance, an "equivalence" between decimal and binary computers, and has been a central toy problem in the field of succinct data structures. Previous solutions required space of n * log2 {\textbar}S{\textbar} + n/logO(1) n bits for constant access. 2. Given a stream of n bits arriving online (for any n, not known in advance), we can output a *prefix-free* encoding that uses n + log2 n + O(loglog n) bits. The encoding and decoding algorithms only require O(log n) bits of memory, and run in constant time per word. This result is interesting in cryptographic applications, as prefix-free codes are the simplest counter-measure to extensions attacks on hash functions, message authentication codes and pseudorandom functions. Our result refutes a conjecture of [Maurer, Sjodin 2005] on the hardness of online prefix-free encodings.},
  citationcount = {56},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure}
}

@article{dodisSpaceTimeTradeoffs1999,
  title = {Space Time Tradeoffs for Graph Properties},
  author = {Dodis, Y. and Khanna, S.},
  year = {1999},
  doi = {10.1007/3-540-48523-6_26},
  abstract = {No abstract available},
  citationcount = {19},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{dolevFindingNeighborhoodQuery1993,
  title = {Finding the Neighborhood of a Query in a Dictionary},
  author = {Dolev, D. and Harari, Yuval and Parnas, Michal},
  year = {1993},
  doi = {10.1109/ISTCS.1993.253486},
  abstract = {Many applications require the retrieval of all words from a fixed dictionary D, that are 'close' to some input string. The paper defines a theoretical framework to study the performance of algorithms for this problem, and provides a basic algorithmic approach. It is shown that a certain class of algorithms, D-oblivious algorithms, can not be optimal both in space and time. This is done by proving a lower bound on the tradeoff between the space and time complexities of D-oblivious algorithms. Several algorithms for this problem are presented, and their performance is compared to that of Ispell, the standard speller of Unix. On the Webster English dictionary the algorithms are shown to be faster than 'Ispell' by a significant factor, while incurring only a small cost in space.<<ETX>>},
  citationcount = {20},
  venue = {[1993] The 2nd Israel Symposium on Theory and Computing Systems},
  keywords = {lower bound,query}
}

@article{dolevNeighborhoodPreservingHashing1994,
  title = {Neighborhood Preserving Hashing and Approximate Queries},
  author = {Dolev, D. and Harari, Yuval and Linial, N. and Nisan, N. and Parnas, Michal},
  year = {1994},
  doi = {10.1137/S089548019731809X},
  abstract = {Let D{$\subseteq\Sigma^n$} be a dictionary. We look for efficient data structures and algorithms to solve the following approximate query problem: Given a query u{$\in\Sigma^n$} list all words v{$\in$}D that are close to u in Hamming distance. The problem reduces to the following combinatorial problem: Hash the vertices of the n-dimensional hypercube into buckets so that (1) the c-neighborhood of each vertex is mapped into at most k buckets and (2) no bucket is too large. Lower and upper bounds are given for the tradeoff between k and the size of the largest bucket. These results are used to derive bounds for the approximate query problem.},
  citationcount = {25},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{dolevSuperconcentratorsGeneralizersAnd1983,
  title = {Superconcentrators, Generalizers and Generalized Connectors with Limited Depth},
  author = {Dolev, D. and Dwork, C. and Pippenger, N. and Wigderson, A.},
  year = {1983},
  doi = {10.1145/800061.808731},
  abstract = {We show that the minimum possible size of an n-superconcentrator with depth 2k{$\geq$}4 is {\texttheta}(n{$\lambda$}(k, n)), where {$\lambda$}(k, .) is the inverse of a certain function at the k-th level of the primitive recursive hierarchy. It follows that the minimum possible depth of an n-superconcentrator with linear size is {\texttheta}({$\beta$}(n)), where {$\beta$} is the inverse of a function growing more rapidly than any primitive recursive function. Similar results hold for generalizers. We give a simple explicit construction for a (d1...dk)-generalizer with depth k and size (d1+...+dk)d1...dk. This is applied to give a simple explicit construction for a generalized n-connector with depth 2k-3 and size (2d1+3d2+...+3dk-1+2dk) d1...dk. These are the best explicit constructions currently available. We also show that, for each fixed k{$\geq$}2, the minimum possible size of a generalized n-connector with depth k is {\textohm}(n1+1/k) and 0((n log n)1+1/k).},
  citationcount = {82},
  venue = {Symposium on the Theory of Computing}
}

@article{dorAllPairsAlmost1996,
  title = {All Pairs Almost Shortest Paths},
  author = {Dor, D. and Halperin, S. and Zwick, Uri},
  year = {1996},
  doi = {10.1109/SFCS.1996.548504},
  abstract = {Let G=(V,E) be an unweighted undirected graph on n vertices. A simple argument shows that computing all distances in G with an additive one-sided error of at most 1 is as hard as Boolean matrix multiplication. Building on recent work of D. Aingworth et al. (1996), we describe an O/spl tilde/(min\{n/sup 3/2/m/sup 1/2/,n/sup 7/3/\}) time algorithm APASP/sub 2/ for computing all distances in G with an additive one-sided error of at most 2. The algorithm APASP/sub 2/ is simple, easy to implement, and faster than the fastest known matrix multiplication algorithm. Furthermore, for every even k{\textquestiondown}2, we describe an O/spl tilde/(min\{n/sup 2-(2)/(k+2)/m/sup (2)/(k+2)/, n/sup 2+(2)/(3k-2)/\}) time algorithm APASP/sub k/ for computing all distances in G with an additive one-sided error of at most k. We also give an O/spl tilde/(n/sup 2/) time algorithm APASP/sub /spl infin// for producing stretch 3 estimated distances in an unweighted and undirected graph on n vertices. No constant stretch factor was previously achieved in O/spl tilde/(n/sup 2/) time. We say that a weighted graph F=(V,E') k-emulates an unweighted graph G=(V,E) if for every u, v/spl isin/V we have /spl delta//sub G/(u,v)/spl les//spl delta//sub F/(u,v)/spl les//spl delta//sub G/(u,v)+k. We show that every unweighted graph on n vertices has a 2-emulator with O/spl tilde/(n/sup 3/2/) edges and a 4-emulator with O/spl tilde/(n/sup 4/3/) edges. These results are asymptotically tight. Finally, we show that any weighted undirected graph on n vertices has a 3-spanner with O/spl tilde/(n/sup 3/2/) edges and that such a 3-spanner can be built in O/spl tilde/(mn/sup 1/2/) time. We also describe an O/spl tilde/(n(m/sup 2/3/+n)) time algorithm for estimating all distances in a weighted undirected graph on n vertices with a stretch factor of at most 3.},
  citationcount = {270},
  venue = {Proceedings of 37th Conference on Foundations of Computer Science}
}

@article{doryNewTradeoffsFor2022,
  title = {New Tradeoffs for Decremental Approximate All-Pairs Shortest Paths},
  author = {Dory, Michal and Forster, S. and Nazari, Yasamin and {de Vos}, Tijn},
  year = {2022},
  doi = {10.48550/arXiv.2211.01152},
  abstract = {We provide new tradeoffs between approximation and running time for the decremental all-pairs shortest paths (APSP) problem. For undirected graphs with m edges and n nodes undergoing edge deletions, we provide four new approximate decremental APSP algorithms, two for weighted and two for unweighted graphs. Our first result is (2+{$\epsilon$})-APSP with total update time O\vphantom\{\}(m\textsuperscript{\{\vphantom\}}1/2\vphantom\{\}n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}) (when m=n\textsuperscript{\{\vphantom\}}1+c\vphantom\{\} for any constant 0\{u,v\})-APSP with total update time O\vphantom\{\}(nm\textsuperscript{\{\vphantom\}}3/4\vphantom\{\}), where the second term is an additive stretch with respect to W\textsubscript{\{\vphantom\}}u,v\vphantom\{\}, the maximum weight on the shortest path from u to v. Our third result is (2+{$\epsilon$})-APSP for unweighted graphs in (m\textsuperscript{\{\vphantom\}}7/4\vphantom\{\}) update time, which for sparse graphs (m=o(n\textsuperscript{\{\vphantom\}}8/7\vphantom\{\})) is the first subquadratic (2+{$\epsilon$})-approximation. Our last result for unweighted graphs is (1+{$\epsilon$},2(k-1))-APSP, for k{$\geq$}2, with O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2-1/k\vphantom\{\}m\textsuperscript{\{\vphantom\}}1/k\vphantom\{\}) total update time (when m=n\textsuperscript{\{\vphantom\}}1+c\vphantom\{\} for any constant c{$>$}0). For comparison, in the special case of (1+{$\epsilon$},2)-approximation, this improves over the state-of-the-art algorithm by [Henzinger, Krinninger, Nanongkai, SICOMP 2016] with total update time of O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2.5\vphantom\{\}). All of our results are randomized, work against an oblivious adversary, and have constant query time.},
  citationcount = {7},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {query,query time,update,update time}
}

@article{downeyFundamentalsOfParameterized2013,
  title = {Fundamentals of Parameterized Complexity},
  author = {Downey, R. and Fellows, M.},
  year = {2013},
  doi = {10.1007/978-1-4471-5559-1},
  abstract = {No abstract available},
  citationcount = {1372},
  venue = {Texts in Computer Science}
}

@article{doyleLinearExpectedTime1976,
  title = {Linear Expected Time of a Simple Union-Find Algorithm},
  author = {Doyle, J. and Rivest, R.},
  year = {1976},
  doi = {10.1016/0020-0190(76)90061-2},
  abstract = {No abstract available},
  citationcount = {37},
  venue = {Information Processing Letters}
}

@article{driscollMakingDataStructures1986,
  title = {Making Data Structures Persistent},
  author = {Driscoll, James R. and Sarnak, Neil and Sleator, D. and Tarjan, R.},
  year = {1986},
  doi = {10.1145/12130.12142},
  abstract = {Abstract This paper is a study of persistence in data structures. Ordinary data structures are ephemeral in the sense that a change to the structure destroys the old version, leaving only the new version available for use. In contrast, a persistent structure allows access to any version, old or new, at any time. We develop simple, systematic, and efficient techniques for making linked data structures persistent. We use our techniques to devise persistent forms of binary search trees with logarithmic access, insertion, and deletion times and O (1) space bounds for insertion and deletion.},
  citationcount = {879},
  venue = {Symposium on the Theory of Computing}
}

@article{duanApproximatingAllPair2018,
  title = {Approximating All-Pair Bounded-Leg Shortest Path and {{APSP-AF}} in Truly-Subcubic Time},
  author = {Duan, Ran and Ren, Hanlin},
  year = {2018},
  doi = {10.4230/LIPIcs.ICALP.2018.42},
  abstract = {In the bounded-leg shortest path (BLSP) problem, we are given a weighted graph G with nonnegative edge lengths, and we want to answer queries of the form "what's the shortest path from u to v, where only edges of length = f are considered. In this article we give an O (n{\textasciicircum}\{(omega+3)/2\}epsilon{\textasciicircum}\{-3/2\}log W) time algorithm to compute a data structure that answers APSP-AF queries in O(log(epsilon{\textasciicircum}\{-1\}log (nW))) time and achieves (1+epsilon)-approximation, where omega {\textexclamdown} 2.373 is the exponent of time complexity of matrix multiplication, W is the upper bound of integer edge lengths, and n is the number of vertices. This is the first truly-subcubic time algorithm for these problems on dense graphs. Our algorithm utilizes the O(n{\textasciicircum}\{(omega+3)/2\}) time max-min product algorithm [Duan and Pettie 2009]. Since the all-pair bottleneck path (APBP) problem, which is equivalent to max-min product, can be seen as all-pair reachability for all flow, our approach indeed shows that these problems are almost equivalent in the approximation sense.},
  citationcount = {3},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,query}
}

@article{duanArtisticVideoStylization2016,
  title = {Artistic Video Stylization for Face},
  author = {Duan, Chong and Gao, Yan and Zhang, Jian},
  year = {2016},
  doi = {10.1109/ICSAI.2016.7811087},
  abstract = {This paper presents a novel method that creates artistic video stylization for face area. Previous work has produced artistic stylized images based on the method of by-example synthesis that uses two sample images which are unfiltered image and filtered image and extends this method called image analogies to create video stylization. Since using the method of image analogies, we can create various styles of video for face by just simply altering the unfiltered image and filtered image. The temporal coherence is maintained by estimating the motion of pixels between the current frame and previous frame. We show experimentally that our method can create artistic video stylization for face efficiently.},
  citationcount = {2},
  venue = {International Conference on Systems and Informatics}
}

@article{duanConnectivityOraclesFailure2010,
  title = {Connectivity Oracles for Failure Prone Graphs},
  author = {Duan, Ran and Pettie, Seth},
  year = {2010},
  doi = {10.1145/1806689.1806754},
  abstract = {Dynamic graph connectivity algorithms have been studied for many years, but typically in the most general possible setting, where the graph can evolve in completely arbitrary ways. In this paper we consider a dynamic subgraph model. We assume there is some fixed, underlying graph that can be preprocessed ahead of time. The graph is subject only to vertices and edges flipping "off" (failing) and "on" (recovering), where queries naturally apply to the subgraph on edges/vertices currently flipped on. This model fits most real world scenarios, where the topology of the graph in question (say a router network or road network) is constantly evolving due to temporary failures but never deviates too far from the ideal failure-free state. We present the first efficient connectivity oracle for graphs susceptible to vertex failures. Given vertices u and v and a set D of d failed vertices, we can determine if there is a path from u to v avoiding D in time polynomial in d log n. There is a tradeoff in our oracle between the space, which is roughly mn{$\varepsilon$}, for 0{\textexclamdown} {$\varepsilon\leq$} 1, and the polynomial query time, which depends on {$\varepsilon$}. If one wanted to achieve the same functionality with existing data structures (based on edge failures or twin vertex failures) the resulting connectivity oracle would either need exorbitant space ({\textohm}(nd)) or update time {\textohm}(dn), that is, linear in the number of vertices. Our connectivity oracle is therefore the first of its kind. As a byproduct of our oracle for vertex failures we reduce the problem of constructing an edge-failure oracle to 2D range searching over the integers. We show there is an  O(m)-space oracle that processes any set of d failed edges in O(d2 log log n) time and, thereafter, answers connectivity queries in O(log log n) time. Our update time is exponentially faster than a recent connectivity oracle of Patrascu and Thorup for bounded d, but slower as a function of d.},
  citationcount = {34},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,dynamic,query,query time,update,update time}
}

@article{duanConnectivityOraclesFor2016,
  title = {Connectivity Oracles for Graphs Subject to Vertex Failures},
  author = {Duan, Ran and Pettie, Seth},
  year = {2016},
  doi = {10.1137/1.9781611974782.31},
  abstract = {We introduce new data structures for answering connectivity queries in graphs subject to batched vertex failures. A deterministic structure processes a batch of d{$\leq$}d\textsubscript{\{\vphantom\}}{$\star$}\vphantom\{\} failed vertices in O\vphantom\{\}(d{$^3$}) time and thereafter answers connectivity queries in O(d) time. It occupies space O(d\textsubscript{\{\vphantom\}}{$\star$}\vphantom\{\}mn). We develop a randomized Monte Carlo version of our data structure with update time O\vphantom\{\}(d{$^2$}), query time O(d), and space O\vphantom\{\}(m) for any failure bound d{$\leq$}n. This is the first connectivity oracle for general graphs that can efficiently deal with an unbounded number of vertex failures. We also develop a more efficient Monte Carlo edge-failure connectivity oracle. Using space O(n{$^2$}n), d edge failures are processed in O(ddn) time and thereafter, connectivity queries are answered in O(n) time, which are correct w.h.p. Our data structures are based on a new decomposition theorem for an undirected graph G=(V,E), which is of independent interest. It states that for any terminal set U{$\subseteq$}V we can remove a set B of {\textbar}U{\textbar}/(s-2) vertices such that the remaining graph contains a Steiner forest for U-B with maximum degree s.},
  citationcount = {30},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,query,query time,update,update time}
}

@article{duanFasterRandomizedWorst2016,
  title = {Faster Randomized Worst-Case Update Time for Dynamic Subgraph Connectivity},
  author = {Duan, Ran and Zhang, Le},
  year = {2016},
  doi = {10.1007/978-3-319-62127-2_29},
  abstract = {No abstract available},
  citationcount = {9},
  venue = {Workshop on Algorithms and Data Structures},
  keywords = {dynamic,update,update time}
}

@article{duanNewDataStructures2010,
  title = {New Data Structures for Subgraph Connectivity},
  author = {Duan, Ran},
  year = {2010},
  doi = {10.1007/978-3-642-14165-2_18},
  abstract = {No abstract available},
  citationcount = {18},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{dudaPatternClassificationAnd1974,
  title = {Pattern Classification and Scene Analysis},
  author = {Duda, R. and Hart, P.},
  year = {1974},
  doi = {10.2307/1573081},
  abstract = {Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis.},
  citationcount = {17382},
  venue = {A Wiley-Interscience publication}
}

@article{dudejaDecrementalMatchingIn2023,
  title = {Decremental Matching in General Weighted Graphs},
  author = {Dudeja, Aditi},
  year = {2023},
  doi = {10.48550/arXiv.2312.08996},
  abstract = {In this paper, we consider the problem of maintaining a (1-{$\varepsilon$})-approximate maximum weight matching in a dynamic graph G, while the adversary makes changes to the edges of the graph. In the fully dynamic setting, where both edge insertions and deletions are allowed, Gupta and Peng gave an algorithm for this problem with an update time of O\vphantom\{\}\textsubscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}({\textsurd}\{m\}). We study a natural relaxation of this problem, namely the decremental model, where the adversary is only allowed to delete edges. For the cardinality version of this problem in general (possibly, non-bipartite) graphs, Assadi, Bernstein, and Dudeja gave a decremental algorithm with update time O\textsubscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}(\{poly\}(n)). However, beating O\vphantom\{\}\textsubscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}({\textsurd}\{m\}) update time remained an open problem for the \{weighted\} version in \{general graphs\}. In this paper, we bridge the gap between unweighted and weighted general graphs for the decremental setting. We give a O\textsubscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}(\{poly\}(n)) update time algorithm that maintains a (1-{$\varepsilon$})-approximate maximum weight matching under adversarial deletions. Like the decremental algorithm of Assadi, Bernstein, and Dudeja, our algorithm is randomized, but works against an adaptive adversary. It also matches the time bound for the cardinality version upto dependencies on {$\varepsilon$} and a R factor, where R is the ratio between the maximum and minimum edge weight in G.},
  citationcount = {2},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {adaptive,dynamic,update,update time}
}

@article{dudekAllNonTrivial2020,
  title = {All Non-Trivial Variants of 3-{{LDT}} Are Equivalent},
  author = {Dudek, Bart{\l}omiej and Gawrychowski, Pawe{\l} and Starikovskaya, Tatiana},
  year = {2020},
  doi = {10.1145/3357713.3384275},
  abstract = {The popular 3-SUM conjecture states that there is no strongly subquadratic time algorithm for checking if a given set of integers contains three distinct elements that sum up to zero. A closely related problem is to check if a given set of integers contains distinct x 1, x 2, x 3 such that x 1+x 2=2x 3. This can be reduced to 3-SUM in almost-linear time, but surprisingly a reverse reduction establishing 3-SUM hardness was not known. We provide such a reduction, thus resolving an open question of Erickson. In fact, we consider a more general problem called 3-LDT parameterized by integer parameters {$\alpha$}1, {$\alpha$}2, {$\alpha$}3 and t. In this problem, we need to check if a given set of integers contains distinct elements x 1, x 2, x 3 such that {$\alpha$}1 x 1+{$\alpha$}2 x 2 +{$\alpha$}3 x 3 = t. For some combinations of the parameters, every instance of this problem is a NO-instance or there exists a simple almost-linear time algorithm. We call such variants trivial. We prove that all non-trivial variants of 3-LDT are equivalent under subquadratic reductions. Our main technical contribution is an efficient deterministic procedure based on the famous Behrend's construction that partitions a given set of integers into few subsets that avoid a chosen linear equation.},
  citationcount = {13},
  venue = {Symposium on the Theory of Computing},
  keywords = {reduction}
}

@article{dudekOnlineContextFree2024,
  title = {Online Context-Free Recognition in Omv Time},
  author = {Dudek, Bart{\l}omiej and Gawrychowski, Pawe{\l}},
  year = {2024},
  doi = {10.4230/LIPIcs.CPM.2024.13},
  abstract = {One of the classical algorithmic problems in formal languages is the context-free recognition problem: for a given context-free grammar and a length-n string, check if the string belongs to the language described by the grammar. Already in 1975, Valiant showed that this can be solved in {\texttildelow} O p n {$\omega$} q time, where {$\omega$} is the matrix multiplication exponent. More recently, Abboud, Backurs, and Vassilevska Williams [FOCS 2015] showed that any improvement on this complexity would imply a breakthrough algorithm for the k -Clique problem. We study the natural online version of this problem, where the input string w r 1 ..n s is given left-to-right, and after having seen every prefix w r 1 ..t s we should output if it belongs to the language. The goal is to maintain the total running time to process the whole input. Even though this version has been extensively studied in the past, the best known upper bound was O p n 3 \{ log 2 n q . We connect the complexity of online context-free recognition to that of Online Matrix-Vector Multiplication, which allows us to improve the upper bound to n 3 \{ 2 {\textohm} p ? log n q .\vphantom{\}\}}},
  citationcount = {1},
  venue = {Annual Symposium on Combinatorial Pattern Matching}
}

@article{dunkerEstimatesForThe2000,
  title = {Estimates for the Small Ball Probabilities of the Fractional Brownian Sheet},
  author = {Dunker, T.},
  year = {2000},
  doi = {10.1023/A:1007897525164},
  abstract = {No abstract available},
  citationcount = {30},
  venue = {No venue available}
}

@article{dunkerMetricEntropyOf1998,
  title = {Metric Entropy of the Integration Operator and Small Ball Probabilities for the {{Brownian}} Sheet},
  author = {Dunker, T. and K{\"u}hn, T. and Lifshits, M. and Linde, W.},
  year = {1998},
  doi = {10.1016/S0764-4442(97)82993-X},
  abstract = {No abstract available},
  citationcount = {21},
  venue = {No venue available}
}

@article{durajEquivalencesBetweenTriangle2019,
  title = {Equivalences between Triangle and Range Query Problems},
  author = {Duraj, Lech and Kleiner, Krzysztof and Polak, Adam and Williams, V. V.},
  year = {2019},
  doi = {10.1137/1.9781611975994.3},
  abstract = {We define a natural class of range query problems, and prove that all problems within this class have the same time complexity (up to polylogarithmic factors). The equivalence is very general, and even applies to online algorithms. This allows us to obtain new improved algorithms for all of the problems in the class. We then focus on the special case of the problems when the queries are offline and the number of queries is linear. We show that our range query problems are runtime-equivalent (up to polylogarithmic factors) to counting for each edge e in an m-edge graph the number of triangles through e. This natural triangle problem can be solved using the best known triangle counting algorithm, running in O(m\textsuperscript{\{\vphantom\}}2{$\omega$}/({$\omega$}+1)\vphantom\{\}){$\leq$}O(m\textsuperscript{\{\vphantom\}}1.41\vphantom\{\}) time. Moreover, if {$\omega$}=2, the O(m\textsuperscript{\{\vphantom\}}2{$\omega$}/({$\omega$}+1)\vphantom\{\}) running time is known to be tight (within m\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} factors) under the 3SUM Hypothesis. In this case, our equivalence settles the complexity of the range query problems. Our problems constitute the first equivalence class with this peculiar running time bound. To better understand the complexity of these problems, we also provide a deeper insight into the family of triangle problems, in particular showing black-box reductions between triangle listing and per-edge triangle detection and counting. As a byproduct of our reductions, we obtain a simple triangle listing algorithm matching the state-of-the-art for all regimes of the number of triangles. We also give some not necessarily tight, but still surprising reductions from variants of matrix products, such as the (,)-product.},
  citationcount = {16},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {query,reduction}
}

@article{duttaTradeoffsInDepth2006,
  title = {Tradeoffs in Depth-Two Superconcentrators},
  author = {Dutta, Chinmoy and Radhakrishnan, J.},
  year = {2006},
  doi = {10.1007/11672142_30},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Symposium on Theoretical Aspects of Computer Science}
}

@article{dvirExtractorsAndRank2007,
  title = {Extractors and Rank Extractors for Polynomial Sources},
  author = {Dvir, Zeev and Gabizon, Ariel and Wigderson, A.},
  year = {2007},
  doi = {10.1007/s00037-009-0258-4},
  abstract = {No abstract available},
  citationcount = {76},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{dvirStaticDataStructure2018,
  title = {Static Data Structure Lower Bounds Imply Rigidity},
  author = {Dvir, Zeev and Golovnev, Alexander and Weinstein, Omri},
  year = {2018},
  doi = {10.1145/3313276.3316348},
  abstract = {We show that static data structure lower bounds in the group (linear) model imply semi-explicit lower bounds on matrix rigidity. In particular, we prove that an explicit lower bound of t {$\geq$} {$\omega$}(log2 n) on the cell-probe complexity of linear data structures in the group model, even against arbitrarily small linear space (s= (1+)n), would already imply a semi-explicit (PNP) construction of rigid matrices with significantly better parameters than the current state of art (Alon, Panigrahy and Yekhanin, 2009). Our results further assert that polynomial (t{$\geq$} n{$\delta$}) data structure lower bounds against near-optimal space, would imply super-linear circuit lower bounds for log-depth linear circuits (a four-decade open question). In the succinct space regime (s=n+o(n)), we show that any improvement on current cell-probe lower bounds in the linear model would also imply new rigidity bounds. Our results rely on a new connection between the ``inner'' and ``outer'' dimensions of a matrix (Paturi and Pudl{\'a}k, 2006), and on a new reduction from worst-case to average-case rigidity, which is of independent interest.},
  citationcount = {27},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {cell probe,data structure,lower bound,reduction,sorted,static}
}

@article{dvokDataStructuresLower2021,
  title = {Data Structures Lower Bounds and Popular Conjectures},
  author = {Dvo{\v r}{\'a}k, P. and Kouck{\'y}, M. and Kr{\'a}l, Karel and Sl{\'i}vov{\'a}, Veronika},
  year = {2021},
  doi = {10.4230/LIPIcs.ESA.2021.39},
  abstract = {In this paper, we investigate the relative power of several conjectures that attracted recently lot of interest. We establish a connection between the Network Coding Conjecture (NCC) of Li and Li [Li and Li, 2004] and several data structure problems such as non-adaptive function inversion of Hellman [M. Hellman, 1980] and the well-studied problem of polynomial evaluation and interpolation. In turn these data structure problems imply super-linear circuit lower bounds for explicit functions such as integer sorting and multi-point polynomial evaluation.},
  citationcount = {4},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,lower bound,non-adaptive}
}

@article{dvokLowerBoundsFor2020,
  title = {Lower Bounds for Semi-Adaptive Data Structures via Corruption},
  author = {Dvo{\v r}{\'a}k, P. and Loff, B.},
  year = {2020},
  doi = {10.4230/LIPIcs.FSTTCS.2020.20},
  abstract = {In a dynamic data structure problem we wish to maintain an encoding of some data in memory, in such a way that we may efficiently carry out a sequence of queries and updates to the data. A long-standing open problem in this area is to prove an unconditional polynomial lower bound of a trade-off between the update time and the query time of an adaptive dynamic data structure computing some explicit function. Ko and Weinstein provided such lower bound for a restricted class of \{\vphantom\}\emph{semi-adaptive}\vphantom\{\}\emph{ data structures, which compute the Disjointness function. There, the data are subsets x{$_1$},{\dots},x\textsubscript{k} and y of  1,{\dots},n , the updates can modify y (by inserting and removing elements), and the queries are an index i{$\in$} 1,{\dots},k  (query i should answer whether x\textsubscript{i} and y are disjoint, i.e., it should compute the Disjointness function applied to (x\textsubscript{i},y)). The semi-adaptiveness places a restriction in how the data structure can be accessed in order to answer a query. We generalize the lower bound of Ko and Weinstein to work not just for the Disjointness, but for any function having high complexity under the smooth corruption bound.}},
  citationcount = {2},
  venue = {Foundations of Software Technology and Theoretical Computer Science},
  keywords = {adaptive,data structure,dynamic,lower bound,query,query time,update,update time}
}

@article{e.demaineRetroactiveDataStructures2007,
  title = {Retroactive Data Structures},
  author = {E. Demaine and J. Iacono and S. Langerman},
  year = {2007},
  journal = {TALG},
  doi = {10.1145/1240233.1240236},
  abstract = {We introduce a new data structuring paradigm in which operations can be performed on a data structure not only in the present, but also in the past. In this new paradigm, called retroactive data structures, the historical sequence of operations performed on the data structure is not fixed. The data structure allows arbitrary insertion and deletion of operations at arbitrary times, subject only to consistency requirements. We initiate the study of retroactive data structures by formally defining the model and its variants. We prove that, unlike persistence, efficient retroactivity is not always achievable. Thus, we present efficient retroactive data structures for queues, doubly ended queues, priority queues, union-find, and decomposable search structures.},
  keywords = {data structure},
  annotation = {Citation Count: 47}
}

@article{edelsbrunnerAlgorithmsInCombinatorial1987,
  title = {Algorithms in Combinatorial Geometry},
  author = {Edelsbrunner, H.},
  year = {1987},
  doi = {10.1007/978-3-642-61568-9},
  abstract = {No abstract available},
  citationcount = {2330},
  venue = {EATCS Monographs in Theoretical Computer Science}
}

@article{edelsbrunnerHalfplanarRangeSearch1986,
  title = {Halfplanar Range Search in Linear Space and {{O}}(N{\textasciicircum}(0.695)) Query Time},
  author = {Edelsbrunner, H. and Welzl, E.},
  year = {1986},
  doi = {10.1016/0020-0190(86)90088-8},
  abstract = {No abstract available},
  citationcount = {88},
  venue = {Information Processing Letters}
}

@article{edelsbrunnerPolygonalIntersectionSearching1982,
  title = {Polygonal Intersection Searching},
  author = {Edelsbrunner, H. and Maurer, H. and Kirkpatrick, D.},
  year = {1982},
  doi = {10.1016/0020-0190(82)90090-4},
  abstract = {No abstract available},
  citationcount = {39},
  venue = {Information Processing Letters}
}

@article{el-zeinImprovedTimeAnd2018,
  title = {Improved Time and Space Bounds for Dynamic Range Mode},
  author = {{El-Zein}, Hicham and He, Meng and Munro, J. and Sandlund, Bryce},
  year = {2018},
  doi = {10.4230/LIPIcs.ESA.2018.25},
  abstract = {Given an array A of n elements, we wish to support queries for the most frequent and least frequent element in a subrange [l,r] of A. We also wish to support updates that change a particular element at index i or insert/ delete an element at index i. For the range mode problem, our data structure supports all operations in O(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}) deterministic time using only O(n) space. This improves two results by Chan et al. \{C14\}: a linear space data structure supporting update and query operations in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}3/4\vphantom\{\}) time and an O(n\textsuperscript{\{\vphantom\}}4/3\vphantom\{\}) space data structure supporting update and query operations in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}) time. For the range least frequent problem, we address two variations. In the first, we are allowed to answer with an element of A that may not appear in the query range, and in the second, the returned element must be present in the query range. For the first variation, we develop a data structure that supports queries in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}) time, updates in O(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}) time, and occupies O(n) space. For the second variation, we develop a Monte Carlo data structure that supports queries in O(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}) time, updates in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}) time, and occupies O\vphantom\{\}(n) space, but requires that updates are made independently of the results of previous queries. The Monte Carlo data structure is also capable of answering k-frequency queries; that is, the problem of finding an element of given frequency in the specified query range. Previously, no dynamic data structures were known for least frequent element or k-frequency queries.},
  citationcount = {7},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,dynamic,query,update}
}

@article{el-zeinSuccinctColorSearching2017,
  title = {Succinct Color Searching in One Dimension},
  author = {{El-Zein}, Hicham and Munro, J. and Nekrich, Yakov},
  year = {2017},
  doi = {10.4230/LIPIcs.ISAAC.2017.30},
  abstract = {In this paper we study succinct data structures for one-dimensional color reporting and color counting problems. We are given a set of n points with integer coordinates in the range [1,m] and every point is assigned a color from the set \{1,...{$\sigma$}\}. A color reporting query asks for the list of distinct colors that occur in a query interval [a,b] and a color counting query asks for the number of distinct colors in [a,b]. We describe a succinct data structure that answers approximate color counting queries in O(1) time and uses \{B\}(n,m) + O(n) + o(\{B\}(n,m)) bits, where \{B\}(n,m) is the minimum number of bits required to represent an arbitrary set of size n from a universe of m elements. Thus we show, somewhat counterintuitively, that it is not necessary to store colors of points in order to answer approximate color counting queries. In the special case when points are in the rank space (i.e., when n=m), our data structure needs only O(n) bits. Also, we show that {\textohm}(n) bits are necessary in that case. Then we turn to succinct data structures for color reporting. We describe a data structure that uses \{B\}(n,m) + nH\_d(S) + o(\{B\}(n,m)) + o(n{$\sigma$}) bits and answers queries in O(k+1) time, where k is the number of colors in the answer, and nH\_d(S) (d=\_{$\sigma$}n) is the d-th order empirical entropy of the color sequence. Finally, we consider succinct color reporting under restricted updates. Our dynamic data structure uses nH\_d(S)+o(n{$\sigma$}) bits and supports queries in O(k+1) time.},
  citationcount = {8},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {data structure,dynamic,query,update}
}

@article{eldarDirectAccessFor2023,
  title = {Direct Access for Answers to Conjunctive Queries with Aggregation},
  author = {Eldar, Idan and Carmeli, Nofar and Kimelfeld, B.},
  year = {2023},
  doi = {10.48550/arXiv.2303.05327},
  abstract = {We study the fine-grained complexity of conjunctive queries with grouping and aggregation. For common aggregate functions (e.g., min, max, count, sum), such a query can be phrased as an ordinary conjunctive query over a database annotated with a suitable commutative semiring. We investigate the ability to evaluate such queries by constructing in loglinear time a data structure that provides logarithmic-time direct access to the answers ordered by a given lexicographic order. This task is nontrivial since the number of answers might be larger than loglinear in the size of the input, so the data structure needs to provide a compact representation of the space of answers. In the absence of aggregation and annotation, past research established a sufficient tractability condition on queries and orders. For queries without self-joins, this condition is not just sufficient, but also necessary (under conventional lower-bound assumptions in fine-grained complexity). We show that all past results continue to hold for annotated databases, assuming that the annotation itself does not participate in the lexicographic order. Yet, past algorithms do not apply to the count-distinct aggregation, which has no efficient representation as a commutative semiring; for this aggregation, we establish the corresponding tractability condition. We then show how the complexity of the problem changes when we include the aggregate and annotation value in the order. We also study the impact of having all relations but one annotated by the multiplicative identity (one), as happens when we translate aggregate queries into semiring annotations, and having a semiring with an idempotent addition, such as the case of min, max, and count-distinct over a logarithmic-size domain.},
  citationcount = {7},
  venue = {International Conference on Database Theory},
  keywords = {data structure,lower bound,query}
}

@article{eliasEfficientStorageAnd1974,
  title = {Efficient Storage and Retrieval by Content and Address of Static Files},
  author = {Elias, P.},
  year = {1974},
  doi = {10.1145/321812.321820},
  abstract = {We consider a set of static files or inventories, each consisting of the same number of entries, each entry a binary word of the same fixed length selected (with replacement) from the set of all binary sequences of that length, and the entries in each file sorted into lexical order. We also consider several retrieval questions of interest for each such file. One is to find the value of the jth entry, another to find the number of entries of value less than k. When a binary representation of such a file is stored in computer memory and an algorithm or machine which knows only the file parameters (i.e. number of entries, number of possible values per entry) accesses some of the stored bits to answer a retrieval question, the number of bits stored and the number of bits accessed per retrieval question are two cost measures for the storage and retrieval task which have been used by Minsky and Papert. Bits stored depends on the representation chosen: bits accessed also depends on the retrieval question asked and on the algorithm used. We give firm lower bounds to minimax measures of bits stored and bits accessed for each of four retrieval questions, and construct representations and algorithms for a bit-addressable machine which come within factors of two or three of attaining all four bounds at once for files of any size. All four factors approach one for large enough files.},
  citationcount = {245},
  venue = {JACM}
}

@article{eliasTheComplexityOf1975,
  title = {The Complexity of Some Simple Retrieval Problems},
  author = {Elias, P. and Flower, Richard A.},
  year = {1975},
  doi = {10.1145/321892.321899},
  abstract = {Four costs of a retrieval algorithm are the number of bits needed to store a representation of a data base, the number of those bits which must be accessed to answer a retrieval question, the number of bits of state information required, and the logic complexity of the algorithm. Firm lower bounds are given to measures of the first three costs for simple binary retrieval problems. Systems are constructed which attain each bound separately. A system which finds the value of the kth bit in an N-bit string attains all bounds simultaneously. For two other more complex retrieval problems there are trading curves between storage and worst-case access, and between storage and average access. Lower and upper bounds to the trading curves are found. Minimal storage is a point of discontinuity on both curves, and for some complex problems large increases in storage are needed to approach minimal access. The cost of a complete updating algorithm is taken to be the number of bits it reads and/or writes in updating the representation of a data base. Lower bounds to measures of this cost are cited. Optimal minimal-storage systems also have minimal update cost. Optimal minimal-access systems with large storage cost also have large update cost, but a small increase in storage for such a system may reduce update cost dramatically. KEY  'ORDS AND PHRASES.' file, storage, retrieval, access, exact match, table lookup, computational complexity, retrieval algorithms, Kraft inequality CR CATEGORIES: 3.70, 3.72, 3.74, 5.25, 5.6},
  citationcount = {45},
  venue = {JACM}
}

@article{elkinSteinerShallowLight2011,
  title = {Steiner Shallow-Light Trees Are Exponentially Lighter than Spanning Ones},
  author = {Elkin, Michael and Solomon, Shay},
  year = {2011},
  doi = {10.1137/13094791X},
  abstract = {For a pair of parameters {$\alpha$},{$\beta\geq$}1, a spanning tree T of a weighted undirected n-vertex graph G=(V,E,w) is called an \{({$\alpha$},{$\beta$})-shallow-light tree\} (shortly, ({$\alpha$},{$\beta$})-SLT)of G with respect to a designated vertex rt{$\in$}V if (1) it approximates all distances from rt to the other vertices up to a factor of {$\alpha$}, and(2) its weight is at most {$\beta$} times the weight of the minimum spanning tree MST(G) of G. The parameter {$\alpha$} (respectively, {$\beta$}) is called the \{root-distortion\}(resp., \{lightness\}) of the tree T. Shallow-light trees (SLTs) constitute a fundamental graph structure, with numerous theoretical and practical applications. In particular, they were used for constructing spanners, in network design, for VLSI-circuit design, for various data gathering and dissemination tasks in wireless and sensor networks, in overlay networks, and in the message-passing model of distributed computing. Tight tradeoffs between the parameters of SLTs were established by Awer buch et al.\{ABP90, ABP91\} and Khuller et al.\{KRY93\}. They showed that for any {$\epsilon\&$}gt,0there always exist (1+{$\epsilon$},O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{{$\epsilon$}\}))-SLTs, and that the upper bound {$\beta$}=O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{{$\epsilon$}\})on the lightness of SLTs cannot be improved. In this paper we show that using Steiner points one can build SLTs with \{logarithmic lightness\}, i.e., {$\beta$}=O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{{$\epsilon$}\}). This establishes an \{exponential separation\} between spanning SLTs and Steiner ones. One particularly remarkable point on our tradeoff curve is {$\epsilon$}=0. In this regime our construction provides a \{shortest-path tree\} with weight at most O(n){$\cdot$}w(MST(G)). Moreover, we prove matching lower bounds that show that all our results are tight up to constant factors. Finally, on our way to these results we settle (up to constant factors) a number of open questions that were raised by Khuller et al.\{KRY93\} in SODA'93.},
  citationcount = {26},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {lower bound}
}

@article{ellenTablesShouldBe1995,
  title = {Tables Should Be Sorted (on Random Access Machines)},
  author = {Ellen, Faith and Miltersen, Peter Bro},
  year = {1995},
  doi = {10.1007/3-540-60220-8_87},
  abstract = {No abstract available},
  citationcount = {32},
  venue = {Workshop on Algorithms and Data Structures}
}

@article{emirisProductsOfEuclidean2017,
  title = {Products of {{Euclidean}} Metrics and Applications to Proximity Questions among Curves},
  author = {Emiris, I. and Psarros, I.},
  year = {2017},
  doi = {10.4230/LIPIcs.SoCG.2018.37},
  abstract = {The problem of Approximate Nearest Neighbor (ANN) search is fundamental in computer science and has benefited from significant progress in the past couple of decades. However, most work has been devoted to pointsets whereas complex shapes have not been sufficiently treated. Here, we focus on distance functions between discretized curves in Euclidean space: they appear in a wide range of applications, from road segments to time-series in general dimension. For {$\ell_p$}-products of Euclidean metrics, for any p, we design simple and efficient data structures for ANN, based on randomized projections, which are of independent interest. They serve to solve proximity problems under a notion of distance between discretized curves, which generalizes both discrete Fr{\'\{}e\vphantom\{\}chet and Dynamic Time Warping distances. These are the most popular and practical approaches to comparing such curves. We offer the first data structures and query algorithms for ANN with arbitrarily good approximation factor, at the expense of increasing space usage and preprocessing time over existing methods. Query time complexity is comparable or significantly improved by our algorithms; our algorithm is especially efficient when the length of the curves is bounded.},
  citationcount = {19},
  venue = {International Symposium on Computational Geometry},
  keywords = {data structure,dynamic,query,query time}
}

@article{emirisProductsOfEuclidean2020,
  title = {Products of Euclidean Metrics, Applied to Proximity Problems among Curves},
  author = {Emiris, I. and Psarros, I.},
  year = {2020},
  doi = {10.1145/3397518},
  abstract = {Approximate Nearest Neighbor (ANN) search is a fundamental computational problem that has benefited from significant progress in the past couple of decades. However, most work has been devoted to pointsets, whereas complex shapes have not been sufficiently addressed. Here, we focus on distance functions between discretized curves in Euclidean space: They appear in a wide range of applications, from road segments and molecular backbones to time-series in general dimension. For {$\ell$}p-products of Euclidean metrics, for any constant p, we propose simple and efficient data structures for ANN based on randomized projections: These data structures are of independent interest. Furthermore, they serve to solve proximity questions under a notion of distance between discretized curves, which generalizes both discrete Fr{\'e}chet and Dynamic Time Warping distance functions. These are two very popular and practical approaches to comparing such curves. We offer, for both approaches, the first data structures and query algorithms for ANN with arbitrarily good approximation factor, at the expense of increasing space usage and preprocessing time over existing methods. Query time complexity is comparable or significantly improved by our methods; our algorithm is especially efficient when the length of the curves is bounded. Finally, we focus on discrete Fr{\'e}chet distance when the ambient space is high dimensional and derive complexity bounds in terms of doubling dimension as well as an improved approximate near neighbor search.},
  citationcount = {5},
  venue = {ACM Trans. Spatial Algorithms Syst.},
  keywords = {data structure,dynamic,query,query time}
}

@article{engelsWhyAreCertain2015,
  title = {Why Are Certain Polynomials Hard?: {{A}} Look at Non-Commutative, Parameterized and Homomorphism Polynomials},
  author = {Engels, C.},
  year = {2015},
  doi = {10.22028/D291-26647},
  abstract = {In this thesis we will try to answer the question why specific polynomials have no small suspected arithmetic circuits. We will look at this general problem in three different ways. First, we study non-commutative computation. Here we show matching upper and lower bounds for the non-commutative permanent for various restricted graph classes. Our main result gives algebraic branching program upper and lower bounds for graphs with connected component size 6 as well as a \#P hardness result. We introduce a measure that characterizes this complexity on these instances. Secondly, we introduce a new framework for arithmetic circuits, similar to fixed parameter tractability in the boolean setting. This framework shows that specific polynomials based on graph problems have the expected complexity as in the counting FPT case. We introduce classes BVW[t] which are close to the boolean setting but hardly use the power of arithmetic circuits. We then introduce VW[t] with modified problems to remedy this situation. Thirdly, we study polynomials defined by graph homomorphisms and show various dichotomy theorems. This shows that even restrictions on the graphs can already give us hard instances. Finally, we stray from our main continuous thread and handle simple heuristics for metric graphs. Instead of studying specific metrics we look at a randomized process giving us shortest path metric instances.},
  citationcount = {2},
  venue = {No venue available}
}

@article{enricopontelliComplexityLateBindingDynamic1998,
  title = {The {{Complexity}} of {{Late-Binding}} in {{Dynamic Object-Oriented Languages}}},
  author = {Enrico Pontelli and D. Ranjan and G. Gupta},
  year = {1998},
  journal = {PLILP/ALP},
  doi = {10.1007/BFb0056616},
  keywords = {dynamic},
  annotation = {Citation Count: 3}
}

@article{enricopontelliComplexityParallelImplementation1997,
  title = {On the {{Complexity}} of {{Parallel Implementation}} of {{Logic Programs}}},
  author = {Enrico Pontelli and D. Ranjan and G. Gupta},
  year = {1997},
  journal = {Foundations of Software Technology and Theoretical Computer Science},
  doi = {10.1007/BFb0058027},
  annotation = {Citation Count: 21}
}

@article{enricopontelliOptimalDataStructure2002,
  title = {An Optimal Data Structure to Handle Dynamic Environments in Non-Deterministic Computations},
  author = {Enrico Pontelli and D. Ranjan and A. D. Pal{\`u}},
  year = {2002},
  journal = {Computer languages, systems \& structures},
  doi = {10.1016/S0096-0551(02)00004-8},
  keywords = {data structure,dynamic},
  annotation = {Citation Count: 9}
}

@article{enricopontelliSimpleOptimalSolution2004,
  title = {A {{Simple Optimal Solution}} for the {{Temporal Precedence Problem}}  on {{Pure Pointer Machines}}},
  author = {Enrico Pontelli and D. Ranjan},
  year = {2004},
  journal = {Theory of Computing Systems},
  doi = {10.1007/s00224-004-1118-x},
  annotation = {Citation Count: 5}
}

@article{eppsteinDynamicConnectivityDigital1997,
  title = {Dynamic Connectivity in Digital Images},
  author = {Eppstein, D.},
  year = {1997},
  doi = {10.1016/S0020-0190(97)00056-2},
  abstract = {No abstract available},
  citationcount = {21},
  venue = {Information Processing Letters},
  keywords = {dynamic}
}

@article{eppsteinMaintenanceOfA1990,
  title = {Maintenance of a Minimum Spanning Forest in a Dynamic Planar Graph},
  author = {Eppstein, D. and Italiano, G. and Tamassia, R. and Tarjan, R. and Westbrook, J. and Yung, M.},
  year = {1990},
  doi = {10.1016/0196-6774(92)90004-V},
  abstract = {No abstract available},
  citationcount = {177},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{eppsteinSeparatorBasedSparsification1993,
  title = {Separator Based Sparsification for Dynamic Planar Graph Algorithms},
  author = {Eppstein, D. and Galil, Z. and Italiano, G. and Spencer, T.},
  year = {1993},
  doi = {10.1145/167088.167159},
  abstract = {We describe algorithms and data structures for maintaining a dynamic planar graph subject to edge insertions and edge deletions that preserve planarity but that can change the embedding. We give a fully dynamic planarity testing algorithm that maintains a graph subject to edge insertions and deletions, and allows queries that test whether the graph is currently planar, or whether a potential new edge would violate planarity, in amortized time O(nl 12) per update or query. We maintain the 2and 3-vertex-connected components, and the 3and 4-edge-connected components of a planar graph in O(n.llz ) time per insertion, deletion or query. We give fully dynamic algorithms for maintaining the connected components, the 2-edge-connected components, and the minimum spanning forest of a planar graph in time (9(log n) per insertion and 0(log2 n) per deletion, assuming that insertions keep the graph planar. All our algorithms improve previous bounds: the improvements are based upon a new type of sparsification combined wit h several properties of separators in planar graphs.},
  citationcount = {50},
  venue = {Symposium on the Theory of Computing}
}

@article{eppsteinSparsificationATechnique1992,
  title = {Sparsification-a Technique for Speeding up Dynamic Graph Algorithms},
  author = {Eppstein, D. and Galil, Z. and Italiano, G. and Nissenzweig, Amnon},
  year = {1992},
  doi = {10.1109/SFCS.1992.267818},
  abstract = {The authors provide data structures that maintain a graph as edges are inserted and deleted, and keep track of the following properties: minimum spanning forests, best swap, graph connectivity, and graph 2-edge-connectivity, in time O(n/sup 1/2/log(m/n)) per change; 3-edge-connectivity, in time O(n/sup 2/3/) per change; 4-edge-connectivity, in time O(n alpha (n)) per change; k-edge-connectivity, in time O(n log n) per change; bipartiteness, 2-vertex-connectivity, and 3-vertex-connectivity, in time O(n log(m/n)) per change; and 4-vertex-connectivity, in time O(n log(m/n)+n alpha (n)) per change. Further results speed up the insertion times to match the bounds of known partially dynamic algorithms. The algorithms are based on a technique that transforms algorithms for sparse graphs into ones that work on any graph, which they call sparsification.<<ETX>>},
  citationcount = {160},
  venue = {Proceedings., 33rd Annual Symposium on Foundations of Computer Science}
}

@article{eppsteinSparsificationATechnique1997,
  title = {Sparsification---a Technique for Speeding up Dynamic Graph Algorithms},
  author = {Eppstein, D. and Galil, Z. and Italiano, G. and Nissenzweig, Amnon},
  year = {1997},
  doi = {10.1145/265910.265914},
  abstract = {We provide data strutures that maintain a graph as edges are inserted and deleted, and keep track of the following properties with the following times: minimum spanning forests, graph connectivity, graph 2-edge connectivity, and bipartiteness in time{\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}1/2{\textexclamdown}/supscrpt{\textquestiondown}) per change; 3-edge connectivity, in time {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}2/3{\textexclamdown}/supscrpt{\textquestiondown}) per change; 4-edge connectivity, in time {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{$\alpha$}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown})) per change; {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}-edge connectivity for constant {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}, in time {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}log{\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) per change;2-vertex connectivity, and 3-vertex connectivity, in the {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) per change; and 4-vertex connectivity, in time {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{$\alpha$}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown})) per change. Further results speed up the insertion times to match the bounds of known partially dynamic algorithms. All our algorithms are based on a new technique that transforms an algorithm for sparse graphs into one that will work on any graph, which we call {\textexclamdown}italic{\textquestiondown}sparsification.{\textexclamdown}/italic{\textquestiondown}},
  citationcount = {318},
  venue = {JACM}
}

@article{erdsOnACombinatorial1973,
  title = {On a Combinatorial Game},
  author = {Erd{\"o}s, P. and Selfridge, J.},
  year = {1973},
  doi = {10.1016/0097-3165(73)90005-8},
  abstract = {No abstract available},
  citationcount = {322},
  venue = {Journal of Combinatorial Theory}
}

@article{erdsOnTheAddition1964,
  title = {On the Addition of Residue Classes Mod p},
  author = {Erd{\"o}s, P. and Heilbronn, H.},
  year = {1964},
  doi = {10.4064/AA-9-2-149-159}
}

@article{erdsSomeRemarksOn1947,
  title = {Some Remarks on the Theory of Graphs},
  author = {Erd{\"o}s, P.},
  year = {1947},
  doi = {10.1090/S0002-9904-1947-08785-1},
  abstract = {The present note consists of some remarks on graphs. A graph G is a set of points some of which are connected by edges. We assume here that no two points are connected by more than one edge. The complementary graph G' of G has the same vertices as G and two points are connected in G' if and only if they are not connected in G. A special case of a theorem of Ramsey can be stated in graph theoretic language as follows: There exists a function f(k, I) of positive integers k, I with the following property. Let there be given a graph G of n*zf(kf I) vertices. Then either G contains a complete graph of order fe, or G' a complete graph of order L (A complete graph is a graph any two vertices of which are connected. The order of a complete graph is the number of its vertices.) I t would be desirable to have a formula for {\textflorin}({\pounds}, I). This a t present we can not do. We have however the following estimates :},
  citationcount = {645},
  venue = {No venue available}
}

@article{ergnComparingSequencesWith2003,
  title = {Comparing Sequences with Segment Rearrangements},
  author = {Erg{\"u}n, Funda and Muthukrishnan, S. and Sahinalp, S. C.},
  year = {2003},
  doi = {10.1007/978-3-540-24597-1_16},
  abstract = {No abstract available},
  citationcount = {34},
  venue = {Foundations of Software Technology and Theoretical Computer Science}
}

@article{ericksonNewLowerBounds1995,
  title = {New Lower Bounds for {{Hopcroft}}'s Problem},
  author = {Erickson, Jeff},
  year = {1995},
  doi = {10.1145/220279.220293},
  abstract = {AbstractWe establish new lower bounds on the complexity of the following basic geometric problem, attributed to John Hopcroft: Given a set ofn points andm hyperplanes in  , is any point contained in any hyperplane? We define a general class ofpartitioning algorithms, and show that in the worst case, for allm andn, any such algorithm requires time {\textohm}(n logm + n2/3m2/3 + m logn) in two dimensions, or {\textohm}(n logm + n5/6m1/2 + n1/2m5/6 + m logn) in three or more dimensions. We obtain slightly higher bounds for the counting version of Hopcroft's problem in four or more dimensions. Our planar lower bound is within a factor of 2O(log*(n+m)) of the best known upper bound, due to Matou{\v s}ek. Previously, the best known lower bound, in any dimension, was {\textohm}(n logm + m logn). We develop our lower bounds in two stages. First we define a combinatorial representation of the relative order type of a set of points and hyperplanes, called amonochromatic cover, and derive lower bounds on its size in the worst case. We then show that the running time of any partitioning algorithm is bounded below by the size of some monochromatic cover. As a related result, using a straightforward adversary argument, we derive aquadratic lower bound on the complexity of Hopcroft's problem in a surprisingly powerful decision tree model of computation.},
  citationcount = {79},
  venue = {SCG '95}
}

@article{evenAnOnLine1981,
  title = {An On-Line Edge-Deletion Problem},
  author = {Even, S. and Shiloach, Y.},
  year = {1981},
  doi = {10.1145/322234.322235},
  abstract = {There is given an undirected graph G -- (V, E) from which edges are deleted one at a time and about which questions of the type, "Are the vertices u and v in the same connected component?" have to be answered "on-line." There is presented an algorithm which maintains a data structure in which each question is answered in constant time and for which the total time involved in answering q questions and maintaining the data structure is O(q + I VI" lED.},
  citationcount = {286},
  venue = {JACM}
}

@article{faginCombiningFuzzyInformation1999,
  title = {Combining Fuzzy Information from Multiple Systems},
  author = {Fagin, Ronald},
  year = {1999},
  doi = {10.1006/jcss.1998.1600},
  abstract = {In a traditional database system, the result of a query is a set of values (those values that satisfy the query). In other data servers, such as a system with queries based on image content, or many text retrieval systems, the result of a query is a sorted list. For example, in the case of a system with queries based on image content, the query might ask for objects that are a particular shade of red, and the result of the query would be a sorted list of objects in the database, sorted by how well the color of the object matches that given in the query. A multimedia system must somehow synthesize both types of queries (those whose result is a set and those whose result is a sorted list) in a consistent manner. In this paper we discuss the solution adopted by Garlic, a multimedia information system being developed at the IBM Almaden Research Center. This solution is based on ``graded'' (or ``fuzzy'') sets. Issues of efficient query evaluation in a multimedia system are very different from those in a traditional database system. This is because the multimedia system receives answers to subqueries from various subsystems, which can be accessed only in limited ways. For the important class of queries that are conjunctions of atomic queries (where each atomic query might be evaluated by a different subsystem), the naive algorithm must retrieve a number of elements that is linear in the database size. In contrast, in this paper an algorithm is given, which has been implemented in Garlic, such that if the conjuncts are independent, then with arbitrarily high probability, the total number of elements retrieved in evaluating the query is sublinear in the database size (in the case of two conjuncts, it is of the order of the square root of the database size). It is also shown that for such queries, the algorithm is optimal. The matching upper and lower bounds are robust, in the sense that they hold under almost any reasonable rule (including the standard min rule of fuzzy logic) for evaluating the conjunction. Finally, we find a query that is provably hard, in the sense that the naive linear algorithm is essentially optimal.},
  citationcount = {803},
  venue = {Journal of computer and system sciences (Print)},
  keywords = {lower bound,query}
}

@article{faginFuzzyQueriesIn1998,
  title = {Fuzzy Queries in Multimedia Database Systems},
  author = {Fagin, Ronald},
  year = {1998},
  doi = {10.1145/275487.275488},
  abstract = {There are essential differences between multimedia databases (which may contain complicated objects, such as images), and traditional databases. These differences lead to interesting new issues, and in particular cause us to consider new typos of queries. Wr example, in a multimedia database it is reasonable and natural to ask for images that are somehow ``similar to'' some fixed image. Furthermore, there are different ways of obtaining and accessing information in a multimedia database than information in a traditional database. For example, in a multimedia database, it might be reasonable to have a query that asks for, say, the top 10 images that are similar to a fixed image. This is in contrast to a rolationnl database, where the answer to a query is simply a set, In this paper, we survey some new issues that arise for multimedia queries, with a particular focus on recent research by the author, developed in the context of the Garlic system at the IBM Almaden Research Center.},
  citationcount = {314},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {query}
}

@article{faloutsosBeyondUniformityAnd1994,
  title = {Beyond Uniformity and Independence: Analysis of {{R-trees}} Using the Concept of Fractal Dimension},
  author = {Faloutsos, C. and Kamel, I.},
  year = {1994},
  doi = {10.1145/182591.182593},
  abstract = {We propose the concept of fractal dimension of a set of points, in order to quantify the deviation from the uniformity distribution. Using measurements on real data sets (road intersections of U.S. counties, star coordinates from NASA's Infrared-Ultraviolet Explorer etc.) we provide evidence that real data indeed are skewed, and, moreover, we show that they behave as mathematical fractals, with a measurable, non-integer fractal dimension. Armed with this tool, we then show its practical use in predicting the performance of spatial access methods, and specifically of the R-trees. We provide the first analysis of R-trees for skewed distributions of points: We develop a formula that estimates the number of disk accesses for range queries, given only the fractal dimension of the point set, and its count. Experiments on real data sets show that the formula is very accurate: the relative error is usually below 5},
  citationcount = {294},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems}
}

@article{farach-coltonApproximateNearestNeighbor1999,
  title = {Approximate Nearest Neighbor Algorithms for {{Hausdorff}} Metrics via Embeddings},
  author = {{Farach-Colton}, Mart{\'i}n and Indyk, P.},
  year = {1999},
  doi = {10.1109/SFFCS.1999.814589},
  abstract = {Hausdorff metrics are used in geometric settings for measuring the distance between sets of points. They have been used extensively in areas such as computer vision, pattern recognition and computational chemistry. While computing the distance between a single pair of sets under the Hausdorff metric has been well studied, no results are known for the nearest-neighbor problem under Hausdorff metrics. Indeed, no results were known for the nearest-neighbor problem for any metric without a norm structure, of which the Hausdorff is one. We present the first nearest-neighbor algorithm for the Hausdorff metric. We achieve our result by embedding Hausdorff metrics into l/sub /spl infin// and by using known nearest-neighbor algorithms for this target metric. We give upper and lower bounds on the number of dimensions needed for such an l/sub /spl infin// embedding. Our bounds require the introduction of new techniques based on superimposed codes and non-uniform sampling.},
  citationcount = {38},
  venue = {40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039)},
  keywords = {lower bound}
}

@article{fariaEfficientAndCompact2016,
  title = {Efficient and Compact Representations of Some Non-Canonical Prefix-Free Codes},
  author = {Fari{\~n}a, A. and Gagie, T. and Manzini, G. and Navarro, G. and Pereira, Alberto Ord{\'o}{\~n}ez},
  year = {2016},
  doi = {10.1007/978-3-319-46049-9_5},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {SPIRE}
}

@article{farzanAlgorithmsInThe2014,
  title = {Algorithms in the Ultra-Wide Word Model},
  author = {Farzan, Arash and {L{\'o}pez-Ortiz}, A. and Nicholson, Patrick K. and Salinger, Alejandro},
  year = {2014},
  doi = {10.1007/978-3-319-17142-5_29},
  abstract = {No abstract available},
  citationcount = {5},
  venue = {Theory and Applications of Models of Computation}
}

@article{farzanAUniformParadigm2012,
  title = {A Uniform Paradigm to Succinctly Encode Various Families of Trees},
  author = {Farzan, Arash and Munro, J.},
  year = {2012},
  doi = {10.1007/s00453-012-9664-0},
  abstract = {No abstract available},
  citationcount = {50},
  venue = {Algorithmica}
}

@article{farzanDynamicSuccinctOrdered2009,
  title = {Dynamic Succinct Ordered Trees},
  author = {Farzan, Arash and Munro, J.},
  year = {2009},
  doi = {10.1007/978-3-642-02927-1_37},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {dynamic}
}

@article{farzanEntropyBoundedRepresentation2010,
  title = {Entropy-Bounded Representation of Point Grids},
  author = {Farzan, Arash and Gagie, T. and Navarro, G.},
  year = {2010},
  doi = {10.1016/j.comgeo.2013.08.002},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {Computational geometry}
}

@article{farzanSuccinctEncodingOf2013,
  title = {Succinct Encoding of Arbitrary Graphs},
  author = {Farzan, Arash and Munro, J.},
  year = {2013},
  doi = {10.1016/j.tcs.2013.09.031},
  abstract = {No abstract available},
  citationcount = {39},
  venue = {Theoretical Computer Science}
}

@article{farzanSuccinctRepresentationOf2011,
  title = {Succinct Representation of Dynamic Trees},
  author = {Farzan, Arash and Munro, J.},
  year = {2011},
  doi = {10.1016/j.tcs.2010.10.030},
  abstract = {No abstract available},
  citationcount = {19},
  venue = {Theoretical Computer Science},
  keywords = {dynamic}
}

@article{farzanSuccinctRepresentationsOf2008,
  title = {Succinct Representations of Arbitrary Graphs},
  author = {Farzan, Arash and Munro, J.},
  year = {2008},
  doi = {10.1007/978-3-540-87744-8_33},
  abstract = {No abstract available},
  citationcount = {38},
  venue = {Embedded Systems and Applications}
}

@article{federAmortizedCommunicationComplexity1995,
  title = {Amortized Communication Complexity},
  author = {Feder, T. and Kushilevitz, E. and Naor, M. and Nisan, N.},
  year = {1995},
  doi = {10.1137/S0097539792235864},
  abstract = {In this work we study the direct-sum problem with respect to communication complexity: Consider a relation f defined over  0,1 \textsuperscript{\{\vphantom\}}n\vphantom\{\}{\texttimes} 0,1 \textsuperscript{\{\vphantom\}}n\vphantom\{\}. Can the communication complexity of simultaneously computing f on l instances (x{$_1$},y{$_1$}),{\dots},(x\textsubscript{\{\vphantom\}}l\vphantom\{\},y\textsubscript{\{\vphantom\}}l\vphantom\{\}) be smaller than the communication complexity of computing f on the l instances, separately? Let the amortized communication complexity of f be the communication complexity of simultaneously computing f on l instances, divided by l. We study the properties of the amortized communication complexity. We show that the amortized communication complexity of a relation can be smaller than its communication complexity. More precisely, we present a partial function whose (deterministic) communication complexity is {$\Theta$}(n) and its amortized (deterministic) communication complexity is O(1). Similarly, for randomized protocols, we present a function whose randomized communication complexity is {$\Theta$}(n) and its amortized randomized communication complexity is O(1). We also give a general lower bound on the amortized communication complexity of any function f in terms of its communication complexity C(f): for every function f the amortized communication complexity of f is {\textohm}({\textsurd}\{C(f)\}-n).},
  citationcount = {122},
  venue = {SIAM journal on computing (Print)}
}

@article{federApproximatingNashEquilibria2007,
  title = {Approximating Nash Equilibria Using Small-Support Strategies},
  author = {Feder, T. and Nazerzadeh, Hamid and Saberi, A.},
  year = {2007},
  doi = {10.1145/1250910.1250961},
  abstract = {We study the problem of finding approximate Nash equilibria of two player games. We show that for any 0{\textexclamdown}{$\varepsilon$}{\textexclamdown}1, there is no 1{\textexclamdown}over{\textquestiondown}1 + {$\varepsilon$} - approximate equilibrium with strategies of support {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}over{\textquestiondown}{$\varepsilon$}{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}).},
  citationcount = {76},
  venue = {ACM Conference on Economics and Computation}
}

@article{feffermanMultiparameterOperatorsAnd1997,
  title = {Multiparameter Operators and Sharp Weighted Inequalities},
  author = {Fefferman, R. and Pipher, J.},
  year = {1997},
  doi = {10.1353/AJM.1997.0011},
  abstract = {This article is concerned with the operators from harmonic analysis which are naturally associated to a multiple parameter family of dilations. We are especially interested here in dealing with questions from the theory of such operators whose answers cannot be obtained by a reduction to the case of product operators. We also introduce a new tool in order to carry out this multiparameter analysis which comes from the classical theory. This is the concept of "sharp weighted inequalities," where, for operators such as the Hilbert transform or classical square function, one asks for Hilbert space inequalities which imply the correct estimates of the norm on LP of these operators as P becomes large. 0. Introduction. In this article, our main goal is to study classes of oper ators associated with multiparameter groups of dilations on Rn. In some sense the consideration of these operators is a natural "next step" or simplest case after those of the classical Calderon-Zygmund theory, and the product space theory. It turns out that in order to obtain estimates for the multiparameter operators we shall be led to the following type of extremely classical question: How does one obtain L2 estimates on R1 for the Hilbert transform (or other singular integrals) which yield sharp estimates on the LP(Rl) operator norm of H as p approaches 1 (or oc)? These "sharp weighted inequalities" are of some independent interest, and their study will be seen to be essentially equivalent to the deep work of Chang, Wilson, and Wolff (3) on the exponential square integrability of functions whose area integral is bounded. They also have other applications; for example, they provide an immediate extension of the Chang-Wilson-Wolff estimates to product spaces as given by Pipher in (12). At this point, let us be more specific about the exact nature of the multipa rameter operators which we shall consider. Consider, in R3, the dilations given by ps,t(x, y, z) = (sx, ty9 stz), for s91 {\textquestiondown} 0. Then there is a maximal operator (first considered by Zygmund) Ml and singular integrals Tl (introduced by Ricci and Stein (13)) naturally associated to these dilations. The maximal operator Af3 is defined by},
  citationcount = {120},
  venue = {No venue available}
}

@article{fehrNewEfficientMethods2009,
  title = {New Efficient Methods for Calculating Watersheds},
  author = {Fehr, E. and Jr., J. S. Andrade and {da Cunha}, S. and {da Silva}, L R and Herrmann, H J and Kadau, D. and Moukarzel, C F and Oliveira, E A},
  year = {2009},
  doi = {10.1088/1742-5468/2009/09/P09007},
  abstract = {We present an advanced algorithm for the determination of watershed lines on digital elevation models (DEMs) which is based on the iterative application of invasion percolation (IP). The main advantage of our method over previously proposed ones is that it has a sub-linear time-complexity. This enables us to process systems comprising up to 108 sites in a few CPU seconds. Using our algorithm we are able to demonstrate, convincingly and with high accuracy, the fractal character of watershed lines. We find the fractal dimension of watersheds to be Df = 1.211 {\textpm} 0.001 for artificial landscapes, Df = 1.10 {\textpm} 0.01 for the Alps and Df = 1.11 {\textpm} 0.01 for the Himalayas.},
  citationcount = {23},
  venue = {No venue available}
}

@article{fehrStatisticalPropertiesOf2011,
  title = {Statistical Properties of Watersheds},
  author = {Fehr, E.},
  year = {2011},
  doi = {10.3929/ETHZ-A-007159253},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {No venue available}
}

@article{feigeRefutingSmoothed3cnf2007,
  title = {Refuting Smoothed {{3CNF}} Formulas},
  author = {Feige, U.},
  year = {2007},
  doi = {10.1109/FOCS.2007.59},
  abstract = {We introduce the following model for generating .semi-random 3CNF formulas. First, an adversary is allowed to pick an arbitrary formula with n varialdes and in clauses. Then, the formula is slightly perturbed at random. Namely, the smoothing operation leaves the variables of the formula unchanged, but flips the polarity of every variable occurrence in the formula independently with probability a. If the density m/n of a 3CNF formula exceeds a certain threshold value (say, 5epsiv-3) then the smoothing operation almost surely results in a non-satisfiable formula. We present a randomized polynomial time refutation algorithm that for every sufficiently dense 3CNF formula manages to refute most of its smoothed instantiations. The density requirement for our refutation algorithm is roughly epsiv-2 radic(n log log n), which almost matches the density Omega( radicn) required bv known algorithms for refuting 3CNF formulas that are completely random.},
  citationcount = {40},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{feldmannTradeoffsForPacket2000,
  title = {Tradeoffs for Packet Classification},
  author = {Feldmann, Anja and Muthukrishnan, S. and Park, Florham},
  year = {2000},
  doi = {10.1109/INFCOM.2000.832493},
  abstract = {We present an algorithmic framework for solving the packet classification problem that allows various access time versus memory tradeoffs. It reduces the multidimensional packet classification problem to solving a few instances of the one-dimensional IP lookup problem. It gives the best known lookup performance with moderately large memory space. Furthermore, it efficiently supports a reasonable number of additions and deletions to the rulesets without degrading the lookup performance. We perform a thorough experimental study of the tradeoffs for the two-dimensional packet classification problem on rulesets derived from datasets collected from AT\&T WorldNet, an Internet service provider.},
  citationcount = {289},
  venue = {Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064)}
}

@article{fengkuangtianzhuImprovedCollectiveInfluence2018,
  title = {Improved Collective Influence of Finding Most Influential Nodes Based on Disjoint-Set Reinsertion},
  author = {Fengkuangtian Zhu},
  year = {2018},
  journal = {Scientific Reports},
  doi = {10.1038/s41598-018-32874-5},
  annotation = {Citation Count: 9}
}

@article{fengOptimalBridgeTwin2024,
  title = {Optimal Bridge, Twin Bridges and beyond: {{Inserting}} Edges into a Road Network to Minimize the Constrained Diameters},
  author = {Feng, Zhidan and Fernau, Henning and Zhu, Binhai},
  year = {2024},
  doi = {10.48550/arXiv.2404.19164},
  abstract = {Given a road network modelled as a planar straight-line graph G=(V,E) with {\textbar}V{\textbar}=n, let (u,v){$\in$}V{\texttimes}V, the shortest path (distance) between u,v is denoted as {$\delta$}\textsubscript{G}(u,v). Let {$\delta$}(G)=\textsubscript{\{\vphantom\}}(u,v)\vphantom\{\}{$\delta$}\textsubscript{G}(u,v), for (u,v){$\in$}V{\texttimes}V, which is called the diameter of G. Given a disconnected road network modelled as two disjoint trees T{$_1$} and T{$_2$}, this paper first aims at inserting one and two edges (bridges) between them to minimize the (constrained) diameter {$\delta$}(T{$_{1}\cup$}T{$_{2}\cup$}I\textsubscript{j}) going through the inserted edges, where I\textsubscript{j},j=1,2, is the set of inserted edges with {\textbar}I{$_{1}\vert$}=1 and {\textbar}I{$_{2}\vert$}=2. The corresponding problems are called the \{\vphantom\}\emph{optimal bridge}\vphantom\{\}\emph{ and }\{\vphantom\}\emph{twin bridges}\vphantom\{\}\emph{ problems. Since when more than one edge are inserted between two trees the resulting graph is becoming more complex, for the general network G we consider the problem of inserting a minimum of k edges such that the shortest distances between a set of m pairs P= (u\_i,v\_i){$\mid$}u\_i,v\_i{$\in$}V,i{$\in$}[m] , {$\delta$}\textsubscript{G}(u\textsubscript{i},v\textsubscript{i})'s, are all decreased. The main results of this paper are summarized as follows: (1) We show that the optimal bridge problem can be solved in O(n{$^2$}) time and that a variation of it has a near-quadratic lower bound unless SETH fails. The proof also implies that the famous 3-SUM problem does have a near-quadratic lower bound for large integers, e.g., each of the n input integers has {\textohm}(n) decimal digits. We then give a simple factor-2 O(nn) time approximation algorithm for the optimal bridge problem. (2) We present an O(n{$^4$}) time algorithm to solve the twin bridges problem, exploiting some new property not in the optimal bridge problem. (3) For the general problem of inserting k edges to reduce the (graph) distances between m given pairs, we show that the problem is NP-complete.}},
  citationcount = {1},
  venue = {Algorithmic Applications in Management},
  keywords = {lower bound}
}

@article{fenwickANewData1994,
  title = {A New Data Structure for Cumulative Frequency Tables},
  author = {Fenwick, P.},
  year = {1994},
  doi = {10.1002/spe.4380240306},
  abstract = {A new method (the `binary indexed tree') is presented for maintaining the cumulative frequencies which are needed to support dynamic arithmetic data compression. It is based on a decomposition of the cumulative frequencies into portions which parallel the binary representation of the index of the table element (or symbol). The operations to traverse the data structure are based on the binary coding of the index. In comparison with previous methods, the binary indexed tree is faster, using more compact data and simpler code. The access time for all operations is either constant or proportional to the logarithm of the table size. In conjunction with the compact data structure, this makes the new method particularly suitable for large symbol alphabets.},
  citationcount = {221},
  venue = {Software, Practice \& Experience}
}

@article{feoFastAlgorithmsFor2010,
  title = {Fast Algorithms for Computing Isogenies between Ordinary Elliptic Curves in Small Characteristic},
  author = {Feo, L. D.},
  year = {2010},
  doi = {10.1016/j.jnt.2010.07.003},
  abstract = {No abstract available},
  citationcount = {20},
  venue = {No venue available}
}

@article{feoFastArithmeticsIn2009,
  title = {Fast Arithmetics in Artin-Schreier Towers over Finite Fields},
  author = {Feo, L. D. and Schost, {\'E}.},
  year = {2009},
  doi = {10.1145/1576702.1576722},
  abstract = {An Artin-Schreier tower over the finite field {\textexclamdown}b{\textquestiondown}F{\textexclamdown}/b{\textquestiondown}{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}p{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown} is a tower of field extensions generated by polynomials of the form {\textexclamdown}i{\textquestiondown}X{\textexclamdown}sup{\textquestiondown}p{\textexclamdown}/sup{\textquestiondown}{\textexclamdown}/i{\textquestiondown}-{\textexclamdown}i{\textquestiondown}X{\textexclamdown}/i{\textquestiondown}-{$\alpha$}. Following Cantor and Couveignes, we give algorithms with quasi-linear time complexity for arithmetic operations in such towers. As an application, we present an implementation of Couveignes' algorithm for computing isogenies between elliptic curves using the {\textexclamdown}i{\textquestiondown}p{\textexclamdown}/i{\textquestiondown}-torsion.},
  citationcount = {20},
  venue = {International Symposium on Symbolic and Algebraic Computation}
}

@article{fernandesPavementPathologiesClassification2014,
  title = {Pavement Pathologies Classification Using Graph-Based Features},
  author = {Fernandes, Kelwin and Ciobanu, Lucian},
  year = {2014},
  doi = {10.1109/ICIP.2014.7025159},
  abstract = {Pavement cracks involve important information to measure road quality. Crack classification is a challenging problem given the diversity of possible cracks, therefore, it is needed to retrieve good features in order to facilitate the learning of predictive models with as few samples as possible. In this paper, we propose a graph-based set of features to efficiently describe cracks. These features proved to have high degree of expressiveness and robustness when used for crack classification. We show that the proposed features succeed in the assessment of 525 images with different kinds of cracks. We proved the robustness of the approach applying different levels of noise to the images and evaluating the classification accuracy.},
  citationcount = {36},
  venue = {International Conference on Information Photonics}
}

@article{ferraginaThePgmIndex2019,
  title = {The {{PGM-index}}},
  author = {Ferragina, P. and Vinciguerra, Giorgio},
  year = {2019},
  doi = {10.14778/3389133.3389135},
  abstract = {We present the first learned index that supports predecessor, range queries and updates within provably efficient time and space bounds in the worst case. In the (static) context of just predecessor and range queries these bounds turn out to be optimal. We call this learned index the Piecewise Geometric Model index (PGM-index). Its flexible design allows us to introduce three variants which are novel in the context of learned data structures. The first variant of the PGM-index is able to adapt itself to the distribution of the query operations, thus resulting in the first known distribution-aware learned index to date. The second variant exploits the repetitiveness possibly present at the level of the learned models that compose the PGM-index to further compress its succinct space footprint. The third one is a multicriteria variant of the PGM-index that efficiently auto-tunes itself in a few seconds over hundreds of millions of keys to satisfy space-time constraints which evolve over time across users, devices and applications. These theoretical achievements are supported by a large set of experimental results on known datasets which show that the fully-dynamic PGM-index improves the space occupancy of existing traditional and learned indexes by up to three orders of magnitude, while still achieving their same or even better query and update time efficiency. As an example, in the static setting of predecessor and range queries, the PGM-index matches the query performance of a cache-optimised static B+-tree within two orders of magnitude (83{\texttimes}) less space; whereas in the fully-dynamic setting, where insertions and deletions are allowed, the PGM-index improves the query and update time performance of a B+-tree by up to 71},
  citationcount = {111},
  venue = {Proceedings of the VLDB Endowment},
  keywords = {data structure,dynamic,query,static,update,update time}
}

@article{fiatHowToFind1989,
  title = {How to Find a Battleship},
  author = {Fiat, A. and Shamir, A.},
  year = {1989},
  doi = {10.1002/net.3230190306},
  abstract = {Consider a ``sea'' of M squares which contains (at some unknown location) a ``battleship'' of K squares. Both the sea and the battleship can assume any rectangular shape. Our goal is to find the battleship by probing at least one of its squares. In this paper we describe a deterministic strategy for this problem which is guaranteed to locate the battleship in at most c1M/K probes, where c1 {$\approx$} 3.065.},
  citationcount = {15},
  venue = {Networks}
}

@article{fiatNonObliviousHashing1988,
  title = {Non-Oblivious Hashing},
  author = {Fiat, A. and Naor, M. and Schmidt, Jeanette P. and Siegel, A.},
  year = {1988},
  doi = {10.1145/62212.62248},
  abstract = {Non-oblivious hashing, where the information gathered by performing ``unsuccessful'' probes determines the probe strategy, is introduced and used to obtain the following results for static lookup on full tables:{\textexclamdown}list{\textquestiondown}{\textexclamdown}item{\textquestiondown}An {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}(1) worst case scheme that requires only logarithmic additional memory (improving on the [FKS84] linear space upper bound). {\textexclamdown}/item{\textquestiondown}{\textexclamdown}item{\textquestiondown}An almost sure {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}(1) probabilistic worst case scheme, without any additional memory (improving on previous logarithmic time upper bounds). {\textexclamdown}/item{\textquestiondown}{\textexclamdown}item{\textquestiondown}Enhancements to hashing: Solving (a) and (b) in the multikey record environment, search can be performed under any key in time {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}(1); finding the nearest neighbor, the rank, etc. in logarithmic time. {\textexclamdown}/item{\textquestiondown}{\textexclamdown}/list{\textquestiondown} Our non-oblivious upper bounds are much better than the appropriate oblivious lower bounds.},
  citationcount = {21},
  venue = {Symposium on the Theory of Computing},
  keywords = {lower bound,static}
}

@article{fiatNonobliviousHashing1992,
  title = {Nonoblivious Hashing},
  author = {Fiat, A. and Naor, M. and Schmidt, Jeanette P. and Siegel, A.},
  year = {1992},
  doi = {10.1145/146585.146591},
  abstract = {Nonoblivious hashing, where information gathered from unsuccessful probes is used to modify subsequent probe strategy, is introduced and used to obtain the following results for static lookup on full tables:{\textexclamdown}list{\textquestiondown}{\textexclamdown}item{\textquestiondown}(1) An {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(1)-time worst-case scheme that uses only logarithmic additional memory, (and no memory when the domain size is linear in the table size), which improves upon previously linear space requirements. {\textexclamdown}/item{\textquestiondown}{\textexclamdown}item{\textquestiondown}(2) An almost sure {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(1)-time probabilistic worst-case scheme, which uses no additional memory and which improves upon previously logarithmic time requirements. {\textexclamdown}/item{\textquestiondown}{\textexclamdown}item{\textquestiondown}(3) Enhancements to hashing: (1) and (2) are solved for multikey recors, where search can be performed under any key in time {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(1); these schemes also permit properties, such as nearest neighbor and rank, to be determined in logarithmic time. {\textexclamdown}/item{\textquestiondown}{\textexclamdown}/list{\textquestiondown}},
  citationcount = {24},
  venue = {JACM},
  keywords = {static}
}

@article{ficiStringProcessingAnd2017,
  title = {String Processing and Information Retrieval},
  author = {Fici, G. and Sciortino, M. and Venturini, R.},
  year = {2017},
  doi = {10.1007/978-3-319-67428-5},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Lecture Notes in Computer Science}
}

@article{fillitreSimplerProofsWith2021,
  title = {Simpler Proofs with Decentralized Invariants},
  author = {Filli{\^a}tre, J.},
  year = {2021},
  doi = {10.1016/J.JLAMP.2021.100645},
  abstract = {No abstract available},
  citationcount = {6},
  venue = {J. Log. Algebraic Methods Program.}
}

@article{filtserNearOptimalLeft2024,
  title = {Near-Optimal {{(1+$\epsilon$)-\{\vphantom\}Approximate\vphantom\{\}}} Fully-Dynamic All-Pairs Shortest Paths in Planar Graphs},
  author = {Filtser, Arnold and Goranci, Gramoz and Patel, Neel and Gutenberg, Maximilian Probst},
  year = {2024},
  doi = {10.1109/FOCS61266.2024.00124},
  abstract = {We study the fully-dynamic all-pair shortest paths (APSP) problem on planar graphs: given an n-\{vertex\} planar graph G=(V,E) undergoing edge insertions and deletions, the goal is to efficiently process these updates and support distance and shortest path queries. We give a (1+{$\epsilon$})-\{approximate\} dynamic algorithm that supports edge updates and distance queries in n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} time, for any 1/\{poly\}(n){$<\epsilon<$}1. Our result is a significant improvement over the best previously known bound of O\vphantom\{\}({\textsurd}\{n\}) on update and query time due to [Abraham, Chechik, and Gavoille, STOC '12], and bypasses a {\textohm}({\textsurd}\{n\}) conditional lower-bound on update and query time for exact fully dynamic planar APSP [Abboud and Dahlgaard, FOCS '16]. The main technical contribution behind our result is to dynamize the planar emulator construction due to [Chang, Krauthgamer, Tan, STOC '22].},
  citationcount = {Unknown},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,lower bound,query,query time,update}
}

@article{fischerAlphabetDependentString2013,
  title = {Alphabet-Dependent String Searching with Wexponential Search Trees},
  author = {Fischer, J. and Gawrychowski, Pawe{\l}},
  year = {2013},
  doi = {10.1007/978-3-319-19929-0_14},
  abstract = {No abstract available},
  citationcount = {52},
  venue = {Annual Symposium on Combinatorial Pattern Matching}
}

@article{fischerANewSuccinct2007,
  title = {A New Succinct Representation of {{RMQ-information}} and Improvements in the Enhanced Suffix Array},
  author = {Fischer, J. and Heun, Volker},
  year = {2007},
  doi = {10.1007/978-3-540-74450-4_41},
  abstract = {No abstract available},
  citationcount = {150},
  venue = {Combinatorics, Algorithms, Probabilistic and Experimental Methodologies}
}

@article{fischerDeterministic3sumHardness2023,
  title = {Deterministic {{3SUM-hardness}}},
  author = {Fischer, Nick and Kaliciak, Piotr and Polak, Adam},
  year = {2023},
  doi = {10.48550/arXiv.2310.12913},
  abstract = {As one of the three main pillars of fine-grained complexity theory, the 3SUM problem explains the hardness of many diverse polynomial-time problems via fine-grained reductions. Many of these reductions are either directly based on or heavily inspired by P{\u \{}a\vphantom\{\}tra{\c \{}s\vphantom\{\}cu's framework involving additive hashing and are thus randomized. Some selected reductions were derandomized in previous work [Chan, He; SOSA'20], but the current techniques are limited and a major fraction of the reductions remains randomized. In this work we gather a toolkit aimed to derandomize reductions based on additive hashing. Using this toolkit, we manage to derandomize almost all known 3SUM-hardness reductions. As technical highlights we derandomize the hardness reductions to (offline) Set Disjointness, (offline) Set Intersection and Triangle Listing -- these questions were explicitly left open in previous work [Kopelowitz, Pettie, Porat; SODA'16]. The few exceptions to our work fall into a special category of recent reductions based on structure-versus-randomness dichotomies. We expect that our toolkit can be readily applied to derandomize future reductions as well. As a conceptual innovation, our work thereby promotes the theory of deterministic 3SUM-hardness. As our second contribution, we prove that there is a deterministic universe reduction for 3SUM. Specifically, using additive hashing it is a standard trick to assume that the numbers in 3SUM have size at most n{$^3$}. We prove that this assumption is similarly valid for deterministic algorithms.},
  citationcount = {3},
  venue = {Information Technology Convergence and Services},
  keywords = {reduction}
}

@article{fischerEfficiencyOfEquivalence1972,
  title = {Efficiency of Equivalence Algorithms},
  author = {Fischer, M.},
  year = {1972},
  doi = {10.1007/978-1-4684-2001-2_14},
  abstract = {No abstract available},
  citationcount = {45},
  venue = {Complexity of Computer Computations}
}

@article{fischerFasterEntropyBounded2009,
  title = {Faster Entropy-Bounded Compressed Suffix Trees},
  author = {Fischer, J. and M{\"a}kinen, V. and Navarro, G.},
  year = {2009},
  doi = {10.1016/J.TCS.2009.09.012},
  abstract = {No abstract available},
  citationcount = {94},
  venue = {Theoretical Computer Science}
}

@article{fischerOptimalStringMining2006,
  title = {Optimal String Mining under Frequency Constraints},
  author = {Fischer, J. and Heun, Volker and Kramer, Stefan},
  year = {2006},
  doi = {10.1007/11871637_17},
  abstract = {No abstract available},
  citationcount = {46},
  venue = {European Conference on Principles of Data Mining and Knowledge Discovery}
}

@article{fischerOptimalSuccinctnessRange2008,
  title = {Optimal Succinctness for Range Minimum Queries},
  author = {Fischer, J.},
  year = {2008},
  doi = {10.1007/978-3-642-12200-2_16},
  abstract = {No abstract available},
  citationcount = {89},
  venue = {Latin American Symposium on Theoretical Informatics},
  keywords = {query}
}

@article{fischerSpaceefficientPreprocessingSchemes2011,
  title = {Space-Efficient Preprocessing Schemes for Range Minimum Queries on Static Arrays},
  author = {Fischer, J. and Heun, Volker},
  year = {2011},
  doi = {10.1137/090779759},
  abstract = {Given a static array of n totally ordered objects, the range minimum query problem is to build a data structure that allows us to answer efficiently subsequent on-line queries of the form ``what is the position of a minimum element in the subarray ranging from i to j?''. We focus on two settings, where (1) the input array is available at query time, and (2) the input array is available only at construction time. In setting (1), we show new data structures (a) of size \textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_2$}n\vphantom\{\}\{c(n)\}-{$\Theta$}(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}n\vphantom\{\}\{c(n)n\}) bits and query time O(c(n)) for any positive integer function c(n){$\in$}O(n\textsuperscript{{$\varepsilon$}}) for an arbitrary constant 0{$<\varepsilon<$}1, or (b) with O(nH\textsubscript{k})+o(n) bits and O(1) query time, where H\textsubscript{k} denotes the empirical entropy of kth order of the input array. In setting (2), we give a data structure of size 2n+o(n) bits and query time O(1). All data structures can be constructed in linear time and almost in-place.},
  citationcount = {232},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,query,query time,static}
}

@article{fischerSuperExponentialComplexity1974,
  title = {Super-Exponential Complexity of Presburger Arithmetic},
  author = {Fischer, M. and Rabin, M.},
  year = {1974},
  doi = {10.1007/978-3-7091-9459-1_5},
  abstract = {No abstract available},
  citationcount = {411},
  venue = {No venue available}
}

@article{fischerTheEffectOf2023,
  title = {The Effect of Sparsity on K-{{Dominating}} Set and Related First-Order Graph Properties},
  author = {Fischer, Nick and K{\"u}nnemann, Marvin and Redzic, Mirza},
  year = {2023},
  doi = {10.48550/arXiv.2312.14593},
  abstract = {We revisit k-Dominating Set, one of the first problems for which a tight n\textsuperscript{k}-o(1) conditional lower bound (for k{$\geq$}3), based on SETH, was shown (P{\u \{}a\vphantom\{\}tra{\c \{}s\vphantom\{\}cu and Williams, SODA 2007). However, the underlying reduction creates dense graphs, raising the question: how much does the sparsity of the graph affect its fine-grained complexity? We first settle the fine-grained complexity of k-Dominating Set in terms of both the number of nodes n and number of edges m. Specifically, we show an mn\textsuperscript{\{\vphantom\}}k-2-o(1)\vphantom\{\} lower bound based on SETH, for any dependence of m on n. This is complemented by an mn\textsuperscript{\{\vphantom\}}k-2+o(1)\vphantom\{\}-time algorithm for all k{$\geq$}3. For the k=2 case, we give a randomized algorithm that employs a Bloom-filter inspired hashing to improve the state of the art of n\textsuperscript{\{\vphantom\}}{$\omega$}+o(1)\vphantom\{\} to m\textsuperscript{\{\vphantom\}}{$\omega$}/2+o(1)\vphantom\{\}. If {$\omega$}=2, this yields a conditionally tight bound for all k{$\geq$}2. To study if k-Dominating Set is special in its sensitivity to sparsity, we consider a class of very related problems. The k-Dominating Set problem belongs to a type of first-order definable graph properties that we call monochromatic basic problems. These problems are the natural monochromatic variants of the basic problems that were proven complete for the class FOP of first-order definable properties (Gao, Impagliazzo, Kolokolova, and Williams, TALG 2019). We show that among these problems, k-Dominating Set is the only one whose fine-grained complexity decreases in sparse graphs. Only for the special case of reflexive properties, is there an additional basic problem that can be solved faster than n\textsuperscript{\{\vphantom\}}k{\textpm}o(1)\vphantom\{\} on sparse graphs. For the natural variant of distance-r k-dominating set, we obtain a hardness of n\textsuperscript{\{\vphantom\}}k-o(1)\vphantom\{\} under SETH for every r{$\geq$}2 already on sparse graphs, which is tight for sufficiently large k.},
  citationcount = {2},
  venue = {arXiv.org},
  keywords = {lower bound,reduction}
}

@article{flajoletAnalyticCombinatorics2009,
  title = {Analytic Combinatorics},
  author = {Flajolet, P. and Sedgewick, R.},
  year = {2009},
  doi = {10.1017/CBO9780511801655},
  abstract = {Analytic Combinatorics is a self-contained treatment of the mathematics underlying the analysis of discrete structures, which has emerged over the past several decades as an essential tool in the understanding of properties of computer programs and scientific models with applications in physics, biology and chemistry. Thorough treatment of a large number of classical applications is an essential aspect of the presentation. Written by the leaders in the field of analytic combinatorics, this text is certain to become the definitive reference on the topic. The text is complemented with exercises, examples, appendices and notes to aid understanding therefore, it can be used as the basis for an advanced undergraduate or a graduate course on the subject, or for self-study.},
  citationcount = {3658},
  venue = {No venue available}
}

@article{flajoletOnBuffonMachines2009,
  title = {On {{Buffon}} Machines and Numbers},
  author = {Flajolet, P. and Pelletier, M. and Soria, Mich{\`e}le},
  year = {2009},
  doi = {10.1137/1.9781611973082.15},
  abstract = {The well-know needle experiment of Buffon can be regarded as an analog (i.e., continuous) device that stochastically "computes" the number 2/{$\pi$} = 0.63661, which is the experiment's probability of success. Generalizing the experiment and simplifying the computational framework, we consider probability distributions, which can be produced perfectly, from a discrete source of unbiased coin flips. We describe and analyse a few simple Buffon machines that generate geometric, Poisson, and logarithmic-series distributions. We provide human-accessible Buffon machines, which require a dozen coin flips or less, on average, and produce experiments whose probabilities of success are expressible in terms of numbers such as {$\pi$}, exp(-1), log2, {\textsurd}3, cos(1/4), {$\zeta$}(5). Generally, we develop a collection of constructions based on simple probabilistic mechanisms that enable one to design Buffon experiments involving compositions of exponentials and logarithms, polylogarithms, direct and inverse trigonometric functions, algebraic and hypergeometric functions, as well as functions defined by integrals, such as the Gaussian error function.},
  citationcount = {31},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{flajoletTheComplexityOf1983,
  title = {The Complexity of Generating an Exponentially Distributed Variate},
  author = {Flajolet, P. and {Saheb-Djahromi}, N.},
  year = {1983},
  doi = {10.1016/0196-6774(86)90014-3},
  abstract = {No abstract available},
  citationcount = {32},
  venue = {J. Algorithms}
}

@article{flatlandUsingModesOf2009,
  title = {Using Modes of Inquiry and Engaging Problems to Link Computer Science and Mathematics},
  author = {Flatland, Robin Y. and Matthews, James R.},
  year = {2009},
  doi = {10.1145/1508865.1509002},
  abstract = {In this paper we show how an engaging problem can be used in both a discrete mathematics course and a programming course as a way to expose students to multiple methods of inquiry and to strengthen the links between the two courses. Since students typically take Discrete Mathematics and a programming course simultaneously, this is an opportunity for them to analyze a problem from multiple perspectives during a single semester. We describe how we have accomplished this using a relatively new problem that is easily stated and has a surprising solution that defies intuition. In the programming course, students experienced a design/empirical approach to the problem by implementing simulations of various solutions and collecting experimental results. By adjusting the emphasis of the programming assignment, we show that it can fit naturally into a range of programming courses, i.e. courses on introductory programming, data structures, and object-oriented techniques. In the Discrete Mathematics course, students analyzed solutions using tools from counting, probability, and calculus. We observed that by linking the two courses using a common problem, our students were more cognizant of inquiry methods and student engagement increased.},
  citationcount = {1},
  venue = {Technical Symposium on Computer Science Education},
  keywords = {data structure}
}

@article{flicknerQueryByImage1995,
  title = {Query by Image and Video Content: {{The QBIC}} System},
  author = {Flickner, M. and Sawhney, H. and Ashley, J. and Huang, Qian and Dom, B. and Gorkani, M. and Hafner, J. and Lee, Denis and Petkovic, D. and Steele, David and Yanker, P.},
  year = {1995},
  doi = {10.1109/2.410146},
  abstract = {Research on ways to extend and improve query methods for image databases is widespread. We have developed the QBIC (Query by Image Content) system to explore content-based retrieval methods. QBIC allows queries on large image and video databases based on example images, user-constructed sketches and drawings, selected color and texture patterns, camera and object motion, and other graphical information. Two key properties of QBIC are (1) its use of image and video content-computable properties of color, texture, shape and motion of images, videos and their objects-in the queries, and (2) its graphical query language, in which queries are posed by drawing, selecting and other graphical means. This article describes the QBIC system and demonstrates its query capabilities. QBIC technology is part of several IBM products. {\textquestiondown}},
  citationcount = {4324},
  venue = {Computer},
  keywords = {query}
}

@article{floydPermutingInformationIn1972,
  title = {Permuting Information in Idealized Two-Level Storage},
  author = {Floyd, R. W.},
  year = {1972},
  doi = {10.1007/978-1-4684-2001-2_10},
  abstract = {No abstract available},
  citationcount = {107},
  venue = {Complexity of Computer Computations}
}

@article{foersterInputDynamicDistributed2021,
  title = {Input-Dynamic Distributed Algorithms for Communication Networks},
  author = {Foerster, Klaus-Tycho and Korhonen, Janne H. and Paz, A. and Rybicki, Joel and Schmid, S.},
  year = {2021},
  doi = {10.1145/3447384},
  abstract = {Consider a distributed task where the communication network is fixed but the local inputs given to the nodes of the distributed system may change over time. In this work, we explore the following question: if some of the local inputs change, can an existing solution be updated efficiently, in a dynamic and distributed manner? To address this question, we define the batch dynamic model in which we are given a bandwidth-limited communication network and a dynamic edge labelling defines the problem input. The task is to maintain a solution to a graph problem on the labeled graph under batch changes. We investigate, when a batch of {$\alpha$} edge label changes arrive,  how much time as a function of {$\alpha$} we need to update an existing solution, and  how much information the nodes have to keep in local memory between batches in order to update the solution quickly. Our work lays the foundations for the theory of input-dynamic distributed network algorithms. We give a general picture of the complexity landscape in this model, design both universal algorithms and algorithms for concrete problems, and present a general framework for lower bounds. In particular, we derive non-trivial upper bounds for two selected, contrasting problems: maintaining a minimum spanning tree and detecting cliques.},
  citationcount = {4},
  venue = {Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  keywords = {communication,dynamic,lower bound,update}
}

@article{fominFasterAlgorithmsFor2009,
  title = {Faster Algorithms for Finding and Counting Subgraphs},
  author = {Fomin, F. and Lokshtanov, D. and Raman, Venkatesh and Saurabh, Saket and Rao, B.},
  year = {2009},
  doi = {10.1016/J.JCSS.2011.10.001},
  abstract = {No abstract available},
  citationcount = {68},
  venue = {Journal of computer and system sciences (Print)}
}

@article{fordCycleTypeOf2021,
  title = {Cycle Type of Random Permutations: {{A}} Toolkit},
  author = {Ford, Kevin},
  year = {2021},
  doi = {10.19086/da.38090},
  abstract = {We prove a number of results, new and old, about the cycle type of a random permutation on S\_n. Underlying our analysis is the idea that the number of cycles of size k is roughly Poisson distributed with parameter 1/k. In particular, we establish strong results about the distribution of the number of cycles whose lengths lie in a fixed but arbitrary set I. Our techniques are motivated by the theory of sieves in number theory.},
  citationcount = {12},
  venue = {No venue available}
}

@article{forsterBootstrappingDynamicDistance2023,
  title = {Bootstrapping Dynamic Distance Oracles},
  author = {Forster, S. and Goranci, Gramoz and Nazari, Yasamin and Skarlatos, Antonis},
  year = {2023},
  doi = {10.48550/arXiv.2303.06102},
  abstract = {Designing approximate all-pairs distance oracles in the fully dynamic setting is one of the central problems in dynamic graph algorithms. Despite extensive research on this topic, the first result breaking the O({\textsurd}\{n\}) barrier on the update time for any non-trivial approximation was introduced only recently by Forster, Goranci and Henzinger [SODA'21] who achieved m\textsuperscript{\{\vphantom\}}1/{$\rho$}+o(1)\vphantom\{\} amortized update time with a O(n)\textsuperscript{\{\vphantom\}}3{$\rho$}-2\vphantom\{\} factor in the approximation ratio, for any parameter {$\rho\geq$}1. In this paper, we give the first constant-stretch fully dynamic distance oracle with a small polynomial update and query time. Prior work required either at least a poly-logarithmic approximation or much larger update time. Our result gives a more fine-grained trade-off between stretch and update time, for instance we can achieve constant stretch of O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{{$\rho^2$}\})\textsuperscript{\{\vphantom\}}4/{$\rho$}\vphantom\{\} in amortized update time O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}{$\rho$}\vphantom\{\}), and query time O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}{$\rho$}/8\vphantom\{\}) for a constant parameter {$\rho<$}1. Our algorithm is randomized and assumes an oblivious adversary. A core technical idea underlying our construction is to design a black-box reduction from decremental approximate hub-labeling schemes to fully dynamic distance oracles, which may be of independent interest. We then apply this reduction repeatedly to an existing decremental algorithm to bootstrap our fully dynamic solution.},
  citationcount = {8},
  venue = {Embedded Systems and Applications},
  keywords = {dynamic,query,query time,reduction,update,update time}
}

@article{forsterDeterministicIncrementalApsp2022,
  title = {Deterministic Incremental {{APSP}} with Polylogarithmic Update Time and Stretch},
  author = {Forster, S. and Nazari, Yasamin and Gutenberg, Maximilian Probst},
  year = {2022},
  doi = {10.1145/3564246.3585213},
  abstract = {We provide the first deterministic data structure that given a weighted undirected graph undergoing edge insertions, processes each update with polylogarithmic amortized update time and answers queries for the distance between any pair of vertices in the current graph with a polylogarithmic approximation in O(loglogn) time. Prior to this work, no data structure was known for partially dynamic graphs, i.e., graphs undergoing either edge insertions or deletions, with less than no(1) update time except for dense graphs, even when allowing randomization against oblivious adversaries or considering only single-source distances.},
  citationcount = {5},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,dynamic,query,update,update time}
}

@article{fournierLowerBoundsFor2014,
  title = {Lower Bounds for Depth 4 Formulas Computing Iterated Matrix Multiplication},
  author = {Fournier, Herv{\'e} and Limaye, N. and Malod, Guillaume and Srinivasan, S.},
  year = {2014},
  doi = {10.1145/2591796.2591824},
  abstract = {We study the arithmetic complexity of iterated matrix multiplication. We show that any multilinear homogeneous depth 4 arithmetic formula computing the product of d generic matrices of size n {\texttimes} n, IMMn,d, has size n{\textohm}({\textsurd}d) as long as d = nO(1). This improves the result of Nisan and Wigderson (Computational Complexity, 1997) for depth 4 set-multilinear formulas. We also study {$\Sigma\Pi$}[O(d/t)] {$\Sigma\Pi$}[t] formulas, which are depth 4 formulas with the stated bounds on the fan-ins of the {$\Pi$} gates. A recent depth reduction result of Tavenas (MFCS, 2013) shows that any n-variate degree d = nO(1) polynomial computable by a circuit of size poly(n) can also be computed by a depth 4 {$\Sigma\Pi$}[O(d/t)] {$\Sigma\Pi$}[t] formula of top fan-in nO(d/t). We show that any such formula computing IMMn,d has top fan-in n{\textohm}(d/t), proving the optimality of Tavenas' result. This also strengthens a result of Kayal, Saha, and Saptharishi (ECCC, 2013) which gives a similar lower bound for an explicit polynomial in VNP.},
  citationcount = {86},
  venue = {SIAM journal on computing (Print)}
}

@article{franciscoExploitingComputationFriendly2017,
  title = {Exploiting Computation-Friendly Graph Compression Methods for Adjacency-Matrix Multiplication},
  author = {Francisco, Alexandre P. and Gagie, T. and Ladra, Susana and Navarro, G.},
  year = {2017},
  doi = {10.1109/DCC.2018.00039},
  abstract = {Computing the product of the (binary) adjacency matrix of a large graph with a real-valued vector is an important operation that lies at the heart of various graph analysis tasks, such as computing PageRank. In this paper we show that some well-known Web and social graph compression formats are computation-friendly, in the sense that they allow boosting the computation. In particular, we show that the format of Boldi and Vigna allows computing the product in time proportional to the compressed graph size. Our experimental results show speedups of at least 2 on graphs that were compressed at least 5 times with respect to the original. We show that other successful graph compression formats enjoy this property as well.},
  citationcount = {7},
  venue = {Data Compression Conference}
}

@article{franciscoGraphCompressionFor2022,
  title = {Graph Compression for Adjacency-Matrix Multiplication},
  author = {Francisco, Alexandre P. and Gagie, T. and K{\"o}ppl, Dominik and Ladra, Susana and Navarro, G.},
  year = {2022},
  doi = {10.1007/s42979-022-01084-2},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {SN Computer Science}
}

@article{frandsenDynamicAlgorithmsDyck1995,
  title = {Dynamic Algorithms for the Dyck Languages},
  author = {Frandsen, G. and Husfeldt, T. and Miltersen, Peter Bro and Rauhe, Theis and Skyum, S{\o}ren},
  year = {1995},
  doi = {10.1007/3-540-60220-8_54},
  abstract = {No abstract available},
  citationcount = {32},
  venue = {Workshop on Algorithms and Data Structures},
  keywords = {dynamic}
}

@article{frandsenDynamicWordProblems1993,
  title = {Dynamic Word Problems},
  author = {Frandsen, G. and Miltersen, Peter Bro and Skyum, Sven},
  year = {1993},
  doi = {10.1109/SFCS.1993.366840},
  abstract = {Let M be a fixed finite monoid. We consider the problem of implementing a data type containing a vector x=(x/sub 1/,x/sub 2/,...,x/sub n/)/spl isin/M/sup n/, initially (1,1,...,1) with two kinds of operations, for each i/spl isin/\{1,...,n\}, a/spl isin/M, an operation change/sub i,a/ which changes x/sub i/ to a and a single operation product returning /spl Pi//sub i=1//sup n/x/sub i/. This is the dynamic word problem. If we in addition for each j/spl isin/\{1,...,n\} have an operation prefix/sub j/ returning /spl Pi//sub i=1//sup j/x/sub i/, we talk about the dynamic prefix problem. We analyze the complexity of these problems in the cell probe or decision assignment tree model for two natural cell sizes, 1 bit and log n bits. We obtain a classification of the complexity based on algebraic properties of M.<<ETX>>},
  citationcount = {53},
  venue = {Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science},
  keywords = {cell probe,dynamic}
}

@article{frandsenLowerBoundsDynamic1998,
  title = {Lower Bounds for Dynamic Algebraic Problems},
  author = {Frandsen, G. and Hansen, J. and Miltersen, Peter Bro},
  year = {1998},
  doi = {10.1006/inco.2001.3046},
  abstract = {We consider dynamic evaluation of algebraic functions (matrix multiplication, determinant, convolution, Fourier transform, etc.) in the model of Reif and Tate; i.e., if f (x1,..., xn)= (y1, ..., ym) is an algebraic problem, we consider serving online requests of the form "change input xi to value v" or "what is the value of output yi?" We present techniques for showing lower bounds on the worst case time complexity per operation for such problems. The first gives lower bounds in a wide range of rather powerful models (for instance, history dependent algebraic computation trees over any infinite subset of a field, the integer RAM, and the generalized real RAM model of Ben-Amram and Galil). Using this technique, we show optimal {\textohm}(n) bounds for dynamic matrix-vector product, dynamic matrix multiplication, and dynamic discriminant and an {\textohm}({\textsurd}n) lower bound for dynamic polynomial multiplication (convolution), providing a good match with Reif and Tate's O({\textsurd}nlogn) upper bound. We also show linear lower bounds for dynamic determinant, matrix adjoint, and matrix inverse and an {\textohm}({\textsurd}n) lower bound for the elementary symmetric functions. The second technique is the communication complexity technique of Miltersen, Nisan, Safra, and Wigderson which we apply to the setting of dynamic algebraic problems, obtaining similar lower bounds in the word RAM model. The third technique gives lower bounds in the weaker straight line program model. Using this technique, we show an {\textohm}((log n)2/log log n) lower bound for dynamic discrete Fourier transform. Technical ingredients of our techniques are the incompressibility technique of Ben-Amram and Galil and the lower bound for depth-two superconcentrators of Radhakrishnan and Ta-Shma. The incompressibility technique is extended to arithmetic computation in arbitrary fields. 2001 Elsevier Science},
  citationcount = {27},
  venue = {Information and Computation},
  keywords = {communication,communication complexity,dynamic,lower bound}
}

@article{frchetteSolvingTheStation2016,
  title = {Solving the Station Repacking Problem},
  author = {Fr{\'e}chette, A. and Newman, N. and {Leyton-Brown}, Kevin},
  year = {2016},
  doi = {10.1609/aaai.v30i1.10077},
  abstract = {We investigate the problem of repacking stations in the FCC's upcoming, multi-billion-dollar "incentive auction". Early efforts to solve this problem considered mixed-integer programming formulations, which we show are unable to reliably solve realistic, national-scale problem instances. We describe the result of a multi-year investigation of alternatives: a solver, SATFC, that has been adopted by the FCC for use in the incentive auction. SATFC is based on a SAT encoding paired with a wide range of techniques: constraint graph decomposition; novel caching mechanisms that allow for reuse of partial solutions from related, solved problems; algorithm configuration; algorithm portfolios; and the marriage of local-search and complete solver strategies. We show that our approach solves virtually all of a set of problems derived from auction simulations within the short time budget required in practice.},
  citationcount = {29},
  venue = {AAAI Conference on Artificial Intelligence}
}

@article{fredericksonDataStructuresFor1983,
  title = {Data Structures for On-Line Updating of Minimum Spanning Trees},
  author = {Frederickson, G.},
  year = {1983},
  doi = {10.1145/800061.808754},
  abstract = {Data structures are presented for the problem of maintaining a minimum spanning tree on-line under the operation of updating the cost of some edge in the graph. For the case of a general graph, maintaining the data structure and updating the tree are shown to take O((@@@@)m) time, where m is the number of edges in the graph. For the case of a planar graph, a data structure is presented which supports an update time of O ((log m)2).},
  citationcount = {212},
  venue = {Symposium on the Theory of Computing}
}

@article{fredericksonDataStructuresFor1985,
  title = {Data Structures for On-Line Updating of Minimum Spanning Trees, with Applications},
  author = {Frederickson, G.},
  year = {1985},
  doi = {10.1137/0214055},
  abstract = {Data structures are presented for the problem of maintaining a minimum spanning tree on-line under the operation of updating the cost of some edge in the graph. For the case of a general graph, mai...},
  citationcount = {386},
  venue = {SIAM journal on computing (Print)}
}

@article{fredericmeunierRainbowEndLine2016,
  title = {The {{Rainbow}} at the {{End}} of the {{Line}} - {{A PPAD Formulation}} of the {{Colorful Carath{\'e}odory Theorem}} with {{Applications}}},
  author = {Fr{\'e}d{\'e}ric Meunier and Wolfgang Mulzer and Pauline Sarrabezolles and Yannik Stein},
  year = {2016},
  journal = {ACM-SIAM Symposium on Discrete Algorithms},
  doi = {10.1137/1.9781611974782.87},
  abstract = {Let \$C\_1,...,C\_\{d+1\}\$ be \$d+1\$ point sets in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$, each containing the origin in its convex hull. A subset \$C\$ of \${\textbackslash}bigcup\_\{i=1\}{\textasciicircum}\{d+1\} C\_i\$ is called a colorful choice (or rainbow) for \$C\_1, {\textbackslash}dots, C\_\{d+1\}\$, if it contains exactly one point from each set \$C\_i\$. The colorful Carath{\textbackslash}'eodory theorem states that there always exists a colorful choice for \$C\_1,{\textbackslash}dots,C\_\{d+1\}\$ that has the origin in its convex hull. This theorem is very general and can be used to prove several other existence theorems in high-dimensional discrete geometry, such as the centerpoint theorem or Tverberg's theorem. The colorful Carath{\textbackslash}'eodory problem (CCP) is the computational problem of finding such a colorful choice. Despite several efforts in the past, the computational complexity of CCP in arbitrary dimension is still open.  We show that CCP lies in the intersection of the complexity classes PPAD and PLS. This makes it one of the few geometric problems in PPAD and PLS that are not known to be solvable in polynomial time. Moreover, it implies that the problem of computing centerpoints, computing Tverberg partitions, and computing points with large simplicial depth is contained in \${\textbackslash}text\{PPAD\} {\textbackslash}cap {\textbackslash}text\{PLS\}\$. This is the first nontrivial upper bound on the complexity of these problems. Finally, we show that our PPAD formulation leads to a polynomial-time algorithm for a special case of CCP in which we have only two color classes \$C\_1\$ and \$C\_2\$ in \$d\$ dimensions, each with the origin in its convex hull, and we would like to find a set with half the points from each color class that contains the origin in its convex hull.},
  annotation = {Citation Count: 20}
}

@article{fredmanALowerBound1981,
  title = {A Lower Bound on the Complexity of Orthogonal Range Queries},
  author = {Fredman, M.},
  year = {1981},
  doi = {10.1145/322276.322281},
  abstract = {Let S be an arbitrary commutaUve semigroup (set of elements closed under a commutative and associative addmon operation, +). Given a set of records wtth d-dimensional key vectors over an ordered key space, such that each record has associated with it a value in S, an orthogonal range query is a request for the sum of the values associated with each record in some specified hypercube (cross product of mtervals). Data structures which accommodate inserUons and delettons of records and orthogonal range queries, such that an arbitrary sequence of n such operations takes time O(n(log n)a), have been presented by G. Lueker and D. Wdlard. It is shown here that fi(n(logn)  ) is a lower bound on the inherent worst case time reqmred to process a sequence of n intermixed insemons, deleuons, and range queries, which imphes that the Lueker and Wdlard data structures are in some sense optimal.},
  citationcount = {112},
  venue = {JACM}
}

@inproceedings{fredmanCellProbeComplexity1989,
  title = {The Cell Probe Complexity of Dynamic Data Structures},
  booktitle = {Proceedings of the Twenty-First Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Fredman, M. and Saks, M.},
  year = {1989},
  month = feb,
  series = {{{STOC}} '89},
  pages = {345--354},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/73007.73040},
  url = {https://dl.acm.org/doi/10.1145/73007.73040},
  urldate = {2024-11-19},
  abstract = {Dynamic data structure problems involve the representation of data in memory in such a way as to permit certain types of modifications of the data (updates) and certain types of questions about the data (queries). This paradigm encompasses many fundamental problems in computer science.The purpose of this paper is to prove new lower and upper bounds on the time per operation to implement solutions to some familiar dynamic data structure problems including list representation, subset ranking, partial sums, and the set union problem. The main features of our lower bounds are: They hold in the cell probe model of computation (A. Yao [18]) in which the time complexity of a sequential computation is defined to be the number of words of memory that are accessed. (The number of bits b in a single word of memory is a parameter of the model). All other computations are free. This model is at least as powerful as a random access machine and allows for unusual representation of data, indirect addressing etc. This contrasts with most previous lower bounds which are proved in models (e.g., algebraic, comparison, pointer manipulation) which require restrictions on the way data is represented and manipulated.The lower bound method presented here can be used to derive amortized complexities, worst case per operation complexities, and randomized complexities.The results occasionally provide (nearly tight) tradeoffs between the number R of words of memory that are read per operation, the number W of memory words rewritten per operation and the size b of each word. For the problems considered here there is a parameter n that represents the size of the data set being manipulated and for these problems b = logn is a natural register size to consider. By letting b vary, our results illustrate the effect of register size on time complexity. For instance, one consequence of the results is that for some of the problems considered here, increasing the register size from logn to polylog(n) only reduces the time complexity by a constant factor. On the other hand, decreasing the register size from logn to 1 increases time complexity by a logn factor for one of the problems we consider and only a loglogn factor for some other problems.The first two specific data structure problems for which we obtain bounds are: List Representation. This problem concerns the representation of an ordered list of at most n (not necessarily distinct) elements from the universe U = \{1, 2,{\dots}, n\}. The operations to be supported are report(k), which returns the kth element of the list, insert(k, u) which inserts element u into the list between the elements in positions k - 1 and k, delete(k), which deletes the kth item.Subset Rank. This problem concerns the representation of a subset S of U = \{1, 2,{\dots}, n\}. The operations that must be supported are the updates ``insert item j into the set'' and ``delete item j from the set'' and the queries rank(j), which returns the number of elements in S that are less than or equal to j.The natural word size for these problems is b = logn, which allows an item of U or an index into the list to be stored in one register. One simple solution to the list representation problem is to maintain a vector v, whose kth entry contains the kth item of the list. The report operation can be done in constant time, but the insert and delete operations may take time linear in the length of the list. Alternatively, one could store the items of the list with each element having a pointer to its predecessor and successor in the list. This allows for constant time updates (given a pointer to the appropriate location), but requires linear cost for queries.This problem can be solved must more efficiently by use of balanced trees (such as AVL trees). When b = logn, the worst case cost per operation using AVL trees is O(logn). If instead b = 1, so that each bit access costs 1, then the AVL three solution requires O(log2n) per operation.It is not hard to find similar upper bounds for the subset rank problem (the algorithms for this problem are actually simpler than AVL trees).The question is: are these upper bounds bet possible? Our results show that the upper bounds for the case of logn bit registers are within a loglogn factor of optimal. On the other hand, somewhat surprisingly, for the case of single bit registers there are implementations for both of these problems that run in time significantly faster than O(log2n) per operation.Let CPROBE(b) denote the cell probe computational model with register size b.Theorem 1. If b {$\leq$} (logn)t for some t, then any CPROBE(b) implementation of either list representation or the subset rank requires {\textohm}(logn/loglogn) amortized time per operation.Theorem 2. Subset rank and list representation have CPROBE(1) implementations with respective complexities O((logn)(loglogn)) and O((logn)(loglogn)2) per operation.Paul Dietz (personal communication) has found an implementation of list representation with logn bit registers that requires only O(logn/loglogn) time per operation, and thus the result of theorem 1 is best possible.The lower bounds of theorem 1 are derived from lower bounds for a third problem: Partial sum mode k. An array A[1],{\dots}, A[N] of integers mod k is to be represented. Updates are add(i, {$\delta$}) which implements A[i] {\textleftarrow} A[i] + {$\delta$}; and queries are sum(j) which returns {$\Sigma$}i{$\leq$}jA[i] (mod k).This problem is demoted PS(n, k). Our main lower bound theorems provide tradeoffs between the number of register rewrites and register reads as a function of n, k, and b. Two corollaries of these results are: Theorem 3. Any CPROBE(b) implementation of PS(n, 2) (partial sums mod 2) requires {\textohm}(logn/(loglogn + logb)) amortized time per operation, and for b {$\geq$} logn, there is an implementation that achieves this. In particular, if b = {$\Theta$}((logn)c) for some constant c, then the optimal time complexity of PS(n, 2) is {\texttheta}(logn/loglogn).Theorem 4. Any CPROBE(1) implementation of PS(n, n) with single bit registers requires {\textohm}((logn/loglogn)2) amortized time per operation, and there is an implementation that achieves O(log2n) time per operation.It can be shown that a lower bound of PS(n, 2) is also a lower bound for both list representation and subset rank (the details, which are not difficult, are omitted from this report), and thus theorem 1 follows from theorem 3. The results of theorem 4 make an interesting contrast with those of theorem 2. For the three problems, list representation, subset rank and PS(n, k), there are standard algorithms that can be implemented on a CPROBE(logn) that use time O(logn per operation, and their implementations on CPROBE(1) require O(log2n) time. Theorem 4 says that for the problem PS(n, n) this algorithm is essentially best possible, while theorem 2 says that for list representation and rank, the algorithm can be significantly improved. In fact, the rank problem an be viewed as a special case of PS(n, n) where the variables take on values on \{0, 1\}, and apparently this specialization is enough to reduce the complexity on a CPROBE(1) by a factor of logn/loglogn, even though on a CPROBE(logn) the complexities of the two problems differ by no more than a loglogn factor.The third problem we consider is the set union problem. This problem concerns the design of a data structure for the on-line manipulation of sets in the following setting. Initially, there are n singleton sets \{1\}, \{2\},{\dots}, \{n\} with i chosen as the name of the set \{i\}. Our data structure is required to implement two operations, Find(j), and Union(A, B, C). The operation Find(j) returns the name of the set containing j. The operation Union(A, B, C) combines the sets with names A and B. The names of the existing sets at any moment must be unique and chosen to be integers in the range from 1 to 2n. The sets existing at any time are disjoint and define a partition of the elements into equivalence classes.A well known data structure for the set union problem represents the sets as trees and stores the name of a set in the root of its corresponding tree. A Union operation is performed by attaching the root of the smaller set as a child of the root of the larger set (weight rule). A Find operation is implemented by following the path from the appropriate node to the root of the tree containing it, and then redirecting to the root the parent pointers of the nodes encountered along this path (path compression). From now on we consider sequences of Union and Find operations consisting of n-1 Union operations and m Find operations with m {$\geq$} n. Tarjan [14] demonstrated that the above algorithm requires time {\texttheta}(m{$\alpha$}(m, n)), where {$\alpha$}(m, n) is an inverse to Ackermann's function, to execute n-1 Union and m Find operations. In particular, if m = {\texttheta}(n), then the running time is almost, but not quite, linear. Tarjan conjectured [14] that no linear time algorithm exists for the set union problem, and provided significant evidence in favor of this conjecture (which we discuss in the following section). We affirm Tarjan's conjecture in the CPROBE(logn) model.Theorem 5. Any CPROBE(logn) implementation of the set union problem requires {\textohm}(m{$\alpha$}(m, n)) time to execute m Find's and n-1 Union's, beginning with n singleton sets.N. Blum [2] has given a logn/loglogn algorithm (worst case time per operation) for the set union problem. This algorithm is also optimal in the CPROBE(polylogn) model.The following Section provides further discussion of these results, Section 3 outlines our lower bound method, and Section 4 contains some proofs.},
  isbn = {978-0-89791-307-2},
  keywords = {cell probe,chronogram,communication,data structure,dynamic,lower bound,query,sorted,update},
  file = {/Users/tulasi/Zotero/storage/6YVFZW2W/Fredman and Saks - 1989 - The cell probe complexity of dynamic data structures.pdf}
}

@article{fredmanComplexityMaintainingArray1982,
  title = {The {{Complexity}} of {{Maintaining}} an {{Array}} and {{Computing Its Partial Sums}}},
  author = {Fredman, Michael L.},
  year = {1982},
  month = jan,
  journal = {J. ACM},
  volume = {29},
  number = {1},
  pages = {250--260},
  issn = {0004-5411},
  doi = {10.1145/322290.322305},
  url = {https://dl.acm.org/doi/10.1145/322290.322305},
  urldate = {2024-11-19},
  keywords = {lower bound},
  annotation = {AHO , A V , HOPCROFT , J E , AND ULLMAN , J D The Design and Analysis of Computer Algorithms Addison-Wesley , Reading, Mass. , 1974 AHO, A V, HOPCROFT, J E, AND ULLMAN, J D The Design and Analysis of Computer Algorithms Addison-Wesley, Reading, Mass., 1974\\
BURKHARD W A Fg{\textasciitilde}I{\textasciitilde}MAN M L ANO KLEIXMAN D J Inherent complexny trade-offs for range query problems Theor. Comput. Sat. (to appear)  BURKHARD W A Fg{\textasciitilde}I{\textasciitilde}MAN M L ANO KLEIXMAN D J Inherent complexny trade-offs for range query problems Theor. Comput. Sat. (to appear)\\
FREDMAN , M L Observations on the complexity of generating quasi-Gray codes SIAM J Comput , 7 , 2 ( 1978 ), 134-146 FREDMAN, M L Observations on the complexity of generating quasi-Gray codes SIAM J Comput, 7, 2 (1978), 134-146},
  file = {/Users/tulasi/Zotero/storage/BCIRI5L6/Fredman - 1982 - The Complexity of Maintaining an Array and Computing Its Partial Sums.pdf}
}

@article{fredmanDataStructuresFor1993,
  title = {Data Structures for Traveling Salesmen},
  author = {Fredman, M. and Johnson, David S. and McGeoch, L. and Ostheimer, G.},
  year = {1993},
  doi = {10.1006/jagm.1995.1018},
  abstract = {The choice of data structure for tour representation plays a critical role in the efficiency of local improvement heuristics for the traveling salesman problem. The tour data structure must permit queries about the relative order of cities in the current tour and must allow sections of the tour to be reversed. The traditional array-based representation of a tour permits the relative order of cities to be determined in small constant time, but requires worst-case ?(N) time (where N is the number of cities) to implement a reversal, which renders it impractical for large instances. This paper considers alternative tour data structures, examining them from both a theoretical and experimental point of view. The first alternative we consider is a data structure based on splay trees, where all queries and updates take amortized time O(log N). We show that this is close to the best possible, because in the cell probe model of computation any data structure must take worst-case amortized time ?(log N/log log N) per operation. Empirically (for random Euclidean instances), splay trees overcome their large constant-factor overhead and catch up to arrays by N = 10,000, pulling ahead by a factor of 4-10 (depending on machine) when N = 100,000. Two alternative tree-based data structures do even better in this range, however. Although both are asymptotically inferior to the splay tree representations, the latter does not appear to pull even with them until N   to 1,000,000.},
  citationcount = {112},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,data structure,query,update}
}

@article{fredmanLowerBoundsDynamic1994,
  title = {Lower Bounds for Dynamic Algorithms},
  author = {Fredman, M.},
  year = {1994},
  doi = {10.1007/3-540-58218-5_15},
  abstract = {No abstract available},
  citationcount = {5},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {dynamic,lower bound}
}

@article{fredmanLowerBoundsFully1998,
  title = {Lower Bounds for Fully Dynamic Connectivity Problems in Graphs},
  author = {Fredman, Michael L. and Henzinger, Monika Rauch},
  year = {1998},
  journal = {Algorithmica},
  volume = {22},
  number = {3},
  pages = {351--362},
  doi = {10.1007/PL00009228},
  abstract = {We prove lower bounds on the complexity of maintaining fully dynamic k-edge or k-vertex connectivity in plane graphs and in (k - 1)-vertex connected graphs. We show an amortized lower bound of {\textflorin}(log n/k(log log n + log b)) per edge insertion, deletion, or query operation in the cell probe model, where b is the word size of the machine and n is the number of vertices in G. We also show an amortized lower bound of {\textflorin}(log n/(log log n + log b)) per operation for fully dynamic planarity testing in embedded graphs. These are the first lower bounds for fully dynamic connectivity problems.},
  keywords = {cell probe,dynamic,Dynamic connectivity testing,Dynamic planarity testing,lower bound},
  file = {/Users/tulasi/Zotero/storage/3NIGVZXF/Henzinger and Fredman - 1998 - Lower Bounds for Fully Dynamic Connectivity Problems in Graphs.pdf}
}

@article{fredmanLowerBoundsOn1981,
  title = {Lower Bounds on the Complexity of Some Optimal Data Structures},
  author = {Fredman, M.},
  year = {1981},
  doi = {10.1137/0210001},
  abstract = {A technique is presented for deriving lower bounds on the complexity of optimal data structures which permit insertions and deletions of records, and queries of the form  where value(r) (the value associated with a record r) lies in a commutative semi-group S, and {$\Gamma$} denotes a set of regions of the space of possible keys. This technique is illustrated with several examples.},
  citationcount = {69},
  venue = {SIAM journal on computing (Print)}
}

@article{fredmanObservationsOnThe1978,
  title = {Observations on the Complexity of Generating Quasi-Gray Codes},
  author = {Fredman, M.},
  year = {1978},
  doi = {10.1137/0207012},
  abstract = {The purpose of this paper is to develop a decision tree-like model for defining and measuring the on-line complexity of algorithms for generating combinatorial objects. For the purpose of illustration, we consider the problem of generating Gray codes and simple generalizations of Gray codes. We include some results pertaining to the generation of certain special codes and, in addition, we present a trade-off theorem. Our model is information theoretical and we emphasize two aspects of complexity; the amount of information that must be gathered and the amount of data structure update required to generate the successor to a given codeword.},
  citationcount = {41},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,information theoretic,update}
}

@article{fredmanStoringASparse1982,
  title = {Storing a Sparse Table with {{O}}(1) Worst Case Access Time},
  author = {Fredman, M. and Komlos, J. and Szemer{\'e}di, E.},
  year = {1982},
  doi = {10.1145/828.1884},
  abstract = {We describe a data structure for representing a set of n items from a universe of m items, which uses space n+o(n) and accommodates membership queries in constant time. Both the data structure and the query algorithm are easy to implement.},
  citationcount = {962},
  venue = {23rd Annual Symposium on Foundations of Computer Science (sfcs 1982)},
  keywords = {data structure,query}
}

@article{fredmanSurpassingTheInformation1993,
  title = {Surpassing the Information Theoretic Bound with Fusion Trees},
  author = {Fredman, M. and Willard, D.},
  year = {1993},
  doi = {10.1016/0022-0000(93)90040-4},
  abstract = {No abstract available},
  citationcount = {454},
  venue = {Journal of computer and system sciences (Print)}
}

@article{fredmanTheComplexityOf1982,
  title = {The Complexity of Partial Match Retrieval in a Dynamic Setting},
  author = {Fredman, M. and Volper, D.},
  year = {1982},
  doi = {10.1016/0196-6774(82)90009-8},
  abstract = {No abstract available},
  citationcount = {6},
  venue = {J. Algorithms}
}

@article{fredmanTheInherentComplexity1980,
  title = {The Inherent Complexity of Dynamic Data Structures Which Accommodate Range Queries},
  author = {Fredman, M.},
  year = {1980},
  doi = {10.1109/SFCS.1980.47},
  abstract = {A formal framework is presented in which to explore the complexity issues of data structures which accommodate various types of range queries. Within this framework, a systematic and reasonably tractable method for assessing inherent complexity is developed. Included among the interesting results are the following: the fact that non-linear lower bounds are readily accessible, and the existence of a complexity gap between linear time and n log n time.},
  citationcount = {26},
  venue = {21st Annual Symposium on Foundations of Computer Science (sfcs 1980)}
}

@article{freedmanOptimalDistanceLabeling2016,
  title = {Optimal Distance Labeling Schemes for Trees},
  author = {Freedman, Ofer and Gawrychowski, Pawe{\l} and Nicholson, Patrick K. and Weimann, Oren},
  year = {2016},
  doi = {10.1145/3087801.3087804},
  abstract = {Labeling schemes seek to assign a short label to each node in a network, so that a function on two nodes (such as distance or adjacency) can be computed by examining their labels alone. For the particular case of trees, following a long line of research, optimal bounds (up to low order terms) were recently obtained for adjacency labeling [FOCS '15], nearest common ancestor labeling [SODA '14], and ancestry labeling [SICOMP '06]. In this paper we obtain optimal bounds for distance labeling. We present labels of size 1/4{\textasciicircum}2n+o({\textasciicircum}2n), matching (up to low order terms) the recent 1/4{\textasciicircum}2n-(n) lower bound [ICALP '16]. Prior to our work, all distance labeling schemes for trees could be reinterpreted as universal trees. A tree T is said to be universal if any tree on n nodes can be found as a subtree of T. A universal tree with {\textbar}T{\textbar} nodes implies a distance labeling scheme with label size {\textbar}T{\textbar}. In 1981, Chung et al. proved that any distance labeling scheme based on universal trees requires labels of size 1/2{\textasciicircum}2 n -n {$\cdot$}n+(n). Our scheme is the first to break this lower bound, showing a separation between distance labeling and universal trees. The {\texttheta} (log2 n) barrier for distance labeling in trees has led researchers to consider distances bounded by k. The size of such labels was shown to be n+(k{\textsurd}\{n\}) in [WADS '01], and then improved to n+(k{\textasciicircum}2((kn)) in [SODA '03] and finally to n+(k(k(n/k))) in [PODC '07]. We show how to construct labels whose size is the minimum between n+(k((n)/k)) and (n {$\cdot$}(k/n)). We complement this with almost tight lower bounds of n+{\textohm}(k(n / (kk))) and {\textohm}(n {$\cdot$}(k/n)). Finally, we consider (1+)-approximate distances. We show that the recent labeling scheme of [ICALP '16] can be easily modified to obtain an ((1/){$\cdot$}n) upper bound and we prove a matching {\textohm}((1/){$\cdot$}n) lower bound.},
  citationcount = {30},
  venue = {ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing},
  keywords = {lower bound}
}

@article{freundImprovedSubquadratic3sum2017,
  title = {Improved Subquadratic {{3SUM}}},
  author = {Freund, Ari},
  year = {2017},
  doi = {10.1007/s00453-015-0079-6},
  abstract = {No abstract available},
  citationcount = {39},
  venue = {Algorithmica}
}

@article{friedmanANoteOn1993,
  title = {A Note on Matrix Rigidity},
  author = {Friedman, J.},
  year = {1993},
  doi = {10.1007/BF01303207},
  abstract = {No abstract available},
  citationcount = {80},
  venue = {Comb.}
}

@article{frigioniDynamicallySwitchingVertices2000,
  title = {Dynamically Switching Vertices in Planar Graphs},
  author = {Frigioni, D. and Italiano, G.},
  year = {2000},
  doi = {10.1007/s004530010032},
  abstract = {No abstract available},
  citationcount = {43},
  venue = {Algorithmica}
}

@article{frischknechtNetworksCannotCompute2012,
  title = {Networks Cannot Compute Their Diameter in Sublinear Time},
  author = {Frischknecht, S. and Holzer, S. and Wattenhofer, Roger},
  year = {2012},
  doi = {10.1137/1.9781611973099.91},
  abstract = {We study the problem of computing the diameter of a network in a distributed way. The model of distributed computation we consider is: in each synchronous round, each node can transmit a different (but short) message to each of its neighbors. We provide an {\textohm}(n) lower bound for the number of communication rounds needed, where n denotes the number of nodes in the network. This lower bound is valid even if the diameter of the network is a small constant. We also show that a (3/2 - e)-approximation of the diameter requires {\textohm} ({\textsurd}n + D) rounds. Furthermore we use our new technique to prove an {\textohm} ({\textsurd}n + D) lower bound on approximating the girth of a graph by a factor 2 - e.},
  citationcount = {161},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{fuentes-seplvedaRunCompressedRank2017,
  title = {Run Compressed Rank/Select for Large Alphabets},
  author = {{Fuentes-Sep{\'u}lveda}, Jos{\'e} and K{\"a}rkk{\"a}inen, Juha and Kosolobov, D. and Puglisi, S.},
  year = {2017},
  doi = {10.1109/DCC.2018.00040},
  abstract = {Given a string of length n that is composed of r runs of letters from the alphabet \{0,1,...,{$\sigma$}-1\} such that 2 {$\leq$} {$\sigma$} {$\leq$} r, we describe a data structure that, provided r {$\leq$} n/log {$\omega$}(1) n, stores the string in rn{$\sigma$}/r + o(r log n{$\sigma$}/r) bits and supports select and access queries in O(log log(n/r)/loglogn) time and rank queries in O(log log(n{$\sigma$}/r)/log) time. We show that r log n({$\sigma$}-1)/r - O(log n/r) bits are necessary for any such data structure and, thus, our solution is succinct. We also describe a data structure that uses (1 + {$\varepsilon$})r log n{$\sigma$}/r + O(r) bits, where {$\varepsilon$} {\textquestiondown} 0 is an arbitrary constant, with the same query times but without the restriction r {$\leq$} n / log {$\omega$}(1) n. By simple reductions to the colored predecessor problem, we show that the query times are optimal in the important case r {$\geq$} 2log{$\delta$} n, for an arbitrary constant {$\delta$} {\textquestiondown} 0. We implement our solution and compare it with the state of the art, showing that the closest competitors consume 31-46},
  citationcount = {3},
  venue = {Data Compression Conference},
  keywords = {data structure,query,query time,reduction}
}

@article{fuLargeScaleImage2013,
  title = {Large-Scale Image Retrieval Based on Boosting Iterative Quantization Hashing with Query-Adaptive Reranking},
  author = {Fu, Haiyan and Kong, Xiangwei and Lu, Jiayin},
  year = {2013},
  doi = {10.1016/j.neucom.2013.05.033},
  abstract = {No abstract available},
  citationcount = {22},
  venue = {Neurocomputing},
  keywords = {adaptive,query}
}

@article{fuPpaDbscanPrivacy2024,
  title = {{{PPA-DBSCAN}}: {{Privacy-preserving}} {\textexclamdown}inline-Formula{\textquestiondown}{\textexclamdown}tex-Math Notation="{{LaTeX}}"{\textquestiondown}{{$\rho$}}{\textexclamdown}/Tex-Math{\textquestiondown}{\textexclamdown}alternatives{\textquestiondown}{\textexclamdown}mml:Math{\textquestiondown}{\textexclamdown}mml:Mi{\textquestiondown}{$\rho$}{\textexclamdown}/Mml:Mi{\textquestiondown}{\textexclamdown}/Mml:Math{\textquestiondown}{\textexclamdown}inline-Graphic Xlink:Href="cheng-Ieq1-3375347.Gif"/{\textquestiondown}{\textexclamdown}/Alternatives{\textquestiondown}{\textexclamdown}/Inline-Formula{\textquestiondown}-Approximate Density-Based Clustering},
  author = {Fu, Jiaxuan and Cheng, Ke and Chang, Zhao and Shen, Yulong},
  year = {2024},
  doi = {10.1109/TDSC.2024.3375347},
  abstract = {Clustering is widely used for data analysis that partitions a set of data into multiple clusters, where objects in the same cluster have similar properties. Data for clustering analysis often comes from different data sources, which makes it important to maintain data privacy. However, existing privacy-preserving clustering schemes either require the support of prior knowledge or are just applicable for small datasets due to impractical costs. To solve this issue, we follow a classical approximate DBSCAN clustering algorithm and adapt it to the privacy-preserving context. Concretely, to construct our secure approximate clustering algorithm, we propose a series of basic secure computation protocols among additively secret-shared values. In addition, we design a crypto-friendly grid partitioning method based on which an efficient and privacy-preserving approximation DBSCAN scheme is derived. Theoretical analysis and experimental results show that our scheme achieves almost the same cluster quality compared to the plain-text exact DBSCAN. Our extensive experiments on different datasets demonstrate that our scheme is accurate and efficient.},
  citationcount = {2},
  venue = {IEEE Transactions on Dependable and Secure Computing}
}

@article{fuWeaklyPrincipalComponent2013,
  title = {Weakly Principal Component Hashing with Multiple Tables},
  author = {Fu, Haiyan and Kong, Xiangwei and Guo, Yanqing and Lu, Jiayin},
  year = {2013},
  doi = {10.1007/978-3-642-35728-2_28},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Conference on Multimedia Modeling}
}

@article{g.blellochParallelComplexityModel1994,
  title = {A {{Parallel Complexity Model}} for {{Functional Languages}}},
  author = {G. Blelloch and John Greiner},
  year = {1994},
  doi = {10.21236/ada288589},
  abstract = {A complexity model based on the lambda-calculus with an appropriate operational semantics in presented and related to various parallel machine models, including the PRAM and hypercube models. The model is used to study parallel algorithms in the context of "sequential" functional languages, and to relate these results to algorithms designed directly for parallel machine models. For example, the paper shows that equally good upper bounds can be achieved for merging two sorted sequences in the pure lambda-calculus with some arithmetic constants as in the EREW PRAM, when they are both mapped onto a more realistic machine such as a hypercube or butterfly network. In particular for n keys and p processors, they both result in an O(n/p + log{\textasciicircum}2p) time algorithm. These results argue that it is possible to get good parallelism in functional languages without adding explicitly parallel constructs. In fact, the lack of random access seems to be a bigger problem than the lack of parallelism.},
  annotation = {Citation Count: 8}
}

@article{g.blellochParallelismSequentialFunctional1995,
  title = {Parallelism in Sequential Functional Languages},
  author = {G. Blelloch and John Greiner},
  year = {1995},
  journal = {Conference on Functional Programming Languages and Computer Architecture},
  doi = {10.1145/224164.224210},
  abstract = {This paper formally studies the question of how much parallelism is available in cal-by-value functional languages with no parallel extensions (i. e., the functional sub;ets of ML or Scheme). In particular we are interested in placing bounds on how much parallelism is available for various problems. To do this we introduce a complexity model, the PAL, based on the call-by-value A-calculus. The model is defined in terms of a profiling semantics and measures complexity in terms of the total work and the parallel depth of a computation. We describe a simulation of the A-PAL (the PAL extended with arithmetic operations) on various parallel machine models, including the butterfly, hypercube, and PRAM models and prove simulation bounds. In particular the simulat ions are workeficient (the processor-time product on the machines is within a constant factor of the work on the A-PAL), and for p processors the slowdown (time on the machines divided by depth on the A-PAL) is proportional to at most O(log p). We also prove bounds for simulating the PRAM on the A-PAL. Based on the model, we describe and analyze tree-based versions of quicksort and merge sort. We show that for an input of size n these algorithms run on the A-PAL model with O(rI log n) work and 0(log2 n) depth (expected case for quicksort ).},
  annotation = {Citation Count: 95}
}

@article{g.brodalOptimalFingerSearch2002,
  title = {Optimal Finger Search Trees in the Pointer Machine},
  author = {G. Brodal and G. Lagogiannis and C. Makris and A. Tsakalidis and K. Tsichlas},
  year = {2002},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/509907.509991},
  abstract = {We develop a new finger search tree with worst-case constant update time in the Pointer Machine (PM) model of computation. This was a major problem in the field of Data Structures and was tantalizingly open for over twenty years while many attempts by researchers were made to solve it. The result comes as a consequence of the innovative mechanism that guides the rebalancing operations combined with incremental multiple splitting and fusion techniques over nodes.},
  keywords = {data structure,update,update time},
  annotation = {Citation Count: 38}
}

@article{g.brodalOptimalSolutionsTemporal2002,
  title = {Optimal {{Solutions}} for the {{Temporal Precedence Problem}}},
  author = {G. Brodal and C. Makris and S. Sioutas and A. Tsakalidis and K. Tsichlas},
  year = {2002},
  journal = {Algorithmica},
  doi = {10.1007/s00453-002-0935-z},
  annotation = {Citation Count: 9}
}

@article{g.gambosiGettingBackUnionFind1988,
  title = {Getting {{Back}} to the {{Past}} in the {{Union-Find Problem}}},
  author = {G. Gambosi and G. Italiano and M. Talamo},
  year = {1988},
  journal = {Symposium on Theoretical Aspects of Computer Science},
  doi = {10.1007/BFb0035827},
  annotation = {Citation Count: 7}
}

@article{g.gambosiSetUnionProblem1991,
  title = {The Set Union Problem with Dynamic Weighted Backtracking},
  author = {G. Gambosi and G. Italiano and M. Talamo},
  year = {1991},
  journal = {BIT},
  doi = {10.1007/BF01933257},
  keywords = {dynamic},
  annotation = {Citation Count: 4}
}

@article{g.gambosiWorstCaseAnalysisSetUnion1989,
  title = {Worst-{{Case Analysis}} of the {{Set-Union Problem}} with {{Extended Backtracking}}},
  author = {G. Gambosi and G. Italiano and M. Talamo},
  year = {1989},
  journal = {Theoretical Computer Science},
  doi = {10.1016/0304-3975(89)90119-9},
  annotation = {Citation Count: 11}
}

@article{g.italianoFullyPersistentData1991,
  title = {Fully {{Persistent Data Structures}} for {{Disjoint Set Union Problems}}},
  author = {G. Italiano and Neil Sarnak},
  year = {1991},
  journal = {Workshop on Algorithms and Data Structures},
  doi = {10.1007/BFb0028283},
  keywords = {data structure},
  annotation = {Citation Count: 6}
}

@article{g.italianoTopicsDataStructures2010,
  title = {Topics in {{Data Structures}}},
  author = {G. Italiano and R. Raman},
  year = {2010},
  journal = {Algorithms and Theory of Computation Handbook},
  doi = {10.1201/9781584888239-C5},
  abstract = {Rajeev Raman King's College, London 5.},
  keywords = {data structure},
  annotation = {Citation Count: 6}
}

@article{g.ivanovApproximateCaratheodorysTheorem2019,
  title = {Approximate {{Carath{\'e}odory}}'s {{Theorem}} in {{Uniformly Smooth Banach Spaces}}},
  author = {G. Ivanov},
  year = {2019},
  journal = {Discrete \& Computational Geometry},
  doi = {10.1007/s00454-019-00130-w},
  annotation = {Citation Count: 11}
}

@article{g.ivanovNodimensionTverbergsTheorem2019,
  title = {No-dimension {{Tverberg}}'s Theorem and Its Corollaries in {{Banach}} Spaces of Type p},
  author = {G. Ivanov},
  year = {2019},
  journal = {Bulletin of the London Mathematical Society},
  doi = {10.1112/blms.12449},
  abstract = {We continue our study of `no-dimension' analogues of basic theorems in combinatorial and convex geometry in Banach spaces. We generalize some results of the paper (Adiprasito, B{\'a}r{\'a}ny and Mustafa, `Theorems of Carath{\'e}odory, Helly, and Tverberg without dimension', Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms (Society for Industrial and Applied Mathematics, San Diego, California, 2019) 2350--2360) and prove no-dimension versions of the colored Tverberg theorem, the selection lemma and the weak {$\varepsilon$} -net theorem in Banach spaces of type p{$>$}1 . To prove these results, we use the original ideas of Adiprasito, B{\'a}r{\'a}ny and Mustafa for the Euclidean case, our no-dimension version of the Radon theorem and slightly modified version of the celebrated Maurey lemma.},
  annotation = {Citation Count: 2}
}

@article{g.sullivanCertificationTrailsData1991,
  title = {Certification Trails for Data Structures},
  author = {G. Sullivan and G. Masson},
  year = {1991},
  journal = {[1991] Digest of Papers. Fault-Tolerant Computing: The Twenty-First International Symposium},
  doi = {10.1109/FTCS.1991.146668},
  abstract = {The applicability of the certification trail technique, a recently introduced and promising approach to fault detection and fault tolerance, is expanded. Previously, certification trails had to be customized to each algorithm application, but here trails appropriate to wide classes of algorithms are developed. These certification trails are based on common data-structure operations such as those carried out using balanced binary trees and heaps. Any algorithm using these sets of operations can therefore employ the certification trail method to achieve software fault tolerance. Constructions of trails for abstract data types such as priority queues and union-find structures are given. These trails are applicable to any data structure implementation of the abstract data type. It is shown that these ideas lead naturally to monitors for data structure operations.{$<<$}ETX{$>>$}},
  keywords = {data structure},
  annotation = {Citation Count: 40}
}

@article{gabowADataStructure2016,
  title = {A Data Structure for Nearest Common Ancestors with Linking},
  author = {Gabow, H.},
  year = {2016},
  doi = {10.1145/3108240},
  abstract = {Consider a forest that evolves via link operations that make the root of one tree the child of a node in another tree. Intermixed with link operations are nca operations, which return the nearest common ancestor of two given nodes when such exists. This article shows that a sequence of m such nca and link operations on a forest of n nodes can be processed online in time O(m{$\alpha$} (m,n)+n). This was previously known only for a restricted type of link operation. The special case where a link only extends a tree by adding a new leaf occurs in Edmonds' algorithm for finding a maximum weight matching on a general graph. Incorporating our algorithm into the implementation of Edmonds' algorithm in [9] achieves time O(n(m + nlog n)) for weighted matching, an arguably optimum asymptotic bound (n and m are the number of vertices and edges, respectively). Our data structure also provides a simple alternative implementation of the incremental-tree set merging algorithm of Gabow and Tarjan [11].},
  citationcount = {5},
  venue = {ACM Trans. Algorithms},
  keywords = {data structure}
}

@article{gabowScalingAndRelated1984,
  title = {Scaling and Related Techniques for Geometry Problems},
  author = {Gabow, H. and Bentley, J. and Tarjan, R.},
  year = {1984},
  doi = {10.1145/800057.808675},
  abstract = {Three techniques in computational geometry are explored: {\textexclamdown}italic{\textquestiondown}Scaling{\textexclamdown}/italic{\textquestiondown} solves a problem by viewing it at increasing levels of numerical precision; {\textexclamdown}italic{\textquestiondown}activation{\textexclamdown}/italic{\textquestiondown} is a restricted type of update operation, useful in sweep algorithms; the {\textexclamdown}italic{\textquestiondown}Cartesian tree{\textexclamdown}/italic{\textquestiondown} is a data structure for problems involving maximums and minimums. These techniques solve the minimum spanning tree problem in R{\textexclamdown}supscrpt{\textquestiondown}k{\textexclamdown}/supscrpt{\textquestiondown}{\textexclamdown}subscrpt{\textquestiondown}1{\textexclamdown}/subscrpt{\textquestiondown} and R{\textexclamdown}supscrpt{\textquestiondown}k{\textexclamdown}/supscrpt{\textquestiondown}{\textexclamdown}subscrpt{\textquestiondown}@@@@{\textexclamdown}/subscrpt{\textquestiondown} in O({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}lg n{\textexclamdown}/italic{\textquestiondown}){\textexclamdown}supscrpt{\textquestiondown}r{\textexclamdown}/supscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}lg lg n{\textexclamdown}/italic{\textquestiondown}) time and O({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) space, where for R{\textexclamdown}supscrpt{\textquestiondown}k{\textexclamdown}/supscrpt{\textquestiondown}{\textexclamdown}subscrpt{\textquestiondown}@@@@{\textexclamdown}/subscrpt{\textquestiondown} and k {$\geq$} 3, r \&equil; k-2; for R{\textexclamdown}supscrpt{\textquestiondown}k{\textexclamdown}/supscrpt{\textquestiondown}{\textexclamdown}subscrpt{\textquestiondown}1{\textexclamdown}/subscrpt{\textquestiondown}, r \&equil; 1, 2, 4 for k \&equil; 3, 4, 5 and r \&equil; k for k {\textquestiondown} 5. Other problems solved include R{\textexclamdown}supscrpt{\textquestiondown}k{\textexclamdown}/supscrpt{\textquestiondown}{\textexclamdown}subscrpt{\textquestiondown}1{\textexclamdown}/subscrpt{\textquestiondown}and R{\textexclamdown}supscrpt{\textquestiondown}k{\textexclamdown}/supscrpt{\textquestiondown} all nearest neighbors, post office and maximum spanning tree; R{\textexclamdown}supscrpt{\textquestiondown}k{\textexclamdown}/supscrpt{\textquestiondown} maxima, R{\textexclamdown}supscrpt{\textquestiondown}k{\textexclamdown}/supscrpt{\textquestiondown} rectangle searching problems, and Z{\textexclamdown}supscrpt{\textquestiondown}k{\textexclamdown}/supscrpt{\textquestiondown}{\textexclamdown}subscrpt{\textquestiondown}p{\textexclamdown}/subscrpt{\textquestiondown} all nearest neighbors (1 {$\leq$} {\textexclamdown}italic{\textquestiondown}p{\textexclamdown}/italic{\textquestiondown} {$\leq$} @@@@).},
  citationcount = {522},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,update}
}

@article{gaedeMultidimensionalAccessMethods1998,
  title = {Multidimensional Access Methods},
  author = {Gaede, V. and G{\"u}nther, O.},
  year = {1998},
  doi = {10.1145/280277.280279},
  abstract = {Search operations in databases require special support at the physical level. This is true for conventional databases as well as spatial databases, where typical search operations include the point query (find all objects that contain a given search point) and the region query (find all objects that overlap a given search region). More than ten years of spatial database research have resulted in a great variety of multidimensional access methods to support such operations. We give an overview of that work. After a brief survey of spatial data management in general, we first present the class of point access methods, which are used to search sets of points in two or more dimensions. The second part of the paper is devoted to spatial access methods to handle extended objects, such as rectangles or polyhedra. We conclude with a discussion of theoretical and experimental results concerning the relative performance of various approaches.},
  citationcount = {1829},
  venue = {CSUR}
}

@article{gagieCompressedDynamicRange2020,
  title = {Compressed Dynamic Range Majority and Minority Data Structures},
  author = {Gagie, T. and He, Meng and Navarro, G.},
  year = {2020},
  doi = {10.1007/s00453-020-00687-6},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Algorithmica},
  keywords = {data structure,dynamic}
}

@article{gagieEfficientAndCompact2014,
  title = {Efficient and Compact Representations of Prefix Codes},
  author = {Gagie, T. and Navarro, G. and Nekrich, Yakov and Pereira, Alberto Ord{\'o}{\~n}ez},
  year = {2014},
  doi = {10.1109/TIT.2015.2452252},
  abstract = {Most of the attention in statistical compression is given to the space used by the compressed sequence, a problem completely solved with optimal prefix codes. However, in many applications, the storage space used to represent the prefix code itself can be an issue. In this paper, we introduce and compare several techniques to store prefix codes. Let N be the sequence length and n be the alphabet size. Then, a naive storage of an optimal prefix code uses O(n log n) bits. Our first technique shows how to use O(n log log(N/n)) bits to store the optimal prefix code. Then, we introduce an approximate technique that, for any 0 {\textexclamdown}; {$\varepsilon$} {\textexclamdown}; 1/2, takes O(n log log(1/E)) bits to store a prefix code with an average codeword length within an additive {$\varepsilon$} of the minimum. Finally, a second approximation takes, for any constant c {\textquestiondown} 1, O(n1/c log n) bits to store a prefix code with an average codeword length at most c times the minimum. In all cases, our data structures allow encoding and decoding of any symbol in O(1) time. We experimentally compare our new techniques with the state of the art, showing that we achieve sixfold-to-eightfold space reductions, at the price of a slower encoding (2.5-8 times slower) and decoding (12-24 times slower). The approximations further reduce this space and improve the time significantly, up to recovering the speed of classical implementations, for a moderate penalty in the average code length. As a byproduct, we compare various heuristic, approximate, and optimal algorithms to generate length-restricted codes, showing that the optimal ones are clearly superior and practical enough to be implemented.},
  citationcount = {6},
  venue = {IEEE Transactions on Information Theory},
  keywords = {data structure,reduction}
}

@article{gagieFastAndCompact2009,
  title = {Fast and Compact Prefix Codes},
  author = {Gagie, T. and Navarro, G. and Nekrich, Yakov},
  year = {2009},
  doi = {10.1007/978-3-642-11266-9_35},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Conference on Current Trends in Theory and Practice of Informatics}
}

@article{gagieFasterCompressedQuadtrees2014,
  title = {Faster Compressed Quadtrees},
  author = {Gagie, T. and {Gonz{\'a}lez-Nova}, Javier I. and Ladra, Susana and Navarro, G. and Seco, Diego},
  year = {2014},
  doi = {10.1109/DCC.2015.57},
  abstract = {Real-world point sets tend to be clustered, so using a machine word for each point is wasteful. In this paper we first bound the number of nodes in the quad tree for a point set in terms of the points' clustering. We then describe aqua tree data structure that uses O (1) bits per node and supports faster queries than previous structures with this property. Finally, we present experimental evidence that our structure is practical.},
  citationcount = {13},
  venue = {Data Compression Conference},
  keywords = {data structure,query}
}

@article{gagieRankAndSelect2016,
  title = {Rank and Select Operations on Sequences},
  author = {Gagie, T.},
  year = {2016},
  doi = {10.1007/978-1-4939-2864-4_638},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {Encyclopedia of Algorithms}
}

@article{gajentaanOnAClass1995,
  title = {On a Class of {{O}}(N2) Problems in Computational Geometry},
  author = {Gajentaan, Anka and Overmars, M.},
  year = {1995},
  doi = {10.1016/0925-7721(95)00022-2},
  abstract = {No abstract available},
  citationcount = {356},
  venue = {Computational geometry}
}

@article{gajjarDistancePreservingSubgraphs2017,
  title = {Distance-Preserving Subgraphs of Interval Graphs},
  author = {Gajjar, Kshitij and Radhakrishnan, J.},
  year = {2017},
  doi = {10.4230/LIPIcs.ESA.2017.39},
  abstract = {We consider the problem of finding small distance-preserving subgraphs of undirected, unweighted interval graphs with k terminal vertices. To start with, we show that finding an optimal distance-preserving subgraph is \{NP\}-hard for general graphs. Then, we show that every interval graph admits a subgraph with O(k) branching vertices that approximates pairwise terminal distances up to an additive term of +1. We also present an interval graph G\textsubscript{\{\vphantom\}}\{int\}\vphantom\{\} for which the +1 approximation is necessary to obtain the O(k) upper bound on the number of branching vertices. In particular, any distance-preserving subgraph of G\textsubscript{\{\vphantom\}}\{int\}\vphantom\{\} has {\textohm}(kk) branching vertices. Furthermore, we prove that every interval graph admits a distance-preserving subgraph with O(kk) branching vertices, implying that the {\textohm}(kk) lower bound for interval graphs is tight. To conclude, we show that there exists an interval graph such that every optimal distance-preserving subgraph of it has O(k) branching vertices and {\textohm}(kk) branching edges, thereby providing a separation between branching vertices and branching edges. The O(k) bound for distance-approximating subgraphs follows from a na{\"i}ve analysis of shortest paths in interval graphs. G\textsubscript{\{\vphantom\}}\{int\}\vphantom\{\} is constructed using bit-reversal permutation matrices. The O(kk) bound for distance-preserving subgraphs uses a divide-and-conquer approach. Finally, the separation between branching vertices and branching edges employs Hansel's lemma for graph covering.},
  citationcount = {11},
  venue = {Embedded Systems and Applications},
  keywords = {lower bound}
}

@article{Gl2013,
  title = {Tight Bounds on Computing Error-Correcting Codes by Bounded-Depth Circuits with Arbitrary Gates},
  author = {G{\'a}l, Anna and Hansen, Kristoffer Arnsfelt and Kouck{\'y}, Michal and Pudl{\'a}k, Pavel and Viola, Emanuele},
  year = {2013},
  journal = {IEEE Transactions on Information Theory},
  volume = {59},
  number = {10},
  pages = {6611--6627},
  doi = {10.1109/TIT.2013.2270275},
  file = {/Users/tulasi/Zotero/storage/LC9N9Q7W/Gl et al. - 2013 - Tight bounds on computing error-correcting codes by bounded-depth circuits with arbitrary gates.pdf}
}

@article{galCellProbeComplexity2007,
  title = {The Cell Probe Complexity of Succinct Data Structures},
  author = {G{\'a}l, Anna and Miltersen, Peter Bro},
  year = {2007},
  month = jun,
  journal = {Theoretical Computer Science},
  series = {Automata, {{Languages}} and {{Programming}}},
  volume = {379},
  number = {3},
  pages = {405--417},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2007.02.047},
  url = {https://www.sciencedirect.com/science/article/pii/S030439750700151X},
  urldate = {2024-11-20},
  abstract = {We consider time-space tradeoffs for static data structure problems in the cell probe model with word size 1 (the bit probe model). In this model, the goal is to represent n-bit data with s=n+r bits such that queries (of a certain type) about the data can be answered by reading at most t bits of the representation. Ideally, we would like to keep both s and t small, but there are tradeoffs between the values of s and t that limit the possibilities of keeping both parameters small. In this paper, we consider the case of succinct representations, where s=n+r for some redundancy r{$\ll$}n. For a Boolean version of the problem of polynomial evaluation with preprocessing of coefficients, we show a lower bound on the redundancy--query time tradeoff of the form (r+1)t{$\geq\Omega$}(n/logn). In particular, for very small redundancies r, we get an almost optimal lower bound stating that the query algorithm has to inspect almost the entire data structure (up to a logarithmic factor). We show similar lower bounds for problems satisfying a certain combinatorial properties of a coding theoretic flavor, and obtain (r+1)t{$\geq\Omega$}(n) for certain problems. Previously, no {$\omega$}(m) lower bounds were known on t in the general model for explicit Boolean problems, even for very small redundancies. By restricting our attention to systematic or index structures {$\phi$} satisfying {$\phi$}(x)=x{$\cdot\phi\ast$}(x) for some map {$\phi\ast$} (where {$\cdot$} denotes concatenation), we show similar lower bounds on the redundancy--query time tradeoff for the natural data structuring problems of Prefix Sum and Substring Search.},
  keywords = {cell probe,data structure,lower bound,Polynomial evaluation with preprocessing,query,query time,static,Succinct data structures,time-space},
  annotation = {H. Buhrman, P.B. Miltersen, J. Radhakrishnan, S. Venkatesh, Are bitvectors optimal? in: Proc. 32th Annual ACM Symposium on Theory of Computing, STOC'00, pp. 449--458\\
A. Chakrabarti, B. Chazelle, B. Gum, A. Lvov, A lower bound on the complexity of approximate nearest-neighbor searching on the Hamming Cube, in: Proc. 31th Annual ACM Symposium on Theory of Computing, STOC'99, pp. 305--311\\
\\
\\
\\
\\
A. G{\'a}l, P.B. Miltersen, The cell probe complexity of succinct data structures, in: Proc. of 30th International Colloquium on Automata, Languages and Programming, ICALP'03, pp. 332--344\\
\\
R. Grossi, J.S. Vitter, Compressed suffix arrays and suffix trees with applications to text indexing and string matching, in: Proc. 32th Annual ACM Symposium on Theory of Computing, STOC'00, pp. 397--406\\
R. Grossi, A. Gupta, J.S. Vitter, High-order entropy-compressed text indexes, in: Proc. 14th Annual ACM-SIAM Symposium on Discrete Algorithms, SODA'03, pp. 841--850\\
\\
\\
\\
\\
U. Manber, S. Wu, GLIMPSE --- A Tool to Search Through Entire Filesystems. White Paper. Available at: http://glimpse.cs.arizona.edu/\\
P.B. Miltersen, The bitprobe complexity measure revisited, in: 10th Annual Symposium on Theoretical Aspects of Computer Science, STACS'93, pp. 662--671\\
\\
\\
\\
\\
M. Naor, L. Schulman, A. Srinivasan, Splitters and near optimal derandomization, in: Proc. of 36th Annual IEEE Symposium on Foundations of Computer Science, FOCS'95, pp. 182--191\\
\\
\\
J. Radhakrishnan, V. Raman, S.S. Rao, Explicit deterministic constructions for membership in the bitprobe model, in: Proc. of 9th Annual European Symposium on Algorithms, ESA'01, pp. 290--299\\
R. Raman, V. Raman, S.S. Rao, Succinct dynamic data structures, in: Proc. of 7th International Workshop on Algorithms and Data Structures, WADS'01, pp. 426--437},
  file = {/Users/tulasi/Zotero/storage/7QSVZN7M/Gl and Miltersen - 2007 - The cell probe complexity of succinct data structures.pdf;/Users/tulasi/Zotero/storage/8RC3XC5S/main.pdf}
}

@article{gallAlgebraicComplexityTheory2014,
  title = {Algebraic Complexity Theory and Matrix Multiplication},
  author = {Gall, F.},
  year = {2014},
  doi = {10.1145/2608628.2627493},
  abstract = {This tutorial will give an overview of algebraic complexity theory focused on bilinear complexity, and describe several powerful techniques to analyze the complexity of computational problems from linear algebra, in particular matrix multiplication. The presentation of these techniques will follow the history of progress on constructing asymptotically fast algorithms for matrix multiplication, and include its most recent developments.},
  citationcount = {729},
  venue = {International Symposium on Symbolic and Algebraic Computation}
}

@article{gallerAnImprovedEquivalence1964,
  title = {An Improved Equivalence Algorithm},
  author = {Galler, B. and Fischer, Michael J.},
  year = {1964},
  doi = {10.1145/364099.364331},
  abstract = {An algorithm for assigning storage on the basis of EQUIVALENCE, DIMENSION and COMMON declarations is presented. The algorithm is based on a tree structure, and has reduced computation time by 40 percent over a previously published algorithm by identifying all equivalence classes with one scan of the EQUIVALENCE declarations. The method is applicable in any problem in which it is necessary to identify equivalence classes, given the element pairs defining the equivalence relation.},
  citationcount = {300},
  venue = {CACM}
}

@article{gallFasterAlgorithmsFor2012,
  title = {Faster Algorithms for Rectangular Matrix Multiplication},
  author = {Gall, F.},
  year = {2012},
  doi = {10.1109/FOCS.2012.80},
  abstract = {Let {$\alpha$} be the maximal value such that the product of an n {\texttimes} n{\textexclamdown}sup{\textquestiondown}{$\alpha$}{\textexclamdown}/sup{\textquestiondown} matrix by an n{\textexclamdown}sup{\textquestiondown}{$\alpha$}{\textexclamdown}/sup{\textquestiondown} {\texttimes} n matrix can be computed with n{\textexclamdown}sup{\textquestiondown}2+o(1){\textexclamdown}/sup{\textquestiondown} arithmetic operations. In this paper we show that {$\alpha$} {\textquestiondown}; 0.30298, which improves the previous record {$\alpha$} {\textquestiondown}; 0.29462 by Coppersmith (Journal of Complexity, 1997). More generally, we construct a new algorithm for multiplying an n {\texttimes} n{\textexclamdown}sup{\textquestiondown}k{\textexclamdown}/sup{\textquestiondown} matrix by an n{\textexclamdown}sup{\textquestiondown}k{\textexclamdown}/sup{\textquestiondown} {\texttimes} n matrix, for any value k {$\neq$} 1. The complexity of this algorithm is better than all known algorithms for rectangular matrix multiplication. In the case of square matrix multiplication (i.e., for k = 1), we recover exactly the complexity of the algorithm by Coppersmith and Winograd (Journal of Symbolic Computation, 1990). These new upper bounds can be used to improve the time complexity of several known algorithms that rely on rectangular matrix multiplication. For example, we directly obtain a O(n{\textexclamdown}sup{\textquestiondown}2.5302{\textexclamdown}/sup{\textquestiondown})-time algorithm for the all-pairs shortest paths problem over directed graphs with small integer weights, where n denotes the number of vertices, and also improve the time complexity of sparse square matrix multiplication.},
  citationcount = {184},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{gallImprovedQuantumAlgorithm2014,
  title = {Improved Quantum Algorithm for Triangle Finding via Combinatorial Arguments},
  author = {Gall, F.},
  year = {2014},
  doi = {10.1109/FOCS.2014.31},
  abstract = {In this paper we present a quantum algorithm solving the triangle finding problem in unweighted graphs with query complexity {\~O}(n5/4), where n denotes the number of vertices in the graph. This improves the previous upper bound O(n9/7) = O(n1.285) recently obtained by Lee, Magniez and Santha. Our result shows, for the first time, that in the quantum query complexity setting unweighted triangle finding is easier than its edge-weighted version, since for finding an edge-weighted triangle Belovs and Rosmanis proved that any quantum algorithm requires O(n9/7/ {\textsurd}log n) queries. Our result also illustrates some limitations of the non-adaptive learning graph approach used to obtain the previous O(n9/7) upper bound since, even over unweighted graphs, any quantum algorithm for triangle finding obtained using this approach requires v(n9/7/ {\textsurd}log n) queries as well. To bypass the obstacles characterized by these lower bounds, our quantum algorithm uses combinatorial ideas exploiting the graph-theoretic properties of triangle finding, which cannot be used when considering edge-weighted graphs or the non-adaptive learning graph approach.},
  citationcount = {54},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {lower bound,non-adaptive,query,query complexity}
}

@article{gallMultipartyQuantumCommunication2017,
  title = {Multiparty Quantum Communication Complexity of Triangle Finding},
  author = {Gall, F. and Nakajima, S.},
  year = {2017},
  doi = {10.4230/LIPIcs.TQC.2017.6},
  abstract = {Triangle finding (deciding if a graph contains a triangle or not) is a central problem in quantum query complexity. The quantum communication complexity of this problem, where the edges of the graph are distributed among the players, was considered recently by Ivanyos et al. in the two- party setting. In this paper we consider its k-party quantum communication complexity with k {\textquestiondown}= 3. Our main result is a  O(m{\textasciicircum}(7/12))-qubit protocol, for any constant number of players k, deciding with high probability if a graph with m edges contains a triangle or not. Our approach makes connections between the multiparty quantum communication complexity of triangle finding and the quantum query complexity of graph collision, a well-studied problem in quantum query complexity.},
  citationcount = {4},
  venue = {Theory of Quantum Computation, Communication, and Cryptography},
  keywords = {communication,communication complexity,query,query complexity}
}

@article{gallQuantumAlgorithmFor2015,
  title = {Quantum Algorithm for Triangle Finding in Sparse Graphs},
  author = {Gall, F. and Nakajima, S.},
  year = {2015},
  doi = {10.1007/s00453-016-0267-z},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {Algorithmica}
}

@article{glThreeQueryLocally2011,
  title = {Three-Query Locally Decodable Codes with Higher Correctness Require Exponential Length},
  author = {G{\'a}l, A. and Mills, Andrew},
  year = {2011},
  doi = {10.1145/2077336.2077338},
  abstract = {Locally decodable codes are error-correcting codes with the extra property that, in order to retrieve the value of a single input position, it is sufficient to read a small number of positions of the codeword. We refer to the probability of getting the correct value as the correctness of the decoding algorithm. A breakthrough result by Yekhanin [2007] showed that 3-query linear locally decodable codes may have subexponential length. The construction of Yekhanin, and the three query constructions that followed, achieve correctness only up to a certain limit which is 1\,-\,3{$\Delta$} for nonbinary codes, where an adversary is allowed to corrupt up to {$\Delta$} fraction of the codeword. The largest correctness for a subexponential length 3-query binary code is achieved in a construction by Woodruff [2008], and it is below 1\,-\,3{$\Delta$}. We show that achieving slightly larger correctness (as a function of {$\Delta$}) requires exponential codeword length for 3-query codes. Previously, there were no larger than quadratic lower bounds known for locally decodable codes with more than 2 queries, even in the case of 3-query linear codes. Our lower bounds hold for linear codes over arbitrary finite fields and for binary nonlinear codes. Considering larger number of queries, we obtain lower bounds for q-query codes for q\,{\textquestiondown}\,3, under certain assumptions on the decoding algorithm that have been commonly used in previous constructions. We also prove bounds on the largest correctness achievable by these decoding algorithms, regardless of the length of the code. Our results explain the limitations on correctness in previous constructions using such decoding algorithms. In addition, our results imply trade-offs on the parameters of error-correcting data structures.},
  citationcount = {14},
  venue = {TOCT},
  keywords = {data structure,lower bound,query}
}

@inproceedings{galTightBoundsComputing2012,
  title = {Tight Bounds on Computing Error-Correcting Codes by Bounded-Depth Circuits with Arbitrary Gates},
  booktitle = {Proceedings of the Forty-Fourth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {G{\'a}l, Anna and Hansen, Kristoffer Arnsfelt and Kouck{\'y}, Michal and Pudl{\'a}k, Pavel and Viola, Emanuele},
  year = {2012},
  month = may,
  pages = {479--494},
  publisher = {ACM},
  address = {New York New York USA},
  doi = {10.1145/2213977.2214023},
  url = {https://dl.acm.org/doi/10.1145/2213977.2214023},
  urldate = {2024-09-20},
  isbn = {978-1-4503-1245-5},
  langid = {english},
  annotation = {A. K. Chandra , S. Fortune , and R. J. Lipton . Lower bounds for constant depth circuits for prefix problems . In 10th Colloquium on Automata, Languages and Programming (ICALP'83) , volume 154 of LNCS , pages 109 -- 117 . Springer , 1983 . A. K. Chandra, S. Fortune, and R. J. Lipton. Lower bounds for constant depth circuits for prefix problems. In 10th Colloquium on Automata, Languages and Programming (ICALP'83), volume 154 of LNCS, pages 109--117. Springer, 1983.\\
\\
\\
\\
\\
L. R. Ford and D. R. Fulkerson . Flows in networks . Princeton University Press , 1962 . L. R. Ford and D. R. Fulkerson. Flows in networks. Princeton University Press, 1962.\\
S. Gelfand , R. Dobrushin , and M. Pinsker . On the complexity of coding . In 2nd International Symposium on Information Theory , pages 177 -- 184 . Akademiai Kiado , 1973 . S. Gelfand, R. Dobrushin, and M. Pinsker. On the complexity of coding. In 2nd International Symposium on Information Theory, pages 177--184. Akademiai Kiado, 1973.\\
\\
G. Hansel . Nombre minimal de contacts de fermature n{\'e}cessaires pour r{\'e}aliser une fonction bool{\'e}enne sym{\'e}trique de n variables . C. R. Acad. Sci. Paris , 258 : 6037 -- 6040 , 1964 . G. Hansel. Nombre minimal de contacts de fermature n{\'e}cessaires pour r{\'e}aliser une fonction bool{\'e}enne sym{\'e}trique de n variables. C. R. Acad. Sci. Paris, 258:6037--6040, 1964.\\
\\
S. Jukna . Boolean Function Complexity: Advances and Frontiers , volume 27 of Algorithms and Combinatorics . Springer , 2012 . S. Jukna. Boolean Function Complexity: Advances and Frontiers, volume 27 of Algorithms and Combinatorics. Springer, 2012.\\
\\
\\
\\
\\
P. B. Miltersen . Error correcting codes, perfect hashing circuits, and deterministic dynamic dictionaries . In 9th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA'98) , pages 556 -- 563 . ACM Press , 1998 . P. B. Miltersen. Error correcting codes, perfect hashing circuits, and deterministic dynamic dictionaries. In 9th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA'98), pages 556--563. ACM Press, 1998.\\
J. G. Oxley . Matroid theory . Oxford University Press , 1992 . J. G. Oxley. Matroid theory. Oxford University Press, 1992.\\
\\
\\
\\
\\
D. Spielman . Computationally Efficient Error-Correcting Codes and Holographic Proofs. PhD thesis , Massachusetts Institute of Technology , 1995 . D. Spielman. Computationally Efficient Error-Correcting Codes and Holographic Proofs. PhD thesis, Massachusetts Institute of Technology, 1995.\\
\\
\\
\\
D. J. Welsh . Matroid theory . Academic Press , London , 1976 . D. J. Welsh. Matroid theory. Academic Press, London, 1976.},
  file = {/Users/tulasi/Zotero/storage/PG3UQGTL/Gl et al. - 2012 - Tight bounds on computing error-correcting codes by bounded-depth circuits with arbitrary gates.pdf}
}

@article{glTightBoundsOn2012,
  title = {Tight Bounds on Computing Error-Correcting Codes by Bounded-Depth Circuits with Arbitrary Gates},
  author = {G{\'a}l, A. and Hansen, Kristoffer Arnsfelt and Kouck{\'y}, M. and Pudl{\'a}k, P. and Viola, Emanuele},
  year = {2012},
  doi = {10.1145/2213977.2214023},
  abstract = {We bound the minimum number w of wires needed to compute any (asymptotically good) error-correcting code C:\{0,1\}{\textexclamdown}sup{\textquestiondown}{\textohm}(n){\textexclamdown}/sup{\textquestiondown}{$\rightarrow$}\{0,1\}{\textexclamdown}sup{\textquestiondown}n{\textexclamdown}/sup{\textquestiondown} with minimum distance {\textohm}(n), using unbounded fan-in circuits of depth d with arbitrary gates. Our main results are: 1) if d=2, then w={$\Theta$}(n (lgn/lglgn){\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}); 2) if d=3, then w={$\Theta$}(nlglgn); 3) if d=2k or d=2k+1 for some integer k {$\geq$} 2, then w={$\Theta$}(n{$\lambda$}{\textexclamdown}sub{\textquestiondown}k{\textexclamdown}/sub{\textquestiondown}(n)), where {$\lambda$}{\textexclamdown}sub{\textquestiondown}1{\textexclamdown}/sub{\textquestiondown}(n)={$\lceil$}lgn{$\rceil$}, {$\lambda$}{\textexclamdown}sub{\textquestiondown}i+1{\textexclamdown}/sub{\textquestiondown}(n)={$\lambda$}{\textexclamdown}sub{\textquestiondown}i{\textexclamdown}/sub{\textquestiondown}*(n), and the * operation gives how many times one has to iterate the function {$\lambda$}{\textexclamdown}sub{\textquestiondown}i{\textexclamdown}/sub{\textquestiondown} to reach a value at most 1 from the argument n; and 4) if d=lg*n, then w=O(n). For depth d=2, our {\textohm}(n (lgn/lglgn){\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}) lower bound gives the largest known lower bound for computing any linear map. The upper bounds imply that a (necessarily dense) generator matrix for our code can be written as the product of two sparse matrices. Using known techniques, we also obtain similar (but not tight) bounds for computing pairwise-independent hash functions. Our lower bounds are based on a superconcentrator-like condition that the graphs of circuits computing good codes must satisfy. This condition is provably intermediate between superconcentrators and their weakenings considered before.},
  citationcount = {31},
  venue = {IEEE Transactions on Information Theory},
  keywords = {lower bound}
}

@article{ganAnAlmostOptimal2022,
  title = {An Almost Optimal Algorithm for Unbounded Search with Noisy Information},
  author = {Gan, Junhao and Wirth, Anthony and Zhang, Xin},
  year = {2022},
  doi = {10.4230/LIPIcs.SWAT.2022.25},
  abstract = {Given a sequence of integers, S = s 1 , s 2 , . . . in ascending order, called the search domain , and an integer t , called the target , the predecessor problem asks for the target index N such that s N is the largest integer in S satisfying s N {$\leq$} t . We consider solving the predecessor problem with the least number of queries to a binary comparison oracle . For each query index i , the oracle returns whether s i {$\leq$} t or s i {\textquestiondown} t . In particular, we study the predecessor problem under the UnboundedNoisy setting, where (i) the search domain S is unbounded , i.e., n = {\textbar}S{\textbar} is unknown or infinite, and (ii) the binary comparison oracle is noisy . We denote the former setting by Unbounded and the latter by Noisy . In Noisy , the oracle, for each query, independently returns a wrong answer with a fixed constant probability 0 {\textexclamdown} p {\textexclamdown} 1 / 2. In particular, even for two queries on the same index i , the answers from the oracle may be different. Furthermore, with a noisy oracle, the goal is to correctly return the target index with probability at least 1 - Q , where 0 {\textexclamdown} Q {\textexclamdown} 1 / 2 is the failure probability . Our first result is an algorithm, called NoS , for Noisy that improves the previous result by Ben-Or and Hassidim [FOCS 2008] from an expected query complexity bound to a worst-case bound. We also achieve an expected query complexity bound, whose leading term has an optimal constant factor, matching the lower bound of Ben-Or and Hassidim. Building on NoS , we propose our NoSU algorithm, which correctly solves the predecessor problem in the UnboundedNoisy setting. We prove that the query complexity of NoSU is P k i =1 (log ( i ) N ) / (1 - H ( p )) + o (log N ) when log Q - 1 {$\in$} o (log N ), where N is the target index, k = log {$\ast$} N , the iterated logarithm, and H ( p ) is the entropy function. This improves the previous bound of O (log( N/Q ) / (1 - H ( p ))) by reducing the coefficient of the leading term from a large constant to 1. Moreover, we show that this upper bound can be further improved to (1 - Q ) P k i =1 (log ( i ) N ) / (1 - H ( p ))+ o (log N ) in expectation, with the constant in the leading term reduced to 1 - Q . Finally, we show that an information-theoretic lower bound on the expected query cost of the predecessor problem in UnboundedNoisy is at least (1 - Q )( P k i =1 log ( i ) N - 2 k ) / (1 - H ( p )) - 10. This implies the constant factor in the leading term of our expected upper bound is indeed optimal.},
  citationcount = {1},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {information theoretic,lower bound,query,query complexity}
}

@article{ganardiLowLatencySliding2022,
  title = {Low-Latency Sliding Window Algorithms for Formal Languages},
  author = {Ganardi, Moses and Jachiet, Louis and Lohrey, Markus and Schwentick, T.},
  year = {2022},
  doi = {10.48550/arXiv.2209.14835},
  abstract = {Low-latency sliding window algorithms for regular and context-free languages are studied, where latency refers to the worst-case time spent for a single window update or query. For every regular language L it is shown that there exists a constant-latency solution that supports adding and removing symbols independently on both ends of the window (the so-called two-way variable-size model). We prove that this result extends to all visibly pushdown languages. For deterministic 1-counter languages we present a \{O\}(n) latency sliding window algorithm for the two-way variable-size model where n refers to the window size. We complement these results with a conditional lower bound: there exists a fixed real-time deterministic context-free language L such that, assuming the OMV (online matrix vector multiplication) conjecture, there is no sliding window algorithm for L with latency n\textsuperscript{\{\vphantom\}}1/2-{$\epsilon$}\vphantom\{\} for any {$\epsilon>$}0, even in the most restricted sliding window model (one-way fixed-size model). The above mentioned results all refer to the unit-cost RAM model with logarithmic word size. For regular languages we also present a refined picture using word sizes \{O\}(1), \{O\}(n), and \{O\}(n).},
  citationcount = {Unknown},
  venue = {Foundations of Software Technology and Theoretical Computer Science},
  keywords = {lower bound,query,update}
}

@article{gangulyFullyFunctionalParameterized2022,
  title = {Fully Functional Parameterized Suffix Trees in Compact Space},
  author = {Ganguly, Arnab and Shah, Rahul and Thankachan, Sharma V.},
  year = {2022},
  doi = {10.4230/LIPIcs.ICALP.2022.65},
  abstract = {Two equal length strings are a parameterized match (p-match) iff there exists a one-to-one function that renames the symbols in one string to those in the other. The Parameterized Suffix Tree (PST) [Baker, STOC' 93] is a fundamental data structure that handles various string matching problems under this setting. The PST of a text T [1 , n ] over an alphabet {$\Sigma$} of size {$\sigma$} takes O ( n log n ) bits of space. It can report any entry in (parameterized) (i) suffix array, (ii) inverse suffix array, and (iii) longest common prefix (LCP) array in O (1) time. Given any pattern P as a query, a position i in T is an occurrence iff T [ i, i + {\textbar} P {\textbar} - 1] and P are a p-match. The PST can count the number of occurrences of P in T in time O ( {\textbar} P {\textbar} log {$\sigma$} ) and then report each occurrence in time proportional to that of accessing a suffix array entry. An important question is, can we obtain a compressed version of PST that takes space close to the text's size of n log {$\sigma$} bits and still support all three functionalities mentioned earlier ? In SODA' 17, Ganguly et al. answered this question partially by presenting an O ( n log {$\sigma$} ) bit index that can support (parameterized) suffix array and inverse suffix array operations in O (log n ) time. However, the compression of the (parameterized) LCP array and the possibility of faster suffix array and inverse suffix array queries in compact space were left open. In this work, we obtain a compact representation of the (parameterized) LCP array. With this result, in conjunction with three new (parameterized) suffix array representations, we obtain the first set of PST representations in o ( n log n ) bits (when log {$\sigma$} = o (log n )) as follows. Here {$\varepsilon$} {\textquestiondown} 0 is an arbitrarily small constant. The first trade-off is an improvement over Ganguly et al.'s result, whereas our third trade-off matches the optimal time performance of Baker's PST while squeezing the space by a factor roughly log {$\sigma$} n . We highlight that our trade-offs match the space-and-time bounds of the best-known compressed text indexes for exact pattern matching and further improvement is highly unlikely.},
  citationcount = {3},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,query}
}

@article{gangulyStabbingColorsIn2017,
  title = {Stabbing Colors in One Dimension},
  author = {Ganguly, Arnab and Hon, W. and Shah, Rahul},
  year = {2017},
  doi = {10.1109/DCC.2017.44},
  abstract = {Given n horizontal segments, each associated with a color from [{$\sigma$}], the Categorical Segment Stabbing problem is to find the distinct K colors stabbed by a vertical line. When the end-points of the segments are distinct and lie in [1, 2n], we present an (2 + {$\varepsilon$})n log {$\sigma$} + O(n)-bit index with O(K/{$\varepsilon$}) query time, where {$\varepsilon\in$} (0, 1].When the end-points are arbitrary real numbers, a standard reduction to the above scenario improves the existing bounds of Janardan and Lopez. We also present results for few other variations: {$\bullet$} reporting the top-k colors that are stabbed, where each color has a fixed priority. {$\bullet$} handling these scenarios when the given segments form a tree range.},
  citationcount = {1},
  venue = {Data Compression Conference},
  keywords = {query,query time,reduction}
}

@article{gaoOnTheUtility2022,
  title = {On the Utility of Buffers in Pick-n-Swap Based Lattice Rearrangement},
  author = {Gao, Kai and Yu, Jingjin},
  year = {2022},
  doi = {10.1109/ICRA48891.2023.10161182},
  abstract = {We investigate the utility of employing multiple buffers in solving a class of rearrangement problems with pick- n-swap manipulation primitives. In this problem, objects stored randomly in a lattice are to be sorted using a robot arm with k 1 swap spaces or buffers, capable of holding up to k objects on its end-effector simultaneously. On the structural side, we show that the addition of each new buffer brings diminishing returns in saving the end-effector travel distance while holding the total number of pick-n-swap operations at a minimum. This is due to an interesting recursive cycle structure in random m-permutation, where the largest cycle covers over 60},
  citationcount = {4},
  venue = {IEEE International Conference on Robotics and Automation}
}

@article{garlingInequalitiesAJourney2007,
  title = {Inequalities: A Journey into Linear Analysis},
  author = {Garling, D. J. H.},
  year = {2007},
  doi = {10.1017/CBO9780511755217.009},
  abstract = {This book contains a wealth of inequalities used in linear analysis, and explains in detail how they are used. The book begins with Cauchy's inequality and ends with Grothendieck's inequality, in between one finds the Loomis-Whitney inequality, maximal inequalities, inequalities of Hardy and of Hilbert, hypercontractive and logarithmic Sobolev inequalities, Beckner's inequality, and many, many more. The inequalities are used to obtain properties of function spaces, linear operators between them, and of special classes of operators such as absolutely summing operators. This textbook complements and fills out standard treatments, providing many diverse applications: for example, the Lebesgue decomposition theorem and the Lebesgue density theorem, the Hilbert transform and other singular integral operators, the martingale convergence theorem, eigenvalue distributions, Lidskii's trace formula, Mercer's theorem and Littlewood's 4/3 theorem. It will broaden the knowledge of postgraduate and research students, and should also appeal to their teachers, and all who work in linear analysis.},
  citationcount = {246},
  venue = {No venue available}
}

@article{gashkovComplexityOfComputation2013,
  title = {Complexity of Computation in Finite Fields},
  author = {Gashkov, S. and Sergeev, I.},
  year = {2013},
  doi = {10.1007/s10958-013-1350-5},
  abstract = {No abstract available},
  citationcount = {30},
  venue = {Journal of Mathematical Sciences}
}

@article{gsieniecPushingTheOnline2019,
  title = {Pushing the Online Matrix-Vector Conjecture off-Line and Identifying Its Easy Cases},
  author = {G{\k a}sieniec, L. and Jansson, J. and Levcopoulos, C. and Lingas, A. and Persson, Mia},
  year = {2019},
  doi = {10.1007/978-3-030-18126-0_14},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Frontiers in Algorithmics}
}

@article{gsieniecPushingTheOnline2021,
  title = {Pushing the {{Online Boolean Matrix-vector Multiplication}} Conjecture off-Line and Identifying Its Easy Cases},
  author = {G{\k a}sieniec, L. and Jansson, J. and Levcopoulos, C. and Lingas, A. and Persson, Mia},
  year = {2021},
  doi = {10.1016/j.jcss.2020.12.004},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Journal of computer and system sciences (Print)}
}

@article{gathenComputingFrobeniusMaps1992,
  title = {Computing {{Frobenius}} Maps and Factoring Polynomials},
  author = {Gathen, J. and Shoup, V.},
  year = {1992},
  doi = {10.1007/BF01272074},
  abstract = {No abstract available},
  citationcount = {210},
  venue = {Symposium on the Theory of Computing}
}

@article{gathenIntervalPartitionsAnd2012,
  title = {Interval Partitions and Polynomial Factorization},
  author = {Gathen, J. and Panario, D. and Richmond, B.},
  year = {2012},
  doi = {10.1007/s00453-011-9537-y},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Algorithmica}
}

@article{gathenModernComputerAlgebra2002,
  title = {Modern Computer Algebra},
  author = {Gathen, J. and Gerhard, J{\"u}rgen},
  year = {2002},
  doi = {10.1145/582475.582479},
  abstract = {Computer algebra systems are now ubiquitous in all areas of science and engineering. This highly successful textbook, widely regarded as the 'bible of computer algebra', gives a thorough introduction to the algorithmic basis of the mathematical engine in computer algebra systems. Designed to accompany one- or two-semester courses for advanced undergraduate or graduate students in computer science or mathematics, its comprehensiveness and reliability has also made it an essential reference for professionals in the area. Special features include: detailed study of algorithms including time analysis; implementation reports on several topics; complete proofs of the mathematical underpinnings; and a wide variety of applications (among others, in chemistry, coding theory, cryptography, computational logic, and the design of calendars and musical scales). A great deal of historical information and illustration enlivens the text. In this third edition, errors have been corrected and much of the Fast Euclidean Algorithm chapter has been renovated.},
  citationcount = {2053},
  venue = {SIGA}
}

@article{gaudryGenus2Point2012,
  title = {Genus 2 Point Counting over Prime Fields},
  author = {Gaudry, P. and Schost, {\'E}.},
  year = {2012},
  doi = {10.1016/j.jsc.2011.09.003},
  abstract = {No abstract available},
  citationcount = {81},
  venue = {Journal of symbolic computation}
}

@article{gawrychowskiFullyDynamicApproximation2020,
  title = {Fully Dynamic Approximation of {{LIS}} in Polylogarithmic Time},
  author = {Gawrychowski, Pawe{\l} and Janczewski, Wojciech},
  year = {2020},
  doi = {10.1145/3406325.3451137},
  abstract = {We revisit the problem of maintaining the longest increasing subsequence (LIS) of an array under (i) inserting an element, and (ii) deleting an element of an array. In a recent breakthrough, Mitzenmacher and Seddighin [STOC 2020] designed an algorithm that maintains an O((1/{\cyrchar\cyrie})O(1/{\cyrchar\cyrie}))-approximation of LIS under both operations with worst-case update time {\~O}(n{\cyrchar\cyrie}), for any constant {\cyrchar\cyrie}{\textquestiondown}0 ({\~O} hides factors polynomial in logn, where n is the length of the input). We exponentially improve on their result by designing an algorithm that maintains an (1+{\cyrchar\cyrie}) approximation of LIS under both operations with worst-case update time {\~O}({\cyrchar\cyrie}-5). Instead of working with the grid packing technique introduced by Mitzenmacher and Seddighin, we take a different approach building on a new tool that might be of independent interest: LIS sparsification. A particularly interesting consequence of our result is an improved solution for the so-called Erd{\H o}s-Szekeres partitioning, in which we seek a partition of a given permutation of \{1,2,{\dots},n\} into O({\textsurd}n) monotone subsequences. This problem has been repeatedly stated as one of the natural examples in which we see a large gap between the decision-tree complexity and algorithmic complexity. The result of Mitzenmacher and Seddighin implies an O(n1+{\cyrchar\cyrie}) time solution for this problem, for any {\cyrchar\cyrie}{\textquestiondown}0. Our algorithm (in fact, its simpler decremental version) further improves this to {\~O}(n).},
  citationcount = {12},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,update,update time}
}

@article{gawrychowskiOptimalDynamicStrings2015,
  title = {Optimal Dynamic Strings},
  author = {Gawrychowski, Pawe{\l} and Karczmarz, Adam and Kociumaka, Tomasz and Lacki, Jakub and Sankowski, P.},
  year = {2015},
  doi = {10.1137/1.9781611975031.99},
  abstract = {In this paper we study the fundamental problem of maintaining a dynamic collection of strings under the following operations: concat - concatenates two strings, split - splits a string into two at a given position, compare - finds the lexicographical order (less, equal, greater) between two strings, LCP - calculates the longest common prefix of two strings. We present an efficient data structure for this problem, where an update requires only O(n) worst-case time with high probability, with n being the total length of all strings in the collection, and a query takes constant worst-case time. On the lower bound side, we prove that even if the only possible query is checking equality of two strings, either updates or queries take amortized {\textohm}(n) time; hence our implementation is optimal. Such operations can be used as a basic building block to solve other string problems. We provide two examples. First, we can augment our data structure to provide pattern matching queries that may locate occurrences of a specified pattern p in the strings in our collection in optimal O({\textbar}p{\textbar}) time, at the expense of increasing update time to O({$^2$}n). Second, we show how to maintain a history of an edited text, processing updates in O(tt) time, where t is the number of edits, and how to support pattern matching queries against the whole history in O({\textbar}p{\textbar}tt) time. Finally, we note that our data structure can be applied to test dynamic tree isomorphism and to compare strings generated by dynamic straight-line grammars.},
  citationcount = {60},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,lower bound,query,update,update time}
}

@article{gawrychowskiSublinearDynamicInterval2022,
  title = {Sublinear Dynamic Interval Scheduling (on One or Multiple Machines)},
  author = {Gawrychowski, Pawe{\l} and Pokorski, Karol},
  year = {2022},
  doi = {10.48550/arXiv.2203.14310},
  abstract = {We revisit the complexity of the classical Interval Scheduling in the dynamic setting. In this problem, the goal is to maintain a set of intervals under insertions and deletions and report the size of the maximum size subset of pairwise disjoint intervals after each update. Nontrivial approximation algorithms are known for this problem, for both the unweighted and weighted versions [Henzinger, Neumann, Wiese, SoCG 2020]. Surprisingly, it was not known if the general exact version admits an exact solution working in sublinear time, that is, without recomputing the answer after each update. Our first contribution is a structure for Dynamic Interval Scheduling with amortized \{O\}\vphantom\{\}(n\textsuperscript{\{\vphantom\}}1/3\vphantom\{\}) update time. Then, building on the ideas used for the case of one machine, we design a sublinear solution for any constant number of machines: we describe a structure for Dynamic Interval Scheduling on m{$\geq$}2 machines with amortized \{O\}\vphantom\{\}(n\textsuperscript{\{\vphantom\}}1-1/m\vphantom\{\}) update time. We complement the above results by considering Dynamic Weighted Interval Scheduling on one machine, that is maintaining (the weight of) the maximum weight subset of pairwise disjoint intervals. We show an almost linear lower bound (conditioned on the hardness of Minimum Weight k-Clique) for the update/query time of any structure for this problem. Hence, in the weighted case one should indeed seek approximate solutions.},
  citationcount = {1},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {dynamic,lower bound,query,query time,update,update time}
}

@article{gawrychowskiSubmatrixMaximumQueries2015,
  title = {Submatrix Maximum Queries in Monge Matrices Are Equivalent to Predecessor Search},
  author = {Gawrychowski, Pawe{\l} and Mozes, S. and Weimann, Oren},
  year = {2015},
  doi = {10.1007/978-3-662-47672-7_47},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {query}
}

@article{gawrychowskiSubmatrixMaximumQueries2020,
  title = {Submatrix Maximum Queries in Monge and Partial Monge Matrices Are Equivalent to Predecessor Search},
  author = {Gawrychowski, Pawe{\l} and Mozes, S. and Weimann, Oren},
  year = {2020},
  doi = {10.1145/3381416},
  abstract = {We present an optimal data structure for submatrix maximum queries in n{\texttimes} n Monge matrices. Our result is a two-way reduction showing that the problem is equivalent to the classical predecessor problem in a universe of polynomial size. This gives a data structure of O(n) space that answers submatrix maximum queries in O(log log n) time, as well as a matching lower bound, showing that O(log log n) query-time is optimal for any data structure of size O(npolylog(n)). Our result settles the problem, improving on the O(log2 n) query time in SODA'12, and on the O(log n) query-time in ICALP'14. In addition, we show that partial Monge matrices can be handled in the same bounds as full Monge matrices. In both previous results, partial Monge matrices incurred additional inverse-Ackermann factors.},
  citationcount = {4},
  venue = {ACM Trans. Algorithms},
  keywords = {data structure,lower bound,query,query time,reduction}
}

@article{gawrychowskiTheNearestColored2017,
  title = {The Nearest Colored Node in a Tree},
  author = {Gawrychowski, Pawe{\l} and Landau, G. M. and Mozes, S. and Weimann, Oren},
  year = {2017},
  doi = {10.1016/j.tcs.2017.08.021},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Theoretical Computer Science}
}

@article{gawrychowskiWeightedAncestorsIn2014,
  title = {Weighted Ancestors in Suffix Trees},
  author = {Gawrychowski, Pawe{\l} and Lewenstein, Moshe and Nicholson, Patrick K.},
  year = {2014},
  doi = {10.1007/978-3-662-44777-2_38},
  abstract = {No abstract available},
  citationcount = {45},
  venue = {Embedded Systems and Applications}
}

@article{geckRewritingWithAcyclic2022,
  title = {Rewriting with Acyclic Queries: {{Mind}} Your Head},
  author = {Geck, Gaetano and Keppeler, Jens and Schwentick, T. and Spinrath, Christopher},
  year = {2022},
  doi = {10.46298/lmcs-19(4:17)2023},
  abstract = {The paper studies the rewriting problem, that is, the decision problem whether, for a given conjunctive query Q and a set \{V\} of views, there is a conjunctive query Q' over \{V\} that is equivalent to Q, for cases where the query, the views, and/or the desired rewriting are acyclic or even more restricted. It shows that, if Q itself is acyclic, an acyclic rewriting exists if there is any rewriting. An analogous statement also holds for free-connex acyclic, hierarchical, and q-hierarchical queries. Regarding the complexity of the rewriting problem, the paper identifies a border between tractable and (presumably) intractable variants of the rewriting problem: for schemas of bounded arity, the acyclic rewriting problem is NP-hard, even if both Q and the views in \{V\} are acyclic or hierarchical. However, it becomes tractable if the views are free-connex acyclic (i.e., in a nutshell, their body is (i) acyclic and (ii) remains acyclic if their head is added as an additional atom).},
  citationcount = {2},
  venue = {International Conference on Database Theory},
  keywords = {query}
}

@article{geffertSofsem2008Theory2008,
  title = {{{SOFSEM}} 2008: {{Theory}} and Practice of Computer Science, 34th Conference on Current Trends in Theory and Practice of Computer Science, Nov{\'y} Smokovec, Slovakia, January 19-25, 2008, Proceedings},
  author = {Geffert, V. and Karhum{\"a}ki, J. and Bertoni, A. and Preneel, B. and N{\'a}vrat, P. and Bielikov{\'a}, M.},
  year = {2008},
  doi = {10.1007/978-3-540-77566-9},
  abstract = {No abstract available},
  citationcount = {86},
  venue = {Conference on Current Trends in Theory and Practice of Informatics}
}

@article{geissmannParallelMinimumCuts2018,
  title = {Parallel Minimum Cuts in Near-Linear Work and Low Depth},
  author = {Geissmann, Barbara and Gianinazzi, Lukas},
  year = {2018},
  doi = {10.1145/3210377.3210393},
  abstract = {We present the first near-linear work and poly-logritharithmic depth algorithm for computing a minimum cut in a graph, while previous parallel algorithms with poly-logarithmic depth required at least quadratic work in the number of vertices. In a graph with n vertices and m edges, our algorithm computes the correct result with high probability in O(m{\l}og{$^4$}n) work and O({\l}og{$^3$}n) depth. This result is obtained by parallelizing a data structure that aggregates weights along paths in a tree and by exploiting the connection between minimum cuts and approximate maximum packings of spanning trees. In addition, our algorithm improves upon bounds on the number of cache misses incurred to compute a minimum cut.},
  citationcount = {23},
  venue = {ACM Symposium on Parallelism in Algorithms and Architectures},
  keywords = {data structure}
}

@article{geissmannParallelMinimumCuts2021,
  title = {Parallel Minimum Cuts in Near-Linear Work and Low Depth},
  author = {Geissmann, Barbara and Gianinazzi, Lukas},
  year = {2021},
  doi = {10.1145/3460890},
  abstract = {We present the first near-linear work and poly-logarithmic depth algorithm for computing a minimum cut in an undirected graph. Previous parallel algorithms with poly-logarithmic depth required at least quadratic work in the number of vertices. In a graph with n vertices and m edges, our randomized algorithm computes the minimum cut with high probability in O(m log4 n) work and O(log3 n) depth. This result is obtained by parallelizing a data structure that aggregates weights along paths in a tree, in addition exploiting the connection between minimum cuts and approximate maximum packings of spanning trees. In addition, our algorithm improves upon bounds on the number of cache misses incurred to compute a minimum cut.},
  citationcount = {Unknown},
  venue = {ACM Transactions on Parallel Computing},
  keywords = {data structure}
}

@article{gelleRecognizingUnionFind2015,
  title = {Recognizing Union-Find Trees Is {{NP-complete}}},
  author = {Gelle, Kitti and Iv{\'a}n, Szabolcs},
  year = {2015},
  doi = {10.1016/j.ipl.2017.11.003},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Information Processing Letters}
}

@article{gelleRecognizingUnionFind2017,
  title = {Recognizing Union-Find Trees Built up Using Union-by-Rank Strategy Is {{NP-complete}}},
  author = {Gelle, Kitti and Iv{\'a}n, Szabolcs},
  year = {2017},
  doi = {10.1007/978-3-319-60252-3_12},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Workshop on Descriptional Complexity of Formal Systems}
}

@article{gelleRecognizingUnionFind2019,
  title = {Recognizing Union-Find Trees Is {{NP-complete}}, Even without Rank Info},
  author = {Gelle, Kitti and Iv{\'a}n, Szabolcs},
  year = {2019},
  doi = {10.1142/s0129054119400276},
  abstract = {Disjoint-Set forests, consisting of Union-Find trees, are data structures having a widespread practical application due to their efficiency. Despite them being well-known, no exact structural characterization of these trees is known (such a characterization exists for Union trees which are constructed without using path compression) for the case assuming union-by-rank strategy for merging. In this paper we provide such a characterization by means of a simple PUSH operation and show that the decision problems whether a (ranked or unranked) tree is a Union-Find tree are NP-complete, complementing our earlier similar result for the union-by-size strategy.},
  citationcount = {1},
  venue = {International Journal of Foundations of Computer Science},
  keywords = {data structure}
}

@article{georghinkelExtensibleApproachImplicit2019,
  title = {An Extensible Approach to Implicit Incremental Model Analyses},
  author = {Georg Hinkel and R. Heinrich and Ralf H. Reussner},
  year = {2019},
  journal = {Journal of Software and Systems Modeling},
  doi = {10.1007/s10270-019-00719-y},
  annotation = {Citation Count: 11}
}

@article{georghinkelImplicitIncrementalModel2021,
  title = {Implicit {{Incremental Model Analyses}} and {{Transformations}}},
  author = {Georg Hinkel},
  year = {2021},
  doi = {10.5445/KSP/1000080522},
  abstract = {In vielen Ingenieursdisziplinen werden Modelle verwendet, um Systeme verschiedenster Art auf einem hohen Abstraktionsgrad zu beschreiben. Auf diesem Abstraktionsgrad ist es haufig einfacher, Aussagen uber den  Zustand des Systems zu treffen.    Wenn sich Modelle eines Systems andern -- beispielsweise, weil sich das System selbst geandert hat -- mussen Analysen auf Grundlage dieses Modells jedoch neu berechnet werden, um weiterhin gultig zu sein. In vielen Fallen ist diese Neuberechnung der Analyseergebnisse zeitkritisch. Da sich oft nur kleine Teile des Modells andern, konnten zwar grose Teile des letzten Analysedurchlaufs durch eine inkrementelle Ausfuhrung der Analyse wiederverwendet werden, in der Praxis ist eine solche Inkrementalisierung aber nicht trivial und oft fehleranfallig.    Eine Losungsmoglichkeit fur dieses Problem bietet der Ansatz der impliziten Inkrementalisierung, bei der ein inkrementeller Algorithmus fur eine gegebene Analyse aus der Batch-Spezifikation abgeleitet wird. Aus der Spezifikation wird ein dynamischer Abhangigkeitsgraph konstruiert, der es erlaubt, nur die Teile einer Analyse neu auszuwerten, die von einer Anderung tatsachlich betroffen sind. Damit lassen sich Vorteile einer Inkrementalisierung nutzen, ohne dass der Code angepasst werden muss und die Lesbarkeit des Analysecodes leidet.    Leider unterstutzen derzeitige Verfahren fur implizite Inkrementalisierung nur eine bestimmte Klasse von Analysen, sind auf eine Inkrementalisierung auf Ebene von einzelnen Instruktionen beschrankt oder benotigen eine explizite Zustandsverwaltung. Auch mit diesen Verbesserungen ist unklar, in welchen Fallen eine Inkrementalisierung Vorteile bringt, da in einigen Szenarien Anderungen Schmetterlingseffekte verursachen konnen und eine Wiederverwertung des letzten Analysedurchlaufs keinerlei Beschleunigungspotential hat.    Diese Dissertation behandelt diese Probleme bei impliziter Inkrementalisierung von Modellanalysen mittels mehrerer Verfahren, die grostenteils kombinierbar sind. Desweiteren wird ein neuer Formalismus vorgestellt,  mit dessen Hilfe Inkrementalisierungssysteme auch fur uni- oder bidirektionale Modelltransformationen einsetzbar sind. Um die Korrektheit der entstehenden inkrementellen Modellanalysen zu definieren und zu zeigen,  wird Inkrementalisierung in Kategorientheorie als Funktor beschrieben.    Ein erstes Verfahren ermoglicht als direkte Konsequenz der formalen Darstellung die Inkrementalisierung auf Ebene von Methodenaufrufen, sodass fur haufig verwendete Operatoren eine optimierte Inkrementalisierung zur Verfugung gestellt werden kann. Durch Erweiterung des Funktors auf Verteilung lassen sich auf ahnliche Weise auch etwaige Speicherprobleme losen.    Ein zweites Verfahren vereinfacht die entstehenden dynamischen Abhangigkeitsgraphen, indem Teile der Analyse durch eine generalisierte Betrachtung von Modellanderungen mit mehreren Strategien zusammengefasst werden konnen. Die Auswahl der Strategien ermoglicht dem Entwickler eine Anpassung der Inkrementalisierung auf einen konkreten Anwendungsfall. Alternativ kann fur ein gegebenes Szenario auch durch automatische Entwurfsraumexploration eine (Pareto-) optimale Konfiguration hinsichtlich Speicherverbrauch und Antwortzeit der Aktualisierung eines Analyseergebnisses nach einer Modellanderung gefunden werden.    Die Kombination dieser Verfahren ermoglicht es, die Performanz von Inkrementalisierungen so zu verbessern, dass diese bis auf einmalige Initialisierung nie schlechter ist als die batchmasige Wiederholung der Analyse, in vielen Fallen aber teils deutlich schneller sein kann. Generische Operatoren, die in vielen Modellanalysen wiederverwendet werden, konnen fur die Inkrementalisierung durch geeignete Algorithmen spezifisch optimiert werden, wahrend komplexe Domanenlogik durch das System optimiert werden kann. Durch den impliziten Ansatz geschehen diese Verbesserungen  vollautomatisch und transparent fur den Entwickler der Modellanalyse.    Obwohl der so geschaffene Ansatz Turing-machtig und somit universell einsetzbar ist, gibt es doch gerade in der modellgetriebenen Entwicklung  eine Klasse von Artefakten, die eine besondere Betrachtung erfordern, da sie sich im Allgemeinen nur schwer mit gewohnlichen objekt-orientierten Sprachen beschreiben lassen: Modelltransformationen. Daher wird in dieser Dissertation ein neuer Formalismus und eine darauf aufbauende Sprache vorgestellt, die Modelltransformationen so beschreiben, dass diese leicht mit Hilfe eines Inkrementalisierungssystems inkrementell ausgefuhrt werden  konnen. Die Synchronisierung einer Modellanderung ist hierbei bewiesen korrekt und hippokratisch.    Alle Verfahren wurden implementiert und in das .NET Modeling Framework integriert, welches Entwickler auf der .NET Plattform bei der modellgetriebenen Entwicklung unterstutzen soll. Die entstandenen Vorteile aller Verfahren hinsichtlich Performanz werden anhand von sieben Fallstudien in verschiedenen Domanen validiert. Insbesondere werden hierzu funf Fallstudien des Transformation Tool Contests (TTC) der Jahre 2015 bis 2017 herangezogen, fur die auch mit anderen Ansatzen verfasste Losungen zur Verfugung stehen. Die Ausdrucksmachtigkeit der Modelltransformationssprache wird durch eine Transformation der in der modellgetriebenen Entwicklung weit verbreiteten Transformationssprache ATL in die neu geschaffene Transformationssprache validiert. Mithilfe dieser Transformation  wird weiterhin die Ausfuhrungsgeschwindigkeit von Modelltransformationen mit der von ATL in einigen Modelltransformationen verglichen.    Die Ergebnisse aus den Fallstudien zeigen gerade bei der Anwendung des Inkrementalisierungssystems auf Modelltransformationen deutliche Performance-Steigerungen im Vergleich zu herkommlichen Modelltransformationen, aber auch gegenuber anderen inkrementellen Modelltransformationssprachen zeigt der vorgestellte Ansatz deutliche Beschleunigungen, teils um mehrere Grosenordnungen. Insbesondere weisen die Fallstudien darauf hin, dass die benotigte Zeit fur die Propagation von Anderungen des Eingabemodells in vielen Fallen unabhangig von der Grose des Eingabemodells ist. Gerade bei grosen Eingabemodellen kommen so sehr hohe Beschleunigungen zustande.    Die Inkrementalisierung einer Analyse ist dabei immer an das Metamodell gebunden. In der Praxis verwenden aber die meisten eingesetzten Metamodelle nur den eingeschrankten Modellierungsstandard EMOF, der teilweise  zu einer unnotigen Komplexitat des Metamodells fuhrt und viele Analysen uberhaupt erst notwendig macht. Eine Erweiterung des Modellierungsstandards kann hier einige Klassen von Modellanalysen komplett uberflussig  machen und andere Analysen deutlich vereinfachen, sowie auch die Performance der entsprechenden Analyse beschleunigen.},
  annotation = {Citation Count: 7}
}

@article{georgiadis2FaultTolerant2023,
  title = {2-Fault-Tolerant Strong Connectivity Oracles},
  author = {Georgiadis, L. and Kosinas, E. and Tsokaktsis, Daniel},
  year = {2023},
  doi = {10.48550/arXiv.2311.00854},
  abstract = {We study the problem of efficiently answering strong connectivity queries under two vertex failures. Given a directed graph G with n vertices, we provide a data structure with O(nh) space and O(h) query time, where h is the height of a decomposition tree of G into strongly connected subgraphs. This immediately implies data structures with O(n\{n\}) space and O(\{n\}) query time for graphs of constant treewidth, and O(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}) space and O({\textsurd}\{n\}) query time for planar graphs. For general directed graphs, we give a refined version of our data structure that achieves O(n{\textsurd}\{m\}) space and O({\textsurd}\{m\}) query time, where m is the number of edges of the graph. We also provide some simple BFS-based heuristics that seem to work remarkably well in practice. In the experimental part, we first evaluate various methods to construct a decomposition tree with small height h in practice. Then we provide efficient implementations of our data structures, and evaluate their empirical performance by conducting an extensive experimental study on graphs taken from real-world applications.},
  citationcount = {Unknown},
  venue = {Workshop on Algorithm Engineering and Experimentation},
  keywords = {data structure,query,query time}
}

@article{georgiadisDataStructuresFor2007,
  title = {Data Structures for Mergeable Trees},
  author = {Georgiadis, L. and Kaplan, Haim and Shafrir, Nira and Tarjan, R. and Werneck, Renato F.},
  year = {2007},
  doi = {10.1145/1921659.1921660},
  abstract = {Motivated by an application in computational geometry, we consider a novel variant of the problem of efficiently maintaining a forest of dynamic rooted trees. This variant includes an operation that merges two tree paths. In contrast to the standard problem, in which a single operation can only add or delete one arc, one merge can add and delete up to a linear number of arcs. In spite of this, we develop three different methods that need only polylogarithmic time per operation. The first method extends a solution of Farach and Thorup [1998] for the special case of paths. Each merge takes O(log2n) amortized time on an n-node forest and each standard dynamic tree operation takes O(log n) time; the latter bound is amortized, worst case, or randomized depending on the underlying data structure. For the special case that occurs in the motivating application, in which arbitrary arc deletions (cuts) do not occur, we give a method that takes O(log n) time per operation, including merging. This is best possible in a model of computation with an {\textohm}(n log n) lower bound for sorting n numbers, since such sorting can be done in O(n) tree operations. For the even-more-special case in which there are no cuts and no parent queries, we give a method that uses standard dynamic trees as a black box: each mergeable tree operation becomes a constant number of standard dynamic tree operations. This third method can also be used in the motivating application, but only by changing the algorithm in the application. Each of our three methods needs different analytical tools and reveals different properties of dynamic trees.},
  citationcount = {13},
  venue = {TALG},
  keywords = {data structure,dynamic,lower bound,query}
}

@article{georgiadisDecrementalDataStructures2017,
  title = {Decremental Data Structures for Connectivity and Dominators in Directed Graphs},
  author = {Georgiadis, L. and Hansen, Thomas Dueholm and Italiano, G. and Krinninger, Sebastian and Parotsidis, Nikos},
  year = {2017},
  doi = {10.4230/LIPIcs.ICALP.2017.42},
  abstract = {We introduce a new dynamic data structure for maintaining the strongly connected components (SCCs) of a directed graph (digraph) under edge deletions, so as to answer a rich repertoire of connectivity queries. Our main technical contribution is a decremental data structure that supports sensitivity queries of the form "are u and v strongly connected in the graph G{$\setminus$}w?", for any triple of vertices u,v,w, while G undergoes deletions of edges. Our data structure processes a sequence of edge deletions in a digraph with n vertices in O(mn\{n\}) total time and O(n{$^2$}\{n\}) space, where m is the number of edges before any deletion, and answers the above queries in constant time. We can leverage our data structure to obtain decremental data structures for many more types of queries within the same time and space complexity. For instance for edge-related queries, such as testing whether two query vertices u and v are strongly connected in G{$\setminus$}e, for some query edge e. As another important application of our decremental data structure, we provide the first nontrivial algorithm for maintaining the dominator tree of a flow graph under edge deletions. We present an algorithm that processes a sequence of edge deletions in a flow graph in O(mn\{n\}) total time and O(n{$^2$}\{n\}) space. For reducible flow graphs we provide an O(mn)-time and O(m+n)-space algorithm. We give a conditional lower bound that provides evidence that these running times may be tight up to subpolynomial factors.},
  citationcount = {11},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,dynamic,lower bound,query}
}

@article{georgiadisDesignOfData2006,
  title = {Design of Data Structures for Mergeable Trees},
  author = {Georgiadis, L. and Tarjan, R. and Werneck, Renato F.},
  year = {2006},
  doi = {10.1145/1109557.1109602},
  abstract = {Motivated by an application in computational topology, we consider a novel variant of the problem of efficiently maintaining dynamic rooted trees. This variant allows an operation that merges two tree paths. In contrast to the standard problem, in which only one tree arc at a time changes, a single merge operation can change many arcs. In spite of this, we develop a data structure that supports merges and all other standard tree operations in O(log2 n) amortized time on an n-node forest. For the special case that occurs in the motivating application, in which arbitrary arc deletions are not allowed, we give a data structure with an O(log n) amortized time bound per operation, which is asymptotically optimal. The analysis of both algorithms is not straightforward and requires ideas not previously used in the study of dynamic trees. We explore the design space of algorithms for the problem and also consider lower bounds for it.},
  citationcount = {14},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,lower bound}
}

@article{georgiadisIncrementalStrongConnectivity2018,
  title = {Incremental Strong Connectivity and 2-Connectivity in Directed Graphs},
  author = {Georgiadis, L. and Italiano, G. and Parotsidis, Nikos},
  year = {2018},
  doi = {10.1007/978-3-319-77404-6_39},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Latin American Symposium on Theoretical Informatics}
}

@article{georgiadisTesting2Vertex2010,
  title = {Testing 2-Vertex Connectivity and Computing Pairs of Vertex-Disjoint s-t Paths in Digraphs},
  author = {Georgiadis, L.},
  year = {2010},
  doi = {10.1007/978-3-642-14165-2_62},
  abstract = {No abstract available},
  citationcount = {38},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{gerhardyTheRoleOf2003,
  title = {The Role of Quantifier Alternations in Cut Elimination},
  author = {Gerhardy, P.},
  year = {2003},
  doi = {10.1305/ndjfl/1117755147},
  abstract = {Extending previous results from the author's master's thesis, subsequently published in the proceedings of CSL 2003, on the complexity of cut elimination for the sequent calculus LK, we discuss the role of quantifier alternations and develop a measure to describe the complexity of cut elimination in terms of quantifier alternations in cut formulas and contractions on such formulas.},
  citationcount = {28},
  venue = {Notre Dame J. Formal Log.}
}

@article{gertsluiterRandomLinkAvoidingLinkageEffects2020,
  title = {{{RandomLink}} - {{Avoiding Linkage-Effects}} by {{Employing Random Effects}} for {{Clustering}}},
  author = {Gert Sluiter and B. Schelling and C. Plant},
  year = {2020},
  journal = {International Conference on Database and Expert Systems Applications},
  doi = {10.1007/978-3-030-59003-1_15},
  annotation = {Citation Count: 0}
}

@article{gfellerTowardsOptimalRange2009,
  title = {Towards Optimal Range Medians},
  author = {Gfeller, Beat and Sanders, P.},
  year = {2009},
  doi = {10.1007/978-3-642-02927-1_40},
  abstract = {No abstract available},
  citationcount = {52},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{ghosalOnConstantDepth2017,
  title = {On Constant Depth Circuits Parameterized by Degree: {{Identity}} Testing and Depth Reduction},
  author = {Ghosal, Purnata and Prakash, O. and Rao, B.},
  year = {2017},
  doi = {10.1007/978-3-319-62389-4_21},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {International Computing and Combinatorics Conference}
}

@article{ghoshFastNumericalMultivariate2023,
  title = {Fast Numerical Multivariate Multipoint Evaluation},
  author = {Ghosh, Sumanta and Harsha, P. and Herdade, Sim{\~a}o and Kumar, Mrinal and Saptharishi, Ramprasad},
  year = {2023},
  doi = {10.1109/FOCS57990.2023.00088},
  abstract = {We design nearly-linear time numerical algorithms for the problem of multivariate multipoint evaluation over the fields of rational, real and complex numbers. We consider both exact and approximate versions of the algorithm. The input to the algorithms are (1) coefficients of an m-variate polynomial f with degree d in each variable, and (2) points \{a\}\textsubscript{\{\vphantom\}}1\vphantom\{\},{\dots},\{a\}\textsubscript{\{\vphantom\}}N\vphantom\{\} each of whose coordinate has absolute value bounded by one. Approximate version: Given additionally an accuracy parameter t, the algorithm computes rational numbers {$\beta$}\textsubscript{\{\vphantom\}}1\vphantom\{\},{\dots},{$\beta$}\textsubscript{\{\vphantom\}}N\vphantom\{\} such that {\textbar}f(\{a\}\textsubscript{\{\vphantom\}}i\vphantom\{\})-{$\beta$}\textsubscript{\{\vphantom\}}i\vphantom\{\}{\textbar}{$\leq$}1/2\textsuperscript{\{\vphantom\}}t\vphantom\{\} for all i, and has a running time of ((Nm+d\textsuperscript{\{\vphantom\}}m\vphantom\{\})t)\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\} for all m and all sufficiently large d. Exact version (when over rationals): Given additionally a bound s on the bit-complexity of all the rational numbers in the input and output, the algorithm computes the rational numbers f(\{a\}\textsubscript{\{\vphantom\}}1\vphantom\{\}),{\dots},f(\{a\}\textsubscript{\{\vphantom\}}N\vphantom\{\}), in time ((Nm+d\textsuperscript{\{\vphantom\}}m\vphantom\{\})s)\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\} for all m and all sufficiently large d. Our results also naturally extend to the case when the input is over the field of real or complex numbers under an appropriate standard model of representation of field elements in such fields.Prior to this work, a nearly-linear time algorithm for multivariate multipoint evaluation (exact or approximate) over any infinite field appears to be known only for the case of univariate polynomials, and was discovered in a recent work of Moroz [Proc. 62nd FOCS, 2021]. In this work, we extend this result from the univariate to the multivariate setting. However, our algorithm is based on ideas that seem to be conceptually different from those of Moroz [Proc. 62nd FOCS, 2021] and crucially relies on a recent algorithm of Bhargava, Ghosh, Guo, Kumar \& Umans [Proc. 63rd FOCS, 2022] for multivariate multipoint evaluation over finite fields, and known efficient algorithms for the problems of rational number reconstruction and fast Chinese remaindering in computational number theory.},
  citationcount = {2},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{gianfeliciNearestNeighborMethods2008,
  title = {Nearest-Neighbor Methods in Learning and Vision (Shakhnarovich, {{G}}. et al., Eds.; 2006) [Book Review]},
  author = {Gianfelici, F.},
  year = {2008},
  doi = {10.1109/TNN.2008.917504},
  abstract = {In this excellent book, the editors deal with the state-of-the-art, current best practices, and some innovative applications of nearest-neighbor methods in learning and vision. This volume brings together contributions of top-level researchers in theory of computation, machine learning, and computer vision with the goal of closing up the gaps between disciplines and current state-of-the-art methods for emerging applications. All the content is well-written, highly relevant, original, and timely. The audience for this book consists of researchers, scientists, engineers, professionals, and academics, working not only in this field, but also in any field that could benefit from these powerful methods. This book can be particularly useful to researchers working on the basis set expansion networks.},
  citationcount = {193},
  venue = {No venue available}
}

@article{gibneyTextIndexingFor2021,
  title = {Text Indexing for Regular Expression Matching},
  author = {Gibney, Daniel and Thankachan, Sharma V.},
  year = {2021},
  doi = {10.3390/a14050133},
  abstract = {Finding substrings of a text T that match a regular expression p is a fundamental problem. Despite being the subject of extensive research, no solution with a time complexity significantly better than O({\textbar}T{\textbar}{\textbar}p{\textbar}) has been found. Backurs and Indyk in FOCS 2016 established conditional lower bounds for the algorithmic problem based on the Strong Exponential Time Hypothesis that helps explain this difficulty. A natural question is whether we can improve the time complexity for matching the regular expression by preprocessing the text T? We show that conditioned on the Online Matrix--Vector Multiplication (OMv) conjecture, even with arbitrary polynomial preprocessing time, a regular expression query on a text cannot be answered in strongly sublinear time, i.e., O({\textbar}T{\textbar}1-{$\varepsilon$}) for any {$\varepsilon$}{\textquestiondown}0. Furthermore, if we extend the OMv conjecture to a plausible conjecture regarding Boolean matrix multiplication with polynomial preprocessing time, which we call Online Matrix--Matrix Multiplication (OMM), we can strengthen this hardness result to there being no solution with a query time that is O({\textbar}T{\textbar}3/2-{$\varepsilon$}). These results hold for alphabet sizes three or greater. We then provide data structures that answer queries in O({\textbar}T{\textbar}{\textbar}p{\textbar}{$\tau$}) time where {$\tau\in$}[1,{\textbar}T{\textbar}] is fixed at construction. These include a solution that works for all regular expressions with Exp{$\tau\cdot\vert$}T{\textbar} preprocessing time and space. For patterns containing only `concatenation' and `or' operators (the same type used in the hardness result), we provide (1) a deterministic solution which requires Exp{$\tau\cdot\vert$}T{\textbar}log2{\textbar}T{\textbar} preprocessing time and space, and (2) when {\textbar}p{\textbar}{$\leq\vert$}T{\textbar}z for z=2o(log{\textbar}T{\textbar}), a randomized solution with amortized query time which answers queries correctly with high probability, requiring Exp{$\tau\cdot\vert$}T{\textbar}2{\textohm}log{\textbar}T{\textbar} preprocessing time and space.},
  citationcount = {10},
  venue = {Algorithms},
  keywords = {data structure,lower bound,query,query time}
}

@article{gilComputing2D1993,
  title = {Computing 2-{{D}} Min, Median, and Max Filters},
  author = {Gil, Joseph and Werman, M.},
  year = {1993},
  doi = {10.1109/34.211471},
  abstract = {Fast algorithms for computing min, median, max, or any other order statistic filter transforms are described. The algorithms take constant time per pixel to compute min or max filters and polylog time per pixel, in the size of the filter, to compute the median filter. A logarithmic time per pixel lower bound for the computation of the median filter is shown. {\textquestiondown}},
  citationcount = {168},
  venue = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {lower bound}
}

@article{girishraguvirDataStructuresFor2019,
  title = {Data Structures for Incremental Interval Coloring},
  author = {GirishRaguvir, J. and Kashyop, Manas Jyoti and Narayanaswamy, N.},
  year = {2019},
  doi = {10.1007/978-3-030-26176-4_40},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {International Computing and Combinatorics Conference},
  keywords = {data structure}
}

@article{girishraguvirDynamicDataStructures2019,
  title = {Dynamic Data Structures for Interval Coloring},
  author = {GirishRaguvir, J. and Kashyop, Manas Jyoti and Narayanaswamy, N.},
  year = {2019},
  doi = {10.1016/J.TCS.2020.06.024},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Theoretical Computer Science},
  keywords = {data structure,dynamic}
}

@article{glaerTwoDimensionalPacket2009,
  title = {Two-Dimensional Packet Classification and Filter Conflict Resolution in the Internet},
  author = {Gla{\ss}er, Christian and Travers, Stephen D.},
  year = {2009},
  doi = {10.1007/S00224-007-9050-5},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {No venue available}
}

@article{gligorEstablishingSoftwareRoot2019,
  title = {Establishing Software Root of Trust Unconditionally},
  author = {Gligor, V. and Woo, Maverick},
  year = {2019},
  doi = {10.14722/ndss.2019.23170},
  abstract = {Root-of-Trust (RoT) establishment ensures either that the state of an untrusted system contains all and only content chosen by a trusted local verifier and the system code begins execution in that state, or that the verifier discovers the existence of unaccounted for content. This ensures program booting into system states that are free of persistent malware. An adversary can no longer retain undetected control of one's local system. We establish RoT unconditionally; i.e., without secrets, trusted hardware modules and instructions, or bounds on the adversary's computational power. The specification of a system's chipset and device controllers, and an external source of true random numbers, such as a commercially available quantum RNG, is all that is needed. Our system specifications are those of a concrete Word Random Access Machine (cWRAM) model -- the closest computation model to a real system with a large instruction set. We define the requirements for RoT establishment and explain their differences from past attestation protocols. Then we introduce a RoT establishment protocol based on a new computation primitive with concrete (non-asymptotic) optimal space-time bounds in adversarial evaluation on the cWRAM. The new primitive is a randomized polynomial, which has kindependent uniform coefficients in a prime order field. Its collision properties are stronger than those of a k-independent (almost) universal hash function in cWRAM evaluations, and are sufficient to prove existence of malware-free states before RoT is established. Preliminary measurements show that randomizedpolynomial performance is practical on commodity hardware even for very large k. To prove the concrete optimality of randomized polynomials, we present a result of independent complexity interest: a Hornerrule program is uniquely optimal whenever the cWRAM execution space and time are simultaneously minimized.},
  citationcount = {22},
  venue = {Network and Distributed System Security Symposium}
}

@article{goelDisjointSetUnion2014,
  title = {Disjoint Set Union with Randomized Linking},
  author = {Goel, Ashish and Khanna, S. and Larkin, Daniel H. and Tarjan, R.},
  year = {2014},
  doi = {10.1137/1.9781611973402.75},
  abstract = {A classic result in the analysis of data structures is that path compression with linking by rank solves the disjoint set union problem in almost-constant amortized time per operation. Recent experiments suggest that in practice, a naive linking method works just as well if not better than linking by rank, in spite of being theoretically inferior. How can this be? We prove that randomized linking is asymptotically as efficient as linking by rank. This result provides theory that matches the experiments, which implicitly do randomized linking as a result of the way the input instances are generated.},
  citationcount = {15},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure}
}

@article{gokajCompletenessTheoremsFor2025,
  title = {Completeness Theorems for K-{{SUM}} and Geometric Friends: {{Deciding}} Fragments of Linear Integer Arithmetic},
  author = {Gokaj, Geri and K{\"u}nnemann, Marvin},
  year = {2025},
  doi = {10.4230/LIPIcs.ITCS.2025.55},
  abstract = {In the last three decades, the k -SUM hypothesis has emerged as a satisfying explanation of long-standing time barriers for a variety of algorithmic problems. Yet to this day, the literature knows of only few proven consequences of a refutation of this hypothesis. Taking a descriptive complexity viewpoint, we ask: What is the largest logically defined class of problems captured by the k -SUM problem? To this end, we introduce a class FOP Z of problems corresponding to deciding sentences in Presburger arithmetic/linear integer arithmetic over finite subsets of integers. We establish two large fragments for which the k -SUM problem is complete under fine-grained reductions: 1. The k -SUM problem is complete for deciding the sentences with k existential quantifiers. 2},
  citationcount = {Unknown},
  venue = {Information Technology Convergence and Services},
  keywords = {reduction}
}

@article{goldImprovedBoundsFor2015,
  title = {Improved Bounds for {{3SUM}}, k-{{SUM}}, and Linear Degeneracy},
  author = {Gold, Omer and Sharir, M.},
  year = {2015},
  doi = {10.4230/LIPIcs.ESA.2017.42},
  abstract = {Given a set of n real numbers, the 3SUM problem is to decide whether there are three of them that sum to zero. Until a recent breakthrough by Gr\{{\o}\}nlund and Pettie [FOCS'14], a simple {$\Theta$}(n{$^2$})-time deterministic algorithm for this problem was conjectured to be optimal. Over the years many algorithmic problems have been shown to be reducible from the 3SUM problem or its variants, including the more generalized forms of the problem, such as k-SUM and k-variate linear degeneracy testing (k-LDT). The conjectured hardness of these problems have become extremely popular for basing conditional lower bounds for numerous algorithmic problems in P. In this paper, we show that the randomized 4-linear decision tree complexity of 3SUM is O(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}), and that the randomized (2k-2)-linear decision tree complexity of k-SUM and k-LDT is O(n\textsuperscript{\{\vphantom\}}k/2\vphantom\{\}), for any odd k{$\geq$}3. These bounds improve (albeit randomized) the corresponding O(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}{\textsurd}\{n\}) and O(n\textsuperscript{\{\vphantom\}}k/2\vphantom\{\}{\textsurd}\{n\}) decision tree bounds obtained by Gr\{{\o}\}nlund and Pettie. Our technique includes a specialized randomized variant of fractional cascading data structure. Additionally, we give another deterministic algorithm for 3SUM that runs in O(n{$^2$}n/n) time. The latter bound matches a recent independent bound by Freund [Algorithmica 2017], but our algorithm is somewhat simpler, due to a better use of word-RAM model.},
  citationcount = {40},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,lower bound}
}

@article{goldreichOnDoublyEfficient2018,
  title = {On Doubly-Efficient Interactive Proof Systems},
  author = {Goldreich, Oded},
  year = {2018},
  doi = {10.1561/0400000084},
  abstract = {An interactive proof system is called doubly-efficient if the prescribed prover strategy can be implemented in polynomial-time and the verifier???s strategy can be implemented in almost-linear time. Such proof systems make the benefits of interactive proof system available to real-life agents who are restricted to polynomial-time computation. On Doubly-Efficient Interactive Proof Systems surveys some of the known results regarding doubly-efficient interactive proof systems. It starts by presenting two simple constructions for t-no-CLIQUE, where the first construction offers the benefit of being generalized to any ???locally characterizable??? set, and the second construction offers the benefit of preserving the combinatorial flavor of the problem. It then turns to two more general constructions of doubly-efficient interactive proof system: the proof system for sets having (uniform) bounded-depth circuits and the proof system for sets that are recognized in polynomial-time and small space. T e presentation of the GKR construction is complete and is somewhat different from the original presentation. A brief overview is provided for the RRR construction.},
  citationcount = {28},
  venue = {Foundations and Trends{\textregistered} in Theoretical Computer Science}
}

@article{goldreichSimpleDoublyEfficient2017,
  title = {Simple Doubly-Efficient Interactive Proof Systems for Locally-Characterizable Sets},
  author = {Goldreich, Oded and Rothblum, G.},
  year = {2017},
  doi = {10.4230/LIPIcs.ITCS.2018.18},
  abstract = {A proof system is called doubly-efficient if the prescribed prover strategy can be implemented in polynomial-time and the verifier's strategy can be implemented in almost-linear-time. We present direct constructions of doubly-efficient interactive proof systems for problems in P that are believed to have relatively high complexity. Specifically, such constructions are presented for t-CLIQUE and t-SUM. In addition, we present a generic construction of such proof systems for a natural class that contains both problems and is in NC (and also in SC). The proof systems presented by us are significantly simpler than the proof systems presented by Goldwasser, Kalai and Rothblum (JACM, 2015), let alone those presented by Reingold, Rothblum, and Rothblum (STOC, 2016).},
  citationcount = {22},
  venue = {Information Technology Convergence and Services}
}

@article{goldsteinConditionalLowerBounds2017,
  title = {Conditional Lower Bounds for Space/Time Tradeoffs},
  author = {Goldstein, Isaac and Kopelowitz, T. and Lewenstein, Moshe and Porat, E.},
  year = {2017},
  doi = {10.1007/978-3-319-62127-2_36},
  abstract = {No abstract available},
  citationcount = {42},
  venue = {Workshop on Algorithms and Data Structures},
  keywords = {lower bound}
}

@article{goldsteinHowHardIs2017,
  title = {How Hard Is It to Find (Honest) Witnesses?},
  author = {Goldstein, Isaac and Kopelowitz, T. and Lewenstein, Moshe and Porat, E.},
  year = {2017},
  doi = {10.4230/LIPIcs.ESA.2016.45},
  abstract = {In recent years much effort has been put into developing polynomial-time conditional lower bounds for algorithms and data structures in both static and dynamic settings. Along these lines we introduce a framework for proving conditional lower bounds based on the well-known 3SUM conjecture. Our framework creates a compact representation of an instance of the 3SUM problem using hashing and domain specific encoding. This compact representation admits false solutions to the original 3SUM problem instance which we reveal and eliminate until we find a true solution. In other words, from all witnesses (candidate solutions) we figure out if an honest one (a true solution) exists. This enumeration of witnesses is used to prove conditional lower bounds on reporting problems that generate all witnesses. In turn, these reporting problems are then reduced to various decision problems using special search data structures which are able to enumerate the witnesses while only using solutions to decision variants. Hence, 3SUM-hardness of the decision problems is deduced. We utilize this framework to show conditional lower bounds for several variants of convolutions, matrix multiplication and string problems. Our framework uses a strong connection between all of these problems and the ability to find witnesses. Specifically, we prove conditional lower bounds for computing partial outputs of convolutions and matrix multiplication for sparse inputs. These problems are inspired by the open question raised by Muthukrishnan 20 years ago. The lower bounds we show rule out the possibility (unless the 3SUM conjecture is false) that almost linear time solutions to sparse input-output convolutions or matrix multiplications exist. This is in contrast to standard convolutions and matrix multiplications that have, or assumed to have, almost linear solutions. Moreover, we improve upon the conditional lower bounds of Amir et al. for histogram indexing, a problem that has been of much interest recently. The conditional lower bounds we show apply for both reporting and decision variants. For the well-studied decision variant, we show a full tradeoff between preprocessing and query time for every alphabet size {\textquestiondown} 2. At an extreme, this implies that no solution to this problem exists with subquadratic preprocessing time and  O(1) query time for every alphabet size {\textquestiondown} 2, unless the 3SUM conjecture is false. This is in contrast to a recent result by Chan and Lewenstein for a binary alphabet. While these specific applications are used to demonstrate the techniques of our framework, we believe that this novel framework is useful for many other problems as well.},
  citationcount = {22},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,dynamic,lower bound,query,query time,static}
}

@article{goldsteinImprovedSpaceTime2018,
  title = {Improved Space-Time Tradeoffs for {{kSUM}}},
  author = {Goldstein, Isaac and Lewenstein, Moshe and Porat, E.},
  year = {2018},
  doi = {10.4230/LIPIcs.ESA.2018.37},
  abstract = {In the kSUM problem we are given an array of numbers a{$_1$},a{$_2$},...,a\textsubscript{n} and we are required to determine if there are k different elements in this array such that their sum is 0. This problem is a parameterized version of the well-studied SUBSET-SUM problem, and a special case is the 3SUM problem that is extensively used for proving conditional hardness. Several works investigated the interplay between time and space in the context of SUBSET-SUM. Recently, improved time-space tradeoffs were proven for kSUM using both randomized and deterministic algorithms. In this paper we obtain an improvement over the best known results for the time-space tradeoff for kSUM. A major ingredient in achieving these results is a general self-reduction from kSUM to mSUM where m1. (iv) An algorithm for 6SUM running in O(n{$^4$}) time using just O(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}) space. (v) A solution to 3SUM on random input using O(n{$^2$}) time and O(n\textsuperscript{\{\vphantom\}}1/3\vphantom\{\}) space, under the assumption of a random read-only access to random bits.},
  citationcount = {Unknown},
  venue = {Embedded Systems and Applications},
  keywords = {reduction,time-space}
}

@article{goldsteinOnTheHardness2019,
  title = {On the Hardness of Set Disjointness and Set Intersection with Bounded Universe},
  author = {Goldstein, Isaac and Lewenstein, Moshe and Porat, E.},
  year = {2019},
  doi = {10.4230/LIPIcs.ISAAC.2019.7},
  abstract = {In the SetDisjointness problem, a collection of m sets S{$_1$},S{$_2$},...,S\textsubscript{m} from some universe U is preprocessed in order to answer queries on the emptiness of the intersection of some two query sets from the collection. In the SetIntersection variant, all the elements in the intersection of the query sets are required to be reported. These are two fundamental problems that were considered in several papers from both the upper bound and lower bound perspective. Several conditional lower bounds for these problems were proven for the tradeoff between preprocessing and query time or the tradeoff between space and query time. Moreover, there are several unconditional hardness results for these problems in some specific computational models. The fundamental nature of the SetDisjointness and SetIntersection problems makes them useful for proving the conditional hardness of other problems from various areas. However, the universe of the elements in the sets may be very large, which may cause the reduction to some other problems to be inefficient and therefore it is not useful for proving their conditional hardness. In this paper, we prove the conditional hardness of SetDisjointness and SetIntersection with bounded universe. This conditional hardness is shown for both the interplay between preprocessing and query time and the interplay between space and query time. Moreover, we present several applications of these new conditional lower bounds. These applications demonstrates the strength of our new conditional lower bounds as they exploit the limited universe size. We believe that this new framework of conditional lower bounds with bounded universe can be useful for further significant applications.},
  citationcount = {7},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {lower bound,query,query time,reduction}
}

@incollection{golovneDerandomizationCellSampling2023,
  title = {Derandomization of {{Cell Sampling}}},
  booktitle = {2023 {{Symposium}} on {{Simplicity}} in {{Algorithms}} ({{SOSA}})},
  author = {Golovne, Alexander and Gur, Tom and Shinkar, Igor},
  year = {2023},
  month = jan,
  series = {Proceedings},
  pages = {278--284},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611977585.ch26},
  url = {https://epubs.siam.org/doi/10.1137/1.9781611977585.ch26},
  urldate = {2025-03-14},
  abstract = {Since 1989, the best known lower bound on static data structures was Siegel's classical cell sampling lower bound. Siegel showed an explicit problem with n inputs and m possible queries such that every data structure that answers queries by probing t memory cells requires space . In this work, we improve this bound for non-adaptive data structures to  for all t {$\geq$} 2.For t = 2, we give a lower bound of s {$>$} m --- o (m), improving on the bound s {$>$} m/2 recently proved by Viola over {$\mathbb{F}$}2 and Siegel's bound  over other finite fields.* The full version of the paper can be accessed at https://arxiv.org/abs/2108.05970},
  keywords = {cell sampling,data structure,lower bound,non-adaptive,query,static}
}

@article{golovnevDataStructuresMeet2019,
  title = {Data Structures Meet Cryptography: {{3SUM}} with Preprocessing},
  author = {Golovnev, Alexander and Guo, Siyao and Horel, Thibaut and Park, Sunoo and Vaikuntanathan, V.},
  year = {2019},
  doi = {10.1145/3357713.3384342},
  abstract = {This paper shows several connections between data structure problems and cryptography against preprocessing attacks. Our results span data structure upper bounds, cryptographic applications, and data structure lower bounds, as summarized next. First, we apply Fiat-Naor inversion, a technique with cryptographic origins, to obtain a data structure upper bound. In particular, our technique yields a suite of algorithms with space S and (online) time T for a preprocessing version of the N-input 3SUM problem where S 3{$\cdot$} T = O(N 6). This disproves a strong conjecture (Goldstein et al., WADS 2017) that there is no data structure that solves this problem for S=N 2-{$\delta$} and T = N 1-{$\delta$} for any constant {$\delta$}{\textquestiondown}0. Secondly, we show equivalence between lower bounds for a broad class of (static) data structure problems and one-way functions in the random oracle model that resist a very strong form of preprocessing attack. Concretely, given a random function F: [N] {$\rightarrow$} [N] (accessed as an oracle) we show how to compile it into a function G F : [N 2] {$\rightarrow$} [N 2] which resists S-bit preprocessing attacks that run in query time T where ST=O(N 2-{$\varepsilon$}) (assuming a corresponding data structure lower bound on 3SUM). In contrast, a classical result of Hellman tells us that F itself can be more easily inverted, say with N 2/3-bit preprocessing in N 2/3 time. We also show that much stronger lower bounds follow from the hardness of kSUM. Our results can be equivalently interpreted as security against adversaries that are very non-uniform, or have large auxiliary input, or as security in the face of a powerfully backdoored random oracle. Thirdly, we give non-adaptive lower bounds for 3SUM which match the best known lower bounds for static data structure problems. Moreover, we show that our lower bound generalizes to a range of geometric problems, such as three points on a line, polygon containment, and others.},
  citationcount = {17},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,lower bound,non-adaptive,query,query time,static}
}

@article{golovnevDerandomizationCellSampling2021,
  title = {Derandomization of Cell Sampling},
  author = {Golovnev, Alexander and Gur, Tom and Shinkar, Igor},
  year = {2021},
  doi = {10.1137/1.9781611977585.ch26},
  abstract = {Since 1989, the best known lower bound on static data structures was Siegel's classical cell sampling lower bound. Siegel showed an explicit problem with n inputs and m possible queries such that every data structure that answers queries by probing t memory cells requires space s{$\geq\Omega$}\vphantom\{\}(n{$\cdot$}(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{m}\vphantom\{\}\{n\})\textsuperscript{\{\vphantom\}}1/t\vphantom\{\}). In this work, we improve this bound for non-adaptive data structures to s{$\geq\Omega$}\vphantom\{\}(n{$\cdot$}(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{m}\vphantom\{\}\{n\})\textsuperscript{\{\vphantom\}}1/(t-1)\vphantom\{\}) for all t{$\geq$}2. For t=2, we give a lower bound of s{$>$}m-o(m), improving on the bound s{$>$}m/2 recently proved by Viola over \{F\}{$_2$} and Siegel's bound s{$\geq\Omega$}\vphantom\{\}({\textsurd}\{mn\}) over other finite fields.},
  citationcount = {Unknown},
  venue = {SIAM Symposium on Simplicity in Algorithms},
  keywords = {cell sampling,data structure,lower bound,non-adaptive,query,static},
  file = {/Users/tulasi/Zotero/storage/WCLRKVRK/Golovne et al. - 2023 - Derandomization of Cell Sampling.pdf}
}

@article{golovnevPolynomialDataStructure2020,
  title = {Polynomial Data Structure Lower Bounds in the Group Model},
  author = {Golovnev, Alexander and Posobin, Gleb and Regev, O. and Weinstein, Omri},
  year = {2020},
  doi = {10.1109/FOCS46700.2020.00074},
  abstract = {Proving super-logarithmic data structure lower bounds in the static group model has been a fundamental challenge in computational geometry since the early 80's. We prove a polynomial (n\textsuperscript{\{\vphantom\}}{\textohm}(1)\vphantom\{\}) lower bound for an explicit range counting problem of n\textsuperscript{\{\vphantom\}}3\vphantom\{\} convex polygons in \{R\}\textsuperscript{\{\vphantom\}}2\vphantom\{\} (each with n\textsuperscript{\{\vphantom\}}O\vphantom\{\}(1)\vphantom\{\} facets/semialgebraic-complexity), against linear storage arithmetic data structures in the group model. Our construction and analysis are based on a combination of techniques in Diophantine approximation, pseudorandomness, and compressed sensing---in particular, on the existence and partial derandomization of optimal binary compressed sensing matrices in the polynomial sparsity regime (k=n\textsuperscript{\{\vphantom\}}1-{$\delta$}\vphantom\{\}). As a byproduct, this establishes a (logarithmic) separation between compressed sensing matrices and the stronger RIP property.},
  citationcount = {1},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,lower bound,static}
}

@article{golovnevTheMinrankOf2016,
  title = {The Minrank of Random Graphs},
  author = {Golovnev, Alexander and Regev, O. and Weinstein, Omri},
  year = {2016},
  doi = {10.4230/LIPIcs.APPROX-RANDOM.2017.46},
  abstract = {The {\textexclamdown}italic{\textquestiondown}minrank{\textexclamdown}/italic{\textquestiondown} of a directed graph {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}G {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} is the minimum rank of a matrix {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}M {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} that can be obtained from the adjacency matrix of {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}G {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} by switching some ones to zeros (i.e., deleting edges) and then setting all diagonal entries to one. This quantity is closely related to the fundamental information-theoretic problems of (linear) {\textexclamdown}italic{\textquestiondown}index coding{\textexclamdown}/italic{\textquestiondown} (Bar-Yossef {\textexclamdown}italic{\textquestiondown}et al.{\textexclamdown}/italic{\textquestiondown}), network coding (Effros {\textexclamdown}italic{\textquestiondown}et al.{\textexclamdown}/italic{\textquestiondown}), and distributed storage (Mazumdar, ISIT, 2014). We prove tight bounds on the minrank of directed Erd{\H o}s--R{\'e}nyi random graphs {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}G(n,p) {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} for all regimes of {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}p{$\in$}[\{0,1\}] {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}. In particular, for any constant {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}p {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}, we show that {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}\{minrk\}(G)={$\Theta$}(n/n) {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} with high probability, where {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}G {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} is chosen from {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}G(n,p) {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}. This bound gives a near quadratic improvement over the previous best lower bound of {\textexclamdown}inline-formula{\textquestiondown} {\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}{\textohm}({\textsurd}\{n\}) {\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} (Haviv and Langberg), and partially settles an open problem raised by Lubetzky and Stav. Our lower bound matches the well-known upper bound obtained by the ``clique covering'' solution and settles the linear index coding problem for random knowledge graphs.},
  citationcount = {26},
  venue = {IEEE Transactions on Information Theory},
  keywords = {information theoretic,lower bound}
}

@article{golynskiCellProbeLower2009,
  title = {Cell Probe Lower Bounds for Succinct Data Structures},
  author = {Golynski, Alexander},
  year = {2009},
  doi = {10.1137/1.9781611973068.69},
  abstract = {In this paper, we consider several static data structure problems in the deterministic cell probe model. We develop a new technique for proving lower bounds for succinct data structures, where the redundancy in the storage can be small compared to the information-theoretic minimum. In fact, we succeed in matching (up to constant factors) the lower order terms of the existing data structures with the lower order terms provided by our lower bound. Using this technique, we obtain (i) the first lower bound for the problem of searching and retrieval of a substring in text; (ii) a cell probe lower bound for the problem of representing permutation {$\pi$} with queries {$\pi$}(i) and {$\pi-$}1(i) that matches the lower order term of the existing data structures, and (iii) a lower bound for representing binary matrices that is also matches upper bounds for some set of parameters. The nature of all these problems is that we are to implement two operations that are in a reciprocal relation to each other (search and retrieval, computing forward and inverse element, operations on rows and columns of a matrix). As far as we know, this paper is the first to provide an insight into such problems.},
  citationcount = {35},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,data structure,information theoretic,lower bound,query,static}
}

@article{golynskiOnTheSize2007,
  title = {On the Size of Succinct Indices},
  author = {Golynski, Alexander and Grossi, R. and Gupta, Ankur and Raman, R. and Rao, S.Srinivasa},
  year = {2007},
  doi = {10.1007/978-3-540-75520-3_34},
  abstract = {No abstract available},
  citationcount = {59},
  venue = {Embedded Systems and Applications}
}

@article{golynskiOptimalIndexesFor2011,
  title = {Optimal Indexes for Sparse Bit Vectors},
  author = {Golynski, Alexander and Orlandi, Alessio and Raman, R. and Rao, S.},
  year = {2011},
  doi = {10.1007/s00453-013-9767-2},
  abstract = {No abstract available},
  citationcount = {17},
  venue = {Algorithmica}
}

@article{golynskiOptimalLowerBounds2006,
  title = {Optimal Lower Bounds for Rank and Select Indexes},
  author = {Golynski, Alexander},
  year = {2006},
  doi = {10.1016/j.tcs.2007.07.041},
  abstract = {No abstract available},
  citationcount = {91},
  venue = {Theoretical Computer Science}
}

@article{golynskiRedundancySuccinctData2008,
  title = {On the Redundancy of Succinct Data Structures},
  author = {Golynski, Alexander and Raman, R. and Rao, S.},
  year = {2008},
  doi = {10.1007/978-3-540-69903-3_15},
  abstract = {No abstract available},
  citationcount = {39},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {data structure}
}

@article{gonzlezACompactRank2019,
  title = {A Compact Rank/Select Data Structure for the Streaming Model},
  author = {Gonz{\'a}lez, Natalia and Arroyuelo, Diego},
  year = {2019},
  doi = {10.1109/SCCC49216.2019.8966418},
  abstract = {For a sorted set S from a universe [1.u] received under the streaming model (i.e., elements are received one at a time, in sorted order), such that at a given time it contains n elements \{x1, {\dots}, xn\}, and whose characteristic bit vector is C\textsubscript{\{\vphantom\}}S\vphantom\{\}=0\textsuperscript{\{\vphantom\}}{$\sigma$}\textsubscript{\{\vphantom\}}1\vphantom\{\}\vphantom\{\}11{$\cdots$}10\textsuperscript{\{\vphantom\}}{$\sigma$}\textsubscript{\{\vphantom\}}2\vphantom\{\}\vphantom\{\}11{$\cdots$}1{$\cdots$}0\textsuperscript{\{\vphantom\}}{$\sigma$}\textsubscript{\{\vphantom\}}g\vphantom\{\}\vphantom\{\}11{$\cdots$}1 (i.e., the set elements are actually arranged in g {$\leq$} n intervals of size {$\geq$} 1), we propose a compact data structure that answers operations select and rank in {$\Theta$}(\{lg\}(q/\{lg\}q)) worst-case time, and append in O(1) amortized time, using 2q\{lg\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{u}-n\vphantom\{\}\{a\}+q\{lg\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}\vphantom\{\}\{a\}+o(q\{lg\}{$\quad$}\{lg\}q) bits of space. The structure is suitable in cases where g q},
  citationcount = {Unknown},
  venue = {International Conference of the Chilean Computer Science Society},
  keywords = {data structure}
}

@article{gonzlezImprovedDynamicRank2008,
  title = {Improved Dynamic Rank-Select Entropy-Bound Structures},
  author = {Gonz{\'a}lez, R. and Navarro, G.},
  year = {2008},
  doi = {10.1007/978-3-540-78773-0_33},
  abstract = {No abstract available},
  citationcount = {29},
  venue = {Latin American Symposium on Theoretical Informatics},
  keywords = {dynamic}
}

@article{gonzlezRankSelectOn2009,
  title = {Rank/Select on Dynamic Compressed Sequences and Applications},
  author = {Gonz{\'a}lez, R. and Navarro, G.},
  year = {2009},
  doi = {10.1016/j.tcs.2009.07.022},
  abstract = {No abstract available},
  citationcount = {38},
  venue = {Theoretical Computer Science},
  keywords = {dynamic}
}

@article{goodmanHandbookOfDiscrete1997,
  title = {Handbook of Discrete and Computational Geometry, Second Edition},
  author = {Goodman, J. and O'Rourke, J.},
  year = {1997},
  doi = {10.1201/9781420035315},
  abstract = {COMBINATORIAL AND DISCRETE GEOMETRY Finite Point Configurations, J. Pach Packing and Covering, G. Fejes Toth Tilings, D. Schattschneider and M. Senechal Helly-Type Theorems and Geometric Transversals, R. Wenger Pseudoline Arrangements, J.E. Goodman Oriented Matroids, J. Richter-Gebert and G.M. Ziegler Lattice Points and Lattice Polytopes, A. Barvinok New! Low-Distortion Embeddings of Finite Metric Spaces, P. Indyk and J. Matousek New! Geometry and Topology of Polygonal Linkages, R. Connelly and E.D. Demaine New! Geometric Graph Theory, J. Pach Euclidean Ramsey Theory, R.L. Graham Discrete Aspects of Stochastic Geometry, R. Schneider Geometric Discrepancy Theory and Uniform Distribution, J.R. Alexander, J. Beck, and W.W.L. Chen Topological Methods, R.T. Zivaljevic Polyominoes, S.W. Golomb and D.A. Klarner POLYTOPES AND POLYHEDRA Basic Properties of Convex Polytopes, M. Henk, J. Richter-Gebert, and G.M. Ziegler Subdivisions and Triangulations of Polytopes, C.W. Lee Face Numbers of Polytopes and Complexes, L.J. Billera and A. Bjoerner Symmetry of Polytopes and Polyhedra, E. Schulte Polytope Skeletons and Paths, G. Kalai Polyhedral Maps, U. Brehm and E. Schulte ALGORITHMS AND COMPLEXITY OF FUNDAMENTAL GEOMETRIC OBJECTS Convex Hull Computations, R. Seidel Voronoi Diagrams and Delaunay Triangulations, S. Fortune Arrangements, D. Halperin Triangulations and Mesh Generation, M. Bern Polygons, J. O'Rourke and S. Suri Shortest Paths and Networks, J.S.B. Mitchell Visibility, J. O'Rourke Geometric Reconstruction Problems, S.S. Skiena New! Curve and Surface Reconstruction, T.K. Dey Computational Convexity, P. Gritzmann and V. Klee Computational Topology, G. Vegter Computational Real Algebraic Geometry, B. Mishra GEOMETRIC DATA STRUCTURES AND SEARCHING Point Location, J. Snoeyink New! Collision and Proximity Queries, M.C. Lin and D. Manocha Range Searching, P.K. Agarwal Ray Shooting and Lines in Space, M. Pellegrini Geometric Intersection, D.M. Mount New! Nearest Neighbors in High-Dimensional Spaces, P. Indyk COMPUTATIONAL TECHNIQUES Randomization and Derandomization, O. Cheong, K. Mulmuley, and E. Ramos Robust Geometric Computation, C.K. Yap Parallel Algorithms in Geometry, M.T. Goodrich Parametric Search, J.S. Salowe New! The Discrepancy Method in Computational Geometry, B. Chazelle APPLICATIONS OF DISCRETE AND COMPUTATIONAL GEOMETRY Linear Programming, M. Dyer, N. Megiddo, and E. Welzl Mathematical Programming, M.H. Todd Algorithmic Motion Planning, M. Sharir Robotics, D. Halperin, L.E. Kavraki, and J.-C. Latombe Computer Graphics, D. Dobkin and S. Teller New! Modeling Motion, L.J. Guibas Pattern Recognition, J. O'Rourke and G.T. Toussaint Graph Drawing, R. Tamassia and G. Liotta Splines and Geometric Modeling, C.L. Bajaj New! Surface Simplification and 3D Geometry Compression, J. Rossignac Manufacturing Processes, R. Janardan and T.C. Woo Solid Modeling, C.M. Hoffmann New! Computation of Robust Statistics: Depth, Median, and Related Measures, P.J. Rousseeuw and A. Struyf New! Geographic Information Systems, M. van Kreveld Geometric Application of the Grassmann-Cayley Algebra, N.L. White Rigidity and Scene Analysis, W. Whiteley Sphere Packing and Coding Theory, G.A. Kabatiansky and J.A. Rush Crystals and Quasicrystals, M. Senechal New! Biological Applications of Computational Topology, H. Edelsbrunner New! GEOMETRIC SOFTWARE Software, J. Joswig Two Computation Geometry Libraries: LEDA and CGAL, L. Kettner and S. Naher Index of Defined Terms New! Index of Cited Authors},
  citationcount = {1053},
  venue = {No venue available},
  keywords = {data structure,query}
}

@article{goodrichTieredVectorsEfficient1999,
  title = {Tiered Vectors: {{Efficient}} Dynamic Arrays for Rank-Based Sequences},
  author = {Goodrich, M. and Kloss, John G.},
  year = {1999},
  doi = {10.1007/3-540-48447-7_21},
  abstract = {No abstract available},
  citationcount = {19},
  venue = {Workshop on Algorithms and Data Structures},
  keywords = {dynamic}
}

@article{gsCommunicationLowerBounds2013,
  title = {Communication Lower Bounds via Critical Block Sensitivity},
  author = {G{\"o}{\"o}s, Mika and Pitassi, T.},
  year = {2013},
  doi = {10.1145/2591796.2591838},
  abstract = {We use critical block sensitivity, a new complexity measure introduced by Huynh and Nordstr{\"o}m (STOC 2012), to study the communication complexity of search problems. To begin, we give a simple new proof of the following central result of Huynh and Nordstr{\"o}m: if S is a search problem with critical block sensitivity b, then every randomised two-party protocol solving a certain two-party lift of S requires {\textohm}(b) bits of communication. Besides simplicity, our proof has the advantage of generalising to the multi-party setting. We combine these results with new critical block sensitivity lower bounds for Tseitin and Pebbling search problems to obtain the following applications. {$\bullet$} Monotone circuit depth: We exhibit a monotone function on n variables whose monotone circuits require depth {\textohm}(n/log n); previously, a bound of {\textohm}({\textsurd}n was known (Raz and Wigderson, JACM 1992). Moreover, we prove a tight {$\Theta$}({\textsurd}n) monotone depth bound for a function in monotone P. This implies an average-case hierarchy theorem within monotone P similar to a result of Filmus et al. (FOCS 2013). {$\bullet$} Proof complexity: We prove new rank lower bounds as well as obtain the first length--space lower bounds for semi-algebraic proof systems, including Lov{\'a}sz--Schrijver and Lasserre (SOS) systems. In particular, these results extend and simplify the works of Beame et al. (SICOMP 2007) and Huynh and Nordstr{\"o}m.},
  citationcount = {95},
  venue = {Symposium on the Theory of Computing}
}

@article{gsDeterministicCommunicationVs2015,
  title = {Deterministic Communication vs. Partition Number},
  author = {G{\"o}{\"o}s, Mika and Pitassi, T. and Watson, Thomas},
  year = {2015},
  doi = {10.1109/FOCS.2015.70},
  abstract = {We show that deterministic communication complexity can be super logarithmic in the partition number of the associated communication matrix. We also obtain near-optimal deterministic lower bounds for the Clique vs. Independent Set problem, which in particular yields new lower bounds for the log-rank conjecture. All these results follow from a simple adaptation of a communication-to-query simulation theorem of Raz and McKenzie (Combinatorica 1999) together with lower bounds for the analogous query complexity questions.},
  citationcount = {148},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{gsQueryToCommunication2017,
  title = {Query-to-Communication Lifting for {{PNP}}},
  author = {G{\"o}{\"o}s, Mika and Kamath, Pritish and Pitassi, T. and Watson, Thomas},
  year = {2017},
  doi = {10.1007/s00037-018-0175-5},
  abstract = {No abstract available},
  citationcount = {30},
  venue = {Computational Complexity}
}

@article{gsQueryToCommunication2017,
  title = {Query-to-Communication Lifting for {{BPP}}},
  author = {G{\"o}{\"o}s, Mika and Pitassi, T. and Watson, Thomas},
  year = {2017},
  doi = {10.1109/FOCS.2017.21},
  abstract = {For any n-bit boolean function f, we show that the randomized communication complexity of the composed function f o g{\textasciicircum}n, where g is an index gadget, is characterized by the randomized decision tree complexity of f. In particular, this means that many query complexity separations involving randomized models (e.g., classical vs. quantum) automatically imply analogous separations in communication complexity.},
  citationcount = {83},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{gsRectanglesAreNonnegative2015,
  title = {Rectangles Are Nonnegative Juntas},
  author = {G{\"o}{\"o}s, Mika and Lovett, Shachar and Meka, Raghu and Watson, Thomas and Zuckerman, David},
  year = {2015},
  doi = {10.1145/2746539.2746596},
  abstract = {We develop a new method to prove communication lower bounds for composed functions of the form f o gn where f is any boolean function on n inputs and g is a sufficiently "hard" two-party gadget. Our main structure theorem states that each rectangle in the communication matrix of f o gn can be simulated by a nonnegative combination of juntas. This is the strongest yet formalization for the intuition that each low-communication randomized protocol can only "query" few inputs of f as encoded by the gadget g. Consequently, we characterize the communication complexity of f o gn in all known one-sided zero-communication models by a corresponding query complexity measure of f. These models in turn capture important lower bound techniques such as corruption, smooth rectangle bound, relaxed partition bound, and extended discrepancy. As applications, we resolve several open problems from prior work: We show that SBPcc (a class characterized by corruption) is not closed under intersection. An immediate corollary is that MAcc {$\neq$} SBPcc. These results answer questions of Klauck (CCC 2003) and Bohler et al. (JCSS 2006). We also show that approximate nonnegative rank of partial boolean matrices does not admit efficient error reduction. This answers a question of Kol et al. (ICALP) for partial matrices.},
  citationcount = {115},
  venue = {SIAM journal on computing (Print)}
}

@article{goranciDynamicEffectiveResistances2018,
  title = {Dynamic Effective Resistances and Approximate Schur Complement on Separable Graphs},
  author = {Goranci, Gramoz and Henzinger, Monika and Peng, Pan},
  year = {2018},
  doi = {10.4230/LIPIcs.ESA.2018.40},
  abstract = {We consider the problem of dynamically maintaining (approximate) all-pairs effective resistances in separable graphs, which are those that contain small balanced separators. We give a fully dynamic algorithm that maintains (1+{$\varepsilon$})-approximations of the all-pairs effective resistances of an n-vertex graph G undergoing edge insertions and deletions with O\vphantom\{\}({\textsurd}\{n\}/{$\varepsilon^2$}) worst-case update time and O\vphantom\{\}({\textsurd}\{n\}/{$\varepsilon^2$}) worst-case query time, if G is guaranteed to be O({\textsurd}\{n\})-separable (i.e., admit a balanced separator of size O({\textsurd}\{n\})) and its separator can be computed in O\vphantom\{\}(n) time. Our algorithm is built upon a dynamic algorithm for maintaining \{approximate Schur complement\} that approximately preserves pairwise effective resistances among a set of terminals for separable graphs, which might be of independent interest. We also show that our algorithm is close to optimal by proving that for any two fixed vertices s and t, there is no incremental or decremental (and thus, also no fully dynamic) algorithm that (1+O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{n\vphantom\}\textsuperscript{\{\vphantom\}}36\vphantom\{\}\vphantom\{\}))-approximates the s-t effective resistance for O({\textsurd}\{n\})-separable graphs with worst-case update time O(n\textsuperscript{\{\vphantom\}}1/2-{$\delta$}\vphantom\{\}) and query time O(n\textsuperscript{\{\vphantom\}}1-{$\delta$}\vphantom\{\}) for any {$\delta>$}0, unless the Online Matrix Vector Multiplication (OMv) conjecture is false. We further show that for \{general\} graphs, no incremental or decremental algorithm can maintain a (1+O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{n\vphantom\}\textsuperscript{\{\vphantom\}}20\vphantom\{\}\vphantom\{\}))-approximation of the s-t effective resistance with worst-case update time O(n\textsuperscript{\{\vphantom\}}1-{$\delta$}\vphantom\{\}) and query-time O(n\textsuperscript{\{\vphantom\}}2-{$\delta$}\vphantom\{\}) for any {$\delta>$}0, unless the OMv conjecture is false.},
  citationcount = {22},
  venue = {Embedded Systems and Applications},
  keywords = {dynamic,query,query time,update,update time}
}

@article{goranciEfficientDataStructures2023,
  title = {Efficient Data Structures for Incremental Exact and Approximate Maximum Flow},
  author = {Goranci, Gramoz and Henzinger, Monika},
  year = {2023},
  doi = {10.4230/LIPIcs.ICALP.2023.69},
  abstract = {We show an (1 + {$\epsilon$} )-approximation algorithm for maintaining maximum s - t flow under m edge insertions in m 1 / 2+ o (1) {$\epsilon$} - 1 / 2 amortized update time for directed, unweighted graphs. This constitutes the first sublinear dynamic maximum flow algorithm in general sparse graphs with arbitrarily good approximation guarantee. Furthermore we give an algorithm that maintains an exact maximum s - t flow under m edge insertions in an n -node graph in {\texttildelow} O ( n 5 / 2 ) total update time. For sufficiently dense graphs, this gives to the first exact incremental algorithm with sub-linear amortized update time for maintaining maximum flows},
  citationcount = {2},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,dynamic,update,update time}
}

@article{goranciIncrementalApproximateMaximum2022,
  title = {Incremental Approximate Maximum Flow in M1/2+o(1) Update Time},
  author = {Goranci, Gramoz and Henzinger, Monika},
  year = {2022},
  doi = {10.48550/arXiv.2211.09606},
  abstract = {We show an (1+{$\epsilon$})-approximation algorithm for maintaining maximum s-t flow under m edge insertions in m\textsuperscript{\{\vphantom\}}1/2+o(1)\vphantom\{\}{$\epsilon$}\textsuperscript{\{\vphantom\}}-1/2\vphantom\{\} amortized update time for directed, unweighted graphs. This constitutes the first sublinear dynamic maximum flow algorithm in general sparse graphs with arbitrarily good approximation guarantee.},
  citationcount = {1},
  venue = {arXiv.org},
  keywords = {dynamic,update,update time}
}

@article{goranciIncrementalApproximateMaximum2025,
  title = {Incremental Approximate Maximum Flow via Residual Graph Sparsification},
  author = {Goranci, Gramoz and Henzinger, Monika and Racke, H. and Sricharan, A. R.},
  year = {2025},
  doi = {10.48550/arXiv.2502.09105},
  abstract = {We give an algorithm that, with high probability, maintains a (1-{$\epsilon$})-approximate s-t maximum flow in undirected, uncapacitated n-vertex graphs undergoing m edge insertions in O\vphantom\{\}(m+nF\textsuperscript{*}/{$\epsilon$}) total update time, where F\textsuperscript{\{\vphantom\}}*\vphantom\{\} is the maximum flow on the final graph. This is the first algorithm to achieve polylogarithmic amortized update time for dense graphs (m={\textohm}(n{$^2$})), and more generally, for graphs where F\textsuperscript{*}=O\vphantom\{\}(m/n). At the heart of our incremental algorithm is the residual graph sparsification technique of Karger and Levine [SICOMP '15], originally designed for computing exact maximum flows in the static setting. Our main contributions are (i) showing how to maintain such sparsifiers for approximate maximum flows in the incremental setting and (ii) generalizing the cut sparsification framework of Fung et al. [SICOMP '19] from undirected graphs to balanced directed graphs.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {static,update,update time}
}

@article{goranciIncrementalExactMin2016,
  title = {Incremental Exact Min-Cut in Polylogarithmic Amortized Update Time},
  author = {Goranci, Gramoz and Henzinger, Monika and Thorup, M.},
  year = {2016},
  doi = {10.1145/3174803},
  abstract = {We present a deterministic incremental algorithm for exactly maintaining the size of a minimum cut with O(log3 n log log2 n) amortized time per edge insertion and O(1) query time. This result partially answers an open question posed by Thorup (2007). It also stays in sharp contrast to a polynomial conditional lower bound for the fully dynamic weighted minimum cut problem. Our algorithm is obtained by combining a sparsification technique of Kawarabayashi and Thorup (2015) or its recent improvement by Henzinger, Rao, and Wang (2017), and an exact incremental algorithm of Henzinger (1997). We also study space-efficient incremental algorithms for the minimum cut problem. Concretely, we show that there exists an O(nlog n/{$\varepsilon$}2) space Monte Carlo algorithm that can process a stream of edge insertions starting from an empty graph, and with high probability, the algorithm maintains a (1+{$\varepsilon$})-approximation to the minimum cut. The algorithm has O(({$\alpha$} (n) log3 n)/{$\varepsilon$} 2) amortized update time and constant query time, where {$\alpha$} (n) stands for the inverse of Ackermann function.},
  citationcount = {29},
  venue = {Embedded Systems and Applications},
  keywords = {dynamic,lower bound,query,query time,update,update time}
}

@article{goranciThePowerOf2017,
  title = {The Power of Vertex Sparsifiers in Dynamic Graph Algorithms},
  author = {Goranci, Gramoz and Henzinger, Monika and Peng, Pan},
  year = {2017},
  doi = {10.4230/LIPIcs.ESA.2017.45},
  abstract = {We introduce a new algorithmic framework for designing dynamic graph algorithms in minor-free graphs, by exploiting the structure of such graphs and a tool called vertex sparsification, which is a way to compress large graphs into small ones that well preserve relevant properties among a subset of vertices and has previously mainly been used in the design of approximation algorithms. Using this framework, we obtain a Monte Carlo randomized fully dynamic algorithm for (1+{$\varepsilon$})-approximating the energy of electrical flows in n-vertex planar graphs with O\vphantom\{\}(r{$\varepsilon$}\textsuperscript{\{\vphantom\}}-2\vphantom\{\}) worst-case update time and O\vphantom\{\}((r+\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}\vphantom\{\}\{{\textsurd}\{r\}\}){$\varepsilon$}\textsuperscript{\{\vphantom\}}-2\vphantom\{\}) worst-case query time, for any r larger than some constant. For r=n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}, this gives O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}{$\varepsilon$}\textsuperscript{\{\vphantom\}}-2\vphantom\{\}) update time and O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}{$\varepsilon$}\textsuperscript{\{\vphantom\}}-2\vphantom\{\}) query time. We also extend this algorithm to work for minor-free graphs with similar approximation and running time guarantees. Furthermore, we illustrate our framework on the all-pairs max flow and shortest path problems by giving corresponding dynamic algorithms in minor-free graphs with both sublinear update and query times. To the best of our knowledge, our results are the first to systematically establish such a connection between dynamic graph algorithms and vertex sparsification. We also present both upper bound and lower bound for maintaining the energy of electrical flows in the incremental subgraph model, where updates consist of only vertex activations, which might be of independent interest.},
  citationcount = {19},
  venue = {Embedded Systems and Applications},
  keywords = {dynamic,lower bound,query,query time,update,update time}
}

@article{goswamiOnTheI2020,
  title = {On the {{I}}/{{O}} Complexity of the k-{{Nearest}} Neighbors Problem},
  author = {Goswami, Mayank and Jacob, R. and Pagh, R.},
  year = {2020},
  doi = {10.1145/3375395.3387649},
  abstract = {We consider static, external memory indexes for exact and approximate versions of the k-nearest neighbor (k-NN) problem, and show new lower bounds under a standard indivisibility assumption: Polynomial space indexing schemes for high-dimensional k-NN in Hamming space cannot take advantage of block transfers: {\'i}(k) block reads are needed to to answer a query. For the l{$\infty$} metric the lower bound holds even if we allow c-appoximate nearest neighbors to be returned, for c {$\in$} (1, 3). The restriction to c {\textexclamdown} 3 is necessary: For every metric there exists an indexing scheme in the indexability model of Hellerstein et al. using space O(kn), where n is the number of points, that can retrieve k 3-approximate nearest neighbors using optimal {$\lceil$}k/B{$\rceil$} I/Os, where B is the block size. For specific metrics, data structures with better approximation factors are possible. For k-NN in Hamming space and every approximation factor c{\textquestiondown}1 there exists a polynomial space data structure that returns k c-approximate nearest neighbors in {$\lceil$}k/B{$\rceil$} I/Os. To show these lower bounds we develop two new techniques: First, to handle that approximation algorithms have more freedom in deciding which result set to return we develop a relaxed version of the {$\lambda$}-set workload technique of Hellerstein et al. This technique allows us to show lower bounds that hold in d {$\geq$} n dimensions. To extend the lower bounds down to d = O(k log(n/k)) dimensions, we develop a new deterministic dimension reduction technique that may be of independent interest.},
  citationcount = {1},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {data structure,lower bound,query,reduction,static}
}

@article{gowersMixingInNon2021,
  title = {Mixing in Non-Quasirandom Groups},
  author = {Gowers, T. and Viola, Emanuele},
  year = {2021},
  doi = {10.4230/LIPIcs.ITCS.2022.80},
  abstract = {We initiate a systematic study of mixing in non-quasirandom groups. Let A and B be two independent, high-entropy distributions over a group G . We show that the product distribution AB is statistically close to the distribution F ( AB ) for several choices of G and F , including: (1) G is the affine group of 2 {\texttimes} 2 matrices, and F sets the top-right matrix entry to a uniform value, (2) G is the lamplighter group, that is the wreath product of Z 2 and Z n , and F is multiplication by a certain subgroup, (3) G is H n where H is non-abelian, and F selects a uniform coordinate and takes a uniform conjugate of it. The obtained bounds for (1) and (2) are tight. This work is motivated by and applied to problems in communication complexity. We consider the 3-party communication problem of deciding if the product of three group elements multiplies to the identity. We prove lower bounds for the groups above, which are tight for the affine and the lamplighter groups.},
  citationcount = {3},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {communication,communication complexity,lower bound}
}

@article{goyalParallelSearchGame2005,
  title = {A Parallel Search Game},
  author = {Goyal, Navin and Saks, M.},
  year = {2005},
  doi = {10.1002/rsa.20068},
  abstract = {We answer in negative a question of G{\'a}l and Miltersen [Proc 30th Int Coll Automata, Languages, and Programming (ICALP) 2003, pp. 332--344] about a combinatorial game arising in the study of time-space trade-offs for data structures. {\copyright} 2005 Wiley Periodicals, Inc. Random Struct. Alg., 2005},
  citationcount = {11},
  venue = {Random Struct. Algorithms},
  keywords = {data structure}
}

@article{grahamConcreteMathematicsA1991,
  title = {Concrete Mathematics - a Foundation for Computer Science},
  author = {Graham, R. and Knuth, D. and Patashnik, Oren},
  year = {1991},
  doi = {10.2307/2324448},
  abstract = {From the Publisher: This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. "More concretely," the authors explain, "it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems." The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. Major topics include: Sums Recurrences Integer functions Elementary number theory Binomial coefficients Generating functions Discrete probability Asymptotic methods This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.},
  citationcount = {2946},
  venue = {No venue available}
}

@article{grayNearestNeighborQueries2010,
  title = {Nearest-Neighbor Queries with Well-Spaced Points},
  author = {Gray, C.},
  year = {2010},
  doi = {10.1109/ISVD.2010.27},
  abstract = {We assume we are given a set of points that have the property that their Voronoi diagram---restricted to some suitable bounding box---consists of only fat cells. We call such points \{well-spaced points\}. We give a linear-sized data structure for finding the nearest neighbor to a query point among well-spaced points in O(n) time. We further show how to extend the results to higher dimensions. Finally, we show how to find the Voronoi diagram of these points in O(nn) time in 3 dimensions.},
  citationcount = {3},
  venue = {International Symposium on Voronoi Diagrams in Science and Engineering},
  keywords = {data structure,query}
}

@inproceedings{green2021further,
  title = {Further Unifying the Landscape of Cell Probe Lower Bounds},
  booktitle = {Symposium on Simplicity in Algorithms ({{SOSA}})},
  author = {Green Larsen, Kasper and Lindegaard Starup, Jonathan and Steensgaard, Jesper},
  year = {2021},
  pages = {224--231},
  publisher = {SIAM},
  doi = {10.1137/1.9781611976496.25},
  abstract = {In a landmark paper, P{\v a}tra{\c s}cu demonstrated how a single lower bound for the static data structure problem of reachability in the butterfly graph, could be used to derive a wealth of new and previous lower bounds via reductions. These lower bounds are tight for numerous static data structure problems. Moreover, he also showed that reachability in the butterfly graph reduces to dynamic marked ancestor, a classic problem used to prove lower bounds for dynamic data structures. Unfortunately, P{\v a}tra{\c s}cu's reduction to marked ancestor loses a lg lg n factor and therefore falls short of fully recovering all the previous dynamic data structure lower bounds that follow from marked ancestor. In this paper, we revisit P{\v a}tra{\c s}cu's work and give a new lossless reduction to dynamic marked ancestor, thereby establishing reachability in the butterfly graph as a single seed problem from which a range of tight static and dynamic data structure lower bounds follow.},
  keywords = {cell probe,data structure,dynamic,lower bound,reduction,static},
  file = {/Users/tulasi/Zotero/storage/CBJ9D3SR/Farach-Colton and Storandt - 2021 - 2021 Proceedings of the Workshop on Algorithm Engineering and Experiments (ALENEX).pdf}
}

@article{grenetDeterministicRootFinding2015,
  title = {Deterministic Root Finding over Finite Fields Using {{Graeffe}} Transforms},
  author = {Grenet, Bruno and Hoeven, J. and Lecerf, Gr{\'e}goire},
  year = {2015},
  doi = {10.1007/s00200-015-0280-5},
  abstract = {No abstract available},
  citationcount = {29},
  venue = {Applicable Algebra in Engineering, Communication and Computing}
}

@article{grenetRandomizedRootFinding2015,
  title = {Randomized Root Finding over Finite {{FFT-fields}} Using Tangent Graeffe Transforms},
  author = {Grenet, Bruno and Hoeven, J. and Lecerf, Gr{\'e}goire},
  year = {2015},
  doi = {10.1145/2755996.2756647},
  abstract = {Consider a finite field Fq whose multiplicative group has smooth cardinality. We study the problem of computing all roots of a polynomial that splits over Fq, which was one of the bottlenecks for fast sparse interpolation in practice. We revisit and slightly improve existing algorithms and then present new randomized ones based on the Graeffe transform. We report on our implementation in the MATHEMAGIX computer algebra system, confirming that our ideas gain by a factor ten at least in practice, for sufficiently large inputs.},
  citationcount = {12},
  venue = {International Symposium on Symbolic and Algebraic Computation}
}

@inproceedings{greveCellProbeLower2010,
  title = {Cell Probe Lower Bounds and Approximations for Range Mode},
  booktitle = {Proc. 37th {{International Colloquium}} on {{Automata}}, {{Languages}}, and {{Programming}}},
  author = {Greve, M. and J{\o}rgensen, A. G. and Larsen, K. D. and Truelsen, J.},
  year = {2010},
  pages = {605--616},
  doi = {10.1007/978-3-642-14165-2_51},
  keywords = {cell probe,lower bound},
  annotation = {Yao, A.C.C.: Should tables be sorted? J. ACM~28(3), 615--628 (1981)\\
Krizanc, D., Morin, P., Smid, M.H.M.: Range mode and range median queries on lists and trees. Nord. J. Comput.~12(1), 1--17 (2005)\\
Petersen, H.: Improved bounds for range mode and range median queries. In: Geffert, V., Karhum{\"a}ki, J., Bertoni, A., Preneel, B., N{\'a}vrat, P., Bielikov{\'a}, M. (eds.) SOFSEM 2008. LNCS, vol.~4910, pp. 418--423. Springer, Heidelberg (2008)\\
Petersen, H., Grabowski, S.: Range mode and range median queries in constant time and sub-quadratic space. Inf. Process. Lett.~109(4), 225--228 (2008)\\
Bose, P., Kranakis, E., Morin, P., Tang, Y.: Approximate range mode and range median queries. In: Proc. 22nd Symposium on Theoretical Aspects of Computer Science, pp. 377--388 (2005)\\
Patrascu, M., Thorup, M.: Higher lower bounds for near-neighbor and further rich problems. In: Proc. of the 47th Annual IEEE Symposium on Foundations of Computer Science, pp. 646--654 (2006)\\
P{\v a}tra{\c s}cu, M.: (Data) structures. In: Proc. 49th Annual IEEE Symposium on Foundations of Computer Science, pp. 434--443 (2008)\\
P{\v a}tra{\c s}cu, M.: Lower bounds for 2-dimensional range counting. In: Proc. 39th ACM Symposium on Theory of Computing, pp. 40--46 (2007)\\
J{\'a}J{\'a}, J., Mortensen, C.W., Shi, Q.: Space-efficient and fast algorithms for multidimensional dominance reporting and counting. In: Fleischer, R., Trippen, G. (eds.) ISAAC 2004. LNCS, vol.~3341, pp. 558--568. Springer, Heidelberg (2004)\\
Afshani, P.: On dominance reporting in 3D. In: Proc. of the 16th Annual European Symposium on Algorithms, pp. 41--51 (2008)\\
Willard, D.E.: Log-logarithmic worst-case range queries are possible in space theta(n). Inf. Process. Lett.~17(2), 81--84 (1983)\\
Miltersen, P.B., Nisan, N., Safra, S., Wigderson, A.: On data structures and asymmetric communication complexity. J. Comput. Syst. Sci.~57(1), 37--49 (1998)\\
Harel, D., Tarjan, R.E.: Fast algorithms for finding nearest common ancestors. SIAM J. Comput.~13(2), 338--355 (1984)\\
Driscoll, J.R., Sarnak, N., Sleator, D.D., Tarjan, R.E.: Making data structures persistent. Journal of Computer and System Sciences~38(1), 86--124 (1989)\\
Jacobson, G.J.: Succinct static data structures. PhD thesis, Carnegie Mellon University, Pittsburgh, PA, USA (1988)},
  file = {/Users/tulasi/Zotero/storage/9SUHY6F8/Greve et al. - 2010 - Cell probe lower bounds and approximations for range mode.pdf}
}

@article{grezDynamicDataStructures2022,
  title = {Dynamic Data Structures for Timed Automata Acceptance},
  author = {Grez, Alejandro and Mazowiecki, Filip and Pilipczuk, Michal and Puppis, Gabriele and Riveros, Cristian},
  year = {2022},
  doi = {10.1007/s00453-022-01025-8},
  abstract = {No abstract available},
  citationcount = {9},
  venue = {Algorithmica},
  keywords = {data structure,dynamic}
}

@article{grezTowardsStreamingEvaluation2020,
  title = {Towards Streaming Evaluation of Queries with Correlation in Complex Event Processing},
  author = {Grez, Alejandro and Riveros, Cristian},
  year = {2020},
  doi = {10.4230/LIPIcs.ICDT.2020.14},
  abstract = {Complex event processing (CEP) has gained a lot of attention for evaluating complex patterns over high-throughput data streams. Recently, new algorithms for the evaluation of CEP patterns have emerged with strong guarantees of efficiency, i.e. constant update-time per tuple and constant-delay enumeration. Unfortunately, these techniques are restricted for patterns with local filters, limiting the possibility of using joins for correlating the data of events that are far apart. In this paper, we embark on the search for efficient evaluation algorithms of CEP patterns with joins. We start by formalizing the so-called partition-by operator, a standard operator in data stream management systems to correlate contiguous events on streams. Although this operator is a restricted version of a join query, we show that partition-by (without iteration) is equally expressive as hierarchical queries, the biggest class of full conjunctive queries that can be evaluated with constant update-time and constant-delay enumeration over streams. To evaluate queries with partition-by we introduce an automata model, called chain complex event automata (chain-CEA), an extension of complex event automata that can compare data values by using equalities and disequalities. We show that this model admits determinization and is expressive enough to capture queries with partition-by. More importantly, we provide an algorithm with constant update time and constant delay enumeration for evaluating any query definable by chain-CEA, showing that all CEP queries with partition-by can be evaluated with these strong guarantees of efficiency.},
  citationcount = {8},
  venue = {International Conference on Database Theory},
  keywords = {query,update,update time}
}

@article{gribelyukAStrongSeparation2024,
  title = {A Strong Separation for Adversarially Robust {$\ell$}0 Estimation for Linear Sketches},
  author = {Gribelyuk, Elena and Lin, Honghao and Woodruff, David P. and Yu, Huacheng and Zhou, Samson},
  year = {2024},
  doi = {10.1109/FOCS61266.2024.00136},
  abstract = {The majority of streaming problems are defined and analyzed in a static setting, where the data stream is any worst-case sequence of insertions and deletions which is fixed in advance. However, many real-world applications require a more flexible model, where an adaptive adversary may select future stream elements after observing the previous outputs of the algorithm. Over the last few years, there has been increased interest in proving lower bounds for natural problems in the adaptive streaming model. In this work, we give the first known adaptive attack against linear sketches for the well-studied {$\ell$}\textsubscript{\{\vphantom\}}0\vphantom\{\}-estimation problem over turnstile, integer streams. For any linear streaming algorithm \{A\} which uses sketching matrix \{A\}{$\varepsilon$}\{Z\}\textsuperscript{\{\vphantom\}}r{\texttimes}n\vphantom\{\}, this attack makes \{O\}\vphantom\{\}(r\textsuperscript{\{\vphantom\}}8\vphantom\{\}) queries and succeeds with high constant probability in breaking the sketch. Additionally, we give an adaptive attack against linear sketches for the {$\ell$}\textsubscript{\{\vphantom\}}0\vphantom\{\}-estimation problem over finite fields \{F\}\textsubscript{\{\vphantom\}}p\vphantom\{\}, which requires a smaller number of \{O\}\vphantom\{\}(r\textsuperscript{\{\vphantom\}}3\vphantom\{\}) queries. Finally, we provide an adaptive attack over \{R\}\textsuperscript{\{\vphantom\}}n\vphantom\{\} against linear sketches A {$\in$}\{R\}\textsuperscript{\{\vphantom\}}r{\texttimes}\{n\}\vphantom\{\} for {$\ell$}\textsubscript{\{\vphantom\}}0\vphantom\{\}-estimation, in the setting where A has all nonzero subdeterminants at least \textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{\{poly\}(r)\}. Our results provide an exponential improvement over the previous number of queries known to break an {$\ell$}\textsubscript{\{\vphantom\}}0\vphantom\{\}-estimation sketch.},
  citationcount = {3},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {adaptive,lower bound,query,static}
}

@article{grigorescuOnNoiseTolerant2011,
  title = {On Noise-Tolerant Learning of Sparse Parities and Related Problems},
  author = {Grigorescu, Elena and Reyzin, L. and Vempala, S.},
  year = {2011},
  doi = {10.1007/978-3-642-24412-4_32},
  abstract = {No abstract available},
  citationcount = {21},
  venue = {International Conference on Algorithmic Learning Theory}
}

@article{grigorievRandomizedComplexityLower1998,
  title = {Randomized Complexity Lower Bounds},
  author = {Grigoriev, D.},
  year = {1998},
  doi = {10.1145/276698.276745},
  abstract = {The complexity lower bound (log N) is proved for randomized computation trees (over reals with branching signs f ; {\textquestiondown}g) for recognizing an arrangement or a polyhedron with N faces. A similar lower bound is proved for randomized computation trees over any zero-characteristic eld with branching signs f=; 6 =g for recognizing an arrangement. As consequences, this provides in particular, the randomized lower bound (n 2) for the KNAPSACK problem (which was proved in case of the randomized computation trees over reals in 11]) and also the randomized lower bound (n log n)},
  citationcount = {14},
  venue = {Symposium on the Theory of Computing}
}

@article{grigorievRandomizedComplexityLower1999,
  title = {Randomized Complexity Lower Bound for Arrangements and Polyhedra},
  author = {Grigoriev, D.},
  year = {1999},
  doi = {10.1007/PL00009425},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Discrete \& Computational Geometry}
}

@article{grigorievRandomizedN21997,
  title = {Randomized {{{\textohm}}}(N2) Lower Bound for Knapsack},
  author = {Grigoriev, D. and Karpinski, Marek},
  year = {1997},
  doi = {10.1145/258533.258555},
  abstract = {We prove f2(n2) complexity lower bound for the general model of randomized computation trees solving the Knapsack Problem, and more generally Restricted Integer Programming. This is the first nontrivial lower bound proven for this model of computation. The method of the proof depends crucially on the new technique for proving lower bounds on the border complexity of a polynomial which could be of independent interest. O Introduction We prove for the first time nonlinear lower bounds on the depth of randomized computation trees (RCTS) (see e.g. [MT82], [S83], [M85a], [GKMS96]) recognizing sets like unions of hyperpianes (i.e. linear arrangements) or intersections of halfspaces (polyhedra). As an application we prove a quadratic lower bound on RCTS solving the knapsack problem, or more generally, the restricted integer programming. Obtaining general lower bounds for randomized computation was an open question for a long time (see Only recently, a nonlinear lower bound was proven in [GKMS96] for a weaker model of randomized d-decision trees (d-RDTs), in which testing polynomials have degrees at most d (for 2-dimensional case the lower bound was proven in [GK93], and for the generic arrangements a lower bound was proved in [GK94]). In particular, for d-RDTs the lower bound Q(n log n) was proven for the Element Distinctness Problem, and also the lower bound fl(nz) was proved for the Knapsack Problem ([GKMS96]). The main difficulty whith proving lower bounds on RCTS is that the degree of testing polyno-mials could be possibly exponential in n. Therefore, we develop in the present paper a new method for obtaining complexity lower bounds for RCTS. The method developed in the present paper cannot be directly applied for the Element Distinctness Problem. In [BKL93] (cf. also [GKMS96]), a linear depth RCT was constructed for a similar problem (permutation problem) \{(z, y) E lR2n : y is a permutation of x\} beating therefore its deterministic Q(n log n) lower bound (cf. [B83]). This example shows that the (still open problem) of complexity of an RCT for the Element Distinctness is quite delicate. We also mention that a linear   lower bound for an RCT recognizing the arrangement UI {\textexclamdown}i{\textexclamdown}n \{xi = O\} or the " orthant " nl {\textexclamdown}i{\textexclamdown}n\{Xi   O\} W*- roved in [GKMS96]. For a stron er-model of randomized analytic decision trees (RADT) a complexity upper bound 0(log2 n) for testing nl{\textexclamdown}i{\textexclamdown}n\{Xi   O\} was proven .-in [GKS96] (for deterministic analytic decision trees the exact complexity {\dots}},
  citationcount = {8},
  venue = {Symposium on the Theory of Computing}
}

@inproceedings{gronlundTightLowerBounds2016,
  title = {Towards {{Tight Lower Bounds}} for {{Range Reporting}} on the {{RAM}}},
  booktitle = {43rd {{International Colloquium}} on {{Automata}}, {{Languages}}, and {{Programming}} ({{ICALP}} 2016)},
  author = {Gr{\o}nlund, Allan and Larsen, Kasper Green},
  year = {2016},
  pages = {92:1-92:12},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  doi = {10.4230/LIPIcs.ICALP.2016.92},
  url = {https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ICALP.2016.92},
  urldate = {2024-11-20},
  abstract = {In the orthogonal range reporting problem, we are to preprocess a set of n points with integer coordinates on a UxU grid. The goal is to support reporting all k points inside an axis-aligned query rectangle. This is one of the most fundamental data structure problems in databases and computational geometry. Despite the importance of the problem its complexity remains unresolved in the word-RAM. On the upper bound side, three best tradeoffs exist, all derived by reducing range reporting to a ball-inheritance problem. Ball-inheritance is a problem that essentially encapsulates all previous attempts at solving range reporting in the word-RAM. In this paper we make progress towards closing the gap between the upper and lower bounds for range reporting by proving cell probe lower bounds for ball-inheritance. Our lower bounds are tight for a large range of parameters, excluding any further progress for range reporting using the ball-inheritance reduction.},
  copyright = {https://creativecommons.org/licenses/by/3.0/legalcode},
  langid = {english},
  keywords = {cell probe,data structure,lower bound,query,reduction,sorted},
  file = {/Users/tulasi/Zotero/storage/RT6KJKRH/Grnlund and Larsen - 2016 - Towards Tight Lower Bounds for Range Reporting on the RAM.pdf}
}

@article{grossiAsymptoticallyOptimalEncodings2017,
  title = {Asymptotically Optimal Encodings of Range Data Structures for Selection and Top-k Queries},
  author = {Grossi, R. and Iacono, John and Navarro, G. and Raman, R. and Satti, S. R.},
  year = {2017},
  doi = {10.1145/3012939},
  abstract = {Given an array A[1, n] of elements with a total order, we consider the problem of building a data structure that solves two queries: (a) selection queries receive a range [i, j] and an integer k and return the position of the kth largest element in A[i, j]; (b) top-k queries receive [i, j] and k and return the positions of the k largest elements in A[i, j]. These problems can be solved in optimal time, O(1+lg k/lg lg n) and O(k), respectively, using linear-space data structures. We provide the first study of the encoding data structures for the above problems, where A cannot be accessed at query time. Several applications are interested in the relative order of the entries of A, and their positions, rather their actual values, and thus we do not need to keep A at query time. In those cases, encodings save storage space: we first show that any encoding answering such queries requires nlg k - O(n+k lg k) bits of space; then, we design encodings using O(nlg k) bits, that is, asymptotically optimal up to constant factors, while preserving optimal query time.},
  citationcount = {10},
  venue = {ACM Trans. Algorithms},
  keywords = {data structure,query,query time}
}

@article{grossiColoredRangeSearching2014,
  title = {Colored Range Searching in Linear Space},
  author = {Grossi, R. and Vind, S{\o}ren},
  year = {2014},
  doi = {10.1007/978-3-319-08404-6_20},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Scandinavian Workshop on Algorithm Theory}
}

@article{grossiCompressedSuffixArrays2000,
  title = {Compressed Suffix Arrays and Suffix Trees with Applications to Text Indexing and String Matching (Extended Abstract)},
  author = {Grossi, R. and Vitter, J.},
  year = {2000},
  doi = {10.1145/335305.335351},
  abstract = {The proliferation of online text, such as found on the World Wide Web and in online databases, motivates the need for space-efficient text indexing methods that support fast string searching. We model this scenario as follows: Consider a text T consisting of n symbols drawn from a fixed alphabet {$\Sigma$}. The text T can be represented in n{\textbar}{$\Sigma\vert$} bits by encoding each symbol with {\textbar}{$\Sigma\vert$} bits. The goal is to support fast online queries for searching any string pattern P of m symbols, with T being fully scanned only once, namely, when the index is created at preprocessing time. The text indexing schemes published in the literature are greedy in terms of space usage: they require {\textohm}(nn) additional bits of space in the worst case. For example, in the standard unit cost RAM, suffix trees and suffix arrays need {\textohm}(n) memory words, each of {\textohm}(n) bits. These indexes are larger than the text itself by a multiplicative factor of {\textohm}(\{\vphantom\}\textsubscript{\{\vphantom\}}{\textbar}{$\Sigma\vert$}\vphantom\{\}n\vphantom\{\}), which is significant when {$\Sigma$} is of constant size, such as in \{ascii\} or \{unicode\}. On the other hand, these indexes support fast searching, either in O(m{\textbar}{$\Sigma\vert$}) time or in O(m+n) time, plus an output-sensitive cost O(\{occ\}) for listing the \{occ\} pattern occurrences. We present a new text index that is based upon compressed representations of suffix arrays and suffix trees. It achieves a fast \{O(m/\vphantom\}\textsubscript{\{\vphantom\}}{\textbar}{$\Sigma\vert$}\vphantom\{\}n+\textsubscript{\{\vphantom\}}{\textbar}{$\Sigma\vert$}\vphantom\{\}\textsuperscript{{$\epsilon$}}n)\vphantom\{\} search time in the worst case, for any constant 0{$<\epsilon\leq$}1, using at most \{({$\epsilon\vphantom\}$}\textsuperscript{\{\vphantom\}}-1\vphantom\{\}+O(1))\,n{\textbar}{$\Sigma\vert$}\vphantom\{\} bits of storage. Our result thus presents for the first time an efficient index whose size is provably linear in the size of the text in the worst case, and for many scenarios, the space is actually sublinear in practice. As a concrete example, the compressed suffix array for a typical 100 MB \{ascii\} file can require 30--40 MB or less, while the raw suffix array requires 500 MB. Our theoretical bounds improve \{both\} time and space of previous indexing schemes. Listing the pattern occurrences introduces a sublogarithmic slowdown factor in the output-sensitive cost, giving O(\{occ\}\,\{\vphantom\}\textsubscript{\{\vphantom\}}{\textbar}{$\Sigma\vert$}\vphantom\{\}\textsuperscript{{$\epsilon$}}n\vphantom\{\}) time as a result. When the patterns are sufficiently long, we can use auxiliary data structures in O(n{\textbar}{$\Sigma\vert$}) bits to obtain a total search bound of O(m/\textsubscript{\{\vphantom\}}{\textbar}{$\Sigma\vert$}\vphantom\{\}n+\{occ\}) time, which is optimal.},
  citationcount = {657},
  venue = {Symposium on the Theory of Computing}
}

@article{grossiDesignOfPractical2013,
  title = {Design of Practical Succinct Data Structures for Large Data Collections},
  author = {Grossi, R. and Ottaviano, G.},
  year = {2013},
  doi = {10.1007/978-3-642-38527-8_3},
  abstract = {No abstract available},
  citationcount = {23},
  venue = {The Sea},
  keywords = {data structure}
}

@article{grossiDynamicCompressedStrings2013,
  title = {Dynamic Compressed Strings with Random Access},
  author = {Grossi, R. and Raman, R. and Rao, S. and Venturini, Rossano},
  year = {2013},
  doi = {10.1007/978-3-642-39206-1_43},
  abstract = {No abstract available},
  citationcount = {28},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {dynamic}
}

@article{grossiMoreHasteLess2009,
  title = {More Haste, Less Waste: {{Lowering}} the Redundancy in Fully Indexable Dictionaries},
  author = {Grossi, R. and Orlandi, Alessio and Raman, R. and Rao, S.},
  year = {2009},
  doi = {10.4230/LIPIcs.STACS.2009.1847},
  abstract = {We consider the problem of representing, in a compressed format, a bit-vector S of m bits with n \{1\}s, supporting the following operations, where b{$\in$} \{0\},\{1\} : \{itemize\}  \{rank\}\textsubscript{b}(S,i) returns the number of occurrences of bit b in the prefix S[1..i];  \{select\}\textsubscript{b}(S,i) returns the position of the ith occurrence of bit b in S. \{itemize\} Such a data structure is called \{fully indexable dictionary (\{fid\})\} [Raman, Raman, and Rao, 2007], and is at least as powerful as predecessor data structures. Viewing S as a set X= x\_1,x\_2,{\dots},x\_n  of n distinct integers drawn from a universe [m]= 1,{\dots},m , the predecessor of integer y{$\in$}[m] in X is given by \{\{select\}\vphantom\}\textsuperscript{\{\vphantom\}}\vphantom\{\}{$_{1}$}\vphantom\{\}(S,\{\{rank\}{$_1$}\}(S,y-1)). \{\{fid\}\}s have many applications in succinct and compressed data structures, as they are often involved in the construction of succinct representation for a variety of abstract data types. Our focus is on space-efficient \{\{fid\}\}s on the \{ram\} model with word size {$\Theta$}(m) and constant time for all operations, so that the time cost is independent of the input size. Given the bitstring S to be encoded, having length m and containing n ones, the minimal amount of information that needs to be stored is B(n,m)={$\lceil$}\{\{m\}\{n\}\}{$\rceil$}. The state of the art in building a \{fid\}for S is given in \{\}[P{\v \{}a\vphantom\{\}tra{\c \{}s\vphantom\{\}cu, 2008] using B(m,n)+O(m/((m/t)\textsuperscript{t}))+O(m\textsuperscript{\{\vphantom\}}3/4\vphantom\{\}) bits, to support the operations in O(t) time. Here, we propose a parametric data structure exhibiting a time/space trade-off such that, for any real constants 00, it uses  bits and performs all the operations in time O(s{$\delta$}\textsuperscript{\{\vphantom\}}-1\vphantom\{\}+{$\varepsilon$}\textsuperscript{\{\vphantom\}}-1\vphantom\{\}). The improvement is twofold: our redundancy can be lowered parametrically and, fixing s=O(1), we get a constant-time \{fid\}whose space is B(n,m)+O(m\textsuperscript{{$\varepsilon$}}/\{poly\}(n)) bits, for sufficiently large m. This is a significant improvement compared to the previous bounds for the general case.},
  citationcount = {50},
  venue = {Symposium on Theoretical Aspects of Computer Science},
  keywords = {data structure}
}

@article{grossiOptimalTradeOffs2010,
  title = {Optimal Trade-Offs for Succinct String Indexes},
  author = {Grossi, R. and Orlandi, Alessio and Raman, R.},
  year = {2010},
  doi = {10.1007/978-3-642-14165-2_57},
  abstract = {No abstract available},
  citationcount = {32},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{grossiRandomAccessTo2013,
  title = {Random Access to High-Order Entropy Compressed Text},
  author = {Grossi, R.},
  year = {2013},
  doi = {10.1007/978-3-642-40273-9_14},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Space-Efficient Data Structures, Streams, and Algorithms}
}

@article{gualResilientLevelAncestor2021,
  title = {Resilient Level Ancestor, Bottleneck, and Lowest Common Ancestor Queries in Dynamic Trees},
  author = {Gual{\`a}, Luciano and Leucci, S. and Ziccardi, Isabella},
  year = {2021},
  doi = {10.1007/s00453-022-01046-3},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Algorithmica},
  keywords = {dynamic,query}
}

@article{gucciaRendicontiDelCircolo1906,
  title = {Rendiconti Del Circolo Matematico Di {{Palermo}}},
  author = {Guccia, G. B.},
  year = {1906},
  doi = {10.1007/BF03014052},
  abstract = {No abstract available},
  citationcount = {231},
  venue = {No venue available}
}

@article{guhaTightLowerBounds2008,
  title = {Tight Lower Bounds for Multi-Pass Stream Computation via Pass Elimination},
  author = {Guha, S. and Mcgregor, A.},
  year = {2008},
  doi = {10.1007/978-3-540-70575-8_62},
  abstract = {No abstract available},
  citationcount = {26},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {lower bound}
}

@article{guoAlgebraicProblemsEquivalent2016,
  title = {Algebraic Problems Equivalent to Beating Exponent 3/2 for Polynomial Factorization over Finite Fields},
  author = {Guo, Zeyu and Narayanan, Anand Kumar and Umans, C.},
  year = {2016},
  doi = {10.4230/LIPIcs.MFCS.2016.47},
  abstract = {The fastest known algorithm for factoring univariate polynomials over finite fields is the Kedlaya-Umans (fast modular composition) implementation of the Kaltofen-Shoup algorithm. It is randomized and takes O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}q+n{$^2$}q) time to factor polynomials of degree n over the finite field \{F\}\textsubscript{q} with q elements. A significant open problem is if the 3/2 exponent can be improved. We study a collection of algebraic problems and establish a web of reductions between them. A consequence is that an algorithm for any one of these problems with exponent better than 3/2 would yield an algorithm for polynomial factorization with exponent better than 3/2.},
  citationcount = {1},
  venue = {International Symposium on Mathematical Foundations of Computer Science},
  keywords = {reduction}
}

@article{guoDensityAwareFeature2020,
  title = {Density-Aware Feature Embedding for Face Clustering},
  author = {Guo, Senhui and Xu, Jing and Chen, Dapeng and Zhang, Chao and Wang, Xiaogang and Zhao, Rui},
  year = {2020},
  doi = {10.1109/cvpr42600.2020.00673},
  abstract = {Clustering has many applications in research and industry. However, traditional clustering methods, such as K-means, DBSCAN and HAC, impose oversimplifying assumptions and thus are not well-suited to face clustering. To adapt to the distribution of realistic problems, a natural approach is to use Graph Convolutional Networks (GCNs) to enhance features for clustering. However, GCNs can only utilize local information, which ignores the overall characterisitcs of the clusters. In this paper, we propose a Density-Aware Feature Embedding Network (DA-Net) for the task of face clustering, which utilizes both local and non-local information, to learn a robust feature embedding. Specifically, DA-Net uses GCNs to aggregate features locally, and then incorporates non-local information using a density chain, which is a chain of faces from low density to high density. This density chain exploits the non-uniform distribution of face images in the dataset. Then, an LSTM takes the density chain as input to generate the final feature embedding. Once this embedding is generated, traditional clustering methods, such as density-based clustering, can be used to obtain the final clustering results. Extensive experiments verify the effectiveness of the proposed feature embedding method, which can achieve state-of-the-art performance on public benchmarks.},
  citationcount = {46},
  venue = {Computer Vision and Pattern Recognition}
}

@article{guptaAFrameworkFor2007,
  title = {A Framework for Dynamizing Succinct Data Structures},
  author = {Gupta, Ankur and Hon, W. and Shah, Rahul and Vitter, J.},
  year = {2007},
  doi = {10.1007/978-3-540-73420-8_46},
  abstract = {No abstract available},
  citationcount = {25},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure}
}

@article{guptaApproachingTheChasm2013,
  title = {Approaching the Chasm at Depth Four},
  author = {Gupta, Ankit and Kamath, Pritish and Kayal, N. and Saptharishi, Ramprasad},
  year = {2013},
  doi = {10.1145/2629541},
  abstract = {Agrawal-Vinay [AV08] and Koiran [Koi12] have recently shown that an exp({$\omega$}({\textsurd}n log2 n)) lower bound for depth four homogeneous circuits computing the permanent with bottom layer of {\texttimes} gates having fanin bounded by {\textsurd}n translates to super-polynomial lower bound for general arithmetic circuits computing the permanent. Motivated by this, we examine the complexity of computing the permanent and determinant via such homogeneous depth four circuits with bounded bottom fanin. We show here that any homogeneous depth four arithmetic circuit with bottom fanin bounded by {\textsurd}n computing the permanent (or the determinant) must be of size exp({\textohm}({\textsurd}n)).},
  citationcount = {115},
  venue = {2013 IEEE Conference on Computational Complexity}
}

@article{guptaCompressedDataStructures2006,
  title = {Compressed Data Structures: Dictionaries and Data-Aware Measures},
  author = {Gupta, Ankur and Hon, W. and Shah, Rahul and Vitter, J.},
  year = {2006},
  doi = {10.1016/j.tcs.2007.07.042},
  abstract = {No abstract available},
  citationcount = {85},
  venue = {Data Compression Conference},
  keywords = {data structure}
}

@article{guptaFurtherResultsOn1993,
  title = {Further Results on Generalized Intersection Searching Problems: {{Counting}}, Reporting, and Dynamization},
  author = {Gupta, Prosenjit and Janardan, Ravi and Smid, M.},
  year = {1993},
  doi = {10.1006/jagm.1995.1038},
  abstract = {In a generalized intersection searching problem, a set, S, of colored geometric objects is to be preprocessed so that given some query object, q, the distinct colors of the objects intersected by q can be reported or counted efficiently. In the dynamic setting, colored objects can be inserted into or deleted from S. These problems generalize the well-studied standard intersection searching problems and are rich in applications. Unfortunately, the techniques known for the standard problems do not yield efficient solutions for the generalized problems. Moreover, previous work on generalized problems applies only to the reporting problems and that too mainly to the static case. In this paper, a uniform framework is presented to solve efficiently the counting/reporting/dynamic versions of a variety of generalized intersection searching problems, including: 1-, 2-, and 3-dimensional range searching, quadrant searching, and 2-dimensional point enclosure searching. Several other related results are also mentioned.},
  citationcount = {105},
  venue = {J. Algorithms}
}

@article{guptaOnlineAndDynamic2016,
  title = {Online and Dynamic Algorithms for Set Cover},
  author = {Gupta, Anupam and Krishnaswamy, Ravishankar and Kumar, Amit and Panigrahi, Debmalya},
  year = {2016},
  doi = {10.1145/3055399.3055493},
  abstract = {In this paper, we give new results for the set cover problem in the fully dynamic model. In this model, the set of "active" elements to be covered changes over time. The goal is to maintain a near-optimal solution for the currently active elements, while making few changes in each timestep. This model is popular in both dynamic and online algorithms: in the former, the goal is to minimize the update time of the solution, while in the latter, the recourse (number of changes) is bounded. We present generic techniques for the dynamic set cover problem inspired by the classic greedy and primal-dual offline algorithms for set cover. The former leads to a competitive ratio of O(lognt), where nt is the number of currently active elements at timestep t, while the latter yields competitive ratios dependent on ft, the maximum number of sets that a currently active element belongs to. We demonstrate that these techniques are useful for obtaining tight results in both settings: update time bounds and limited recourse, exhibiting algorithmic techniques common to these two parallel threads of research.},
  citationcount = {82},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,update,update time}
}

@article{gurevichPermutationIndexingFast2013,
  title = {Permutation Indexing: Fast Approximate Retrieval from Large Corpora},
  author = {Gurevich, M. and Sarl{\'o}s, Tam{\'a}s},
  year = {2013},
  doi = {10.1145/2505515.2505646},
  abstract = {Inverted indexing is a ubiquitous technique used in retrieval systems including web search. Despite its popularity, it has a drawback - query retrieval time is highly variable and grows with the corpus size. In this work we propose an alternative technique, permutation indexing, where retrieval cost is strictly bounded and has only logarithmic dependence on the corpus size. Our approach is based on two novel techniques: (a) partitioning of the term space into overlapping clusters of terms that frequently co-occur in queries, and (b) a data structure for compactly encoding results of all queries composed of terms in a cluster as continuous sequences of document ids. Then, query results are retrieved by fetching few small chunks of these sequences. There is a price though: our encoding is lossy and thus returns approximate result sets. The fraction of the true results returned, recall, is controlled by the level of redundancy. The more space is allocated for the permutation index the higher is the recall. We analyze permutation indexing both theoretically under simplified document and query models, and empirically on a realistic document and query collections. We show that although permutation indexing can not replace traditional retrieval methods, since high recall cannot be guaranteed on all queries, it covers up to 77},
  citationcount = {Unknown},
  venue = {International Conference on Information and Knowledge Management},
  keywords = {data structure,query}
}

@article{guruswamiExpanderBasedConstructions2001,
  title = {Expander-Based Constructions of Efficiently Decodable Codes},
  author = {Guruswami, V. and Indyk, P.},
  year = {2001},
  doi = {10.1109/SFCS.2001.959942},
  abstract = {We present several novel constructions of codes which share the common thread of using expander (or expander-like) graphs as a component. The expanders enable the design of efficient decoding algorithms that correct a large number of errors through various forms of "voting" procedures. We consider both the notions of unique and list decoding, and in all cases obtain asymptotically good codes which are decodable up to a "maximum" possible radius and either: (a) achieve a similar rate as the previously best known codes but come with significantly faster algorithms, or (b) achieve a rate better than any prior construction with similar error-correction properties. Among our main results are: i) codes of rate /spl Omega/(/spl epsi//sup 2/) over constant-sized alphabet that can be list decoded in quadratic time from (1-/spl epsi/) errors; ii) codes of rate /spl Omega/(/spl epsi/) over constant-sized alphabet that can be uniquely decoded from (1/2-/spl epsi/) errors in near-linear time (this matches AG-codes with much faster algorithms); iii) linear-time encodable and decodable binary codes of positive rate (in fact, rate /spl Omega/(/spl epsi//sup 2/)) that can correct up to (1/4-/spl epsi/) fraction errors.},
  citationcount = {122},
  venue = {Proceedings IEEE International Conference on Cluster Computing}
}

@article{guruswamiListDecodingFrom2001,
  title = {List Decoding from Erasures: Bounds and Code Constructions},
  author = {Guruswami, V.},
  year = {2001},
  doi = {10.1109/TIT.2003.815776},
  abstract = {We consider the problem of list decoding from erasures. We establish lower and upper bounds on the rate of a (linear) code that can be list decoded with list size L when up to a fraction p of its symbols are adversarially erased. Our results show that in the limit of large L, the rate of such a code approaches the capacity (1 - p) of the erasure channel. Such nicely list decodable codes are then used as inner codes in a suitable concatenation scheme to give a uniformly constructive family of asymptotically good binary linear codes of rate {\textohm}(2/ lg(1/)) that can be efficiently list decoded using lists of size O(1/) from up to a fraction (1-) of erasures. This improves previous results from [14] in this vein, which achieveda rate of {\textohm}(3 lg(1/)).},
  citationcount = {77},
  venue = {IEEE Transactions on Information Theory}
}

@article{guruswamiListDecodingOf2001,
  title = {List Decoding of Error Correcting Codes},
  author = {Guruswami, V.},
  year = {2001},
  doi = {10.1007/b104335},
  abstract = {No abstract available},
  citationcount = {239},
  venue = {No venue available}
}

@article{gustedtACompactData1995,
  title = {A Compact Data Structure and Parallel Algorithms for Permutation Graphs},
  author = {Gustedt, J. and Morvan, M. and Viennot, L.},
  year = {1995},
  doi = {10.1007/3-540-60618-1_89},
  abstract = {No abstract available},
  citationcount = {13},
  venue = {International Workshop on Graph-Theoretic Concepts in Computer Science},
  keywords = {data structure}
}

@article{gutenbergDecrementalSsspIn2020,
  title = {Decremental {{SSSP}} in Weighted Digraphs: {{Faster}} and against an Adaptive Adversary},
  author = {Gutenberg, Maximilian Probst and {Wulff-Nilsen}, Christian},
  year = {2020},
  doi = {10.1137/1.9781611975994.155},
  abstract = {Given a dynamic digraph G=(V,E) undergoing edge deletions and given s{$\in$}V and {$\epsilon>$}0, we consider the problem of maintaining (1+{$\epsilon$})-approximate shortest path distances from s to all vertices in G over the sequence of deletions. Even and Shiloach (J. ACM'81) give a deterministic data structure for the exact version of the problem with total update time O(mn). Henzinger et al. (STOC'14, ICALP'15) give a Monte Carlo data structure for the approximate version with improved total update time O(mn\textsuperscript{\{\vphantom\}}0.9+o(1)\vphantom\{\}W) where W is the ratio between the largest and smallest edge weight. A drawback of their data structure is that they only work against an oblivious adversary, meaning that the sequence of deletions needs to be fixed in advance. This limits its application as a black box inside algorithms. We present the following (1+{$\epsilon$})-approximate data structures: (1) the first data structure is Las Vegas and works against an adaptive adversary; it has total expected update time (m\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}n\textsuperscript{\{\vphantom\}}4/3\vphantom\{\}) for unweighted graphs and (m\textsuperscript{\{\vphantom\}}3/4\vphantom\{\}n\textsuperscript{\{\vphantom\}}5/4\vphantom\{\}W) for weighted graphs, (2) the second data structure is Las Vegas and assumes an oblivious adversary; it has total expected update time ({\textsurd}mn\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}) for unweighted graphs and (m\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}n\textsuperscript{\{\vphantom\}}4/3\vphantom\{\}W) for weighted graphs, (3) the third data structure is Monte Carlo and is correct w.h.p. against an oblivious adversary; it has total expected update time ((mn)\textsuperscript{\{\vphantom\}}7/8\vphantom\{\}W)=(mn\textsuperscript{\{\vphantom\}}3/4\vphantom\{\}W). Each of our data structures can be queried at any stage of G in constant worst-case time; if the adversary is oblivious, a query can be extended to also report such a path in time proportional to its length. Our update times are faster than those of Henzinger et al. for all graph densities.},
  citationcount = {34},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {adaptive,data structure,dynamic,query,update,update time}
}

@article{gutenbergDeterministicAlgorithmsFor2020,
  title = {Deterministic Algorithms for Decremental Approximate Shortest Paths: {{Faster}} and Simpler},
  author = {Gutenberg, Maximilian Probst and {Wulff-Nilsen}, Christian},
  year = {2020},
  doi = {10.1137/1.9781611975994.154},
  abstract = {In the decremental (1+{$\epsilon$})-approximate Single-Source Shortest Path (SSSP) problem, we are given a graph G=(V,E) with n={\textbar}V{\textbar},m={\textbar}E{\textbar}, undergoing edge deletions, and a distinguished source s{$\in$}V, and we are asked to process edge deletions efficiently and answer queries for distance estimates \{dist\}\vphantom\{\}\textsubscript{G}(s,v) for each v{$\in$}V, at any stage, such that \{dist\}\textsubscript{G}(s,v){$\leq$}\{dist\}\vphantom\{\}\textsubscript{G}(s,v){$\leq$}(1+{$\epsilon$})\{dist\}\textsubscript{G}(s,v). In the decremental (1+{$\epsilon$})-approximate All-Pairs Shortest Path (APSP) problem, we are asked to answer queries for distance estimates \{dist\}\vphantom\{\}\textsubscript{G}(u,v) for every u,v{$\in$}V. In this article, we consider the problems for undirected, unweighted graphs. We present a new \{deterministic\} algorithm for the decremental (1+{$\epsilon$})-approximate SSSP problem that takes total update time O(mn\textsuperscript{\{\vphantom\}}0.5+o(1)\vphantom\{\}). Our algorithm improves on the currently best algorithm for dense graphs by Chechik and Bernstein [STOC 2016] with total update time O\vphantom\{\}(n{$^2$}) and the best existing algorithm for sparse graphs with running time O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}1.25\vphantom\{\}{\textsurd}\{m\}) [SODA 2017] whenever m=O(n\textsuperscript{\{\vphantom\}}1.5-o(1)\vphantom\{\}). In order to obtain this new algorithm, we develop several new techniques including improved decremental cover data structures for graphs, a more efficient notion of the heavy/light decomposition framework introduced by Chechik and Bernstein and the first clustering technique to maintain a dynamic \{sparse\} emulator in the deterministic setting. As a by-product, we also obtain a new simple deterministic algorithm for the decremental (1+{$\epsilon$})-approximate APSP problem with near-optimal total running time O\vphantom\{\}(mn/{$\epsilon$}) matching the time complexity of the sophisticated but rather involved algorithm by Henzinger, Forster and Nanongkai [FOCS 2013].},
  citationcount = {33},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,query,update,update time}
}

@article{gutenbergNewAlgorithmsAnd2020,
  title = {New Algorithms and Hardness for Incremental Single-Source Shortest Paths in Directed Graphs},
  author = {Gutenberg, Maximilian Probst and Williams, V. V. and Wein, Nicole},
  year = {2020},
  doi = {10.1145/3357713.3384236},
  abstract = {In the dynamic Single-Source Shortest Paths (SSSP) problem, we are given a graph G=(V,E) subject to edge insertions and deletions and a source vertex s{$\in$} V, and the goal is to maintain the distance d(s,t) for all t{$\in$} V. Fine-grained complexity has provided strong lower bounds for exact partially dynamic SSSP and approximate fully dynamic SSSP [ESA'04, FOCS'14, STOC'15]. Thus much focus has been directed towards finding efficient partially dynamic (1+{\cyrchar\cyrie})-approximate SSSP algorithms [STOC'14, ICALP'15, SODA'14, FOCS'14, STOC'16, SODA'17, ICALP'17, ICALP'19, STOC'19, SODA'20, SODA'20]. Despite this rich literature, for directed graphs there are no known deterministic algorithms for (1+{\cyrchar\cyrie})-approximate dynamic SSSP that perform better than the classic ES-tree [JACM'81]. We present the first such algorithm. We present a deterministic data structure for incremental SSSP in weighted directed graphs with total update time {\~O}(n 2 logW/{\cyrchar\cyrie} O(1)) which is near-optimal for very dense graphs; here W is the ratio of the largest weight in the graph to the smallest. Our algorithm also improves over the best known partially dynamic randomized algorithm for directed SSSP by Henzinger et al. [STOC'14, ICALP'15] if m={$\omega$}(n 1.1). Complementing our algorithm, we provide improved conditional lower bounds. Henzinger et al. [STOC'15] showed that under the OMv Hypothesis, the partially dynamic exact s-t Shortest Path problem in undirected graphs requires amortized update or query time m 1/2-o(1), given polynomial preprocessing time. Under a new hypothesis about finding Cliques, we improve the update and query lower bound for algorithms with polynomial preprocessing time to m 0.626-o(1). Further, under the k-Cycle hypothesis, we show that any partially dynamic SSSP algorithm with O(m 2-{\cyrchar\cyrie}) preprocessing time requires amortized update or query time m 1-o(1), which is essentially optimal. All previous conditional lower bounds that come close to our bound [ESA'04,FOCS'14] only held for ``combinatorial'' algorithms, while our new lower bound does not make such restrictions.},
  citationcount = {30},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,dynamic,lower bound,query,query time,update,update time}
}

@article{guttmanRTreesA1984,
  title = {R-Trees: A Dynamic Index Structure for Spatial Searching},
  author = {Guttman, A.},
  year = {1984},
  doi = {10.1145/602259.602266},
  abstract = {In order to handle spatial data efficiently, as required in computer aided design and geo-data applications, a database system needs an index mechanism that will help it retrieve data items quickly according to their spatial locations However, traditional indexing methods are not well suited to data objects of non-zero size located m multi-dimensional spaces In this paper we describe a dynamic index structure called an R-tree which meets this need, and give algorithms for searching and updating it. We present the results of a series of tests which indicate that the structure performs well, and conclude that it is useful for current database systems in spatial applications},
  citationcount = {8297},
  venue = {ACM SIGMOD Conference}
}

@article{guusregtsRegularityLemmasBanach2015,
  title = {Regularity Lemmas in a {{Banach}} Space Setting},
  author = {Guus Regts},
  year = {2015},
  journal = {European journal of combinatorics (Print)},
  doi = {10.1016/J.EJC.2015.06.006},
  annotation = {Citation Count: 0}
}

@article{h.gabowLineartimeAlgorithmSpecial1983,
  title = {A Linear-Time Algorithm for a Special Case of Disjoint Set Union},
  author = {H. Gabow and R. Tarjan},
  year = {1983},
  journal = {Journal of computer and system sciences (Print)},
  doi = {10.1145/800061.808753},
  abstract = {This paper presents a linear-time algorithm for the special case of the disjoint set union problem in which the structure of the unions (defined by a ``union tree'') is known in advance. The algorithm executes an intermixed sequence of m union and find operations on n elements in 0(m+n) time and 0(n) space. This is a slight but theoretically significant improvement over the fastest known algorithm for the general problem, which runs in 0(m\&agr;(m+n, n)+n) time and 0(n) space, where \&agr; is a functional inverse of Ackermann's function. Used as a subroutine, the algorithm gives similar improvements in the efficiency of algorithms for solving a number of other problems, including two-processor scheduling, the off-line min problem, matching on convex graphs, finding nearest common ancestors off-line, testing a flow graph for reducibility, and finding two disjoint directed spanning trees. The algorithm obtains its efficiency by combining a fast algorithm for the general problem with table look-up on small sets, and requires a random access machine for its implementation. The algorithm extends to the case in which single-node additions to the union tree are allowed. The extended algorithm is useful in finding maximum cardinality matchings on nonbipartite graphs.},
  annotation = {Citation Count: 765}
}

@article{h.gabowScalingAlgorithmWeighted1985,
  title = {A Scaling Algorithm for Weighted Matching on General Graphs},
  author = {H. Gabow},
  year = {1985},
  journal = {26th Annual Symposium on Foundations of Computer Science (sfcs 1985)},
  doi = {10.1109/SFCS.1985.3},
  abstract = {This paper presents an algorithm for maximum matching on general graphs with integral edge weights, running in time O(n3/4m lg N), where n, m and N are the number of vertices, number of edges, and largest edge weight magnitude, respectively. The best previous bound is O(n(mlg lg lgd n + n lg n)) where d is the density of the graph. The algorithm finds augmenting paths in batches by scaling the weights. The algorithm extends to degree-constrained subgraphs and hence to shortest paths on undirected graphs, the Chinese postman problem and finding a maximum cut of a planar graph. It speeds up Christofides' travelling salesman approximation algorithm from O(n3) to O(n2.75 lg n). A list splitting problem that arises in Edmonds' matching algorithm is solved in O(m{$\alpha$}(m,n)) time, where m is the number of operations on a universe of n elements; the list splitting algorithm does not use set merging. Applications are given to update problems for red-green matching, the cardinality Chinese postman problem and the maximum cardinality plane cut problem; also to the all-pairs shortest paths problem on undirected graphs with lengths plus or minus one.},
  keywords = {update},
  annotation = {Citation Count: 114}
}

@article{h.l.poutreAlphaalgorithmsIncrementalPlanarity1994,
  title = {Alpha-Algorithms for Incremental Planarity Testing (Preliminary Version)},
  author = {H. L. Poutr{\'e}},
  year = {1994},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/195058.195439},
  abstract = {In this paper, a data structure is presented for incremental planarity testing. At any moment, the data structure can answer the following type of query: given two nodes in the graph, can an edge be inserted between these nodes such that planarity is preserved, while edges (preserving planarity) are inserted from time to time. Starting from an " empty " graph of n nodes, the data structure runs in O (n + rn. a(m, n)) time, where m is the total number of queries and edge insertions. The data structure allows for insertions of nodes also (in the same time bounds, taking n as the final number of nodes), As a co-result, the problem of finding a maximal planar subgraph of a given (static) graph can be solved in O(n + e.a(e, n)) time, where e is the number of edges in the graph.},
  keywords = {data structure,query,static},
  annotation = {Citation Count: 39}
}

@article{h.l.poutreLowerBoundsUnionfind1990,
  title = {Lower Bounds for the Union-Find and the Split-Find Problem on Pointer Machines},
  author = {H. L. Poutr{\'e}},
  year = {1990},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/100216.100221},
  abstract = {A well-known result of Tarjan states that for all n and m n there exists a sequence of n\&1 Union and m Find operations that needs at least 0(m .:(m, n)) execution steps on a pointer machine that satisfies the separation condition. Later the bound was extended to 0(n+m .:(m, n)) for all m and n. In this paper we prove that this bound holds on a general pointer machine without the separation condition and we prove that the same bound holds for the Split Find problem as well. ] 1996 Academic Press, Inc.},
  keywords = {lower bound},
  annotation = {Citation Count: 57}
}

@article{h.l.poutreLowerBoundsUnionFind1996,
  title = {Lower {{Bounds}} for the {{Union-Find}} and the {{Sp}};It-{{Find Problem}} on {{Pointer Machines}}},
  author = {H. L. Poutr{\'e}},
  year = {1996},
  journal = {Journal of computer and system sciences (Print)},
  doi = {10.1006/jcss.1996.0008},
  abstract = {A well-known result of Tarjan states that for allnandm?nthere exists a sequence ofn?1 Union andmFind operations that needs at least?(m.?(m, n)) execution steps on a pointer machine that satisfies the separation condition. Later the bound was extended to?(n+m.?(m, n)) for allmandn. In this paper we prove that this bound holds on a general pointer machine without the separation condition and we prove that the same bound holds for the Split?Find problem as well.},
  keywords = {lower bound},
  annotation = {Citation Count: 21}
}

@article{h.mannilaComplexityUnificationSequences1986,
  title = {On the {{Complexity}} of {{Unification Sequences}}},
  author = {H. Mannila and E. Ukkonen},
  year = {1986},
  journal = {International Conference on Logic Programming},
  doi = {10.1007/3-540-16492-8_69},
  annotation = {Citation Count: 18}
}

@article{h.mannilaSetUnionProblem1986,
  title = {The {{Set Union Problem}} with {{Backtracking}}},
  author = {H. Mannila and E. Ukkonen},
  year = {1986},
  journal = {International Colloquium on Automata, Languages and Programming},
  doi = {10.1007/3-540-16761-7_73},
  annotation = {Citation Count: 28}
}

@article{h.mannilaTimeParameterArbitrary1988,
  title = {Time {{Parameter}} and {{Arbitrary Deunions}} in the {{Set Union Problem}}},
  author = {H. Mannila and E. Ukkonen},
  year = {1988},
  journal = {Scandinavian Workshop on Algorithm Theory},
  doi = {10.1007/3-540-19487-8_4},
  annotation = {Citation Count: 12}
}

@article{h.mannilaUnificationsDeunificationsTheir1990,
  title = {Unifications, Deunifications, and Their Complexity},
  author = {H. Mannila and E. Ukkonen},
  year = {1990},
  journal = {BIT},
  doi = {10.1007/BF01933209},
  annotation = {Citation Count: 0}
}

@article{h.schmidtReasoningConcurrentObjects1995,
  title = {Reasoning about Concurrent Objects},
  author = {H. Schmidt and Jing Chen},
  year = {1995},
  journal = {Proceedings 1995 Asia Pacific Software Engineering Conference},
  doi = {10.1109/APSEC.1995.496957},
  abstract = {Embedded specifications in object-oriented (OO) languages such as Eiffel and Sather are based on a rigorous approach towards validation, compatibility and reusability of sequential programs. The underlying method of "design-by-contract" is based on Hoare logic for which concurrency extensions exist. However concurrent OO languages are still in their infancy. They have inherently imperative facets, such as object identity, sharing, and synchronisation, which cannot be ignored in the semantics. Any marriage of objects and concurrency requires a trade-off in a space of intertwined qualities. The paper summarises our work on a type system, calculus and an operational model for concurrent objects in a minimal extension of the Eiffel and Sather languages (cSather). We omit concurrency control constructs and instead use assertions as synchronisation constraints for asynchronous functions. We show that this provides a framework in which subtyping and concurrency can coexist.},
  annotation = {Citation Count: 21}
}

@article{haeuplerDynamicDeterministicConstant2024,
  title = {Dynamic Deterministic Constant-Approximate Distance Oracles with {{n{\textsuperscript{\{\vphantom\}}}$\epsilon$\vphantom\{\}}} Worst-Case Update Time},
  author = {Haeupler, Bernhard and Long, Yaowei and Saranurak, Thatchaphol},
  year = {2024},
  doi = {10.1109/FOCS61266.2024.00121},
  abstract = {We present a new distance oracle in the fully dynamic setting: given a weighted undirected graph G = (V, E) with n vertices undergoing both edge insertions and deletions, and an arbitrary parameter {$\epsilon\in$}[1/\textsuperscript{\{\vphantom\}}c\vphantom\{\}n,1 where c {\textquestiondown} 0 is a small constant, we can deterministically maintain a data structure with O(n\textsuperscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}) worst-case update time that, given any pair of vertices (u, v), returns a 2\textsuperscript{\{\vphantom\}}\{poly\}(1/{$\epsilon$})\vphantom\{\} -approximate distance between u and v in poly(1/E) log log n query time. Our algorithm significantly advances the state-of-the-art in two aspects, both for fully dynamic algorithms and even decremental algorithms. First, no existing algorithm with worst-case update time guarantees a o(n)-approximation while also achieving an n2-{\textohm}(1)update and n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} query time, while our algorithm offers a constant O\textsubscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}(1) -approximation with O(n\textsuperscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}) update time and o\textsubscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\} (log log n) query time. Second, even if amortized update time is allowed, it is the first deterministic constant-approximation algorithm with n\textsuperscript{\{\vphantom\}}1-{\textohm}(1)\vphantom\{\} update and query time. The best result in this direction is the recent deterministic distance oracle by Chuzhoy and Zhang [STOC 2023] which achieves an approxi- mation of (log log n)\textsuperscript{\{\vphantom\}}2\textsuperscript{\{\vphantom\}}O(1/{$\epsilon^3$})\vphantom\{\}\vphantom\{\} with amortized update time of O(n\textsuperscript{\{\vphantom\}}{$\epsilon$})\vphantom\{\} and query time of 2\textsuperscript{\{\vphantom\}}\{p\}{$\circ$}1\{y\}(1/{$\epsilon$})\vphantom\{\}n log log n. We obtain the result by dynamizing tools related to length- constrained expanders [Haeupler-Racke-Ghaffari, STOC 2022; Haeupler-Hershkowitz-Tan, FOCS 2024]. Our technique com- pletely bypasses the 40-year-old Even-Shiloach tree, which has remained the most pervasive tool in the area but is inherently amortized.},
  citationcount = {4},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,dynamic,query,query time,update,update time}
}

@article{haeuplerIncrementalCycleDetection2011,
  title = {Incremental Cycle Detection, Topological Ordering, and Strong Component Maintenance},
  author = {Haeupler, Bernhard and Kavitha, T. and Mathew, Rogers and Sen, S. and Tarjan, R.},
  year = {2011},
  doi = {10.1145/2071379.2071382},
  abstract = {We present two online algorithms for maintaining a topological order of a directed n-vertex acyclic graph as arcs are added, and detecting a cycle when one is created. Our first algorithm handles m arc additions in O(m3/2) time. For sparse graphs (m/n = O(1)), this bound improves the best previous bound by a logarithmic factor, and is tight to within a constant factor among algorithms satisfying a natural locality property. Our second algorithm handles an arbitrary sequence of arc additions in O(n5/2) time. For sufficiently dense graphs, this bound improves the best previous bound by a polynomial factor. Our bound may be far from tight: we show that the algorithm can take {\textohm}(n22{\textsurd}2 lg n) time by relating its performance to a generalization of the k-levels problem of combinatorial geometry. A completely different algorithm running in {$\Theta$}(n2 log n) time was given recently by Bender, Fineman, and Gilbert. We extend both of our algorithms to the maintenance of strong components, without affecting the asymptotic time bounds.},
  citationcount = {92},
  venue = {ACM Trans. Algorithms}
}

@article{hagerupAGuidedTour1990,
  title = {A Guided Tour of Chernoff Bounds},
  author = {Hagerup, T. and R{\"u}b, Christine},
  year = {1990},
  doi = {10.1016/0020-0190(90)90214-I},
  abstract = {No abstract available},
  citationcount = {549},
  venue = {Information Processing Letters}
}

@article{hagerupDeterministicDictionaries2001,
  title = {Deterministic Dictionaries},
  author = {Hagerup, T. and Miltersen, Peter Bro and Pagh, R.},
  year = {2001},
  doi = {10.1006/jagm.2001.1171},
  abstract = {It is shown that a static dictionary that offers constant-time access to n elements with w-bit keys and occupies O(n) words of memory can be constructed deterministically in O(nlogn) time on a unit-cost RAM with word length w and a standard instruction set including multiplication. Whereas a randomized construction working in linear expected time was known, the running time of the best previous deterministic algorithm was ?(n2). Using a standard dynamization technique, the first deterministic dynamic dictionary with constant lookup time and sublinear update time is derived. The new algorithms are weakly nonuniform; i.e., they require access to a fixed number of precomputed constants dependent on w. The main technical tools employed are unit-cost error-correcting codes, word parallelism, and derandomization using conditional expectations.},
  citationcount = {105},
  venue = {J. Algorithms}
}

@article{hagerupMaintainingDiscreteProbability1993,
  title = {Maintaining Discrete Probability Distributions Optimally},
  author = {Hagerup, T. and Mehlhorn, K. and Munro, I. and Lingas, Andrzej and Karlsson, R. and Carlsson, Svante},
  year = {1993},
  doi = {10.1007/3-540-56939-1_77},
  abstract = {No abstract available},
  citationcount = {55},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{hagerupSortingSearchingWord1998,
  title = {Sorting and Searching on the Word {{RAM}}},
  author = {Hagerup, T.},
  year = {1998},
  doi = {10.1007/BFb0028575},
  abstract = {No abstract available},
  citationcount = {210},
  venue = {Symposium on Theoretical Aspects of Computer Science}
}

@article{haimkaplanDynamicRectangularIntersection2003,
  title = {Dynamic Rectangular Intersection with Priorities},
  author = {Haim Kaplan and Eyal Molad and R. Tarjan},
  year = {2003},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/780542.780635},
  abstract = {We present efficient data structures to maintain dynamic set of rectangles, each with priority assigned to it, such that we can efficiently find the rectangle of maximum priority containing a query point. Our data structures support insertions and deletions of rectangles. In one dimension, when rectangles are intervals, our most efficient data structure supports queries and insertions in O(log n) time, deletions in O(log n loglog n) time and requires linear space. When intervals are guaranteed to be nonoverlapping (but one can be nested within the other) we obtain a simpler data structure that supports all operations in O(log n) time.},
  keywords = {data structure,dynamic,query},
  annotation = {Citation Count: 41}
}

@article{haimkaplanSortingSignedPermutations2005,
  title = {Sorting Signed Permutations by Reversals, Revisited},
  author = {Haim Kaplan and Elad Verbin},
  year = {2005},
  journal = {Journal of computer and system sciences (Print)},
  doi = {10.1016/j.jcss.2004.12.002},
  annotation = {Citation Count: 26}
}

@article{haimkaplanThinHeapsThick2008,
  title = {Thin Heaps, Thick Heaps},
  author = {Haim Kaplan and R. Tarjan},
  year = {2008},
  journal = {TALG},
  doi = {10.1145/1328911.1328914},
  abstract = {The Fibonacci heap was devised to provide an especially efficient implementation of Dijkstra's shortest path algorithm. Although asyptotically efficient, it is not as fast in practice as other heap implementations. Expanding on ideas of H{\o}yer [1995], we describe three heap implementations (two versions of thin heaps and one of thick heaps) that have the same amortized efficiency as Fibonacci heaps, but need less space and promise better practical performance. As part of our development, we fill in a gap in H{\o}yer's analysis.},
  annotation = {Citation Count: 26}
}

@article{haltonOnTheEfficiency1960,
  title = {On the Efficiency of Certain Quasi-Random Sequences of Points in Evaluating Multi-Dimensional Integrals},
  author = {Halton, J.},
  year = {1960},
  doi = {10.1007/BF01386213},
  abstract = {No abstract available},
  citationcount = {1934},
  venue = {No venue available}
}

@article{hamidouneOnCompleteSubsets2007,
  title = {On Complete Subsets of the Cyclic Group},
  author = {Hamidoune, Y. O. and Llad{\'o}, A. and Serra, O.},
  year = {2007},
  doi = {10.1016/j.jcta.2007.12.007},
  abstract = {No abstract available},
  citationcount = {28},
  venue = {Journal of Combinatorial Theory}
}

@article{hamlinPrivateAnonymousData2019,
  title = {Private Anonymous Data Access},
  author = {Hamlin, Ariel and Ostrovsky, R. and Weiss, Mor and Wichs, Daniel},
  year = {2019},
  doi = {10.1007/978-3-030-17656-3_9},
  abstract = {No abstract available},
  citationcount = {29},
  venue = {IACR Cryptology ePrint Archive}
}

@article{hamlinTwoServerDistributed2020,
  title = {Two-Server Distributed {{ORAM}} with Sublinear Computation and Constant Rounds},
  author = {Hamlin, Ariel and Varia, Mayank},
  year = {2020},
  doi = {10.1007/978-3-030-75248-4_18},
  abstract = {No abstract available},
  citationcount = {9},
  venue = {IACR Cryptology ePrint Archive}
}

@article{hampapuramOptimalBiweightedBinary1993,
  title = {Optimal Bi-Weighted Binary Trees and the Complexity of Maintaining Partial Sums},
  author = {Hampapuram, Haripriyan and Fredman, M.},
  year = {1993},
  doi = {10.1109/SFCS.1993.366839},
  abstract = {Let A be an array. The partial sum problem concerns the design of a data structure for implementing the following operations. The operation update(j,x) has the effect, A[j]/spl larr/A[j]+x, and the query operation sum(j) returns the partial sum, /spl Sigma//sub i=1//sup j/A[i]. Our interest centers upon the optimal efficiency with which sequences of such operations can be performed, and we derive new upper and lower bounds in the semi-group model of computation. Our analysis relates the optimal complexity of the partial sum problem to optimal binary trees relative to a type of weighting scheme that defines the notion of bi-weighted binary tree.<<ETX>>},
  citationcount = {23},
  venue = {Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science},
  keywords = {data structure,lower bound,query,update}
}

@article{hampapuramOptimalBiweightedBinary1999,
  title = {Optimal Biweighted Binary Trees and the Complexity of Maintaining Partial Sums},
  author = {Hampapuram, Haripriyan and Fredman, M.},
  year = {1999},
  doi = {10.1137/S0097539795291598},
  abstract = {Let A be an array. The partial sum problem concerns the design of a data structure for implementing the following operations. The operation update (j,x) has the effect A[j]{\textleftarrow}A[j]+x\,, and the query operation (j) returns the partial sum {$\sum$}\textsubscript{\{\vphantom\}}i=1\vphantom\{\}\textsuperscript{j}\,A[i]\,. Our interest centers upon the optimal efficiency with which sequences of such operations can be performed, and we derive new upper and lower bounds in the semigroup model of computation. Our analysis relates the optimal complexity of the partial sum problem to optimal binary trees relative to a type of weighting scheme that defines the notion of biweighted binary tree.},
  citationcount = {7},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,lower bound,query,update}
}

@article{hamzaisericFastCharacterizationSegmental2022,
  title = {Fast Characterization of Segmental Duplication Structure in Multiple Genome Assemblies},
  author = {Hamza Iseric and C. Alkan and Faraz Hach and Ibrahim Numanagi{\'c}},
  year = {2022},
  journal = {Algorithms for Molecular Biology},
  doi = {10.1186/s13015-022-00210-2},
  annotation = {Citation Count: 21}
}

@article{hanauerFasterFullyDynamic2020,
  title = {Faster Fully Dynamic Transitive Closure in Practice},
  author = {Hanauer, Kathrin and Henzinger, Monika and Schulz, Christian},
  year = {2020},
  doi = {10.4230/LIPIcs.SEA.2020.14},
  abstract = {The fully dynamic transitive closure problem asks to maintain reachability information in a directed graph between arbitrary pairs of vertices, while the graph undergoes a sequence of edge insertions and deletions. The problem has been thoroughly investigated in theory and many specialized algorithms for solving it have been proposed in the last decades. In two large studies [Frigioni ea, 2001; Krommidas and Zaroliagis, 2008], a number of these algorithms have been evaluated experimentally against simple static algorithms for graph traversal, showing the competitiveness and even superiority of the simple algorithms in practice, except for very dense random graphs or very high ratios of queries. A major drawback of those studies is that only small and mostly randomly generated graphs are considered. In this paper, we engineer new algorithms to maintain all-pairs reachability information which are simple and space-efficient. Moreover, we perform an extensive experimental evaluation on both generated and real-world instances that are several orders of magnitude larger than those in the previous studies. Our results indicate that our new algorithms outperform all state-of-the-art algorithms on all types of input considerably in practice.},
  citationcount = {14},
  venue = {The Sea},
  keywords = {dynamic,query,static}
}

@article{hanauerFullyDynamicFour2021,
  title = {Fully Dynamic Four-Vertex Subgraph Counting},
  author = {Hanauer, Kathrin and Henzinger, Monika and Hua, Q.},
  year = {2021},
  doi = {10.4230/LIPIcs.SAND.2022.18},
  abstract = {This paper presents a comprehensive study of algorithms for maintaining the number of all connected four-vertex subgraphs in a dynamic graph. Specifically, our algorithms maintain the number of paths of length three in deterministic amortized O ( m 12 ) update time, and any other connected four-vertex subgraph which is not a clique in deterministic amortized update time O ( m 23 ). Queries can be answered in constant time. We also study the query times for subgraphs containing an arbitrary edge that is supplied only with the query as well as the case where only subgraphs containing a vertex s that is fixed beforehand are considered. For length-3 paths, paws, 4-cycles, and diamonds our bounds match or are not far from (conditional) lower bounds: Based on the OMv conjecture we show that any dynamic algorithm that detects the existence of paws, diamonds, or 4-cycles or that counts length-3 paths takes update time {\textohm}( m 1 / 2 - {$\delta$} ). Additionally, for 4-cliques and all connected induced subgraphs, we show a lower bound of {\textohm}( m 1 - {$\delta$} ) for any small constant {$\delta$} {\textquestiondown} 0 for the amortized update time, assuming the static combinatorial 4-clique conjecture holds. This shows that the O ( m ) algorithm by Eppstein et al. [9] for these subgraphs cannot be improved by a polynomial factor.},
  citationcount = {9},
  venue = {Symposium on Algorithmic Foundations of Dynamic Networks},
  keywords = {dynamic,lower bound,query,query time,static,update,update time}
}

@article{hanauerFullyDynamicSingle2019,
  title = {Fully Dynamic Single-Source Reachability in Practice: {{An}} Experimental Study},
  author = {Hanauer, Kathrin and Henzinger, Monika and Schulz, Christian},
  year = {2019},
  doi = {10.1137/1.9781611976007.9},
  abstract = {Given a directed graph and a source vertex, the fully dynamic single-source reachability problem is to maintain the set of vertices that are reachable from the given vertex, subject to edge deletions and insertions. It is one of the most fundamental problems on graphs and appears directly or indirectly in many and varied applications. While there has been theoretical work on this problem, showing both linear conditional lower bounds for the fully dynamic problem and insertions-only and deletions-only upper bounds beating these conditional lower bounds, there has been no experimental study that compares the performance of fully dynamic reachability algorithms in practice. Previous experimental studies in this area concentrated only on the more general all-pairs reachability or transitive closure problem and did not use real-world dynamic graphs. In this paper, we bridge this gap by empirically studying an extensive set of algorithms for the single-source reachability problem in the fully dynamic setting. In particular, we design several fully dynamic variants of well-known approaches to obtain and maintain reachability information with respect to a distinguished source. Moreover, we extend the existing insertions-only or deletions-only upper bounds into fully dynamic algorithms. Even though the worst-case time per operation of all the fully dynamic algorithms we evaluate is at least linear in the number of edges in the graph (as is to be expected given the conditional lower bounds) we show in our extensive experimental evaluation that their performance differs greatly, both on generated as well as on real-world instances.},
  citationcount = {7},
  venue = {Workshop on Algorithm Engineering and Experimentation},
  keywords = {dynamic,lower bound}
}

@article{hanauerRecentAdvancesIn2021,
  title = {Recent Advances in Fully Dynamic Graph Algorithms -- a Quick Reference Guide},
  author = {Hanauer, Kathrin and Henzinger, Monika and Schulz, Christian},
  year = {2021},
  doi = {10.1145/3555806},
  abstract = {In recent years, significant advances have been made in the design and analysis of fully dynamic algorithms. However, these theoretical results have received very little attention from the practical perspective. Few of the algorithms are implemented and tested on real datasets, and their practical potential is far from understood. Here, we present a quick reference guide to recent engineering and theory results in the area of fully dynamic graph algorithms.},
  citationcount = {35},
  venue = {ACM Journal of Experimental Algorithmics},
  keywords = {dynamic}
}

@article{hansenBisimulationsForAsynchronous1996,
  title = {Bisimulations for Asynchronous Mobile Processes},
  author = {Hansen, Martin and H{\"u}ttel, Hans and Kleist, Josva},
  year = {1996},
  doi = {10.7146/BRICS.V3I8.19971},
  abstract = {Within the past few years there has been renewed interest in the study of value-passing process calculi as a consequence of the emergence of the pi-calculus. Here, [MPW89] have determined two variants of the notion of bisimulation, late and early bisimilarity. Most recently [San93] has proposed the new notion of open bisimulation equivalence. In this paper we consider Plain LAL, a mobile process calculus which differs from the pi-calculus in the sense that the communication of data values happens asynchronously. The surprising result is that in the presence of asynchrony, the open, late and early bisimulation equivalences coincide - this in contrast to the pi-calculus where they are distinct. The result allows us to formulate a common equational theory which is sound and complete for finite terms of Plain LAL.},
  citationcount = {26},
  venue = {No venue available}
}

@article{hanyuliSurveyAlgorithmsNash2024,
  title = {A Survey on Algorithms for {{Nash}} Equilibria in Finite Normal-Form Games},
  author = {Hanyu Li and Wenhan Huang and Zhijian Duan and D. Mguni and Kun Shao and Jun Wang and Xiaotie Deng},
  year = {2024},
  journal = {Computer Science Review},
  doi = {10.1016/j.cosrev.2023.100613},
  annotation = {Citation Count: 4}
}

@article{har-peledApproximateNearestNeighbor2012,
  title = {Approximate Nearest Neighbor: {{Towards}} Removing the Curse of Dimensionality},
  author = {{Har-Peled}, Sariel and Indyk, P. and Motwani, R.},
  year = {2012},
  doi = {10.4086/toc.2012.v008a014},
  abstract = {We present two algorithms for the approximate nearest neighbor problem in high dimensional spaces. For data sets of size n living in IR d , the algorithms require space that is only polynomial in n and d , while achieving query times that are sub-linear in n and polynomial in d . We also show applications to other high-dimensional geometric problems, such as the approximate minimum spanning tree.},
  citationcount = {4554},
  venue = {Theory of Computing},
  keywords = {query,query time}
}

@article{har-peledAReplacementFor2001,
  title = {A Replacement for {{Voronoi}} Diagrams of near Linear Size},
  author = {{Har-Peled}, Sariel},
  year = {2001},
  doi = {10.1109/SFCS.2001.959884},
  abstract = {For a set P of n points in R/sup d/, we define a new type of space decomposition. The new diagram provides an /spl epsi/-approximation to the distance function associated with the Voronoi diagram of P, while being of near linear size, for d/spl ges/2. This contrasts with the standard Voronoi diagram that has /spl Omega/ (n/sup [d/2]/) complexity in the worst case.},
  citationcount = {184},
  venue = {Proceedings IEEE International Conference on Cluster Computing}
}

@article{har-peledProximityInThe2015,
  title = {Proximity in the Age of Distraction: {{Robust}} Approximate Nearest Neighbor Search},
  author = {{Har-Peled}, Sariel and Mahabadi, S.},
  year = {2015},
  doi = {10.1137/1.9781611974782.1},
  abstract = {We introduce a new variant of the nearest neighbor search problem, which allows for some coordinates of the dataset to be arbitrarily corrupted or unknown. Formally, given a dataset of n points P= x\_1,{\dots},x\_n  in high-dimensions, and a parameter k, the goal is to preprocess the dataset, such that given a query point q, one can compute quickly a point x{$\in$}P, such that the distance of the query to the point x is minimized, when ignoring the "optimal" k coordinates. Note, that the coordinates being ignored are a function of both the query point and the point returned. We present a general reduction from this problem to answering ANN queries, which is similar in spirit to LSH (locality sensitive hashing) [IM98]. Specifically, we give a sampling technique which achieves a bi-criterion approximation for this problem. If the distance to the nearest neighbor after ignoring k coordinates is r, the data-structure returns a point that is within a distance of O(r) after ignoring O(k) coordinates. We also present other applications and further extensions and refinements of the above result. The new data-structures are simple and (arguably) elegant, and should be practical -- specifically, all bounds are polynomial in all relevant parameters (including the dimension of the space, and the robustness parameter k).},
  citationcount = {8},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,query,reduction}
}

@article{har-peledRangeMedians2008,
  title = {Range Medians},
  author = {{Har-Peled}, Sariel and Muthukrishnan, S.},
  year = {2008},
  doi = {10.1007/978-3-540-87744-8_42},
  abstract = {No abstract available},
  citationcount = {18},
  venue = {Embedded Systems and Applications}
}

@article{harelFastAlgorithmsFor1984,
  title = {Fast Algorithms for Finding Nearest Common Ancestors},
  author = {Harel, D. and Tarjan, R.},
  year = {1984},
  doi = {10.1137/0213024},
  abstract = {We consider the following problem: Given a collection of rooted trees, answer on-line queries of the form, ``What is the nearest common ancester of vertices x and y?'' We show that any pointer machine that solves this problem requires {\textohm}(n) time per query in the worst case, where n is the total number of vertices in the trees. On the other hand, we present an algorithm for a random access machine with uniform cost measure (and a bound of {\textohm}(n) on the number of bits per word) that requires O(1) time per query and O(n) preprocessing time, assuming that the collection of trees is static. For a version of the problem in which the trees can change between queries, we obtain an almost-linear-time (and linear-space) algorithm.},
  citationcount = {1210},
  venue = {SIAM journal on computing (Print)}
}

@article{harperOptimalNumberingsAnd1966,
  title = {Optimal Numberings and Isoperimetric Problems on Graphs},
  author = {Harper, L. H.},
  year = {1966},
  doi = {10.1016/S0021-9800(66)80059-5},
  abstract = {No abstract available},
  citationcount = {439},
  venue = {No venue available}
}

@article{harrylangDeterministicCoresetsStochastic2019,
  title = {Deterministic {{Coresets}} for {{Stochastic Matrices}} with {{Applications}} to {{Scalable Sparse PageRank}}},
  author = {Harry Lang and Cenk Baykal and Najib Abu Samra and Tony Tannous and Dan Feldman and D. Rus},
  year = {2019},
  journal = {Theory and Applications of Models of Computation},
  doi = {10.1007/978-3-030-14812-6_25},
  annotation = {Citation Count: 3}
}

@article{hstadAlmostOptimalLower1986,
  title = {Almost Optimal Lower Bounds for Small Depth Circuits},
  author = {H{\aa}stad, J.},
  year = {1986},
  doi = {10.1145/12130.12132},
  abstract = {We give improved lower bounds for the size of small depth circuits computing several functions. In particular we prove almost optimal lower bounds for the size of parity circuits. Further we show that there are functions computable in polynomial size and depth k but requires exponential size when the depth is restricted to k 1. Our Main Lemma which is of independent interest states that by using a random restriction we can convert an AND of small ORs to an OR of small ANDs and conversely. Warning: Essentially this paper has been published in Advances for Computing and is hence subject to copyright restrictions. It is for personal use only.},
  citationcount = {827},
  venue = {Symposium on the Theory of Computing},
  keywords = {lower bound}
}

@article{hastieDiscriminantAdaptiveNearest1995,
  title = {Discriminant Adaptive Nearest Neighbor Classification},
  author = {Hastie, T. and Tibshirani, R.},
  year = {1995},
  doi = {10.1109/34.506411},
  abstract = {Nearest neighbor classification expects the class conditional probabilities to be locally constant, and suffers from bias in high dimensions We propose a locally adaptive form of nearest neighbor classification to try to finesse this curse of dimensionality. We use a local linear discriminant analysis to estimate an effective metric for computing neighborhoods. We determine the local decision boundaries from centroid information, and then shrink neighborhoods in directions orthogonal to these local decision boundaries, and elongate them parallel to the boundaries. Thereafter, any neighborhood-based classifier can be employed, using the modified neighborhoods. The posterior probabilities tend to be more homogeneous in the modified neighborhoods. We also propose a method for global dimension reduction, that combines local dimension information. In a number of examples, the methods demonstrate the potential for substantial improvements over nearest neighbour classification.},
  citationcount = {963},
  venue = {IEEE Transactions on Pattern Analysis and Machine Intelligence}
}

@article{hatamiStructureOfProtocols2016,
  title = {Structure of Protocols for {{XOR}} Functions},
  author = {Hatami, Hamed and Hosseini, Kaave and Lovett, Shachar},
  year = {2016},
  doi = {10.1109/FOCS.2016.38},
  abstract = {Let f be a boolean function on n variables. Its associated XOR function is the two-party function F(x, y) = f(x xor y). We show that, up to polynomial factors, the deterministic communication complexity of F is equal to the parity decision tree complexity of f. This relies on a novel technique of entropy reduction for protocols, combined with existing techniques in Fourier analysis and additive combinatorics.},
  citationcount = {60},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{hauchanFPTASComputingNash2018,
  title = {An {{FPTAS}} for {{Computing Nash Equilibrium}} in {{Resource Graph Games}}},
  author = {Hau Chan and A. Jiang},
  year = {2018},
  journal = {International Joint Conference on Artificial Intelligence},
  doi = {10.24963/ijcai.2018/21},
  abstract = {We consider the problem of computing a mixed-strategy Nash equilibrium (MSNE)~in resource graph games (RGGs),~a compact representation for games with an exponential number of strategies.~In an RGG,~each player's pure strategy is a subset of resources, represented by a binary vector, and her pure strategy~set is represented compactly using~ a set of linear inequality constraints.~Given the pure strategies of the players, each player's utility depends on the resource graph~and the numbers of times the neighboring resources are used. RGGs are general enough to capture a wide variety of games studied in literature, including congestion games and security games.In this paper, we provide the first~Fully Polytnomial Time Approximation Scheme (FPTAS) for computing an MSNE in any symmetric multilinear RGG~where its constraint moralized resource graph (a graph formed between the moralized resource graph~and the constraints defining the strategy polytope) has bounded treewidth.~Our FPTAS can be generalized to compute optimal MSNE, and to games with~a constant number of player types.~As a consequence, our FPTAS provides new approximation results for~security~games, network congestion games,~and bilinear games.},
  annotation = {Citation Count: 2}
}

@article{hausslerEpsilonNetsAnd1986,
  title = {Epsilon-Nets and Simplex Range Queries},
  author = {Haussler, D. and Welzl, E.},
  year = {1986},
  doi = {10.1145/10515.10522},
  abstract = {We present a new technique for half-space and simplex range query using {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) space and {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}a{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown}) query time, where {\textexclamdown}italic{\textquestiondown}a{\textexclamdown}/italic{\textquestiondown} {\textexclamdown} {\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}(d-1)/{\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}-1) + 1 + {$\gamma$} for all dimensions {\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown} {$\geq$} 2 and {\textexclamdown}italic{\textquestiondown}{$\gamma$}{\textexclamdown}/italic{\textquestiondown} {\textquestiondown} 0. These bounds are better than those previously published for all {\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown} {$\geq$} 2. The technique uses random sampling to build a partition-tree structure. We introduce the concept of an {\textexclamdown}italic{\textquestiondown}{$\varepsilon$}{\textexclamdown}/italic{\textquestiondown}-net for an abstract set of ranges to describe the desired result of this random sampling and give necessary and sufficient conditions that a random sample is an {\textexclamdown}italic{\textquestiondown}{$\varepsilon$}{\textexclamdown}/italic{\textquestiondown}-net with high probability. We illustrate the application of these ideas to other range query problems.},
  citationcount = {472},
  venue = {SCG '86}
}

@article{hausslerNetsAnd1987,
  title = {-Nets and Simplex Range Queries},
  author = {Haussler, D. and Welzl, E.},
  year = {1987},
  doi = {10.1007/BF02187876},
  abstract = {No abstract available},
  citationcount = {684},
  venue = {Discrete \& Computational Geometry}
}

@article{hausslerSpherePackingNumbers1995,
  title = {Sphere Packing Numbers for Subsets of the Boolean N-{{Cube}} with Bounded Vapnik-Chervonenkis Dimension},
  author = {Haussler, D.},
  year = {1995},
  doi = {10.1016/0097-3165(95)90052-7},
  abstract = {No abstract available},
  citationcount = {403},
  venue = {Journal of Combinatorial Theory}
}

@article{hazanHowHardIs2009,
  title = {How Hard Is It to Approximate the Best {{Nash}} Equilibrium?},
  author = {Hazan, Elad and Krauthgamer, Robert},
  year = {2009},
  doi = {10.1137/090766991},
  abstract = {The quest for a PTAS for Nash equilibrium in a two-player game seeks to circumvent the PPAD-completeness of an (exact) Nash equilibrium by finding an approximate equilibrium, and has emerged as a major open question in Algorithmic Game Theory. A closely related problem is that of finding an equilibrium maximizing a certain objective, such as the social welfare. This optimization problem was shown to be NP-hard by Gilboa and Zemel [Games and Economic Behavior 1989]. However, this NP-hardness is unlikely to extend to finding an approximate equilibrium, since the latter admits a quasi-polynomial time algorithm, as proved by Lipton, Markakis and Mehta [Proc. of 4th EC, 2003]. We show that this optimization problem, namely, finding in a two-player game an approximate equilibrium achieving large social welfare is unlikely to have a polynomial time algorithm. One interpretation of our results is that the quest for a PTAS for Nash equilibrium should not extend to a PTAS for finding the best Nash equilibrium, which stands in contrast to certain algorithmic techniques used so far (e.g. sampling and enumeration). Technically, our result is a reduction from a notoriously difficult problem in modern Combinatorics, of finding a planted (but hidden) clique in a random graph G(n, 1/2). Our reduction starts from an instance with planted clique size k = O(log n). For comparison, the currently known algorithms due to Alon, Krivelevich and Sudakov [Random Struct. \& Algorithms, 1998], and Krauthgamer and Feige [Random Struct. \& Algorithms, 2000], are effective for a much larger clique size k = {\textohm}({\textsurd}n).},
  citationcount = {125},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{hazanTwoPartyDirect2017,
  title = {Two-Party Direct-Sum Questions through the Lens of Multiparty Communication Complexity},
  author = {Hazan, Itay and Kushilevitz, E.},
  year = {2017},
  doi = {10.4230/LIPIcs.DISC.2017.26},
  abstract = {Direct-sum questions in (two-party) communication complexity ask whether two parties, Alice and Bob, can compute the value of a function f on l inputs (x\_1,y\_1),...,(x\_l,y\_l) more efficiently than by applying the best protocol for f, independently on each input (x\_i,y\_i). In spite of significant efforts to understand these questions (under various communication-complexity measures), the general question is still far from being well understood. In this paper, we offer a multiparty view of these questions: The direct-sum setting is just a two-player system with Alice having inputs x\_1,...,x\_l, Bob having inputs y\_1,...,y\_l and the desired output is f(x\_1,y\_1),...,f(x\_l,y\_l). The naive solution of solving the l problems independently, is modeled by a network with l (disconnected) pairs of players Alice i and Bob i, with inputs x\_i,y\_i respectively, and communication only within each pair. Then, we consider an intermediate ("star") model, where there is one Alice having l inputs x\_1,...,x\_l and l players Bob\_1,...,Bob\_l holding y\_1,...,y\_l, respectively (in fact, we consider few variants of this intermediate model, depending on whether communication between each Bob i and Alice is point-to-point or whether we allow broadcast). Our goal is to get a better understanding of the relation between the two extreme models (i.e., of the two-party direct-sum question). If, for instance, Alice and Bob can do better (for some complexity measure) than solving the l problems independently, we wish to understand what intermediate model already allows to do so (hereby understanding the "source" of such savings). If, on the other hand, we wish to prove that there is no better solution than solving the l problems independently, then our approach gives a way of breaking the task of proving such a statement into few (hopefully, easier) steps. We present several results of both types. Namely, for certain complexity measures, communication problems f and certain pairs of models, we can show gaps between the complexity of solving f on l instances in the two models in question; while, for certain other complexity measures and pairs of models, we can show that such gaps do not exist (for any communication problem f). For example, we prove that if only point-to-point communication is allowed in the intermediate "star" model, then significant savings are impossible in the public-coin randomized setting. On the other hand, in the private-coin randomized setting, if Alice is allowed to broadcast messages to all Bobs in the "star" network, then some savings are possible. While this approach does not lead yet to new results on the original two-party direct-sum question, we believe that our work gives new insights on the already-known direct-sum results, and may potentially lead to more such results in the future.},
  citationcount = {1},
  venue = {International Symposium on Distributed Computing},
  keywords = {communication,communication complexity}
}

@article{heDynamicPathQueries2018,
  title = {Dynamic Path Queries in Linear Space},
  author = {He, Meng and Munro, J. I. and Zhou, Gelin},
  year = {2018},
  doi = {10.1007/s00453-018-0413-x},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Algorithmica},
  keywords = {dynamic,query}
}

@article{heDynamicRangeSelection2011,
  title = {Dynamic Range Selection in Linear Space},
  author = {He, Meng and Munro, J. and Nicholson, Patrick K.},
  year = {2011},
  doi = {10.1007/978-3-642-25591-5_18},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {dynamic}
}

@article{hellersteinModelIndexabilityIts2002,
  title = {On a Model of Indexability and Its Bounds for Range Queries},
  author = {Hellerstein, J. M. and Koutsoupias, E. and Miranker, D. P. and Papadimitriou, C. H. and Samoladas, V.},
  year = {2002},
  journal = {Journal of the ACM},
  volume = {49},
  number = {1},
  pages = {35--55},
  doi = {10.1145/505241.505244},
  keywords = {query},
  file = {/Users/tulasi/Zotero/storage/3CC59ZPE/Hellerstein et al. - 2002 - On a model of indexability and its bounds for range queries.pdf}
}

@article{hellersteinOnTheAnalysis1997,
  title = {On the Analysis of Indexing Schemes},
  author = {Hellerstein, J. and Koutsoupias, E. and Papadimitriou, C.},
  year = {1997},
  doi = {10.1145/263661.263688},
  abstract = {We consider the problem of indexing general database workloads (combinations of data sets and sets of potential queries). We define a framework for measuring the efficiency of an indexing scheme for a workload based on two characterizations: storage redundancy (how many times each item in the data set is stored), and access overhead (how many times more blocks than necessary does a query retrieve). Using this framework we present some initial results, showing upper and lower bounds and trade-offs between them in the case of multi-dimensional range queries and set queries.},
  citationcount = {158},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems}
}

@article{hellFullyDynamicAlgorithm2001,
  title = {A {{Fully Dynamic Algorithm}} for {{Recognizing}} and {{Representing Proper Interval Graphs}}},
  author = {Hell, Pavol and Shamir, Ron and Sharan, Roded},
  year = {2001},
  month = jan,
  journal = {SIAM J. Comput.},
  volume = {31},
  number = {1},
  pages = {289--305},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/S0097539700372216},
  url = {https://epubs.siam.org/doi/10.1137/S0097539700372216},
  urldate = {2025-04-21},
  abstract = {Our main result is a linear-time (that is, time \$O(m + n)\$) algorithm to recognize and represent proper circular-arc graphs. The best previous algorithm, due to A. Tucker, has time complexity \$O(n{\textasciicircum}2 )\$. We take advantage of the fact that (among connected graphs) proper circular-arc graphs are precisely the graphs orientable as local tournaments, and we use a new characterization of local tournaments. The algorithm depends on repeated representation of portions of the input graph as proper interval graphs. Thus we also find it useful to give a new linear-time algorithm to represent proper interval graphs. This latter algorithm also depends on an orientation characterization of proper interval graphs. It is conceptually simple and does not use complex data structures. As a byproduct of the correctness proof of the algorithm, we also obtain a new proof of a characterization of proper interval graphs by forbidden subgraphs.},
  keywords = {cell probe,dynamic,lower bound,sorted},
  file = {/Users/tulasi/Zotero/storage/WPU32AVK/Hell et al. - 2001 - A Fully Dynamic Algorithm for Recognizing and Representing Proper Interval Graphs.pdf}
}

@article{hemaspaandraSigactNewsComplexity2010,
  title = {{{SIGACT}} News Complexity Theory Column 67},
  author = {Hemaspaandra, Lane A.},
  year = {2010},
  doi = {10.1145/1855118.1886592},
  abstract = {The satisfiability problem has emerged as the queen of the complexity zoo. She is the quintessential NP-complete hard-to-find but easy-to-recognize search problem in computer science. There are hundreds if not thousands of problems that are now known to be equivalent to SAT, and our rich theory of complexity classes is centered around its queen. In the world of communication complexity, the set disjointness problem has similarly emerged as the quintessential hard-to-find but easy-to-recognize problem. There is an impressive collection of problems in many diverse areas whose hardness boils down to the hardness of the set disjointness problem in some model of communication complexity. Moreover, we will argue that proving lower bounds for the set disjointness function in a particular communication},
  citationcount = {9},
  venue = {SIGA},
  keywords = {communication,communication complexity,lower bound}
}

@article{henzingerACombinatorialCut2020,
  title = {A Combinatorial Cut-Toggling Algorithm for Solving Laplacian Linear Systems},
  author = {Henzinger, Monika and Jin, Billy and Peng, Richard and Williamson, David P.},
  year = {2020},
  doi = {10.1007/s00453-023-01154-8},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {Algorithmica}
}

@article{henzingerAStatic21997,
  title = {A Static 2-Approximation Algorithm for Vertex Connectivity and Incremental Approximation Algorithms for Edge and Vertex Connectivity},
  author = {Henzinger, Monika},
  year = {1997},
  doi = {10.1006/jagm.1997.0855},
  abstract = {This paper presents insertions-only algorithms for maintaining the exact and/or approximate size of the minimum edge cut and the minimum vertex cut of a graph. The algorithms output the approximate or exact sizekin timeO(1) and a cut of sizekin time linear in its size. For the minimum edge cut problem and for any 0{\textexclamdown}??1, the amortized time per insertion isO(1/?2) for a (2+?)-approximation,O((log?)((logn)/?)2) for a (1+?)-approximation, andO(?logn) for the exact size, wherenis the number of nodes in the graph and ? is the size of the minimum cut. The (2+?)-approximation algorithm and the exact algorithm are deterministic; the (1+?)-approximation algorithm is randomized. We also present a static 2-approximation algorithm for the size ? of the minimum vertex cut in a graph, which takes timeO(n2min(n,?)). This is a factor of ? faster than the best algorithm for computing the exact size, which takes timeO((?3n+?n2)min(n,?)). We give an insertions-only algorithm for maintaining a (2+?)-approximation of the minimum vertex cut with amortized insertion timeO(n/?).},
  citationcount = {34},
  venue = {J. Algorithms}
}

@article{henzingerCertificatesAndFast1995,
  title = {Certificates and Fast Algorithms for Biconnectivity in Fully-Dynamic Graphs},
  author = {Henzinger, Monika and Poutr{\'e}, H. L.},
  year = {1995},
  doi = {10.1007/3-540-60313-1_142},
  abstract = {No abstract available},
  citationcount = {28},
  venue = {Embedded Systems and Applications}
}

@article{henzingerConditionalHardnessFor2017,
  title = {Conditional Hardness for Sensitivity Problems},
  author = {Henzinger, Monika and Lincoln, Andrea and Neumann, S. and Williams, V. V.},
  year = {2017},
  doi = {10.4230/LIPIcs.ITCS.2017.26},
  abstract = {In recent years it has become popular to study dynamic problems in a sensitivity setting: Instead of allowing for an arbitrary sequence of updates, the sensitivity model only allows to apply batch updates of small size to the original input data. The sensitivity model is particularly appealing since recent strong conditional lower bounds ruled out fast algorithms for many dynamic problems, such as shortest paths, reachability, or subgraph connectivity. In this paper we prove conditional lower bounds for these and additional problems in a sensitivity setting. For example, we show that under the Boolean Matrix Multiplication (BMM) conjecture combinatorial algorithms cannot compute the (4/3-{$\varepsilon$})-approximate diameter of an undirected unweighted dense graph with truly subcubic preprocessing time and truly subquadratic update/query time. This result is surprising since in the static setting it is not clear whether a reduction from BMM to diameter is possible. We further show under the BMM conjecture that many problems, such as reachability or approximate shortest paths, cannot be solved faster than by recomputation from scratch even after only one or two edge insertions. We extend our reduction from BMM to Diameter to give a reduction from All Pairs Shortest Paths to Diameter under one deletion in weighted graphs. This is intriguing, as in the static setting it is a big open problem whether Diameter is as hard as APSP. We further get a nearly tight lower bound for shortest paths after two edge deletions based on the APSP conjecture. We give more lower bounds under the Strong Exponential Time Hypothesis. Many of our lower bounds also hold for static oracle data structures where no sensitivity is required. Finally, we give the first algorithm for the (1+{$\varepsilon$})-approximate radius, diameter, and eccentricity problems in directed or undirected unweighted graphs in case of single edges failures. The algorithm has a truly subcubic running time for graphs with a truly subquadratic number of edges; it is tight w.r.t. the conditional lower bounds we obtain.},
  citationcount = {25},
  venue = {Information Technology Convergence and Services},
  keywords = {data structure,dynamic,lower bound,query,query time,reduction,static,update}
}

@article{henzingerConstantTimeDynamic2020,
  title = {Constant-Time Dynamic Weight Approximation for Minimum Spanning Forest},
  author = {Henzinger, Monika and Peng, Pan},
  year = {2020},
  doi = {10.1016/j.ic.2021.104805},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Information and Computation},
  keywords = {dynamic}
}

@article{henzingerConstantTimeDynamic2022,
  title = {Constant-Time Dynamic ({{$\Delta$}} +1)-Coloring},
  author = {Henzinger, Monika and Peng, Pan},
  year = {2022},
  doi = {10.1145/3501403},
  abstract = {We give a fully dynamic (Las-Vegas style) algorithm with constant expected amortized time per update that maintains a proper ({$\Delta$} +1)-vertex coloring of a graph with maximum degree at most {$\Delta$}. This improves upon the previous O(log {$\Delta$})-time algorithm by Bhattacharya et al. (SODA 2018). Our algorithm uses an approach based on assigning random ranks to vertices and does not need to maintain a hierarchical graph decomposition. We show that our result does not only have optimal running time but is also optimal in the sense that already deciding whether a {$\Delta$}-coloring exists in a dynamically changing graph with maximum degree at most {$\Delta$} takes {\textohm} (log n) time per operation.},
  citationcount = {4},
  venue = {Symposium on Theoretical Aspects of Computer Science},
  keywords = {dynamic,update}
}

@article{henzingerDecrementalSinglesourceShortest2014,
  title = {Decremental Single-Source Shortest Paths on Undirected Graphs in near-Linear Total Update Time},
  author = {Henzinger, Monika and Krinninger, Sebastian and Nanongkai, Danupon},
  year = {2014},
  doi = {10.1109/FOCS.2014.24},
  abstract = {The decremental single-source shortest paths (SSSP) problem concerns maintaining the distances between a given source node s to every node in an n-node m-edge graph G undergoing edge deletions. While its static counterpart can be easily solved in near-linear time, this decremental problem is much more challenging even in the undirected unweighted case. In this case, the classic O(mn) total update time of Even and Shiloach (JACM 1981) has been the fastest known algorithm for three decades. With the loss of a (1 + {$\varepsilon$})-approximation factor, the running time was recently improved to O(n2+o(1)) by Bernstein and Roditty (SODA 2011), and more recently to O(n1.8+o(1) + m1+o(1)) by Henzinger, Krinninger, and Nanongkai (SODA 2014). In this paper, we finally bring the running time of this case down to near-linear: We give a (1 + {$\varepsilon$})-approximation algorithm with O(m1+o(1)) total update time, thus obtaining near-linear time. Moreover, we obtain O(m1+o(1) log W) time for the weighted case, where the edge weights are integers from 1 to W. The only prior work on weighted graphs in o(mn log W) time is the O(mn0.986 log W)-time algorithm by Henzinger, Krinninger, and Nanongkai (STOC 2014) which works for the general weighted directed case. In contrast to the previous results which rely on maintaining a sparse emulator, our algorithm relies on maintaining a so-called sparse (d, {$\varepsilon$})-hop set introduced by Cohen (JACM 2000) in the PRAM literature. A (d, {$\varepsilon$})-hop set of a graph G = (V, E) is a set E' of weighted edges such that the distance between any pair of nodes in G can be (1 + {$\varepsilon$})-approximated by their d-hop distance (given by a path containing at most d edges) on G'=(V, E{$\cup$}E'). Our algorithm can maintain an (no(1), {$\varepsilon$})-hop set of near-linear size in near-linear time under edge deletions. It is the first of its kind to the best of our knowledge. To maintain the distances on this hop set, we develop a monotone bounded-hop Even-Shiloach tree. It results from extending and combining the monotone Even-Shiloach tree of Henzinger, Krinninger, and Nanongkai (FOCS 2013) with the bounded-hop SSSP technique of Bernstein (STOC 2013). These two new tools might be of independent interest.},
  citationcount = {108},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {static,update,update time}
}

@article{henzingerDynamicApproximateAllpairs2013,
  title = {Dynamic Approximate All-Pairs Shortest Paths: {{Breaking}} the {{O}}(Mn) Barrier and Derandomization},
  author = {Henzinger, Monika and Krinninger, Sebastian and Nanongkai, Danupon},
  year = {2013},
  doi = {10.1137/140957299},
  abstract = {We study dynamic (1 + {$\epsilon$})-approximation algorithms for the all-pairs shortest paths problem in unweighted undirected n-node m-edge graphs under edge deletions. The fastest algorithm for this problem is a randomized algorithm with a total update time of (mn) and constant query time by Roditty and Zwick (FOCS 2004). The fastest deterministic algorithm is from a 1981 paper by Even and Shiloach (JACM 1981); it has a total update time of O(mn2) and constant query time. We improve these results as follows: (1) We present an algorithm with a total update time of (n5/2) and constant query time that has an additive error of two in addition to the 1 + {$\epsilon$} multiplicative error. This beats the previous (mn) time when m = {\textohm}(n3/2). Note that the additive error is unavoidable since, even in the static case, an O(n3-{$\delta$})-time (a so-called truly sub cubic) combinatorial algorithm with 1 + {$\epsilon$} multiplicative error cannot have an additive error less than 2 - {$\epsilon$}, unless we make a major breakthrough for Boolean matrix multiplication (Dor, Halperin and Zwick FOCS 1996) and many other long-standing problems (Vassilevska Williams and Williams FOCS 2010). The algorithm can also be turned into a (2 + {$\epsilon$})-approximation algorithm (without an additive error) with the same time guarantees, improving the recent (3 + {$\epsilon$})-approximation algorithm with (n5/2+O(1{\textsurd}(log n))) running time of Bernstein and Roditty (SODA 2011) in terms of both approximation and time guarantees. (2) We present a deterministic algorithm with a total update time of (mn) and a query time of O(log log n). The algorithm has a multiplicative error of 1 + {$\epsilon$} and gives the first improved deterministic algorithm since 1981. It also answers an open question raised by Bernstein in his STOC 2013 paper. In order to achieve our results, we introduce two new techniques: (1) A lazy Even-Shiloach tree algorithm which maintains a bounded-distance shortest-paths tree on a certain type of emulator called locally persevering emulator. (2) A derandomization technique based on moving Even-Shiloach trees as a way to derandomize the standard random set argument. These techniques might be of independent interest.},
  citationcount = {82},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,query,query time,static,update,update time}
}

@article{henzingerDynamicApproximateMaximum2020,
  title = {Dynamic Approximate Maximum Independent Set of Intervals, Hypercubes and Hyperrectangles},
  author = {Henzinger, Monika and Neumann, S. and Wiese, Andreas},
  year = {2020},
  doi = {10.4230/LIPIcs.SoCG.2020.51},
  abstract = {Independent set is a fundamental problem in combinatorial optimization. While in general graphs the problem is essentially inapproximable, for many important graph classes there are approximation algorithms known in the offline setting. These graph classes include interval graphs and geometric intersection graphs, where vertices correspond to intervals/geometric objects and an edge indicates that the two corresponding objects intersect. We present dynamic approximation algorithms for independent set of intervals, hypercubes and hyperrectangles in d dimensions. They work in the fully dynamic model where each update inserts or deletes a geometric object. All our algorithms are deterministic and have worst-case update times that are polylogarithmic for constant d and {$\epsilon>$}0, assuming that the coordinates of all input objects are in [0,N]\textsuperscript{d} and each of their edges has length at least 1. We obtain the following results: {$\bullet$} For weighted intervals, we maintain a (1+{$\epsilon$})-approximate solution. {$\bullet$} For d-dimensional hypercubes we maintain a (1+{$\epsilon$})2\textsuperscript{\{\vphantom\}}d\vphantom\{\}-approximate solution in the unweighted case and a O(2\textsuperscript{\{\vphantom\}}d\vphantom\{\})-approximate solution in the weighted case. Also, we show that for maintaining an unweighted (1+{$\epsilon$})-approximate solution one needs polynomial update time for d{$\geq$}2 if the ETH holds. {$\bullet$} For weighted d-dimensional hyperrectangles we present a dynamic algorithm with approximation ratio (1+{$\epsilon$})\textsuperscript{\{\vphantom\}}d-1\vphantom\{\}N.},
  citationcount = {19},
  venue = {International Symposium on Computational Geometry},
  keywords = {dynamic,update,update time}
}

@article{henzingerFasterSubmodularMaximization2023,
  title = {Faster Submodular Maximization for Several Classes of Matroids},
  author = {Henzinger, Monika and Liu, Paul and Vondr{\'a}k, J. and Zheng, D.},
  year = {2023},
  doi = {10.48550/arXiv.2305.00122},
  abstract = {The maximization of submodular functions have found widespread application in areas such as machine learning, combinatorial optimization, and economics, where practitioners often wish to enforce various constraints; the matroid constraint has been investigated extensively due to its algorithmic properties and expressive power. Recent progress has focused on fast algorithms for important classes of matroids given in explicit form. Currently, nearly-linear time algorithms only exist for graphic and partition matroids [ICALP '19]. In this work, we develop algorithms for monotone submodular maximization constrained by graphic, transversal matroids, or laminar matroids in time near-linear in the size of their representation. Our algorithms achieve an optimal approximation of 1-1/e-{$\epsilon$} and both generalize and accelerate the results of Ene and Nguyen [ICALP '19]. In fact, the running time of our algorithm cannot be improved within the fast continuous greedy framework of Badanidiyuru and Vondr{\'a}k [SODA '14]. To achieve near-linear running time, we make use of dynamic data structures that maintain bases with approximate maximum cardinality and weight under certain element updates. These data structures need to support a weight decrease operation and a novel FREEZE operation that allows the algorithm to freeze elements (i.e. force to be contained) in its basis regardless of future data structure operations. For the laminar matroid, we present a new dynamic data structure using the top tree interface of Alstrup, Holm, de Lichtenberg, and Thorup [TALG '05] that maintains the maximum weight basis under insertions and deletions of elements in O(n) time. For the transversal matroid the FREEZE operation corresponds to requiring the data structure to keep a certain set S of vertices matched, a property that we call S-stability.},
  citationcount = {4},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,dynamic,update}
}

@article{henzingerFineGrainedComplexity2022,
  title = {Fine-Grained Complexity Lower Bounds for Families of Dynamic Graphs},
  author = {Henzinger, Monika and Paz, A. and Sricharan, A. R.},
  year = {2022},
  doi = {10.48550/arXiv.2208.07572},
  abstract = {A dynamic graph algorithm is a data structure that answers queries about a property of the current graph while supporting graph modifications such as edge insertions and deletions. Prior work has shown strong conditional lower bounds for general dynamic graphs, yet graph families that arise in practice often exhibit structural properties that the existing lower bound constructions do not possess. We study three specific graph families that are ubiquitous, namely constant-degree graphs, power-law graphs, and expander graphs, and give the first conditional lower bounds for them. Our results show that even when restricting our attention to one of these graph classes, any algorithm for fundamental graph problems such as distance computation or approximation or maximum matching, cannot simultaneously achieve a sub-polynomial update time and query time. For example, we show that the same lower bounds as for general graphs hold for maximum matching and ( s, t )-distance in constant-degree graphs, power-law graphs or expanders. Namely, in an m -edge graph, there exists no dynamic algorithms with both O ( m 1 / 2 - {$\epsilon$} ) update time and O ( m 1 - {$\epsilon$} ) query time, for any small {$\epsilon$} {\textquestiondown} 0. Note that for ( s, t )-distance the trivial dynamic algorithm achieves an almost matching upper bound of constant update time and O ( m ) query time. We prove similar bounds for the other graph families and for other fundamental problems such as densest subgraph detection and perfect matching. ``The Design of Modern Fully Dynamic Data Structures (MoDynStruct)'' and from the Austrian Science Fund (FWF) project ``Fast Algorithms for a Reactive Network Layer (ReactNet)'', P 33775-N, with additional funding from the netidee SCIENCE Stiftung , 2020--2024},
  citationcount = {4},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,dynamic,lower bound,query,query time,update,update time}
}

@article{henzingerFullyDynamicCoresets2020,
  title = {Fully-Dynamic Coresets},
  author = {Henzinger, Monika and Kale, S.},
  year = {2020},
  doi = {10.4230/LIPIcs.ESA.2020.57},
  abstract = {With input sizes becoming massive, coresets -- small yet representative summary of the input -- are relevant more than ever. A weighted set C\textsubscript{w} that is a subset of the input is an {$\varepsilon$}-coreset if the cost of any feasible solution S with respect to C\textsubscript{w} is within [1\{{\textpm}\}{$\varepsilon$}] of the cost of S with respect to the original input. We give a very general technique to compute coresets in the fully-dynamic setting where input points can be added or deleted. Given a static {$\varepsilon$}-coreset algorithm that runs in time t(n,{$\varepsilon$},{$\lambda$}) and computes a coreset of size s(n,{$\varepsilon$},{$\lambda$}), where n is the number of input points and 1\{-\}{$\lambda$} is the success probability, we give a fully-dynamic algorithm that computes an {$\varepsilon$}-coreset with worst-case update time O((n){$\cdot$}t(s(n,{$\varepsilon$}/n,{$\lambda$}/n),{$\varepsilon$}/n,{$\lambda$}/n)) (this bound is stated informally), where the success probability is 1\{-\}{$\lambda$}. Our technique is a fully-dynamic analog of the merge-and-reduce technique that applies to insertion-only setting. Although our space usage is O(n), we work in the presence of an adaptive adversary, and we show that {\textohm}(n) space is required when adversary is adaptive. As a consequence, we get fully-dynamic {$\varepsilon$}-coreset algorithms for k-median and k-means with worst-case update time O({$\varepsilon$}\textsuperscript{\{\vphantom\}}-2\vphantom\{\}k{$^{25}$}n{$^3$}k) and coreset size O({$\varepsilon$}\textsuperscript{\{\vphantom\}}-2\vphantom\{\}kn{$^2$}k) ignoring n and (1/{$\varepsilon$}) factors and assuming that {$\varepsilon$},{$\lambda$}={\textohm}(1/poly(n)). These are the first fully-dynamic algorithms for k-median and k-means with worst-case update times O(poly(k,n,{$\varepsilon$}\textsuperscript{\{\vphantom\}}-1\vphantom\{\})). We also give conditional lower bound on update/query time for any fully-dynamic (4-{$\delta$})-approximation algorithm for k-means.},
  citationcount = {27},
  venue = {Embedded Systems and Applications},
  keywords = {adaptive,dynamic,lower bound,query,query time,static,update,update time}
}

@inproceedings{henzingerFullyDynamicCycleequivalence1994,
  title = {Fully Dynamic Cycle-Equivalence in Graphs},
  booktitle = {Proceedings 35th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Henzinger, M.R.},
  year = {1994},
  month = nov,
  pages = {744--755},
  doi = {10.1109/SFCS.1994.365718},
  url = {https://ieeexplore.ieee.org/document/365718},
  urldate = {2025-04-16},
  abstract = {Two edges e/sub 1/ and e/sub 2/ of an undirected graph are cycle-equivalent iff all cycles that contain e/sub 1/ also contain e/sub 2/, i.e., iff e/sub 1/ and e/sub 2/ are a cut-edge pair. The cycle-equivalence classes of the control-flow graph are used in optimizing compilers to speed up existing control-flow and data-flow algorithms. While the cycle-equivalence classes can be computed in linear time, we present the first fully dynamic algorithm for maintaining the cycle-equivalence relation. In an n-node graph our data structure executes an edge insertion or deletion in O(/spl radic/n log n) time and answers the query whether two given edges are cycle-equivalent in O(log/sup 2/ n) time. We also present an algorithm for plane graphs with O(log n) update and query time and for planar graphs with O(log n) insertion time and O(log/sup 2/ n) query and deletion time. Additionally, we show a lower bound of /spl Omega/(log n/log log n) for the amortized time per operation for the dynamic cycle-equivalence problem in the cell probe model.{$<>$}},
  keywords = {Algorithm design and analysis,Availability,cell probe,Centralized control,Computer science,Data analysis,data structure,Data structures,dynamic,Heuristic algorithms,lower bound,Optimizing compilers,Probes,query,query time,Testing,update},
  file = {/Users/tulasi/Zotero/storage/IRR9LS8J/Henzinger - 1994 - Fully dynamic cycle-equivalence in graphs.pdf}
}

@article{henzingerImprovedAlgorithmsDecremental2015,
  title = {Improved Algorithms for Decremental Single-Source Reachability on Directed Graphs},
  author = {Henzinger, Monika and Krinninger, Sebastian and Nanongkai, Danupon},
  year = {2015},
  doi = {10.1007/978-3-662-47672-7_59},
  abstract = {No abstract available},
  citationcount = {41},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{henzingerImprovedDataStructures2000,
  title = {Improved Data Structures for Fully Dynamic Biconnectivity},
  author = {Henzinger, Monika},
  year = {2000},
  doi = {10.1137/S0097539794263907},
  abstract = {We present fully dynamic algorithms for maintaining the biconnected components in general and plane graphs. A fully dynamic algorithm maintains a graph during a sequence of insertions and deletions of edges or isolated vertices. Let m be the number of edges and n be the number of vertices in a graph. The time per operation of the best deterministic algorithms is O({\textsurd}n) in general graphs and O(log n) in plane graphs for fully dynamic connectivity and O(min m2/3,n\vphantom\{\}) in general graphs and O({\textsurd}n) in plane graphs for fully dynamic biconnectivity. We improve the later running times to O({\textsurd}\{mn\}) in general graphs and O(log 2 n) in plane graphs. Our algorithm for general graphs can also find the biconnected components of all vertices in time O(n).},
  citationcount = {51},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,dynamic}
}

@article{henzingerImprovedSamplingWith1996,
  title = {Improved Sampling with Applications to Dynamic Graph Algorithms},
  author = {Henzinger, Monika and Thorup, M.},
  year = {1996},
  doi = {10.1007/3-540-61440-0_136},
  abstract = {No abstract available},
  citationcount = {20},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{henzingerIncrementalAndFully2016,
  title = {Incremental and Fully Dynamic Subgraph Connectivity for Emergency Planning},
  author = {Henzinger, Monika and Neumann, S.},
  year = {2016},
  doi = {10.4230/LIPIcs.ESA.2016.48},
  abstract = {During the last 10 years it has become popular to study dynamic graph problems in a emergency planning or sensitivity setting: Instead of considering the general fully dynamic problem, we only have to process a single batch update of size d; after the update we have to answer queries. In this paper, we consider the dynamic subgraph connectivity problem with sensitivity d: We are given a graph of which some vertices are activated and some are deactivated. After that we get a single update in which the states of up to d vertices are changed. Then we get a sequence of connectivity queries in the subgraph of activated vertices. We present the first fully dynamic algorithm for this problem which has an update and query time only slightly worse than the best decremental algorithm. In addition, we present the first incremental algorithm which is tight with respect to the best known conditional lower bound; moreover, the algorithm is simple and we believe it is implementable and efficient in practice.},
  citationcount = {11},
  venue = {Embedded Systems and Applications},
  keywords = {dynamic,lower bound,query,query time,update}
}

@article{henzingerOnTheComplexity2021,
  title = {On the Complexity of Weight-Dynamic Network Algorithms},
  author = {Henzinger, Monika and Paz, A. and Schmid, S.},
  year = {2021},
  doi = {10.23919/IFIPNetworking52078.2021.9472803},
  abstract = {While operating communication networks adaptively may improve utilization and performance, frequent adjustments also introduce an algorithmic challenge: the re-optimization of traffic engineering solutions is time-consuming and may limit the granularity at which a network can be adjusted. This paper is motivated by question whether the reactivity of a network can be improved by re-optimizing solutions dynamically rather than from scratch, especially if inputs such as link weights do not change significantly. This paper explores to what extent dynamic algorithms can be used to speed up fundamental tasks in network operations. We specifically investigate optimizations related to traffic engineering (namely shortest paths and maximum flow computations), but also consider spanning tree and matching applications. While prior work on dynamic graph algorithms focusses on link insertions and deletions, we are interested in the practical problem of link weight changes. We revisit existing upper bounds in the weight-dynamic model, and present several novel lower bounds on the amortized runtime for recomputing solutions. In general, we find that the potential performance gains depend on the application, and there are also strict limitations on what can be achieved, even if link weights change only slightly.},
  citationcount = {5},
  venue = {2021 IFIP Networking Conference (IFIP Networking)},
  keywords = {adaptive,communication,dynamic,lower bound}
}

@article{henzingerOnTheComplexity2023,
  title = {On the Complexity of Algorithms with Predictions for Dynamic Graph Problems},
  author = {Henzinger, Monika and Lincoln, Andrea and Saha, B. and Seybold, Martin P. and Ye, Christopher},
  year = {2023},
  doi = {10.48550/arXiv.2307.16771},
  abstract = {\{\vphantom\}\emph{Algorithms with predictions}\vphantom\{\}\emph{ incorporate machine learning predictions into algorithm design. A plethora of recent works incorporated predictions to improve on worst-case optimal bounds for online problems. In this paper, we initiate the study of complexity of dynamic data structures with predictions, including dynamic graph algorithms. Unlike in online algorithms, the main goal in dynamic data structures is to maintain the solution }\{\vphantom\}\emph{efficiently}\vphantom\{\}\emph{ with every update. Motivated by work in online algorithms, we investigate three natural models of predictions: (1) {$\varepsilon$}-accurate predictions where each predicted request matches the true request with probability at least {$\varepsilon$}, (2) list-accurate predictions where a true request comes from a list of possible requests, and (3) bounded delay predictions where the true requests are some (unknown) permutations of the predicted requests. For {$\varepsilon$}-accurate predictions, we show that lower bounds from the non-prediction setting of a problem carry over, up to a 1-{$\varepsilon$} factor. Then we give general reductions among the prediction models for a problem, showing that lower bounds for bounded delay imply lower bounds for list-accurate predictions, which imply lower bounds for {$\varepsilon$}-accurate predictions. Further, we identify two broad problem classes based on lower bounds due to the Online Matrix Vector (OMv) conjecture. Specifically, we show that dynamic problems that are }\{\vphantom\}\emph{locally correctable}\vphantom\{\}\emph{ have strong conditional lower bounds for list-accurate predictions that are equivalent to the non-prediction setting, unless list-accurate predictions are perfect. Moreover, dynamic problems that are }\{\vphantom\}\emph{locally reducible}\vphantom\{\}\emph{ have a smooth transition in the running time. We categorize problems accordingly and give upper bounds that show that our lower bounds are almost tight, including problems in dynamic graphs.}},
  citationcount = {10},
  venue = {Information Technology Convergence and Services},
  keywords = {data structure,dynamic,lower bound,reduction,update}
}

@article{henzingerRandomizedDynamicGraph1995,
  title = {Randomized Dynamic Graph Algorithms with Polylogarithmic Time per Operation},
  author = {Henzinger, Monika and King, Valerie},
  year = {1995},
  doi = {10.1145/225058.225269},
  abstract = {This paper solves a longstanding open problem in fully dynamic algorithms: We present the first fully dynamic algorithms that maintain connectivity, bipartiteness, and approximate minimum spanning trees in polylogarithmic time per edge insertion or deletion. The algorithms are designed using a new dynamic technique that combines a novel graph decomposition with randomization. They are Las-Vegas type randomized algorithms which use simple data structures and have a small constant factor. Let n denote the number of nodes in the graph. For a sequence of V(m0) operations, where m0 is the number of edges in the initial graph, the expected time for p updates is O( p log 3 n) (Throughout the paper the logarithms are base 2.) for connectivity and bipartiteness. The worst-case time for one query is O(log n/log log n). For the k-edge witness problem ("Does the removal of k given edges disconnect the graph?") the expected time for p updates is O( p log 3 n) and the expected time for q queries is O(qk log 3 n). Given a graph with k different weights, the minimum spanning tree can be maintained during a sequence of p updates in expected time O( pk log 3 n). This implies an algorithm to maintain a 1 1 e-approximation of the minimum spanning tree in expected time O((p log 3 n log U)/e) for p updates, where the weights of the edges are between 1 and U.},
  citationcount = {144},
  venue = {Symposium on the Theory of Computing}
}

@article{henzingerRandomizedFullyDynamic1999,
  title = {Randomized Fully Dynamic Graph Algorithms with Polylogarithmic Time per Operation},
  author = {Henzinger, Monika and King, Valerie},
  year = {1999},
  doi = {10.1145/320211.320215},
  abstract = {This paper solves a longstanding open problem in fully dynamic algorithms: We present the first fully dynamic algorithms that maintain connectivity, bipartiteness, and approximate minimum spanning trees in polylogarithmic time per edge insertion or deletion. The algorithms are designed using a new dynamic technique that combines a novel graph decomposition with randomization. They are Las-Vegas type randomized algorithms which use simple data structures and have a small constant factor. Let {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} denote the number of nodes in the graph. For a sequence of \&OHgr;({\textexclamdown}italic{\textquestiondown}m{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}subscrpt{\textquestiondown}0{\textexclamdown}/subscrpt{\textquestiondown}) operations, where {\textexclamdown}italic{\textquestiondown}m{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}subscrpt{\textquestiondown}0{\textexclamdown}/subscrpt{\textquestiondown} is the number of edges in the initial graph, the expected time for {\textexclamdown}italic{\textquestiondown}p{\textexclamdown}/italic{\textquestiondown} updates is {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}p{\textexclamdown}/italic{\textquestiondown} log{\textexclamdown}supscrpt{\textquestiondown}3{\textexclamdown}/supscrpt{\textquestiondown} {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) (througout the paper the logarithms are based 2) for connectivity and bipartiteness. The worst-case time for one query is {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}/log log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}). For the {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}-edge witness problem (``Does the removal of {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown} given edges disconnect the graph?'') the expected time for {\textexclamdown}italic{\textquestiondown}p{\textexclamdown}/italic{\textquestiondown} updates is {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}p{\textexclamdown}/italic{\textquestiondown} log{\textexclamdown}supscrpt{\textquestiondown}3{\textexclamdown}/supscrpt{\textquestiondown} {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) and the expected time for {\textexclamdown}italic{\textquestiondown}q{\textexclamdown}/italic{\textquestiondown} queries is {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}qk{\textexclamdown}/italic{\textquestiondown} log{\textexclamdown}supscrpt{\textquestiondown}3{\textexclamdown}/supscrpt{\textquestiondown} {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}). Given a graph with {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown} different weights, the minimum spanning tree can be maintained during a sequence of {\textexclamdown}italic{\textquestiondown}p{\textexclamdown}/italic{\textquestiondown} updates in expected time {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}pk{\textexclamdown}/italic{\textquestiondown} log{\textexclamdown}supscrpt{\textquestiondown}3{\textexclamdown}/supscrpt{\textquestiondown} {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}). This implies an algorithm to maintain a 1 + {$\varepsilon$}-approximation of the minimum spanning tree in expected time {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(({\textexclamdown}italic{\textquestiondown}p{\textexclamdown}/italic{\textquestiondown} log{\textexclamdown}supscrpt{\textquestiondown}3{\textexclamdown}/supscrpt{\textquestiondown} {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} log{\textexclamdown}italic{\textquestiondown}U{\textexclamdown}/italic{\textquestiondown})/{$\varepsilon$}) for {\textexclamdown}italic{\textquestiondown}p{\textexclamdown}/italic{\textquestiondown} updates, where the weights of the edges are between 1 and {\textexclamdown}italic{\textquestiondown}U{\textexclamdown}/italic{\textquestiondown}.},
  citationcount = {282},
  venue = {JACM}
}

@article{henzingerSamplingToProvide1997,
  title = {Sampling to Provide or to Bound: {{With}} Applications to Fully Dynamic Graph Algorithms},
  author = {Henzinger, Monika and Thorup, M.},
  year = {1997},
  doi = {10.1002/(SICI)1098-2418(199712)11:4%3C369::AID-RSA5%3E3.0.CO;2-X},
  abstract = {In dynamic graph algorithms the following provide-or-bound problem has to be solved quickly: Given a set S containing a subset R and a way of generating random elements from S testing for membership in R, either (i) provide an element of R, or (ii) give a (small) upper bound on the size of R that holds with high probability. We give an optimal algorithm for this problem. This algorithm improves the time per operation for various dynamic graph algorithms by a factor of O(log n). For example, it improves the time per update for fully dynamic connectivity from O(log3n) to O(log2n).},
  citationcount = {54},
  venue = {Random Struct. Algorithms}
}

@article{henzingerSublineartimeDecrementalAlgorithms2014,
  title = {Sublinear-Time Decremental Algorithms for Single-Source Reachability and Shortest Paths on Directed Graphs},
  author = {Henzinger, Monika and Krinninger, Sebastian and Nanongkai, Danupon},
  year = {2014},
  doi = {10.1145/2591796.2591869},
  abstract = {We consider dynamic algorithms for maintaining Single-Source Reachability (SSR) and approximate Single-Source Shortest Paths (SSSP) on n-node m-edge directed graphs under edge deletions (decremental algorithms). The previous fastest algorithm for SSR and SSSP goes back three decades to Even and Shiloach (JACM 1981); it has O(1) query time and O(mn) total update time (i.e., linear amortized update time if all edges are deleted). This algorithm serves as a building block for several other dynamic algorithms. The question whether its total update time can be improved is a major, long standing, open problem. In this paper, we answer this question affirmatively. We obtain a randomized algorithm which, in a simplified form, achieves an {\~O}(mn0.984) expected total update time for SSR and (1 + {$\varepsilon$})-approximate SSSP, where {\~O}({$\cdot$}) hides poly log n. We also extend our algorithm to achieve roughly the same running time for Strongly Connected Components (SCC), improving the algorithm of Roditty and Zwick (FOCS 2002), and an algorithm that improves the {\~O} (mn log W)-time algorithm of Bernstein (STOC 2013) for approximating SSSP on weighted directed graphs, where the edge weights are integers from 1 to W. All our algorithms have constant query time in the worst case.},
  citationcount = {66},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,query,query time,update,update time}
}

@article{henzingerSublinearTimeMaintenance2015,
  title = {Sublinear-Time Maintenance of Breadth-First Spanning Trees in Partially Dynamic Networks},
  author = {Henzinger, Monika and Krinninger, Sebastian and Nanongkai, Danupon},
  year = {2015},
  doi = {10.1145/3146550},
  abstract = {We study the problem of maintaining a breadth-first spanning tree (BFS tree) in partially dynamic distributed networks modeling a sequence of either failures or additions of communication links (but not both). We present deterministic (1+{$\varepsilon$})-approximation algorithms whose amortized time (over some number of link changes) is sublinear in D, the maximum diameter of the network. Our technique also leads to a deterministic (1+{$\varepsilon$})-approximate incremental algorithm for single-source shortest paths in the sequential (usual RAM) model. Prior to our work, the state of the art was the classic exact algorithm of Even and Shiloach (1981), which is optimal under some assumptions (Roditty and Zwick 2011; Henzinger et al. 2015). Our result is the first to show that, in the incremental setting, this bound can be beaten in certain cases if some approximation is allowed.},
  citationcount = {5},
  venue = {ACM Trans. Algorithms},
  keywords = {communication,dynamic}
}

@article{henzingerTheComplexityOf2022,
  title = {The Complexity of Average-Case Dynamic Subgraph Counting},
  author = {Henzinger, Monika and Lincoln, Andrea and Saha, B.},
  year = {2022},
  doi = {10.1137/1.9781611977073.23},
  abstract = {,},
  citationcount = {13},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {dynamic}
}

@article{henzingerTheStateOf2018,
  title = {The State of the Art in Dynamic Graph Algorithms},
  author = {Henzinger, Monika},
  year = {2018},
  doi = {10.1007/978-3-319-73117-9_3},
  abstract = {No abstract available},
  citationcount = {13},
  venue = {Conference on Current Trends in Theory and Practice of Informatics},
  keywords = {dynamic}
}

@inproceedings{henzingerUnifyingStrengtheningHardness2015,
  title = {Unifying and {{Strengthening Hardness}} for {{Dynamic Problems}} via the {{Online Matrix-Vector Multiplication Conjecture}}},
  booktitle = {Proceedings of the Forty-Seventh Annual {{ACM}} Symposium on {{Theory}} of {{Computing}}},
  author = {Henzinger, Monika and Krinninger, Sebastian and Nanongkai, Danupon and Saranurak, Thatchaphol},
  year = {2015},
  month = jun,
  series = {{{STOC}} '15},
  pages = {21--30},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2746539.2746609},
  url = {https://dl.acm.org/doi/10.1145/2746539.2746609},
  urldate = {2024-11-19},
  abstract = {Consider the following Online Boolean Matrix-Vector Multiplication problem: We are given an n x n matrix M and will receive n column-vectors of size n, denoted by v1, ..., vn, one by one. After seeing each vector vi, we have to output the product Mvi before we can see the next vector. A naive algorithm can solve this problem using O(n3) time in total, and its running time can be slightly improved to O(n3/log2 n) [Williams SODA'07]. We show that a conjecture that there is no truly subcubic (O(n3-{$\varepsilon$})) time algorithm for this problem can be used to exhibit the underlying polynomial time hardness shared by many dynamic problems. For a number of problems, such as subgraph connectivity, Pagh's problem, d-failure connectivity, decremental single-source shortest paths, and decremental transitive closure, this conjecture implies tight hardness results. Thus, proving or disproving this conjecture will be very interesting as it will either imply several tight unconditional lower bounds or break through a common barrier that blocks progress with these problems. This conjecture might also be considered as strong evidence against any further improvement for these problems since refuting it will imply a major breakthrough for combinatorial Boolean matrix multiplication and other long-standing problems if the term "combinatorial algorithms" is interpreted as "Strassen-like algorithms" [Ballard et al. SPAA'11].The conjecture also leads to hardness results for problems that were previously based on diverse problems and conjectures -- such as 3SUM, combinatorial Boolean matrix multiplication, triangle detection, and multiphase -- thus providing a uniform way to prove polynomial hardness results for dynamic algorithms; some of the new proofs are also simpler or even become trivial. The conjecture also leads to stronger and new, non-trivial, hardness results, e.g., for the fully-dynamic densest subgraph and diameter problems.},
  isbn = {978-1-4503-3536-2},
  keywords = {dynamic,lower bound},
  annotation = {M. Blaser . Lower bounds for online matrix-vector multiplication . Manuscript , 2014 . M. Blaser. Lower bounds for online matrix-vector multiplication. Manuscript, 2014.\\
\\
\\
\\
\\
R. Duan . New data structures for subgraph connectivity. In Automata, Languages and Programming , 37th International Colloquium, ICALP 2010, Bordeaux, France, July 6--10, 2010, Proceedings, Part I , pages 201 -- 212 , 2010 . R. Duan. New data structures for subgraph connectivity. In Automata, Languages and Programming, 37th International Colloquium, ICALP 2010, Bordeaux, France, July 6--10, 2010, Proceedings, Part I, pages 201--212, 2010.\\
\\
J. Fakcharoenphol , T. Kumpijit , D. Nanongkai , T. Saranurak , and P. Sukprasert . Fully-dynamic minimum cut on plane graphs in poylogarithmic time . Manuscript , 2014 . J. Fakcharoenphol, T. Kumpijit, D. Nanongkai, T. Saranurak, and P. Sukprasert. Fully-dynamic minimum cut on plane graphs in poylogarithmic time. Manuscript, 2014.\\
\\
\\
\\
\\
\\
\\
\\
M. Henzinger , S. Krinninger , and D. Nanongkai . Improved algorithms for decremental single-source reachability on directed graphs . In Manuscript , 2015 . M. Henzinger, S. Krinninger, and D. Nanongkai. Improved algorithms for decremental single-source reachability on directed graphs. In Manuscript, 2015.\\
\\
D. Hermelin , A. Levy , O. Weimann , and R. Yuster . Distance oracles for vertex-labeled graphs. In Automata, Languages and Programming - 38th International Colloquium, ICALP 2011, Zurich, Switzerland, July 4--8, 2011, Proceedings, Part II , pages 490 -- 501 , 2011 . D. Hermelin, A. Levy, O. Weimann, and R. Yuster. Distance oracles for vertex-labeled graphs. In Automata, Languages and Programming - 38th International Colloquium, ICALP 2011, Zurich, Switzerland, July 4--8, 2011, Proceedings, Part II, pages 490--501, 2011.\\
\\
\\
T. Kopelowitz , S. Pettie , and E. Porat . 3sum hardness in (dynamic) data structures. CoRR, abs/1407.6756 , 2014 . T. Kopelowitz, S. Pettie, and E. Porat. 3sum hardness in (dynamic) data structures. CoRR, abs/1407.6756, 2014.\\
\\
\\
T. Motzkin . Evaluation of polynomials and evaluation of rational functions . Bull. Amer. Math. Soc , 61 ( 163 ): 10 , 1955 . T. Motzkin. Evaluation of polynomials and evaluation of rational functions. Bull. Amer. Math. Soc, 61(163):10, 1955.\\
\\
\\
\\
\\
\\
\\
\\
\\
P. Sankowski . Faster dynamic matchings and vertex connectivity. In N. Bansal, K. Pruhs, and C. Stein, editors , Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007 , pages 118 -- 126 . SIAM, 2007 . P. Sankowski. Faster dynamic matchings and vertex connectivity. In N. Bansal, K. Pruhs, and C. Stein, editors, Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, pages 118--126. SIAM, 2007.\\
\\
\\
\\
\\
\\
R. Williams . Matrix-vector multiplication in sub-quadratic time: (some preprocessing required) . In SODA , pages 995 -- 1001 , 2007 . R. Williams. Matrix-vector multiplication in sub-quadratic time: (some preprocessing required). In SODA, pages 995--1001, 2007.},
  file = {/Users/tulasi/Zotero/storage/W8X42VHM/Henzinger et al. - 2015 - Unifying and Strengthening Hardness for Dynamic Problems via the Online Matrix-Vector Multiplication.pdf}
}

@article{henzingerUnifyingStrengtheningHardness2015a,
  title = {Unifying and Strengthening Hardness for Dynamic Problems via the Online Matrix-Vector Multiplication Conjecture},
  author = {Henzinger, Monika and Krinninger, Sebastian and Nanongkai, Danupon and Saranurak, Thatchaphol},
  year = {2015},
  doi = {10.1145/2746539.2746609},
  abstract = {Consider the following Online Boolean Matrix-Vector Multiplication problem: We are given an n x n matrix M and will receive n column-vectors of size n, denoted by v1, ..., vn, one by one. After seeing each vector vi, we have to output the product Mvi before we can see the next vector. A naive algorithm can solve this problem using O(n3) time in total, and its running time can be slightly improved to O(n3/log2 n) [Williams SODA'07]. We show that a conjecture that there is no truly subcubic (O(n3-{$\varepsilon$})) time algorithm for this problem can be used to exhibit the underlying polynomial time hardness shared by many dynamic problems. For a number of problems, such as subgraph connectivity, Pagh's problem, d-failure connectivity, decremental single-source shortest paths, and decremental transitive closure, this conjecture implies tight hardness results. Thus, proving or disproving this conjecture will be very interesting as it will either imply several tight unconditional lower bounds or break through a common barrier that blocks progress with these problems. This conjecture might also be considered as strong evidence against any further improvement for these problems since refuting it will imply a major breakthrough for combinatorial Boolean matrix multiplication and other long-standing problems if the term "combinatorial algorithms" is interpreted as "Strassen-like algorithms" [Ballard et al. SPAA'11]. The conjecture also leads to hardness results for problems that were previously based on diverse problems and conjectures -- such as 3SUM, combinatorial Boolean matrix multiplication, triangle detection, and multiphase -- thus providing a uniform way to prove polynomial hardness results for dynamic algorithms; some of the new proofs are also simpler or even become trivial. The conjecture also leads to stronger and new, non-trivial, hardness results, e.g., for the fully-dynamic densest subgraph and diameter problems.},
  citationcount = {295},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,lower bound}
}

@article{henzingerUpperAndLower2019,
  title = {Upper and Lower Bounds for Fully Retroactive Graph Problems},
  author = {Henzinger, Monika and Wu, Xiaowei},
  year = {2019},
  doi = {10.1007/978-3-030-83508-8_34},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Workshop on Algorithms and Data Structures},
  keywords = {lower bound}
}

@article{hePathQueryData2020,
  title = {Path Query Data Structures in Practice},
  author = {He, Meng and Kazi, Serikzhan},
  year = {2020},
  doi = {10.4230/LIPIcs.SEA.2020.27},
  abstract = {We perform experimental studies on data structures that answer path median, path counting, and path reporting queries in weighted trees. These query problems generalize the well-known range median query problem in arrays, as well as the 2d orthogonal range counting and reporting problems in planar point sets, to tree structured data. We propose practical realizations of the latest theoretical results on path queries. Our data structures, which use tree extraction, heavy-path decomposition and wavelet trees, are implemented in both succinct and pointer-based form. Our succinct data structures are further specialized to be plain or entropy-compressed. Through experiments on large sets, we show that succinct data structures for path queries may present a viable alternative to standard pointer-based realizations, in practical scenarios. Compared to na\{{\"i}\}ve approaches that compute the answer by explicit traversal of the query path, our succinct data structures are several times faster in path median queries and perform comparably in path counting and path reporting queries, while being several times more space-efficient. Plain pointer-based realizations of our data structures, requiring a few times more space than the na\{{\"i}\}ve ones, yield up to 100-times speed-up over them.},
  citationcount = {1},
  venue = {The Sea},
  keywords = {data structure,query}
}

@article{hermelinDistanceOraclesFor2011,
  title = {Distance Oracles for Vertex-Labeled Graphs},
  author = {Hermelin, D. and Levy, Avivit and Weimann, Oren and Yuster, R.},
  year = {2011},
  doi = {10.1007/978-3-642-22012-8_39},
  abstract = {No abstract available},
  citationcount = {24},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{hershbergerFullyDynamic21992,
  title = {Fully Dynamic 2-Edge-Connectivity in Planar Graphs},
  author = {Hershberger, J. and Henzinger, Monika and Suri, S.},
  year = {1992},
  doi = {10.1007/3-540-55706-7_20},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {Scandinavian Workshop on Algorithm Theory}
}

@article{hershManagingGigabytesCompressing2001,
  title = {Managing Gigabytes---Compressing and Indexing Documents and Images (Second Edition)},
  author = {Hersh, W.},
  year = {2001},
  doi = {10.1023/A:1011472308196},
  abstract = {No abstract available},
  citationcount = {191},
  venue = {Information retrieval (Boston)}
}

@article{heSpaceEfficientData2011,
  title = {Space Efficient Data Structures for Dynamic Orthogonal Range Counting},
  author = {He, Meng and Munro, J.},
  year = {2011},
  doi = {10.1016/j.comgeo.2013.08.007},
  abstract = {No abstract available},
  citationcount = {13},
  venue = {Computational geometry},
  keywords = {data structure,dynamic}
}

@article{heSuccinctAndImplicit2013,
  title = {Succinct and Implicit Data Structures for Computational Geometry},
  author = {He, Meng},
  year = {2013},
  doi = {10.1007/978-3-642-40273-9_15},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {Space-Efficient Data Structures, Streams, and Algorithms},
  keywords = {data structure}
}

@article{heSuccinctDataStructures2012,
  title = {Succinct Data Structures for Path Queries},
  author = {He, Meng and Munro, J. and Zhou, Gelin},
  year = {2012},
  doi = {10.1007/978-3-642-33090-2_50},
  abstract = {No abstract available},
  citationcount = {16},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,query}
}

@article{heSuccinctRepresentationsOf2010,
  title = {Succinct Representations of Dynamic Strings},
  author = {He, Meng and Munro, J. I.},
  year = {2010},
  doi = {10.1007/978-3-642-16321-0_35},
  abstract = {No abstract available},
  citationcount = {41},
  venue = {SPIRE},
  keywords = {dynamic}
}

@article{hiraharaHardnessSelfAmplification2022,
  title = {Hardness Self-Amplification from Feasible Hard-Core Sets},
  author = {Hirahara, Shuichi and Shimizu, Nobutaka},
  year = {2022},
  doi = {10.1109/FOCS54457.2022.00058},
  abstract = {We consider the question of hardness self-amplification: Given a Boolean function f that is hard to compute on an o (1)-fraction of inputs drawn from some distribution, can we prove that f is hard to compute on a (\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{2\}-o(1))-fraction of inputs drawn from the same distribution? We prove hardness self-amplification results for natural distributional problems studied in fine-grained average-case complexity, such as the problem of counting the number of the triangles modulo 2 in a random tripartite graph and the online vector-matrix-vector multiplication problem over \{F\}\textsubscript{\{\vphantom\}}2\vphantom\{\}. More generally, we show that any problem that can be decomposed into "computationally disjoint" subsets of inputs admits hardness self-amplification. This is proved by generalizing the security proof of the NisanWigderson pseudorandom generator, in which case nearly disjoint subsets of inputs are considered. At the core of our proof techniques is a new notion of feasible hard-core set, which generalizes Impagliazzo's hard-core set [Impagliazzo, FOCS'95]. We show that any weak average-case hard function f has a feasible hard-core set H: any small H-oracle circuit (that is allowed to make queries q to H if f(q) can be computed without the oracle) fails to compute f on a (\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{2\}-o(1))-fraction of inputs in H.},
  citationcount = {2},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {query}
}

@article{hiraharaHardnessSelfAmplification2023,
  title = {Hardness Self-Amplification: {{Simplified}}, Optimized, and Unified},
  author = {Hirahara, Shuichi and Shimizu, Nobutaka},
  year = {2023},
  doi = {10.1145/3564246.3585189},
  abstract = {Strong (resp. weak) average-case hardness refers to the properties of a computational problem in which a large (resp. small) fraction of instances are hard to solve. We develop a general framework for proving hardness self-amplification, that is, the equivalence between strong and weak average-case hardness. Using this framework, we prove hardness self-amplification for popular problems, such as matrix multiplication, online matrix-vector multiplication, triangle counting of Erd{\H o}s--R{\'e}nyi random graphs, and the planted clique problem. As a corollary, we obtain the first search-to-decision reduction for the planted clique problem in a high-error regime. Our framework simplifies, improves, and unifies the previous hardness self-amplification results. Our approach uses a one-query upward self-reduction, that is, a reduction that maps a small instance to a large instance. We demonstrate that this reduction yields hardness self-amplification if the bipartite graph, whose left and right vertices correspond to small and large instances, respectively, has an expansion property. Our key technical contribution is to show the expansion property of the bipartite graph naturally constructed from the planted clique problem by using the coupling method of Markov chains.},
  citationcount = {2},
  venue = {Symposium on the Theory of Computing},
  keywords = {query,reduction}
}

@article{hirotaEfficientAlgorithmsFor2023,
  title = {Efficient Algorithms for Enumerating Maximal Common Subsequences of Two Strings},
  author = {Hirota, Miyuji and Sakai, Y.},
  year = {2023},
  doi = {10.48550/arXiv.2307.10552},
  abstract = {We propose efficient algorithms for enumerating maximal common subsequences (MCSs) of two strings. Efficiency of the algorithms are estimated by the preprocessing-time, space, and delay-time complexities. One algorithm prepares a cubic-space data structure in cubic time to output each MCS in linear time. This data structure can be used to search for particular MCSs satisfying some condition without performing an explicit enumeration. Another prepares a quadratic-space data structure in quadratic time to output each MCS in linear time, and the other prepares a linear-space data structure in quadratic time to output each MCS in linearithmic time.},
  citationcount = {3},
  venue = {arXiv.org},
  keywords = {data structure}
}

@article{hoeffdingProbabilityInequalitiesFor1963,
  title = {Probability Inequalities for Sum of Bounded Random Variables},
  author = {Hoeffding, W.},
  year = {1963},
  doi = {10.1007/978-1-4612-0865-5_26},
  abstract = {No abstract available},
  citationcount = {4653},
  venue = {No venue available}
}

@article{hoevenAmortizedMultiPoint2022,
  title = {Amortized Multi-Point Evaluation of Multivariate Polynomials},
  author = {Hoeven, J. and Lecerf, Gr{\'e}goire},
  year = {2022},
  doi = {10.1016/j.jco.2022.101693},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {Journal of Complexity}
}

@article{hoevenCompositionModuloPowers2017,
  title = {Composition modulo Powers of Polynomials},
  author = {Hoeven, J. and Lecerf, Gr{\'e}goire},
  year = {2017},
  doi = {10.1145/3087604.3087634},
  abstract = {Modular composition is the problem to compose two univariate polynomials modulo a third one. For polynomials with coefficients in a finite field, Kedlaya and Umans proved in 2008 that the theoretical bit complexity for performing this task could be made arbitrarily close to linear. Unfortunately, beyond its major theoretical impact, this result has not led to practically faster implementations yet. In this paper, we study the more specific case of composition modulo the power of a polynomial. First we extend previously known algorithms for power series composition to this context. We next present a fast direct reduction of our problem to power series composition.},
  citationcount = {14},
  venue = {International Symposium on Symbolic and Algebraic Computation},
  keywords = {reduction}
}

@article{hoevenFastMultivariateMulti2020,
  title = {Fast Multivariate Multi-Point Evaluation Revisited},
  author = {Hoeven, J. and Lecerf, Gr{\'e}goire},
  year = {2020},
  doi = {10.1016/J.JCO.2019.04.001},
  abstract = {No abstract available},
  citationcount = {29},
  venue = {Journal of Complexity}
}

@article{hoevenModularCompositionVia2018,
  title = {Modular Composition via Factorization},
  author = {Hoeven, J. and Lecerf, Gr{\'e}goire},
  year = {2018},
  doi = {10.1016/J.JCO.2018.05.002},
  abstract = {No abstract available},
  citationcount = {16},
  venue = {Journal of Complexity}
}

@article{hoevenOnTheComplexity2020,
  title = {On the Complexity Exponent of Polynomial System Solving},
  author = {Hoeven, J. and Lecerf, Gr{\'e}goire},
  year = {2020},
  doi = {10.1007/s10208-020-09453-0},
  abstract = {No abstract available},
  citationcount = {20},
  venue = {Foundations of Computational Mathematics}
}

@article{hoevenUltimateComplexityFor2020,
  title = {Ultimate Complexity for Numerical Algorithms},
  author = {Hoeven, J. and Lecerf, Gr{\'e}goire},
  year = {2020},
  doi = {10.1145/3419048.3419049},
  abstract = {Most numerical algorithms are designed for single or double precision floating point arithmetic, and their complexity is measured in terms of the total number of floating point operations. The resolution of problems with high condition numbers (e.g. when approaching a singularity or degeneracy) may require higher working precisions, in which case it is important to take the precision into account when doing complexity analyses. In this paper, we propose a new "ultimate complexity" model, which focuses on analyzing the cost of numerical algorithms for "sufficiently large" precisions. As an example application we will present an ultimately softly linear time algorithm for modular composition of univariate polynomials.},
  citationcount = {2},
  venue = {ACM Communications in Computer Algebra}
}

@article{hoffmannComparisonOfStandard2010,
  title = {Comparison of Standard and Zipf-Based Document Retrieval Heuristics},
  author = {Hoffmann, Benjamin},
  year = {2010},
  doi = {10.18419/OPUS-2685},
  abstract = {Document retrieval is the task to retrieve from a possibly huge collection of documents those which are most similar to a given query document. In this paper, we present a new heuristic for inexact top K retrieval. It is similar to the well-known index elimination heuristic and is based on Zipf's law, a statistical law observable in natural language texts. We compare the two heuristics with regard to retrieval performance and execution time. Therefore, we use a text collection consisting of scientific articles from various computer science conferences and journals. It turns out that our new approach is not better than index elimination. Interestingly, a combination of both heuristics yields the best results.},
  citationcount = {Unknown},
  venue = {No venue available},
  keywords = {query}
}

@article{hoffmannSimilaritySearchWith2010,
  title = {Similarity Search with Set Intersection as a Distance Measure},
  author = {Hoffmann, Benjamin},
  year = {2010},
  doi = {10.18419/OPUS-2669},
  abstract = {This thesis deals with a fundamental algorithmic problem. Given a database of sets and a query set, we want to determine a set from the database that has a maximal intersection with the query set. It is allowed to preprocess the database so that queries can be answered efficiently. We solve the approximate version of this problem. We investigate two randomized input models which are derived from real inputs. We present a deterministic algorithm for each of them. Under the assumption that the database and the query set follow one of these models, the corresponding algorithm determines with high probability a set from the database that has no maximal intersection with the query set, but an intersection that achieves a large proportion of the maximal size. Depending on the model, the query time is either quasi-linear in the sum of the database size and the number of different elements from all sets, or it is polylogarithmic in the database size. Thus, both algorithms are significantly faster than a naive algorithm intersecting the query set with each single database set. Die vorliegende Arbeit beschaftigt sich mit einem elementaren Problem aus dem Gebiet der Algorithmentheorie. Gegeben sei eine Datenbank von Mengen und eine Anfragemenge. Das Ziel ist, moglichst effizient eine Menge der Datenbank zu bestimmen, die einen Schnitt maximaler Grose mit der Anfragemenge besitzt. Dabei ist es erlaubt, die Datenbank vorzuverarbeiten. Wir prasentieren Losungen fur die Approximationsvariante dieses Problems. Wir untersuchen zwei aus der Praxis hergeleitete Eingabemodelle und stellen fur jedes Modell einen deterministischen Algorithmus vor. Verhalten sich die Datenbank und die Anfragemenge gemas einem dieser Modelle, dann bestimmt der entsprechende Algorithmus mit hoher Wahrscheinlichkeit eine Menge der Datenbank, deren Schnittgrose mit der Anfragemenge zwar nicht maximal ist, jedoch einen hohen Anteil der maximalen Grose erreicht. Die Anfragezeit ist je nach Modell entweder quasilinear in der Summe der Datenbankgrose und der Anzahl der verschiedenen Elemente aller Mengen, oder polylogarithmisch in der Datenbankgrose. Somit sind beide Algorithmen deutlich schneller als ein naiver Algorithmus, der die Grose des Schnittes zwischen der Anfragemenge und jeder einzelnen Menge der Datenbank bestimmt.},
  citationcount = {5},
  venue = {No venue available},
  keywords = {query,query time}
}

@article{hollandSuccinctListIndexing2022,
  title = {Succinct List Indexing in Optimal Time},
  author = {Holland, W.},
  year = {2022},
  doi = {10.4230/LIPIcs.ISAAC.2022.65},
  abstract = {An indexed list supports (efficient) access to both the offsets and the items of an arbitrarily ordered set under the effect of insertions and deletions . Existing solutions are engaged in a space-time trade-off. On the one hand, time efficient solutions are composed as a package of data structures: a linked-list, a hash table and a tree-type structure to support indexing. This arrangement observes a memory commitment that is outside the information theoretic lower bound (for ordered sets) by a factor of 12. On the other hand, the memory lower bound can be satisfied, up to an additive lower order term, trivially with an array. However, operations incur time costs proportional to the length of the array. We revisit the list indexing problem by attempting to balance the competing demands of space and time efficiency. We prepare the first succinct indexed list that supports efficient query and update operations. To implement an ordered set of size n , drawn from the universe \{ 1 , . . . , m \} , the solution occupies n (log m + o (log n )) bits (with high probability) and admits all operations optimally in O (log n/ log log n ) time.},
  citationcount = {1},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {data structure,information theoretic,lower bound,query,update}
}

@article{holmDynamicBridgeFinding2017,
  title = {Dynamic Bridge-Finding in {{O\vphantom\{\}($^2$n)}} Amortized Time},
  author = {Holm, J. and Rotenberg, E. and Thorup, M.},
  year = {2017},
  doi = {10.1137/1.9781611975031.3},
  abstract = {We present a deterministic fully-dynamic data structure for maintaining information about the bridges in a graph. We support updates in O((log n)2) amortized time, and can find a bridge in the component of any given vertex, or a bridge separating any two given vertices, in O(log n/ log log n) worst case time. Our bounds match the current best for bounds for deterministic fully-dynamic connectivity up to log log n factors. The previous best dynamic bridge finding was an O((log n)3) amortized time algorithm by Thorup [STOC2000], which was a bittrick-based improvement on the O((log n)4) amortized time algorithm by Holm et al.[STOC98, JACM2001]. Our approach is based on a different and purely combinatorial improvement of the algorithim of Holm et al., which by itself gives a new combinatorial O((log n)3) amortized time algorithm. Combining it with Thorup's bittrick, we get down to the claimed O((log n)2) amortized time. Essentially the same new trick can be applied to the biconnectivity data structure from [STOC98, JACM2001], improving the amortized update time to O((log n)3). We also offer improvements in space. We describe a general trick which applies to both of our new algorithms, and to the old ones, to get down to linear space, where the previous best use O(m + n log n log log n). Our result yields an improved running time for deciding whether a unique perfect matching exists in a static graph.},
  citationcount = {28},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,static,update,update time}
}

@article{holmDynamicPlanarEmbeddings2015,
  title = {Dynamic Planar Embeddings of Dynamic Graphs},
  author = {Holm, J. and Rotenberg, E.},
  year = {2015},
  doi = {10.1007/s00224-017-9768-7},
  abstract = {No abstract available},
  citationcount = {14},
  venue = {Theory of Computing Systems},
  keywords = {dynamic}
}

@article{holmFasterFullyDynamic2014,
  title = {Faster Fully-Dynamic Minimum Spanning Forest},
  author = {Holm, J. and Rotenberg, E. and {Wulff-Nilsen}, Christian},
  year = {2014},
  doi = {10.1007/978-3-662-48350-3_62},
  abstract = {No abstract available},
  citationcount = {33},
  venue = {Embedded Systems and Applications},
  keywords = {dynamic}
}

@article{holmFullyDynamicPlanarity2019,
  title = {Fully-Dynamic Planarity Testing in Polylogarithmic Time},
  author = {Holm, J. and Rotenberg, E.},
  year = {2019},
  doi = {10.1145/3357713.3384249},
  abstract = {Given a dynamic graph subject to insertions and deletions of edges, a natural question is whether the graph presently admits a planar embedding. We give a deterministic fully-dynamic algorithm for general graphs, running in amortized O(log3 n) time per edge insertion or deletion, that maintains a bit indicating whether or not the graph is presently planar. This is an exponential improvement over the previous best algorithm [Eppstein, Galil, Italiano, Spencer, 1996] which spends amortized O({\textsurd}n) time per update.},
  citationcount = {16},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,update}
}

@article{holmGoodRDivisions2024,
  title = {Good R-{{Divisions}} Imply Optimal Amortized Decremental Biconnectivity},
  author = {Holm, J. and Rotenberg, E.},
  year = {2024},
  doi = {10.4230/LIPIcs.STACS.2021.42},
  abstract = {We present a data structure that, given a graph G of n vertices and m edges, and a suitable pair of nested r-divisions of G, preprocesses G in  O ( m + n ) time and handles any series of edge-deletions in O(m) total time while answering queries to pairwise biconnectivity in worst-case O(1) time. In case the vertices are not biconnected, the data structure can return a cutvertex separating them in worst-case O(1) time. As an immediate consequence, this gives optimal amortized decremental biconnectivity, 2-edge connectivity, and connectivity for large classes of graphs, including planar graphs and other minor free graphs.},
  citationcount = {4},
  venue = {Symposium on Theoretical Aspects of Computer Science},
  keywords = {data structure,query}
}

@article{holmPlanarReachabilityIn2014,
  title = {Planar Reachability in Linear Space and Constant Time},
  author = {Holm, J. and Rotenberg, E. and Thorup, M.},
  year = {2014},
  doi = {10.1109/FOCS.2015.30},
  abstract = {We show how to represent a planar digraph in linear space so that reach ability queries can be answered in constant time. The data structure can be constructed in linear time. This representation of reach ability is thus optimal in both time and space, and has optimal construction time. The previous best solution used O(n log n) space for constant query time [Thorup FOCS'01].},
  citationcount = {17},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,query,query time}
}

@article{holmPolyLogarithmicDeterministic2001,
  title = {Poly-Logarithmic Deterministic Fully-Dynamic Algorithms for Connectivity, Minimum Spanning Tree, 2-Edge, and Biconnectivity},
  author = {Holm, J. and Lichtenberg, K. D. and Thorup, M.},
  year = {2001},
  doi = {10.1145/502090.502095},
  abstract = {Deterministic fully dynamic graph algorithms are presented for connectivity, minimum spanning tree, 2-edge connectivity, and biconnectivity. Assuming that we start with no edges in a graph with {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} vertices, the amortized operation costs are {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) for connectivity, {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log{\textexclamdown}sup{\textquestiondown}4{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) for minimum spanning forest, 2-edge connectivity, and {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log{\textexclamdown}sup{\textquestiondown}5{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) biconnectivity.},
  citationcount = {462},
  venue = {JACM}
}

@article{holmSplayTopTrees2022,
  title = {Splay Top Trees},
  author = {Holm, J. and Rotenberg, E. and Ryhl, Alice},
  year = {2022},
  doi = {10.1137/1.9781611977585.ch28},
  abstract = {The top tree data structure is an important and fundamental tool in dynamic graph algorithms. Top trees have existed for decades, and today serve as an ingredient in many state-of-the-art algorithms for dynamic graphs. In this work, we give a new direct proof of the existence of top trees, facilitating simpler and more direct implementations of top trees, based on ideas from splay trees. This result hinges on new insights into the structure of top trees, and in particular the structure of each root path in a top tree.},
  citationcount = {3},
  venue = {SIAM Symposium on Simplicity in Algorithms},
  keywords = {data structure,dynamic}
}

@article{holmWorstCaseDeterministic2022,
  title = {Worst-Case Deterministic Fully-Dynamic Planar 2-Vertex Connectivity},
  author = {Holm, J. and Hoog, I. and Rotenberg, E.},
  year = {2022},
  doi = {10.48550/arXiv.2209.14079},
  abstract = {We study dynamic planar graphs with n vertices, subject to edge deletion, edge contraction, edge insertion across a face, and the splitting of a vertex in specified corners. We dynamically maintain a combinatorial embedding of such a planar graph, subject to connectivity and 2 -vertex-connectivity (biconnectivity) queries between pairs of vertices. Whenever a query pair is connected and not biconnected, we find the first and last cutvertex separating them. Additionally, we allow local changes to the embedding by flipping the embedding of a subgraph that is connected by at most two vertices to the rest of the graph. We support all queries and updates in deterministic, worst-case, O (log 2 n ) time, using an O ( n ) -sized data structure. Previously, the best for fully-dynamic biconnectivity (subject to our set of operations) was an amortised {\texttildelow} O (log 3 n ) for general graphs, and algorithms with worst-case polylogarithmic update times were known only in the partially dynamic (insertion-only or deletion-only) setting.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {data structure,dynamic,query,update,update time}
}

@article{holmWorstCaseDeterministic2023,
  title = {Worst-Case Deterministic Fully-Dynamic Biconnectivity in Changeable Planar Embeddings},
  author = {Holm, J. and Hoog, I. and Rotenberg, E.},
  year = {2023},
  doi = {10.4230/LIPIcs.SoCG.2023.40},
  abstract = {We study dynamic planar graphs with n vertices, subject to edge deletion, edge contraction, edge insertion across a face, and the splitting of a vertex in specified corners. We dynamically maintain a combinatorial embedding of such a planar graph, subject to connectivity and 2-vertex-connectivity (biconnectivity) queries between pairs of vertices. Whenever a query pair is connected and not biconnected, we find the first and last cutvertex separating them. Additionally, we allow local changes to the embedding by flipping the embedding of a subgraph that is connected by at most two vertices to the rest of the graph. We support all queries and updates in deterministic, worst-case, O (log 2 n ) time, using an O ( n )-sized data structure.},
  citationcount = {Unknown},
  venue = {International Symposium on Computational Geometry},
  keywords = {data structure,dynamic,query,update}
}

@article{honBreakingATime2003,
  title = {Breaking a Time-and-Space Barrier in Constructing Full-Text Indices},
  author = {Hon, W. and Sadakane, K. and Sung, W.},
  year = {2003},
  doi = {10.1109/SFCS.2003.1238199},
  abstract = {Suffix trees and suffix arrays are the most prominent full-text indices, and their construction algorithms are well studied. It has been open for a long time whether these indices can be constructed in both O(n log n) time and O(n log n)-bit working space, where n denotes the length of the text. In the literature, the fastest algorithm runs in O(n) time, while it requires O(n log n)-bit working space. On the other hand, the most space-efficient algorithm requires O(n)-bit working space while it runs in O(n log n) time. This paper breaks the long-standing time-and-space barrier under the unit-cost word RAM. We give an algorithm for constructing the suffix array which takes O(n) time and O(n)-bit working space, for texts with constant-size alphabets. Note that both the time and the space bounds are optimal. For constructing the suffix tree, our algorithm requires O(n log/sup /spl epsi//n) time and O(n)-bit working space for any 0 {\textexclamdown} /spl epsi/ {\textexclamdown} 1. Apart from that, our algorithm can also be adopted to build other existing full-text indices, such as Compressed Suffix Tree, Compressed Suffix Arrays and FM-index. We also study the general case where the size of the alphabet A is not constant. Our algorithm can construct a suffix array and a suffix tree using optimal O(n log {\textbar}A{\textbar})-bit working space while running in O(n log log {\textbar}A{\textbar}) time and O(n log/sup /spl epsi//n) time, respectively. These are the first algorithms that achieve 0(n log n) time with optimal working space, under a reasonable assumption that log {\textbar}A{\textbar} = o(log n).},
  citationcount = {168},
  venue = {44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.}
}

@article{honCompressedPersistentIndex2013,
  title = {Compressed Persistent Index for Efficient Rank/Select Queries},
  author = {Hon, W. and Lee, Lap-Kei and Sadakane, K. and Tsakalidis, Konstantinos},
  year = {2013},
  doi = {10.1007/978-3-642-40104-6_35},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Workshop on Algorithms and Data Structures},
  keywords = {query}
}

@article{hongIOComplexity1981,
  title = {I/{{O}} Complexity: {{The}} Red-Blue Pebble Game},
  author = {Hong, Jia-Wei and Kung, H. T.},
  year = {1981},
  doi = {10.1145/800076.802486},
  abstract = {In this paper, the red-blue pebble game is proposed to model the input-output complexity of algorithms. Using the pebble game formulation, a number of lower bound results for the I/O requirement are proven. For example, it is shown that to perform the n-point FFT or the ordinary n{\texttimes}n matrix multiplication algorithm with O(S) memory, at least {\textohm}(n log n/log S) or {\textohm}(n3/@@@@S), respectively, time is needed for the I/O. Similar results are obtained for algorithms for several other problems. All of the lower bounds presented are the best possible in the sense that they are achievable by certain decomposition schemes. Results of this paper may provide insight into the difficult task of balancing I/O and computation in special-purpose system designs. For example, for the n-point FFT, the lower bound on I/O time implies that an S-point device achieving a speed-up ratio of order log S over the conventional O(n log n) time implementation is all one can hope for.},
  citationcount = {559},
  venue = {Symposium on the Theory of Computing}
}

@article{honSpaceEfficientFramework2009,
  title = {Space-Efficient Framework for Top-k String Retrieval Problems},
  author = {Hon, W. and Shah, Rahul and Vitter, J.},
  year = {2009},
  doi = {10.1109/FOCS.2009.19},
  abstract = {Given a set \{D\}= d\_1,d\_2,...,d\_D  of Dstrings of total length n, our task is to report the "most relevant"strings for a given query pattern P. This involves somewhat more advanced query functionality than the usual pattern matching, as some notion of "most relevant" is involved. In information retrieval literature, this task is best achieved by using inverted indexes. However, inverted indexes work only for some predefined set of patterns. In the pattern matching community, the most popular pattern-matching data structures are suffix trees and suffix arrays. However, a typical suffix tree search involves going through all the occurrences of the pattern over the entire string collection, which might be a lot more than the required relevant documents. The first formal framework to study such kind of retrieval problems was given by [Muthukrishnan, 2002]. He considered two metrics for relevance: frequency and proximity. He took a threshold-based approach on these metrics and gave data structures taking O(n) words of space. We study this problem in a slightly different framework of reporting the top k most relevant documents (in sorted order) under similar and more general relevance metrics. Our framework gives linear space data structure with optimal query times for arbitrary score functions. As a corollary, it improves the space utilization for the problems in [Muthukrishnan, 2002] while maintaining optimal query performance. We also develop compressed variants of these data structures for several specific relevance metrics.},
  citationcount = {104},
  venue = {2009 50th Annual IEEE Symposium on Foundations of Computer Science}
}

@article{honSuccinctDataStructures2003,
  title = {Succinct Data Structures for Searchable Partial Sums},
  author = {Hon, W. and Sadakane, K. and Sung, W.},
  year = {2003},
  doi = {10.1007/978-3-540-24587-2_52},
  abstract = {No abstract available},
  citationcount = {44},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {data structure}
}

@article{honSuccinctDataStructures2011,
  title = {Succinct Data Structures for {{Searchable Partial Sums}} with Optimal Worst-Case Performance},
  author = {Hon, W. and Sadakane, K. and Sung, W.},
  year = {2011},
  doi = {10.1016/j.tcs.2011.05.023},
  abstract = {No abstract available},
  citationcount = {25},
  venue = {Theoretical Computer Science},
  keywords = {data structure}
}

@article{hoogDynamicEmbeddingsOf2022,
  title = {Dynamic Embeddings of Dynamic Single-Source Upward Planar Graphs},
  author = {Hoog, I. and Parada, I. and Rotenberg, E.},
  year = {2022},
  doi = {10.48550/arXiv.2209.14094},
  abstract = {A directed graph G is upward planar if it admits a planar embedding such that each edge is y-monotone. Unlike planarity testing, upward planarity testing is NP-hard except in restricted cases, such as when the graph has the single-source property (i.e. each connected component only has one source). In this paper, we present a dynamic algorithm for maintaining a combinatorial embedding \{E\}(G) of a single-source upward planar graph subject to edge deletions, edge contractions, edge insertions upwards across a face, and single-source-preserving vertex splits through specified corners. We furthermore support changes to the embedding \{E\}(G) on the form of subgraph flips that mirror or slide the placement of a subgraph that is connected to the rest of the graph via at most two vertices. All update operations are supported as long as the graph remains upward planar, and all queries are supported as long as the graph remains single-source. Updates that violate upward planarity are identified as such and rejected by our update algorithm. We dynamically maintain a linear-size data structure on G which supports incidence queries between a vertex and a face, and upward-linkability of vertex pairs. If a pair of vertices are not upwards-linkable, we facilitate one-flip-linkable queries that point to a subgraph flip that makes them linkable, if any such flip exists. We support all updates and queries in O({$^2$}n) time.},
  citationcount = {Unknown},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,dynamic,query,update}
}

@article{hopcroftSetMergingAlgorithms1973,
  title = {Set Merging Algorithms},
  author = {Hopcroft, J. and Ullman, J.},
  year = {1973},
  doi = {10.1137/0202024},
  abstract = {This paper considers the problem of merging sets formed from a total of n items in such a way that at any time, the name of a set containing a given item can be ascertained. Two algorithms using different data structures are discussed. The execution times of both algorithms are bounded by a constant times nG(n), where G(n) is a function whose asymptotic growth rate is less than that of any finite number of logarithms of n.},
  citationcount = {174},
  venue = {SIAM journal on computing (Print)}
}

@article{howatExploitingNonUniform2012,
  title = {Exploiting Non-Uniform Query Distributions in Data Structuring Problems},
  author = {Howat, John},
  year = {2012},
  doi = {10.22215/etd/2012-09696},
  abstract = {This thesis examines data structures with query times that are a function of the distribution of queries made to them. When a query distribution exhibits nonuniformity---as is often the case in many applications---the sequence of queries can often be executed faster: Several such problems are considered. For the dictionary problem, this thesis presents the first binary search tree to achieve the working-set property for individual queries in the worst case, a data structure that supports searching from arbitrary temporal fingers, and a data structure that supports a stronger version of the unified property. For the predecessor search problem in bounded universes, this thesis presents a data structure that answers queries in time that is a function of the distance between the query and its answers (which leads to several appliations in the areas of nearest neighbour search and range searching), as well as several data structures that answer queries in time that is a function of the entropy of the query distribution and various space requirements.},
  citationcount = {1},
  venue = {No venue available},
  keywords = {data structure,query,query time}
}

@article{hsuOnMultidimensionalAnd2017,
  title = {On Multidimensional and Monotone K-{{SUM}}},
  author = {Hsu, Chloe Ching-Yun and Umans, C.},
  year = {2017},
  doi = {10.4230/LIPIcs.MFCS.2017.50},
  abstract = {The well-known k-SUM conjecture is that integer k-SUM requires time Omega(n{\textasciicircum}\{\{k/2\}-o(1)\}). Recent work has studied multidimensional k-SUM in F\_p{\textasciicircum}d, where the best known algorithm takes time {\~O}(n{\textasciicircum}\{\{k/2\}\}). Bhattacharyya et al. [ICS 2011] proved a min(2{\textasciicircum}\{{\textohm}(d)\},n{\textasciicircum}\{{\textohm}(k)\}) lower bound for k-SUM in F\_p{\textasciicircum}d under the Exponential Time Hypothesis. We give a more refined lower bound under the standard k-SUM conjecture: for sufficiently large p, k-SUM in F\_p{\textasciicircum}d requires time Omega(n{\textasciicircum}\{k/2-o(1)\}) if k is even, and Omega(n{\textasciicircum}\{\{k/2\}-2k(log k)/(log p)-o(1)\}) if k is odd. For a special case of the multidimensional problem, bounded monotone d-dimensional 3SUM, Chan and Lewenstein [STOC 2015] gave a surprising {\~O}(n{\textasciicircum}\{2-2/(d+13)\}) algorithm using additive combinatorics. We show this algorithm is essentially optimal. To be more precise, bounded monotone d-dimensional 3SUM requires time Omega(n{\textasciicircum}\{2-\vphantom\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{4}$}\vphantom\{\}\{d\}-o(1)\vphantom\{\}) under the standard 3SUM conjecture, and time Omega(n{\textasciicircum}\{2-\vphantom\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{2}$}\vphantom\{\}\{d\}-o(1)\vphantom\{\}) under the so-called strong 3SUM conjecture. Thus, even though one might hope to further exploit the structural advantage of monotonicity, no substantial improvements beyond those obtained by Chan and Lewenstein are possible for bounded monotone d-dimensional 3SUM.},
  citationcount = {7},
  venue = {International Symposium on Mathematical Foundations of Computer Science},
  keywords = {lower bound}
}

@article{huAlgorithmsForA2020,
  title = {Algorithms for a Topology-Aware Massively Parallel Computation Model},
  author = {Hu, Xiao and Koutris, Paraschos and Blanas, Spyros},
  year = {2020},
  doi = {10.1145/3452021.3458318},
  abstract = {Most of the prior work in massively parallel data processing assumes homogeneity, i.e., every computing unit has the same computational capability and can communicate with every other unit with the same latency and bandwidth. However, this strong assumption of a uniform topology rarely holds in practical settings, where computing units are connected through complex networks. To address this issue, Blanas et al. 2020topology recently proposed a topology-aware massively parallel computation model that integrates the network structure and heterogeneity in the modeling cost. The network is modeled as a directed graph, where each edge is associated with a cost function that depends on the data transferred between the two endpoints. The computation proceeds in synchronous rounds and the cost of each round is measured as the maximum cost over all the edges in the network. In this work, we take the first step into investigating three fundamental data processing tasks in this topology-aware parallel model: set intersection, cartesian product, and sorting. We focus on network topologies that are tree topologies, and present both lower bounds as well as (asymptotically) matching upper bounds. Instead of assuming a worst-case distribution as in previous results, the optimality of our algorithms is with respect to the initial data distribution among the network nodes. Apart from the theoretical optimality of our results, our protocols are simple, use a constant number of rounds, and we believe can be implemented in practical settings as well.},
  citationcount = {5},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {lower bound}
}

@article{huangFastRectangularMatrix1998,
  title = {Fast Rectangular Matrix Multiplication and Applications},
  author = {Huang, X. and Pan, V.},
  year = {1998},
  doi = {10.1006/jcom.1998.0476},
  abstract = {First we study asymptotically fast algorithms for rectangular matrix multiplication. We begin with new algorithms for multiplication of ann{\texttimes}nmatrix by ann{\texttimes}n2matrix in arithmetic timeO(n?),?=3.333953?, which is less by 0.041 than the previous record 3.375477?. Then we present fast multiplication algorithms for matrix pairs of arbitrary dimensions, estimate the asymptotic running time as a function of the dimensions, and optimize the exponents of the complexity estimates. For a large class of input matrix pairs, we improve the known exponents. Finally we show three applications of our results: (a)we decrease from 2.851 to 2.837 the known exponent of the work bounds for fast deterministic (NC) parallel evaluation of the determinant, the characteristic polynomial, and the inverse of ann{\texttimes}nmatrix, as well as for the solution to a nonsingular linear system ofnequations, (b)we asymptotically accelerate the known sequential algorithms for the univariate polynomial composition modxn, yielding the complexity boundO(n1.667) versus the old record ofO(n1.688), and for the univariate polynomial factorization over a finite field, and (c)we improve slightly the known complexity estimates for computing basic solutions to the linear programming problem withmconstraints andnvariables.},
  citationcount = {253},
  venue = {Journal of Complexity}
}

@article{huangFullyDynamicConnectivity2016,
  title = {Fully Dynamic Connectivity in {{O}}(Log n(Log Log n)2) Amortized Expected Time},
  author = {Huang, Shang-En and Huang, Dawei and Kopelowitz, T. and Pettie, Seth},
  year = {2016},
  doi = {10.46298/theoretics.23.6},
  abstract = {Dynamic connectivity is one of the most fundamental problems in dynamic graph algorithms. We present a randomized Las Vegas dynamic connectivity data structure with O(n(n){$^2$}) amortized expected update time and O(n/n) worst case query time, which comes very close to the cell probe lower bounds of Patrascu and Demaine (2006) and Patrascu and Thorup (2011).},
  citationcount = {47},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,data structure,dynamic,lower bound,query,query time,update,update time}
}

@article{huangHighSpeedVariable2023,
  title = {High-Speed Variable Polynomial Toeplitz Hash Algorithm Based on {{FPGA}}},
  author = {Huang, Si-Cheng and Huang, Shan and Yin, Hua-Lei and Ma, Qing-Li and Yin, Ze-Jie},
  year = {2023},
  doi = {10.3390/e25040642},
  abstract = {In the Quantum Key Distribution (QKD) network, authentication protocols play a critical role in safeguarding data interactions among users. To keep pace with the rapid advancement of QKD technology, authentication protocols must be capable of processing data at faster speeds. The Secure Hash Algorithm (SHA), which functions as a cryptographic hash function, is a key technology in digital authentication. Irreducible polynomials can serve as characteristic functions of the Linear Feedback Shift Register (LFSR) to rapidly generate pseudo-random sequences, which in turn form the foundation of the hash algorithm. Currently, the most prevalent approach to hardware implementation involves performing block computations and pipeline data processing of the Toeplitz matrix in the Field-Programmable Gate Array (FPGA) to reach a maximum computing rate of 1 Gbps. However, this approach employs a fixed irreducible polynomial as the characteristic polynomial of the LFSR, which results in computational inefficiency as the highest bit of the polynomial restricts the width of parallel processing. Moreover, an attacker could deduce the irreducible polynomials utilized by an algorithm based on the output results, creating a serious concealed security risk. This paper proposes a method to use FPGA to implement variational irreducible polynomials based on a hashing algorithm. Our method achieves an operational rate of 6.8 Gbps by computing equivalent polynomials and updating the Toeplitz matrix with pipeline operations in real-time, which accelerates the authentication protocol while also significantly enhancing its security. Moreover, the optimization of this algorithm can be extended to quantum randomness extraction, leading to a considerable increase in the generation rate of random numbers.},
  citationcount = {Unknown},
  venue = {Entropy}
}

@article{huangSimplerAnalysesOf2023,
  title = {Simpler Analyses of Union-Find},
  author = {Huang, Zhiyi and Lambert, Chris and Nie, Zipei and Peng, Richard},
  year = {2023},
  doi = {10.48550/arXiv.2308.09021},
  abstract = {We analyze union-find using potential functions motivated by continuous algorithms, and give alternate proofs of the O(\{n\}), O(\textsuperscript{\{\vphantom\}}*\vphantom\{\}n), O(\textsuperscript{\{\vphantom\}}**\vphantom\{\}n), and O({$\alpha$}(n)) amortized cost upper bounds. The proof of the O(\{n\}) amortized bound goes as follows. Let each node's potential be the square root of its size, i.e., the size of the subtree rooted from it. The overall potential increase is O(n) because the node sizes increase geometrically along any tree path. When compressing a path, each node on the path satisfies that either its potential decreases by {\textohm}(1), or its child's size along the path is less than the square root of its size: this can happen at most O(\{n\}) times along any tree path.},
  citationcount = {Unknown},
  venue = {arXiv.org}
}

@article{huangSubsetSamplingAnd2023,
  title = {Subset Sampling and Its Extensions},
  author = {Huang, Jinchao and Wang, Si-Yang},
  year = {2023},
  doi = {10.48550/arXiv.2307.11585},
  abstract = {This paper studies the \{subset sampling\} problem. The input is a set \{S\} of n records together with a function \textbf{\{\vphantom\}}p\vphantom\{\} that assigns each record v{$\in$}\{S\} a probability \textbf{\{\vphantom\}}p\vphantom\{\}(v). A query returns a random subset X of \{S\}, where each record v{$\in$}\{S\} is sampled into X independently with probability \textbf{\{\vphantom\}}p\vphantom\{\}(v). The goal is to store \{S\} in a data structure to answer queries efficiently. If \{S\} fits in memory, the problem is interesting when \{S\} is dynamic. We develop a dynamic data structure with \{O\}(1+{$\mu$}\textsubscript{\{\vphantom\}}\{S\}\vphantom\{\}) expected \{query\} time, \{O\}(n) space and \{O\}(1) amortized expected \{update\}, \{insert\} and \{delete\} time, where {$\mu$}\textsubscript{\{\vphantom\}}\{S\}\vphantom\{\}={$\sum$}\textsubscript{\{\vphantom\}}v{$\in$}\{S\}\vphantom\{\}\textbf{\{\vphantom\}}p\vphantom\{\}(v). The query time and space are optimal. If \{S\} does not fit in memory, the problem is difficult even if \{S\} is static. Under this scenario, we present an I/O-efficient algorithm that answers a \{query\} in \{O\}((\textsuperscript{*}\textsubscript{B}n)/B+({$\mu$}\{S\}/B)\textsubscript{\{\vphantom\}}M/B\vphantom\{\}(n/B)) amortized expected I/Os using \{O\}(n/B) space, where M is the memory size, B is the block size and \textsuperscript{*}\textsubscript{B}n is the number of iterative {$_2$}(.) operations we need to perform on n before going below B. In addition, when each record is associated with a real-valued key, we extend the \{subset sampling\} problem to the \{range subset sampling\} problem, in which we require that the keys of the sampled records fall within a specified input range [a,b]. For this extension, we provide a solution under the dynamic setting, with \{O\}(n+{$\mu$}\textsubscript{\{\vphantom\}}\{S\}{$\cap$}[a,b]\vphantom\{\}) expected \{query\} time, \{O\}(n) space and \{O\}(n) amortized expected \{update\}, \{insert\} and \{delete\} time.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {data structure,dynamic,query,query time,static,update}
}

@article{hubcekStrongerLowerBounds2019,
  title = {Stronger Lower Bounds for Online {{ORAM}}},
  author = {Hub{\'a}cek, Pavel and Kouck{\'y}, M. and Kr{\'a}l, Karel and Sl{\'i}vov{\'a}, Veronika},
  year = {2019},
  doi = {10.1007/978-3-030-36033-7_10},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {Theory of Cryptography Conference},
  keywords = {lower bound}
}

@article{hubrechtsPointCountingIn2006,
  title = {Point Counting in Families of Hyperelliptic Curves in Characteristic 2},
  author = {Hubrechts, Hendrik},
  year = {2006},
  doi = {10.1112/S1461157000001376},
  abstract = {Let E\_G be a family of hyperelliptic curves over F2{\textasciicircum}(alg cl) with general Weierstrass equation given over a very small field F. We describe in this paper an algorithm to compute the zeta function of E\_g for g in a degree n extension field of F, which has as time complexity O(n{\textasciicircum}3) and memory requirements O(n{\textasciicircum}2). With a slightly different algorithm we can get time O(n{\textasciicircum}2,667) and memory O(n{\textasciicircum}2,5), and the computation of O(n) curves of the family can be done in time and space O(n{\textasciicircum}3). All these algorithms are polynomial in the genus.},
  citationcount = {25},
  venue = {LMS J. Comput. Math.}
}

@article{hubrechtsPointCountingIn2006,
  title = {Point Counting in Families of Hyperelliptic Curves},
  author = {Hubrechts, Hendrik},
  year = {2006},
  doi = {10.1007/s10208-007-9000-2},
  abstract = {No abstract available},
  citationcount = {33},
  venue = {Foundations of Computational Mathematics}
}

@article{huBuildingAnOptimal2018,
  title = {Building an Optimal Point-Location Structure in {{O}}(Sort(n)) {{I}}/Os},
  author = {Hu, Xiaocheng and Sheng, Cheng and Tao, Yufei},
  year = {2018},
  doi = {10.1007/s00453-018-0518-2},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Algorithmica}
}

@article{huComputingComplexTemporal2022,
  title = {Computing Complex Temporal Join Queries Efficiently},
  author = {Hu, Xiao and Sintos, Stavros and Gao, Junyang and Agarwal, P. and Yang, Jun},
  year = {2022},
  doi = {10.1145/3514221.3517893},
  abstract = {This paper studies multi-way join queries over temporal data, where each tuple is associated with a valid time interval indicating when the tuple is valid. A temporal join requires that joining tuples' valid intervals intersect. Previous work on temporal joins has focused on joining two relations, but pairwise processing is often inefficient because it may generate unnecessarily large intermediate results. This paper investigates how to efficiently process complex temporal joins involving multiple relations. We also consider a useful extension, durable temporal joins, which further selects results with long enough valid intervals so they are not merely transient patterns. We classify temporal join queries into different classes based on their computational complexity. We identify the class of r-hierarchical joins and show that a linear-time algorithm exists for a temporal join if and only it is r-hierarchical (assuming the 3SUM conjecture holds). We further propose output-sensitive algorithms for non-r-hierarchical joins. We implement our algorithms and evaluate them on both synthetic and real datasets.},
  citationcount = {6},
  venue = {SIGMOD Conference},
  keywords = {query}
}

@article{huComputingTheDifference2023,
  title = {Computing the Difference of Conjunctive Queries Efficiently},
  author = {Hu, Xiao and Wang, Qichen},
  year = {2023},
  doi = {10.1145/3589298},
  abstract = {We investigate how to efficiently compute the difference result of two (or multiple) conjunctive queries, which is the last operator in relational algebra to be unraveled. The standard approach in practical database systems is to materialize the results for every input query as a separate set, and then compute the difference of two (or multiple) sets. This approach is bottlenecked by the complexity of evaluating every input query individually, which could be very expensive, particularly when there are only a few results in the difference. In this paper, we introduce a new approach by exploiting the structural property of input queries and rewriting the original query by pushing the difference operator down as much as possible. We show that for a large class of difference queries, this approach can lead to a linear-time algorithm, in terms of the input size and (final) output size, i.e., the number of query results that survive from the difference operator. We complete this result by showing the hardness of computing the remaining difference queries in linear time. Although a linear-time algorithm is hard to achieve in general, we also provide some heuristics that can provably improve the standard approach. At last, we compare our approach with standard SQL engines over graph and benchmark datasets. The experiment results demonstrate order-of-magnitude speedups achieved by our approach over the vanilla SQL engine.},
  citationcount = {7},
  venue = {Proc. ACM Manag. Data},
  keywords = {query}
}

@article{huConnectivityOraclesFor2023,
  title = {Connectivity Oracles for Predictable Vertex Failures},
  author = {Hu, Bingbing and Kosinas, E. and Polak, Adam},
  year = {2023},
  doi = {10.48550/arXiv.2312.08489},
  abstract = {The problem of designing connectivity oracles supporting vertex failures is one of the basic data structures problems for undirected graphs. It is already well understood: previous works [Duan--Pettie STOC'10; Long--Saranurak FOCS'22] achieve query time linear in the number of failed vertices, and it is conditionally optimal as long as we require preprocessing time polynomial in the size of the graph and update time polynomial in the number of failed vertices. We revisit this problem in the paradigm of algorithms with predictions: we ask if the query time can be improved if the set of failed vertices can be predicted beforehand up to a small number of errors. More specifically, we design a data structure that, given a graph G=(V,E) and a set of vertices predicted to fail D\vphantom\{\}{$\subseteq$}V of size d={\textbar}D\vphantom\{\}{\textbar}, preprocesses it in time O\vphantom\{\}(d{\textbar}E{\textbar}) and then can receive an update given as the symmetric difference between the predicted and the actual set of failed vertices D\vphantom\{\}D=(D\vphantom\{\}{$\setminus$}D){$\cup$}(D{$\setminus$}D\vphantom\{\}) of size {$\eta$}={\textbar}D\vphantom\{\}D{\textbar}, process it in time O\vphantom\{\}({$\eta^4$}), and after that answer connectivity queries in G{$\setminus$}D in time O({$\eta$}). Viewed from another perspective, our data structure provides an improvement over the state of the art for the \{fully dynamic subgraph connectivity problem\} in the \{sensitivity setting\} [Henzinger--Neumann ESA'16]. We argue that the preprocessing time and query time of our data structure are conditionally optimal under standard fine-grained complexity assumptions.},
  citationcount = {2},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,dynamic,query,query time,update,update time}
}

@article{huFastMatrixMultiplication2024,
  title = {Fast Matrix Multiplication for Query Processing},
  author = {Hu, Xiao},
  year = {2024},
  doi = {10.1145/3651599},
  abstract = {This paper studies how to use fast matrix multiplication to speed up query processing. As observed, computing a two-table join and then projecting away the join attribute is essentially the Boolean matrix multiplication problem, which can be significantly improved with fast matrix multiplication. Moving beyond this basic two-table query, we introduce output-sensitive algorithms for general join-project queries using fast matrix multiplication. These algorithms have achieved a polynomially large improvement over the classic Yannakakis framework. To the best of our knowledge, this is the first theoretical improvement for general acyclic join-project queries since 1981.},
  citationcount = {2},
  venue = {Proc. ACM Manag. Data},
  keywords = {query}
}

@article{huInstanceAndOutput2019,
  title = {Instance and Output Optimal Parallel Algorithms for Acyclic Joins},
  author = {Hu, Xiao and Yi, K.},
  year = {2019},
  doi = {10.1145/3294052.3319698},
  abstract = {Massively parallel join algorithms have received much attention in recent years, while most prior work has focused on worst-optimal algorithms. However, the worst-case optimality of these join algorithms relies on hard instances having very large output sizes, which rarely appear in practice. A stronger notion of optimality is \emph{output-optimal, which requires an algorithm to be optimal within the class of all instances sharing the same input and output size. An even stronger optimality is instance-optimal, i.e., the algorithm is optimal on every single instance, but this may not always be achievable. In the traditional RAM model of computation, the classical Yannakakis algorithm is instance-optimal on any acyclic join. But in the massively parallel computation (MPC) model, the situation becomes much more complicated. We first show that for the class of r-hierarchical joins, instance-optimality can still be achieved in the MPC model. Then, we give a new MPC algorithm for an arbitrary acyclic join with load O({\o}verp+{\textsurd}{$\cdot$}{\O}UT{\o}verp), where ,{\O}UT are the input and output sizes of the join, and p is the number of servers in the MPC model. This improves the MPC version of the Yannakakis algorithm by an O({\textsurd}{\O}UT{\o}ver) factor. Furthermore, we show that this is output-optimal when {\O}UT=O(p{$\cdot$}), for every acyclic but non-r-hierarchical join. Finally, we give the first output-sensitive lower bound for the triangle join in the MPC model, showing that it is inherently more difficult than acyclic joins.}},
  citationcount = {19},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {lower bound}
}

@article{huNonBooleanOmv2024,
  title = {Non-Boolean Omv: {{One}} More Reason to Believe Lower Bounds for Dynamic Problems},
  author = {Hu, Bingbing and Polak, Adam},
  year = {2024},
  doi = {10.48550/arXiv.2409.15970},
  abstract = {Most of the known tight lower bounds for dynamic problems are based on the Online Boolean Matrix-Vector Multiplication (OMv) Hypothesis, which is not as well studied and understood as some more popular hypotheses in fine-grained complexity. It would be desirable to base hardness of dynamic problems on a more believable hypothesis. We propose analogues of the OMv Hypothesis for variants of matrix multiplication that are known to be harder than Boolean product in the offline setting, namely: equality, dominance, min-witness, min-max, and bounded monotone min-plus products. These hypotheses are a priori weaker assumptions than the standard (Boolean) OMv Hypothesis. Somewhat surprisingly, we show that they are actually equivalent to it. This establishes the first such fine-grained equivalence class for dynamic problems.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {dynamic,lower bound}
}

@article{huntAFastAlgorithm1977,
  title = {A Fast Algorithm for Computing Longest Common Subsequences},
  author = {Hunt, J. W. and Szymanski, T. G.},
  year = {1977},
  doi = {10.1145/359581.359603},
  abstract = {Previously published algorithms for finding the longest common subsequence of two sequences of length n have had a best-case running time of O(n2). An algorithm for this problem is presented which has a running time of O((r + n) log n), where r is the total number of ordered pairs of positions at which the two sequences match. Thus in the worst case the algorithm has a running time of O(n2 log n). However, for those applications where most positions of one sequence match relatively few positions in the other sequence, a running time of O(n log n) can be expected.},
  citationcount = {748},
  venue = {CACM}
}

@article{huOnTheI2015,
  title = {On the {{I}}/{{O}} Complexity of Dynamic Distinct Counting},
  author = {Hu, Xiaocheng and Tao, Yufei and Yang, Yi and Zhang, Shengyu and Zhou, Shuigeng},
  year = {2015},
  doi = {10.4230/LIPIcs.ICDT.2015.265},
  abstract = {In dynamic distinct counting, we want to maintain a multi-setS of integers under insertions to answer eciently the query: how many distinct elements are there in S? In external memory, the problem admits two standard solutions. The first one maintainsS in a hash structure, so that the distinct count can be incrementally updated after each insertion using O(1) expected I/Os. A query is answered for free. The second one storesS in a linked list, and thus supports an insertion in O(1/B) amortized I/Os. A query can be answered in O( N logM/B N ) I/Os by sorting, where N ={\textbar}S{\textbar}, B is the block size, and M is the memory size. In this paper, we show that the above two naive solutions are already optimal within a polylog factor. Specifically, for any Las Vegas structure using N O(1) blocks, if its expected amortized insertion cost is o( 1 logB ), then it must incur ( N B logB ) expected I/Os answering a query in the worst case, under the (realistic) condition that N is a polynomial of B. This means that the problem is repugnant to update buering: the query cost jumps from 0 dramatically to almost},
  citationcount = {Unknown},
  venue = {International Conference on Database Theory},
  keywords = {dynamic,query,update}
}

@article{huOutputOptimalAlgorithms2024,
  title = {Output-Optimal Algorithms for Join-Aggregate Queries},
  author = {Hu, Xiao},
  year = {2024},
  doi = {10.48550/arXiv.2406.05536},
  abstract = {One of the most celebrated results of computing join-aggregate queries defined over commutative semi-rings is the classic Yannakakis algorithm proposed in 1981. It is known that the runtime of the Yannakakis algorithm is O(N+) for any free-connex query, where N is the input size of the database and  is the output size of the query result. This is already output-optimal. However, only an upper bound O(N{$\cdot$}) on the runtime is known for the large remaining class of acyclic but non-free-connex queries. Alternatively, one can convert a non-free-connex query into a free-connex one using tree decomposition techniques and then run the Yannakakis algorithm. This approach takes O(N\textsuperscript{\{\vphantom\}}\#\vphantom\{\}+) time, where \# is the \{\vphantom\}\emph{free-connex sub-modular width}\vphantom\{\}\emph{ of the input query. But, none of these results is known to be output-optimal. In this paper, we show a matching lower and upper bound {$\Theta$}(N{$\cdot$}\textsuperscript{\{\vphantom\}}1-\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{\}\vphantom\{\}+) for computing general acyclic join-aggregate queries by }\{\vphantom\}\emph{semiring algorithms, where  is the free-connex fractional hypertree width}\vphantom\{\}\emph{ of the query. For example, =1 for free-connex queries, =2 for line queries (a.k.a. chain matrix multiplication), and =k for star queries (a.k.a. star matrix multiplication) with k relations. While this measure has been defined before, we are the first to use it to characterize the output-optimal complexity of acyclic join-aggregate queries. To our knowledge, this has been the first polynomial improvement over the Yannakakis algorithm in the last 40 years and completely resolves the open question of an output-optimal algorithm for computing acyclic join-aggregate queries.}},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {query}
}

@article{huOutputOptimalMassively2019,
  title = {Output-Optimal Massively Parallel Algorithms for Similarity Joins},
  author = {Hu, Xiao and Yi, K. and Tao, Yufei},
  year = {2019},
  doi = {10.1145/3311967},
  abstract = {Parallel join algorithms have received much attention in recent years due to the rapid development of massively parallel systems such as MapReduce and Spark. In the database theory community, most efforts have been focused on studying worst-case optimal algorithms. However, the worst-case optimality of these join algorithms relies on the hard instances having very large output sizes. In the case of a two-relation join, the hard instance is just a Cartesian product, with an output size that is quadratic in the input size. In practice, however, the output size is usually much smaller. One recent parallel join algorithm by Beame et al. has achieved output-optimality (i.e., its cost is optimal in terms of both the input size and the output size), but their algorithm only works for a 2-relation equi-join and has some imperfections. In this article, we first improve their algorithm to true optimality. Then we design output-optimal algorithms for a large class of similarity joins. Finally, we present a lower bound, which essentially eliminates the possibility of having output-optimal algorithms for any join on more than two relations.},
  citationcount = {23},
  venue = {ACM Transactions on Database Systems},
  keywords = {lower bound}
}

@article{huOutputOptimalParallel2017,
  title = {Output-Optimal Parallel Algorithms for Similarity Joins},
  author = {Hu, Xiao and Tao, Yufei and Yi, K.},
  year = {2017},
  doi = {10.1145/3034786.3056110},
  abstract = {Parallel join algorithms have received much attention in recent years, due to the rapid development of massively parallel systems such as MapReduce and Spark. In the database theory community, most efforts have been focused on studying worst-optimal algorithms. However, the worst-case optimality of these join algorithms relies on the hard instances having very large output sizes. In the case of a two-relation join, the hard instance is just a Cartesian product, with an output size that is quadratic in the input size. In practice, however, the output size is usually much smaller. One recent parallel join algorithm by Beame et al.[8] has achieved output-optimality, i.e., its cost is optimal in terms of both the input size and the output size, but their algorithm only works for a 2-relation equi-join, and has some imperfections. In this paper, we first improve their algorithm to true optimality. Then we design output-optimal algorithms for a large class of similarity joins. Finally, we present a lower bound, which essentially eliminates the possibility of having output-optimal algorithms for any join on more than two relations.},
  citationcount = {27},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {lower bound}
}

@article{huSemiGroupRange2017,
  title = {Semi-Group Range Sum Revisited: {{Query-space}} Lower Bound Tightened},
  author = {Hu, Xiaocheng and Tao, Yufei and Yang, Yi and Zhou, Shuigeng},
  year = {2017},
  doi = {10.1007/s00453-017-0307-3},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Algorithmica},
  keywords = {lower bound,query}
}

@article{husfeldtFullyDynamicTransitive1994,
  title = {Fully Dynamic Transitive Closure in Plane Dags with One Source and One Sink},
  author = {Husfeldt, T.},
  year = {1994},
  doi = {10.1007/3-540-60313-1_144},
  abstract = {No abstract available},
  citationcount = {19},
  venue = {Embedded Systems and Applications},
  keywords = {dynamic}
}

@article{husfeldtHardnessResultsDynamic1998,
  title = {Hardness Results for Dynamic Problems by Extensions of Fredman and Saks' Chronogram Method},
  author = {Husfeldt, T. and Rauhe, Theis},
  year = {1998},
  doi = {10.1007/BFb0055041},
  abstract = {No abstract available},
  citationcount = {28},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {chronogram,dynamic}
}

@inproceedings{husfeldtLowerBoundsDynamic1996,
  title = {Lower Bounds for Dynamic Transitive Closure, Planar Point Location, and Parentheses Matching},
  booktitle = {Algorithm {{Theory}} --- {{SWAT}}'96},
  author = {Husfeldt, Thore and Rauhe, Theis and Skyum, S{\o}ren},
  editor = {Karlsson, Rolf and Lingas, Andrzej},
  year = {1996},
  pages = {198--211},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-61422-2_132},
  abstract = {We give a number of new lower bounds in the cell probe model with logarithmic cell size, which entails the same bounds on the random access computer with logarithmic word size and unit cost operations.},
  isbn = {978-3-540-68529-6},
  langid = {english},
  keywords = {cell probe,dynamic,lower bound,Membership Problem,Polylogarithmic Time,Query Operation,Query Time,Random Access Machine},
  file = {/Users/tulasi/Zotero/storage/YIGHY9Q4/Husfeldt et al. - 1996 - Lower bounds for dynamic transitive closure, planar point location, and parentheses matching.pdf}
}

@article{husfeldtNewLowerBound2003a,
  title = {New Lower Bound Techniques for Dynamic Partial Sums and Related Problems},
  author = {Husfeldt, Thore and Rauhe, Theis},
  year = {2003},
  journal = {SIAM Journal on Computing},
  volume = {32},
  number = {3},
  pages = {736--753},
  doi = {10.1137/S0097539701391592},
  keywords = {dynamic,lower bound},
  file = {/Users/tulasi/Zotero/storage/TDNG2XKX/Husfeldt and Rauhe - 2003 - New Lower Bound Techniques for Dynamic Partial Sums and Related Problems.pdf}
}

@article{httelRecursivePingPong2003,
  title = {Recursive Ping-Pong Protocols},
  author = {H{\"u}ttel, Hans and Srba, Jiri},
  year = {2003},
  doi = {10.7146/BRICS.V10I47.21819},
  abstract = {This paper introduces a process calculus with recursion which allows us to express an unbounded number of runs of the ping-pong protocols introduced by Dolev and Yao. We study the decidability issues associated with two common approaches to checking security properties, namely reachability analysis and bisimulation checking. Our main result is that our channel-free and memory-less calculus is Turing powerful, assuming that at least three principals are involved. We also investigate the expressive power of the calculus in the case of two participants. Here, our main results are that reachability and, under certain conditions, also strong bisimilarity become decidable.},
  citationcount = {15},
  venue = {No venue available}
}

@article{huynhOnTheVirtue2012,
  title = {On the Virtue of Succinct Proofs: Amplifying Communication Complexity Hardness to Time-Space Trade-Offs in Proof Complexity},
  author = {Huynh, Trinh N. D. and Nordstr{\"o}m, Jakob},
  year = {2012},
  doi = {10.1145/2213977.2214000},
  abstract = {An active line of research in proof complexity over the last decade has been the study of proof space and trade-offs between size and space. Such questions were originally motivated by practical SAT solving, but have also led to the development of new theoretical concepts in proof complexity of intrinsic interest and to results establishing nontrivial relations between space and other proof complexity measures. By now, the resolution proof system is fairly well understood in this regard, as witnessed by a sequence of papers leading up to [Ben-Sasson and Nordstrom 2008, 2011] and [Beame, Beck, and Impagliazzo 2012]. However, for other relevant proof systems in the context of SAT solving, such as polynomial calculus (PC) and cutting planes (CP), very little has been known. Inspired by [BN08, BN11], we consider CNF encodings of so-called pebble games played on graphs and the approach of making such pebbling formulas harder by simple syntactic modifications. We use this paradigm of hardness amplification to make progress on the relatively longstanding open question of proving time-space trade-offs for PC and CP. Namely, we exhibit a family of modified pebbling formulas \{F\_n\} such that: - The formulas F\_n have size O(n) and width O(1). - They have proofs in length O(n) in resolution, which generalize to both PC and CP. - Any refutation in CP or PCR (a generalization of PC) in length L and space s must satisfy s log L {\textquestiondown}{$\approx$} {\textsurd}[4]\{n\}. A crucial technical ingredient in these results is a new two-player communication complexity lower bound for composed search problems in terms of block sensitivity, a contribution that we believe to be of independent interest.},
  citationcount = {68},
  venue = {Symposium on the Theory of Computing}
}

@article{i.hoogFullyAdaptiveDynamicConnectivity2024,
  title = {Fully-{{Adaptive Dynamic Connectivity}} of {{Square Intersection Graphs}}},
  author = {I. Hoog and Andr{\'e} Nusser and Eva Rotenberg and F. Staals},
  year = {2024},
  journal = {International Symposium on Mathematical Foundations of Computer Science},
  doi = {10.48550/arXiv.2406.20065},
  abstract = {A classical problem in computational geometry and graph algorithms is: given a dynamic set S of geometric shapes in the plane, efficiently maintain the connectivity of the intersection graph of S. Previous papers studied the setting where, before the updates, the data structure receives some parameter P. Then, updates could insert and delete disks as long as at all times the disks have a diameter that lies in a fixed range [1/P, 1]. The state-of-the-art for storing disks in a dynamic connectivity data structure is a data structure that uses O(Pn) space and that has amortized O(P log{\textasciicircum}4 n) expected amortized update time. Connectivity queries between disks are supported in O( log n / loglog n) time. The state-of-the-art for Euclidean disks immediately implies a data structure for connectivity between axis-aligned squares that have their diameter in the fixed range [1/P, 1], with an improved update time of O(P log{\textasciicircum}4 n) amortized time. We restrict our attention to axis-aligned squares, and study fully-dynamic square intersection graph connectivity. Our result is fully-adaptive to the aspect ratio, spending time proportional to the current aspect ratio \{{\textbackslash}psi\}, as opposed to some previously given maximum P. Our focus on squares allows us to simplify and streamline the connectivity pipeline from previous work. When \$n\$ is the number of squares and \{{\textbackslash}psi\} is the aspect ratio after insertion (or before deletion), our data structure answers connectivity queries in O(log n / loglog n) time. We can update connectivity information in O(\{{\textbackslash}psi\} log{\textasciicircum}4 n + log{\textasciicircum}6 n) amortized time. We also improve space usage from O(P n log n) to O(n log{\textasciicircum}3 n log \{{\textbackslash}psi\}) -- while generalizing to a fully-adaptive aspect ratio -- which yields a space usage that is near-linear in n for any polynomially bounded \{{\textbackslash}psi\}.},
  keywords = {adaptive,data structure,dynamic,information,query,update,update time},
  annotation = {Citation Count: 0}
}

@article{iaconoMergeableDictionaries2010,
  title = {Mergeable Dictionaries},
  author = {Iacono, J. and {\"O}zkan, {\"O}zg{\"u}r},
  year = {2010},
  doi = {10.1007/978-3-642-14165-2_15},
  abstract = {No abstract available},
  citationcount = {10},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@incollection{iaconoUsingHashingSolve2012,
  title = {Using Hashing to Solve the Dictionary Problem (in External Memory)},
  booktitle = {Proceedings of the 2012 {{Annual Acm-Siam Symposium}} on {{Discrete Algorithms}} (Soda)},
  author = {Iacono, John and P{\u a}tra{\c s}cu, Mihai},
  year = {2012},
  month = jan,
  series = {Proceedings},
  pages = {570--582},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611973099.48},
  url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611973099.48},
  urldate = {2024-09-20},
  abstract = {We consider the dictionary problem in external memory and improve the update time of the well-known buffer tree by roughly a logarithmic factor. For any {$\lambda$} {$>$} max(lg lg n, logM/B (n/B)\vphantom\{\}, we can support updates in time O({$\lambda$}/B) and queries in sublogarithmic time, O(log{$\lambda$} n). We also present a lower bound in the cell-probe model showing that our data structure is optimal.In the RAM, hash tables have been use to solve the dictionary problem faster than binary search for more than half a century. By contrast, our data structure is the first to beat the comparison barrier in external memory. Ours is also the first data structure to depart convincingly from the indivisibility paradigm.},
  isbn = {978-1-61197-210-8},
  keywords = {cell probe,data structure,lower bound,query,update,update time},
  file = {/Users/tulasi/Zotero/storage/F2SCAR98/Iacono and Ptracu - 2012 - Using hashing to solve the dictionary problem (in external memory).pdf}
}

@article{idrisEfficientQueryProcessing2019,
  title = {Efficient Query Processing for Dynamically Changing Datasets},
  author = {Idris, Muhammad and Ugarte, M. and Vansummeren, Stijn and Voigt, H. and Lehner, Wolfgang},
  year = {2019},
  doi = {10.1145/3371316.3371325},
  abstract = {The ability to efficiently analyze changing data is a key requirement of many real-time analytics applications. Traditional approaches to this problem were developed around the notion of Incremental View Maintenance (IVM), and are based either on the materialization of subresults (to avoid their recomputation) or on the recomputation of subresults (to avoid the space overhead of materialization). Both techniques are suboptimal: instead of materializing results and subresults, one may also maintain a data structure that supports efficient maintenance under updates and from which the full query result can quickly be enumerated. In two previous articles, we have presented algorithms for dynamically evaluating queries that are easy to implement, efficient, and can be naturally extended to evaluate queries from a wide range of application domains. In this paper, we discuss our algorithm and its complexity, explaining the main components behind its efficiency. Finally, we show experiments that compare our algorithm to a state-of-the-art (Higher-order) IVM engine, as well as to a prominent complex event recognition engine. Our approach outperforms the competitor systems by up to two orders of magnitude in processing time, and one order in memory consumption.},
  citationcount = {12},
  venue = {SGMD},
  keywords = {data structure,dynamic,query,update}
}

@article{idrisGeneralDynamicYannakakis2019,
  title = {General Dynamic {{Yannakakis}}: Conjunctive Queries with Theta Joins under Updates},
  author = {Idris, Muhammad and Ugarte, M. and Vansummeren, Stijn and Voigt, H. and Lehner, Wolfgang},
  year = {2019},
  doi = {10.1007/s00778-019-00590-9},
  abstract = {No abstract available},
  citationcount = {28},
  venue = {The VLDB journal},
  keywords = {dynamic,query,update}
}

@article{idrisTheDynamicYannakakis2017,
  title = {The Dynamic Yannakakis Algorithm: {{Compact}} and Efficient Query Processing under Updates},
  author = {Idris, Muhammad and Ugarte, M. and Vansummeren, Stijn},
  year = {2017},
  doi = {10.1145/3035918.3064027},
  abstract = {Modern computing tasks such as real-time analytics require refresh of query results under high update rates. Incremental View Maintenance (IVM) approaches this problem by materializing results in order to avoid recomputation. IVM naturally induces a trade-off between the space needed to maintain the materialized results and the time used to process updates. In this paper, we show that the full materialization of results is a barrier for more general optimization strategies. In particular, we present a new approach for evaluating queries under updates. Instead of the materialization of results, we require a data structure that allows: (1) linear time maintenance under updates, (2) constant-delay enumeration of the output, (3) constant-time lookups in the output, while (4) using only linear space in the size of the database. We call such a structure a Dynamic Constant-delay Linear Representation (DCLR) for the query. We show that DYN, a dynamic version of the Yannakakis algorithm, yields DCLRs for the class of free-connex acyclic CQs. We show that this is optimal in the sense that no DCLR can exist for CQs that are not free-connex acyclic. Moreover, we identify a sub-class of queries for which DYN features constant-time update per tuple and show that this class is maximal. Finally, using the TPC-H and TPC-DS benchmarks, we experimentally compare DYN and a higher-order IVM (HIVM) engine. Our approach is not only more efficient in terms of memory consumption (as expected), but is also consistently faster in processing updates.},
  citationcount = {91},
  venue = {SIGMOD Conference},
  keywords = {data structure,dynamic,query,update}
}

@article{impagliazzoWhichProblemsHave1998,
  title = {Which Problems Have Strongly Exponential Complexity?},
  author = {Impagliazzo, R. and Paturi, R. and Zane, F.},
  year = {1998},
  doi = {10.1109/SFCS.1998.743516},
  abstract = {For several NP-complete problems, there have been a progression of better but still exponential algorithms. In this paper we address the relative likelihood of sub-exponential algorithms for these problems. We introduce a generalized reduction which we call sub-exponential reduction family (SERF) that preserves sub-exponential complexity. We show that Circuit-SAT is SERF-complete for all NP-search problems, and that for any fixed k, k-SAT, k-Colorability, k-Set Cover Independent Set, Clique, Vertex Cover are SERF-complete for the class SNP of search problems expressible by second order existential formulas whose first order part is universal. In particular, sub-exponential complexity for any one of the above problems implies the same for all others. We also look at the issue of proving strongly exponential lower bounds (that is, bounds of the form 2/sup /spl Omega/(n)/) for AC/sup 0/. This problem is even open far depth-3 circuits. In fact, such a bound for depth-3 circuits with even limited (at most n/sup /spl epsiv//) fan-infer bottom-level gates would imply a nonlinear size lower bound for logarithmic depth circuits. We show that with high probability even degree 2 random GF(2) polynomials require strongly exponential site for /spl Sigma//sub 3//sup k/ circuits for k=o(loglogn). We thus exhibit a much smaller space of 2(0(/sup n2/)) functions such that almost every function in this class requires strongly exponential size /spl Sigma//sub 3//sup k/ circuits. As a corollary, we derive a pseudorandom generator (requiring O(n/sup 2/) bits of advice) that maps n bits into a larger number of bits so that computing parity on the range is hard for /spl Sigma//sub 3//sup k/ circuits. Our main technical lemma is an algorithm that, for any fixed /spl epsiv/{\textquestiondown}0, represents an arbitrary k-CNF formula as a disjunction of 2/sup /spl epsiv/n/ k-CNF formulas that are sparse, e.g., each having O(n) clauses.},
  citationcount = {1525},
  venue = {Proceedings 39th Annual Symposium on Foundations of Computer Science (Cat. No.98CB36280)}
}

@article{impagliazzoWhichProblemsHave2001,
  title = {Which Problems Have Strongly Exponential Complexity?},
  author = {Impagliazzo, Russell and Paturi, Ramamohan and Zane, Francis},
  year = {2001},
  month = dec,
  journal = {Journal of Computer and System Sciences},
  volume = {63},
  number = {4},
  pages = {512--530},
  issn = {0022-0000},
  doi = {10.1006/jcss.2001.1774},
  url = {https://www.sciencedirect.com/science/article/pii/S002200000191774X},
  urldate = {2024-09-20},
  abstract = {For several NP-complete problems, there have been a progression of better but still exponential algorithms. In this paper, we address the relative likelihood of sub-exponential algorithms for these problems. We introduce a generalized reduction that we call Sub-exponential Reduction Family (SERF) that preserves sub-exponential complexity. We show that Circuit-SAT is SERF-complete for all NP-search problems, and that for any fixed k3, k-SAT, k-Colorability, k-Set Cover, Independent Set, Clique, and Vertex Cover, are SERF-complete for the class SNP of search problems expressible by second-order existential formulas whose first-order part is universal. In particular, sub-exponential complexity for any one of the above problems implies the same for all others. We also look at the issue of proving strongly exponential lower bounds for AC0, that is, bounds of the form 2{\textohm}(n). This problem is even open for depth-3 circuits. In fact, such a bound for depth-3 circuits with even limited (at most n{$\varepsilon$}) fan-in for bottom-level gates would imply a nonlinear size lower bound for logarithmic depth circuits. We show that with high probability even random degree 2 GF(2) polynomials require strongly exponential size for {$\Sigma$}k3 circuits for k=o(loglogn). We thus exhibit a much smaller space of 2O(n2) functions such that almost every function in this class requires strongly exponential size {$\Sigma$}k3 circuits. As a corollary, we derive a pseudorandom generator (requiring O(n2) bits of advice) that maps n bits into a larger number of bits so that computing parity on the range is hard for {$\Sigma$}k3 circuits. Our main technical lemma is an algorithm that, for any fixed {$\varepsilon>$}0, represents an arbitrary k-CNF formula as a disjunction of 2{$\varepsilon$}nk-CNF formulas that are sparse, that is, each disjunct has O(n) clauses.},
  keywords = {lower bound,reduction},
  annotation = {T. Feder, and, R. Motwani, Worst-case time bounds for coloring and satisfiability problems, manuscript, September 1998.\\
\\
\\
\\
\\
\\
\\
\\
O. Kullmann, and, H. Luckhard, Deciding propositional tautologies: Algorithms and their complexity, submitted.\\
\\
R. Paturi, P. Pudl{\'a}k, and F. Zane, Satisfiability coding lemma, in Proceedings of the 38th Annual IEEE Symposium on Foundations of Computer Science, October 1997, pp. 567--574.\\
R. Paturi, P. Pudl{\'a}k, M. E. Saks, and F. Zane, An improved exponential-time algorithm for k-SAT, in 1998 Annual IEEE Symposium on Foundations of Computer Science, pp. 628--637.},
  file = {/Users/tulasi/Zotero/storage/PK3VB47L/Impagliazzo et al. - 2001 - Which problems have strongly exponential complexity.pdf}
}

@article{indykApproximateNearestNeighbor2002,
  title = {Approximate Nearest Neighbor Algorithms for {{Frechet}} Distance via Product Metrics},
  author = {Indyk, P.},
  year = {2002},
  doi = {10.1145/513400.513414},
  abstract = {The Nearest Neighbor Search (NNS) problem is: Given a set P of n points in a metric space X, preprocess P so as to e{\AE}ciently answer queries for nding the point in P closest to a query point q. NNS and its approximate versions are among the most extensively studied problems in the elds of Computational Geometry and Algorithms, resulting in discovery of many e{\AE}cient algorithms. In particular, for the case when the metric spaceX is a low-dimensional Euclidean space l 2 , it is known how to construct data structure for exact [5, 15] or approximate [4, 13, 12, 9] NNS with query time (d+log n). Unfortunately, those data structures require space exponential in d. More recently, several data structures using space (quasi)-polynomial in n and d, and query time sublinear in n, have been discovered for approximate NNS under l1 and l2 [14, 12, 11] and l1 [10] norms. While many metrics of interest for nearest neighbor search are norms, quite a few of them are not. A prominent example of the latter is the Frechet metric. Unlike the norms, Frechet metric is de ned for sequences of points (possibly in nite and of di erent length), not vectors. More speci cally, the Frechet distance between two sequences of points is de ned as a minimum, over all alignments between the two sequences, of the maximum distance between two corresponding points (see Preliminaries for formal de nition). Frechet metric is a natural measure of similarity between sequences of points (e.g., handwritten signatures). It also has been studied in the Computational Geometry literature, e.g., in [2] (see also [3]). Unfortunately, the optimum alignment between any two sequences is not xed (i.e., it could be},
  citationcount = {86},
  venue = {SCG '02}
}

@article{indykApproximateNearestNeighbors2001,
  title = {On Approximate Nearest Neighbors under {{L}}\_infty Norm},
  author = {Indyk, Piotr},
  year = {2001},
  journal = {Journal of Computer and System Sciences},
  volume = {63},
  number = {4},
  pages = {627--638},
  doi = {10.1006/jcss.2001.1781},
  annotation = {A. Ambainis, manuscript, 1998.\\
P. K. Agarwal, A. Gionis, L. Guibas, and, R. Motwani, Rank-based similarity searching, in preparation, 1998.\\
\\
\\
\\
A. Borodin, R. Ostrovsky, and, Y. Rabani, Lower bounds for high dimensional nearest neighbor search and related problems, in, STOC'99.\\
\\
\\
\\
\\
\\
\\
\\
\\
M. Farach-Colton, and, P. Indyk, Approximate nearest neighbor algorithms for Hausdorff metric via embeddings, manuscript, 1998.\\
\\
A. Gionis, P. Indyk, and, R. Motwani, Similarity search in high dimensions via hashing, manuscript, 1997.\\
P. Indyk, Deterministic superimposed coding with applications to pattern matching, in, Proceedings of the 38th Symposium on Foundations of Computer Science, 1997.\\
\\
\\
\\
\\
\\
\\
J. Matou{\v s}ek, On embedding expanders into lp spaces, Israel J. Math.},
  file = {/Users/tulasi/Zotero/storage/2CANZ3RB/Indyk - 2001 - On Approximate Nearest Neighbors under l Norm.pdf}
}

@article{indykDeterministicSuperimposedCoding1997,
  title = {Deterministic Superimposed Coding with Applications to Pattern Matching},
  author = {Indyk, P.},
  year = {1997},
  doi = {10.1109/SFCS.1997.646101},
  abstract = {A superimposed code is a set of binary vectors having the property that no vector is contained in a boolean sum (i.e. bitwise OR) of a small number of others. Such codes are used in information retrieval for constructing so-called signature files; they also have applications in other areas. In this paper we introduce a new notion of data-dependent superimposed codes and give a deterministic algorithm for constructing short such codes. We then show that these codes can be used to achieve an almost optimal de-randomization of several pattern matching algorithms, including the almost-linear algorithm for tree pattern matching developed recently. Thus, we give the first almost-linear time deterministic algorithms for these problems.},
  citationcount = {41},
  venue = {Proceedings 38th Annual Symposium on Foundations of Computer Science}
}

@article{indykOnApproximateNearest1998,
  title = {On Approximate Nearest Neighbors in Non-{{Euclidean}} Spaces},
  author = {Indyk, P.},
  year = {1998},
  doi = {10.1109/SFCS.1998.743438},
  abstract = {The nearest neighbor search (NNS) problem is the following: Given a set of n points P=\{p/sub 1/,...,p/sub n/\} in some metric space X, preprocess P so as to efficiently answer queries which require finding a point in P closest to a query point q/spl isin/X. The approximate nearest neighbor search (c-NNS) is a relaxation of NNS which allows to return any point within c times the distance to the nearest neighbor (called c-nearest neighbor). This problem is of major and growing importance to a variety of applications. In this paper we give an algorithm for (4log/sub 1+/spl rho//log4d+3)-NNS algorithm in l/sub /spl infin///sup d/ with O(dn/sup 1+/spl rho//logn) storage and O(dlogn) query time. In particular this yields the first algorithm for O(1)-NNS for l/sub /spl infin// with subexponential storage. The preprocessing time is linear in the size of the data structure. The algorithm can be also used (after simple modifications) to output the exact nearest neighbor in time bounded bounded O(dlogn) plus the number of (4log/sub 1+/spl rho//log4d+3)-nearest neighbors of the query point. Building on this result, we also obtain an approximation algorithm for a general class of product metrics. Finally: we show that for any c{\textexclamdown}3 the c-NNS problem in l/sub /spl infin// is provably hard for a version of the indexing model introduced by Hellerstein et al. (1997).},
  citationcount = {52},
  venue = {Proceedings 39th Annual Symposium on Foundations of Computer Science (Cat. No.98CB36280)}
}

@article{indykStableDistributionsPseudorandom2000,
  title = {Stable Distributions, Pseudorandom Generators, Embeddings and Data Stream Computation},
  author = {Indyk, P.},
  year = {2000},
  doi = {10.1109/SFCS.2000.892082},
  abstract = {In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: we show how to maintain (using only O(log n//spl epsiv//sup 2/) words of storage) a sketch C(p) of a point p/spl isin/l/sub 1//sup n/ under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate {\textbar}p-q{\textbar}/sub 1/ up to a factor of (1+/spl epsiv/) with large probability. We obtain another sketch function C' which maps l/sub 1//sup n/ into a normed space l/sub 1//sup m/ (as opposed to C), such that m=m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l/sub 1/ norm we give an explicit embedding of l/sub 2//sup n/ into l/sub l//sup nO(log n)/ with distortion (1+1/n/sup /spl theta/(1)/) and a non-constructive embedding of l/sub 2//sup n/ into l/sub 1//sup O(n)/ with distortion (1+/spl epsiv/) such that the embedding can be represented using only O(n log/sup 2/ n) bits (as opposed to at least n/sup 2/ used by earlier methods).},
  citationcount = {728},
  venue = {Proceedings 41st Annual Symposium on Foundations of Computer Science}
}

@article{indykTightLowerBounds2003,
  title = {Tight Lower Bounds for the Distinct Elements Problem},
  author = {Indyk, P. and Woodruff, David P.},
  year = {2003},
  doi = {10.1109/SFCS.2003.1238202},
  abstract = {We prove strong lower bounds for the space complexity of (/spl epsi/, /spl delta/)-approximating the number of distinct elements F/sub 0/ in a data stream. Let m be the size of the universe from which the stream elements are drawn. We show that any one-pass streaming algorithm for (/spl epsi/, /spl delta/)-approximating F/sub 0/ must use /spl Omega/(1//spl epsi//sup 2/) space when /spl epsi/ = /spl Omega/(m/sup -1/(9 + k)/), for any k {\textquestiondown} 0, improving upon the known lower bound of /spl Omega/(1//spl epsi/) for this range of /spl epsi/. This lower bound is tight up to a factor of log log m for small /spl epsi/ and log 1//spl epsi/ for large /spl epsi/. Our lower bound is derived from a reduction from the one-way communication complexity of approximating a Boolean function in Euclidean space. The reduction makes use of a low-distortion embedding from an l/sub 2/ to l/sub 1/ norm.},
  citationcount = {155},
  venue = {44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings.}
}

@article{ishaiCryptographyWithConstant2008,
  title = {Cryptography with Constant Computational Overhead},
  author = {Ishai, Yuval and Kushilevitz, E. and Ostrovsky, R. and Sahai, A.},
  year = {2008},
  doi = {10.1145/1374376.1374438},
  abstract = {Current constructions of cryptographic primitives typically involve a large multiplicative computational overhead that grows with the desired level of security. We explore the possibility of implementing basic cryptographic primitives, such as encryption, authentication, signatures, and secure two-party computation, while incurring only a constant computational overhead compared to insecure implementations of the same tasks. Here we make the usual security requirement that the advantage of any polynomial-time attacker must be negligible in the input length. We obtain affirmative answers to this question for most central cryptographic primitives under plausible, albeit sometimes nonstandard, intractability assumptions. We start by showing that pairwise-independent hash functions can be computed by linear-size circuits, disproving a conjecture of Mansour, Nisan, and Tiwari (STOC 1990). This construction does not rely on any unproven assumptions and is of independent interest. Our hash functions can be used to construct message authentication schemes with constant overhead from any one-way function. Under an intractability assumption that generalizes a previous assumption of Alekhnovich (FOCS 2003), we get (public and private key) encryption schemes with constant overhead. Using an exponentially strong version of the previous assumption, we get signature schemes of similar complexity. Assuming the existence of pseudorandom generators in NC z with polynomial stretch together with the existence of an (arbitrary) oblivious transfer protocol, we get similar results for the seemingly very complex task of secure two-party computation. More concretely, we get general protocols for secure two-party computation in the semi-honest model in which the two parties can be implemented by circuits whose size is a constant multiple of the size s of the circuit to be evaluated. In the malicious model, we get protocols whose communication complexity is a constant multiple of s and whose computational complexity is slightly super-linear in s. For natural relaxations of security in the malicious model that are still meaningful in practice, we can also keep the computational complexity linear in s. These results extend to the case of a constant number of parties, where an arbitrary subset of the parties can be corrupted. Our protocols rely on non-black-box techniques, and suggest the intriguing possibility that the ultimate efficiency in this area of cryptography can be obtained via such techniques.},
  citationcount = {176},
  venue = {Symposium on the Theory of Computing}
}

@article{istominaFineGrainedReductions2023,
  title = {Fine-Grained Reductions around {{CFL-reachability}}},
  author = {Istomina, A. and Grigorev, S. and Shemetova, E.},
  year = {2023},
  doi = {10.48550/arXiv.2306.15967},
  abstract = {In this paper we study the fine-grained complexity of the CFL reachability problem. We first present one of the existing algorithms for the problem and an overview of conditional lower bounds based on widely believed hypotheses. We then use the existing reduction techniques to obtain new conditional lower bounds on CFL reachability and related problems. We also devise a faster algorithm for the problem in case of bounded path lengths and a technique that may be useful in finding new conditional lower bounds.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {lower bound,reduction}
}

@article{italianoAmortizedEfficiencyOf1986,
  title = {Amortized Efficiency of a Path Retrieval Data Structure},
  author = {Italiano, G.},
  year = {1986},
  doi = {10.1016/0304-3975(86)90098-8},
  abstract = {No abstract available},
  citationcount = {153},
  venue = {Theoretical Computer Science}
}

@article{italianoFullyDynamicConnectivity2008,
  title = {Fully Dynamic Connectivity: {{Upper}} and Lower Bounds},
  author = {Italiano, G.},
  year = {2008},
  doi = {10.1007/978-1-4939-2864-4_153},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Encyclopedia of Algorithms},
  keywords = {dynamic,lower bound}
}

@article{italianoFullyDynamicPlanarity1993,
  title = {Fully Dynamic Planarity Testing in Planar Embedded Graphs (Extended Abstract)},
  author = {Italiano, G. and Poutr{\'e}, H. L. and Henzinger, Monika},
  year = {1993},
  doi = {10.1007/3-540-57273-2_57},
  abstract = {No abstract available},
  citationcount = {30},
  venue = {Embedded Systems and Applications},
  keywords = {dynamic}
}

@article{ittaiabrahamAsynchronousResourceDiscovery2003,
  title = {Asynchronous Resource Discovery},
  author = {Ittai Abraham and D. Dolev},
  year = {2003},
  journal = {ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing},
  doi = {10.1145/872035.872055},
  abstract = {Consider a dynamic, large-scale communication infrastructure (e.g., the Internet) where nodes (e.g., in a peer to peer system) can communicate only with nodes whose id (e.g., IP address) are known to them. One of the basic building blocks of such a distributed system is resource discovery - efficiently discovering the ids of the nodes that currently exist in the system. We present both upper and lower bounds for the resource discovery problem. For the original problem raised by Harchol-Balter, Leighton, and Lewin [3] we present an {\textohm}2(n log n) message complexity lower bound for asynchronous networks whose size is unknown. For this model, we give an asymptotically message optimal algorithm that improves the bit complexity of Kutten and Peleg [4]. When each node knows the size of its connected component, we provide a novel and highly efficient algorithm with near linear O(n{$\alpha$}(n, n)) message complexity (where {$\alpha$} is the inverse of Ackerman's function). In addition, we define and study the Ad-hoc Resource Discovery Problem, which is a practical relaxation of the original problem. Our algorithm for ad-hoc resource discovery has near linear O(n{$\alpha$}(n, n)) message complexity. The algorithm efficiently deals with dynamic node additions to the system, thus addressing an open question of [3]. We present a {\textohm}(n{$\alpha$}(n, n)) lower bound for the Ad-hoc Resource Discovery Problem, showing that our algorithm is asymptotically message optimal.},
  keywords = {communication,dynamic,lower bound},
  annotation = {Citation Count: 28}
}

@article{ivanyosTradingGrhFor2008,
  title = {Trading {{GRH}} for Algebra: {{Algorithms}} for Factoring Polynomials and Related Structures},
  author = {Ivanyos, G. and Karpinski, Marek and R{\'o}nyai, L. and Saxena, Nitin},
  year = {2008},
  doi = {10.1090/S0025-5718-2011-02505-6},
  abstract = {In this paper we develop techniques that eliminate the need of the Generalized Riemann Hypothesis (GRH) from various (almost all) known results about deterministic polynomial factoring over finite fields. Our main result shows that given a polynomial f(x) of degree n over a finite field k, we can find in deterministic poly(n log n , log{\textbar}k{\textbar}) time either a nontrivial factor of f(x) or a nontrivial automorphism of k[x]/(f(x)) of order n. This main tool leads to various new GRH-free results, most striking of which are: 1. Given a noncommutative algebra A of dimension n over a finite field k. There is a deterministic poly(n logn , log{\textbar}k{\textbar}) time algorithm to find a zero divisor in A. This is the best known deterministic GRH-free result since R{\textasciiacute} (1990) first studied the problem of finding zero divisors in finite algebras and showed that this problem has the same complexity as factoring polynomials over finite fields. 2. Given a positive integer r {\textquestiondown} 4 such that either 4{\textbar}r or r has two distinct prime factors. There is a deterministic polynomial time algorithm to find a nontrivial factor of ther-th cyclotomic polynomial over a finite field. This is the best known deterministic GRH-free result since Evdokimov (1989) showed that cyclotomic polynomials can be factored over finite fields in deterministic polynomial time assuming GRH. In this paper, following the seminal work of Lenstra (1991) on constructing isomorphisms between finite fields, we further generalize classical Galois theory constructs},
  citationcount = {18},
  venue = {Mathematics of Computation}
}

@article{iwataFastDynamicGraph2014,
  title = {Fast Dynamic Graph Algorithms for Parameterized Problems},
  author = {Iwata, Yoichi and Oka, Keigo},
  year = {2014},
  doi = {10.1007/978-3-319-08404-6_21},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {dynamic}
}

@article{izumiDeterministicFaultTolerant2022,
  title = {Deterministic Fault-Tolerant Connectivity Labeling Scheme},
  author = {Izumi, Taisuke and Emek, Y. and Wadayama, T. and Masuzawa, T.},
  year = {2022},
  doi = {10.1145/3583668.3594584},
  abstract = {The f-fault-tolerant connectivity labeling (f-FTC labeling) is a scheme of assigning each vertex and edge with a small-size label such that one can determine the connectivity of two vertices s and t under the presence of at most f faulty edges only from the labels of s, t, and the faulty edges. This paper presents a new deterministic f-FTC labeling scheme attaining O(f2 polylog(n))-bit label size and a polynomial construction time, which settles the open problem left by Dory and Parter [18]. The key ingredient of our construction is to develop a deterministic counterpart of the graph sketch technique by Ahn, Guha, and McGreger [4], via some natural connection with the theory of error-correcting codes. This technique removes one major obstacle in de-randomizing the Dory-Parter scheme. The whole scheme is obtained by combining this technique with a new deterministic graph sparsification algorithm derived from the seminal {$\epsilon$}-net theory, which is also of independent interest. As byproducts, our result deduces the first deterministic fault-tolerant approximate distance labeling scheme with a non-trivial performance guarantee and an improved deterministic fault-tolerant compact routing. The authors believe that our new technique is potentially useful in the future exploration of more efficient FTC labeling schemes and other related applications based on graph sketches.},
  citationcount = {3},
  venue = {ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing}
}

@article{izumiTriangleFindingAnd2017,
  title = {Triangle Finding and Listing in {{CONGEST}} Networks},
  author = {Izumi, Taisuke and Gall, F.},
  year = {2017},
  doi = {10.1145/3087801.3087811},
  abstract = {Triangle-free graphs play a central role in graph theory, and triangle detection (or triangle finding) as well as triangle enumeration (triangle listing) play central roles in the field of graph algorithms. In distributed computing, algorithms with sublinear round complexity for triangle finding and listing have recently been developed in the powerful CONGEST clique model, where communication is allowed between any two nodes of the network. In this paper we present the first algorithms with sublinear complexity for triangle finding and triangle listing in the standard CONGEST model, where the communication topology is the same as the topology of the network. More precisely, we give randomized algorithms for triangle finding and listing with round complexity O(n2/3(log n)2/3) and O(n3/4log n), respectively, where n denotes the number of nodes of the network. We also show a lower bound {\textohm}(n1/3/log n) on the round complexity of triangle listing, which also holds for the CONGEST clique model.},
  citationcount = {54},
  venue = {ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing},
  keywords = {communication,lower bound}
}

@article{j.d.loeraDiscreteUbiquitousTheorems2017,
  title = {The Discrete yet Ubiquitous Theorems of {{Carath{\'e}odory}}, {{Helly}}, {{Sperner}}, {{Tucker}}, and {{Tverberg}}},
  author = {J. D. Loera and X. Goaoc and Fr{\'e}d{\'e}ric Meunier and Nabil H. Mustafa},
  year = {2017},
  journal = {Bulletin of the American Mathematical Society},
  doi = {10.1090/BULL/1653},
  abstract = {We discuss five discrete results: the lemmas of Sperner and Tucker from combinatorial topology and the theorems of Carath{\textbackslash}'eodory, Helly, and Tverberg from combinatorial geometry. We explore their connections and emphasize their broad impact in application areas such as game theory, graph theory, mathematical optimization, computational geometry, etc.},
  annotation = {Citation Count: 83}
}

@article{j.gustedtEfficientUnionFindPlanar1996,
  title = {Efficient {{Union-Find}} for {{Planar Graphs}} and Other {{Sparse Graph Classes}} ({{Extended Abstract}})},
  author = {J. Gustedt},
  year = {1996},
  journal = {International Workshop on Graph-Theoretic Concepts in Computer Science},
  doi = {10.1007/3-540-62559-3_16},
  annotation = {Citation Count: 1}
}

@article{j.gustedtEfficientUnionFindPlanar1998,
  title = {Efficient {{Union-Find}} for {{Planar Graphs}} and Other {{Sparse Graph Classes}}},
  author = {J. Gustedt},
  year = {1998},
  journal = {Theoretical Computer Science},
  doi = {10.1016/S0304-3975(97)00291-0},
  annotation = {Citation Count: 28}
}

@article{j.leeuwenChapter7Fundamental1992,
  title = {Chapter 7 {{Fundamental}} Algorithms and Data Structures},
  author = {J. Leeuwen and P. Widmayer},
  year = {1992},
  doi = {10.1016/S0927-0507(05)80204-X},
  keywords = {data structure},
  annotation = {Citation Count: 1}
}

@article{j.troppIntroductionMatrixConcentration2015,
  title = {An {{Introduction}} to {{Matrix Concentration Inequalities}}},
  author = {J. Tropp},
  year = {2015},
  journal = {Found. Trends Mach. Learn.},
  doi = {10.1561/2200000048},
  abstract = {In recent years, random matrices have come to play a major role in computational mathematics, but most of the classical areas of random matrix theory remain the province of experts. Over the last decade, with the advent of matrix concentration inequalities, research has advanced to the point where we can conquer many (formerly) challenging problems with a page or two of arithmetic. The aim of this monograph is to describe the most successful methods from this area along with some interesting examples that these techniques can illuminate.},
  annotation = {Citation Count: 1117}
}

@article{j.westbrookMaintainingBridgeconnectedBiconnected1992,
  title = {Maintaining Bridge-Connected and Biconnected Components on-Line},
  author = {J. Westbrook and R. Tarjan},
  year = {1992},
  journal = {Algorithmica},
  doi = {10.1007/BF01758773},
  annotation = {Citation Count: 99}
}

@article{jacobLowerBoundsFor2018,
  title = {Lower Bounds for Oblivious Data Structures},
  author = {Jacob, R. and Larsen, Kasper Green and Nielsen, J.},
  year = {2018},
  doi = {10.1137/1.9781611975482.149},
  abstract = {An oblivious data structure is a data structure where the memory access patterns reveals no information about the operations performed on it. Such data structures were introduced by Wang et al. [ACM SIGSAC'14] and are intended for situations where one wishes to store the data structure at an untrusted server. One way to obtain an oblivious data structure is simply to run a classic data structure on an oblivious RAM (ORAM). Until very recently, this resulted in an overhead of {$\omega$}(n) for the most natural setting of parameters. Moreover, a recent lower bound for ORAMs by Larsen and Nielsen [CRYPTO'18] show that they always incur an overhead of at least {\textohm}(n) if used in a black box manner. To circumvent the {$\omega$}(n) overhead, researchers have instead studied classic data structure problems more directly and have obtained efficient solutions for many such problems such as stacks, queues, deques, priority queues and search trees. However, none of these data structures process operations faster than {$\Theta$}(n), leaving open the question of whether even faster solutions exist. In this paper, we rule out this possibility by proving {\textohm}(n) lower bounds for oblivious stacks, queues, deques, priority queues and search trees.},
  citationcount = {34},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,lower bound}
}

@article{jacobsonSpaceEfficientStatic1989,
  title = {Space-Efficient Static Trees and Graphs},
  author = {Jacobson, G.},
  year = {1989},
  doi = {10.1109/SFCS.1989.63533},
  abstract = {Data structures that represent static unlabeled trees and planar graphs are developed. The structures are more space efficient than conventional pointer-based representations, but (to within a constant factor) they are just as time efficient for traversal operations. For trees, the data structures described are asymptotically optimal: there is no other structure that encodes n-node trees with fewer bits per node, as N grows without bound. For planar graphs (and for all graphs of bounded page number), the data structure described uses linear space: it is within a constant factor of the most succinct representation.<<ETX>>},
  citationcount = {744},
  venue = {30th Annual Symposium on Foundations of Computer Science}
}

@article{jainADirectProduct2016,
  title = {A Direct Product Theorem for Two-Party Bounded-Round Public-Coin Communication Complexity},
  author = {Jain, Rahul and Pereszl{\'e}nyi, A. and Yao, Penghui},
  year = {2016},
  doi = {10.1007/s00453-015-0100-0},
  abstract = {No abstract available},
  citationcount = {32},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{jainADirectSum2003,
  title = {A Direct Sum Theorem in Communication Complexity via Message Compression},
  author = {Jain, Rahul and Radhakrishnan, J. and Sen, P.},
  year = {2003},
  doi = {10.1007/3-540-45061-0_26},
  abstract = {No abstract available},
  citationcount = {108},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{jjSpaceEfficientAnd2004,
  title = {Space-Efficient and Fast Algorithms for Multidimensional Dominance Reporting and Counting},
  author = {J{\'a}J{\'a}, J. and Mortensen, C. and Shi, Qingmin},
  year = {2004},
  doi = {10.1007/978-3-540-30551-4_49},
  abstract = {No abstract available},
  citationcount = {109},
  venue = {International Symposium on Algorithms and Computation}
}

@article{jakublackiOptimalDecrementalConnectivity2014,
  title = {Optimal {{Decremental Connectivity}} in {{Planar Graphs}}},
  author = {Jakub Lacki and P. Sankowski},
  year = {2014},
  journal = {Theory of Computing Systems},
  doi = {10.1007/s00224-016-9709-x},
  annotation = {Citation Count: 17}
}

@article{jambulapatiRegularizedBoxSimplex2022,
  title = {Regularized Box-Simplex Games and Dynamic Decremental Bipartite Matching},
  author = {Jambulapati, Arun and Jin, Yujia and Sidford, Aaron and Tian, Kevin},
  year = {2022},
  doi = {10.48550/arXiv.2204.12721},
  abstract = {Box-simplex games are a family of bilinear minimax objectives which encapsulate graph-structured problems such as maximum flow [41], optimal transport [29], and bipartite matching [5]. We develop efficient near-linear time, high-accuracy solvers for regularized variants of these games. Beyond the immediate applications of such solvers for computing Sinkhorn distances, a prominent tool in machine learning, we show that these solvers can be used to obtain improved running times for maintaining a (fractional) {$\epsilon$} -approximate maximum matching in a dynamic decremental bipartite graph against an adaptive adversary. We give a generic framework which reduces this dynamic matching problem to solving regularized graph-structured optimization problems to high accuracy. Through our reduction framework, our regularized box-simplex game solver implies a new algorithm for dynamic decremental bipartite matching in total time e O ( m {$\cdot$} {$\epsilon$} - 3 ), from an initial graph with m edges and n nodes. We further show how to use recent advances in flow optimization [11] to improve our runtime to m 1+ o (1) {$\cdot$} {$\epsilon$} - 2 , thereby demonstrating the versatility of our reduction-based approach. These results improve upon the previous best runtime of e O ( m {$\cdot$} {$\epsilon$} - 4 ) [6] and illustrate the utility of using regularized optimization problem solvers for designing dynamic algorithms.},
  citationcount = {10},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {adaptive,dynamic,reduction}
}

@article{janardanGeneralizedIntersectionSearching1993,
  title = {Generalized Intersection Searching Problems},
  author = {Janardan, Ravi and Lopez, M.},
  year = {1993},
  doi = {10.1142/S021819599300004X},
  abstract = {A new class of geometric intersection searching problems is introduced, which generalizes previously-considered intersection searching problems and is rich in applications. In a standard intersection searching problem, a set S of n geometric objects is to be preprocessed so that the objects that are intersected by a query object q can be reported efficiently. In a generalized problem, the objects in S come aggregated in disjoint groups and what is of interest are the groups, not the objects, that are intersected by q. Although this problem can be solved easily by using an algorithm for the standard problem, the query time can be {\textohm}(n) even though the output size is just O(1). In this paper, algorithms with efficient, output-size-sensitive query times are presented for the generalized versions of a number of intersection searching problems, including: interval intersection searching, orthogonal segment intersection searching, orthogonal range searching, point enclosure searching, rectangle intersection searching, and segment intersection searching. In addition, the algorithms are also space-efficient.},
  citationcount = {82},
  venue = {International journal of computational geometry and applications}
}

@article{janardhankulkarniMinimumBirkhoffvonNeumann2017,
  title = {Minimum {{Birkhoff-von Neumann Decomposition}}},
  author = {Janardhan Kulkarni and Euiwoong Lee and Mohit Singh},
  year = {2017},
  journal = {Conference on Integer Programming and Combinatorial Optimization},
  doi = {10.1007/978-3-319-59250-3_28},
  annotation = {Citation Count: 19}
}

@article{jansonWileyInterscienceSeries2011,
  title = {Wiley-interscience Series in Discrete Mathematics and Optimization},
  author = {Janson, S. and {\L}uczak, Tomasz and Rucinski, A.},
  year = {2011},
  doi = {10.1002/9781118032718.SCARD},
  abstract = {No abstract available},
  citationcount = {452},
  venue = {No venue available}
}

@article{janssonCompressedDynamicTries2007,
  title = {Compressed Dynamic Tries with Applications to {{LZ-compression}} in Sublinear Time and Space},
  author = {Jansson, J. and Sadakane, K. and Sung, W.},
  year = {2007},
  doi = {10.1007/978-3-540-77050-3_35},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {Foundations of Software Technology and Theoretical Computer Science},
  keywords = {dynamic}
}

@article{janssonCramCompressedRandom2010,
  title = {{{CRAM}}: {{Compressed}} Random Access Memory},
  author = {Jansson, J. and Sadakane, K. and Sung, W.},
  year = {2010},
  doi = {10.1007/978-3-642-31594-7_43},
  abstract = {No abstract available},
  citationcount = {28},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{janssonLinkedDynamicTries2013,
  title = {Linked Dynamic Tries with Applications to {{LZ-compression}} in Sublinear Time and Space},
  author = {Jansson, J. and Sadakane, K. and Sung, W.},
  year = {2013},
  doi = {10.1007/s00453-013-9836-6},
  abstract = {No abstract available},
  citationcount = {20},
  venue = {Algorithmica},
  keywords = {dynamic}
}

@article{jayantiARandomizedConcurrent2016,
  title = {A Randomized Concurrent Algorithm for Disjoint Set Union},
  author = {Jayanti, S. and Tarjan, R.},
  year = {2016},
  doi = {10.1145/2933057.2933108},
  abstract = {Disjoint set union is a basic problem in data structures with a wide variety of applications. We extend a known efficient sequential algorithm for this problem to obtain a simple and efficient concurrent wait-free algorithm running on an asynchronous parallel random access machine (APRAM). Crucial to our result is the use of randomization. Under a certain independence assumption, for a problem instance in which there are n elements, m operations, and l processes, our algorithm does {\texttheta}(m ({$\alpha$}\{n, m/nl) + log (nl/m + 1 ))) expected work, where the expectation is over the random choices made by the algorithm and {$\alpha$} is a functional inverse of Ackermann's function. In addition, each operation takes O(log n) steps with high probability. We point out some gaps in the earlier work on this problem by Anderson and Woll [1]. More importantly, our algorithm is significantly simpler than theirs. Finally, under our independence assumption our algorithm achieves speedup close to linear for applications in which all or most of the processes can be kept busy, thereby partially answering an open problem posed by them.\vphantom\}},
  citationcount = {22},
  venue = {ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing},
  keywords = {data structure}
}

@article{jayantiConcurrentDisjointSet2020,
  title = {Concurrent Disjoint Set Union},
  author = {Jayanti, S. and Tarjan, R.},
  year = {2020},
  doi = {10.1007/s00446-020-00388-x},
  abstract = {No abstract available},
  citationcount = {12},
  venue = {Distributed computing}
}

@article{jayantiRandomizedConcurrentSet2019,
  title = {Randomized Concurrent Set Union and Generalized Wake-Up},
  author = {Jayanti, S. and Tarjan, R. and {Boix-Adser{\`a}}, Enric},
  year = {2019},
  doi = {10.1145/3293611.3331593},
  abstract = {We consider the disjoint set union problem in the asynchronous shared memory multiprocessor computation model. We design a randomized algorithm that performs at most O(log n) work per operation (with high probability), and performs at most O(m \#8226; ({$\alpha$}(n, m/(np)) + log(np/m + 1)) total work in expectation for a problem instance with m operations on n elements solved by p processes. Our algorithm is the first to have work bounds that grow sublinearly with p against an adversarial scheduler. We use Jayanti's Wake Up problem and our newly defined Generalized Wake Up problem to prove several lower bounds on concurrent set union. We show an {\textohm}(log min \{n,p\}) expected work lower bound on the cost of any single operation on a set union algorithm. This shows that our single-operation upper bound is optimal across all algorithms when p = n{\textohm}(1). Furthermore, we identify a class of "symmetric algorithms'' that captures the complexities of all the known algorithms for the disjoint set union problem, and prove an {\textohm}(m{$\bullet$}({$\alpha$}(n, m(np)) + log(np/m + 1))) expected total work lower bound on algorithms of this class, thereby showing that our algorithm has optimal total work complexity for this class. Finally, we prove that any randomized algorithm, symmetric or not, cannot breach an {\textohm}(m {$\bullet$}({$\alpha$}(n, m/n) + log log(np/m + 1))) expected total work lower bound.},
  citationcount = {21},
  venue = {ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing},
  keywords = {lower bound}
}

@inproceedings{jayramCellprobeLowerBounds2003,
  title = {Cell-Probe Lower Bounds for the Partial Match Problem},
  booktitle = {Proceedings of the Thirty-Fifth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Jayram, T. S. and Khot, Subhash and Kumar, Ravi and Rabani, Yuval},
  year = {2003},
  month = jun,
  series = {{{STOC}} '03},
  pages = {667--672},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/780542.780639},
  url = {https://dl.acm.org/doi/10.1145/780542.780639},
  urldate = {2024-11-19},
  abstract = {Given a database of n points in (0,1)d, the partial match problem is: In response to a query x in (0, 1, *)d, find a database point y such that for every i whenever xi {$\neq$} *, we have xi = yi. In this paper we show randomized lower bounds in the cell-probe model for this well-studied problem[18, 11, 19, 16, 4, 6 ].Our lower bounds follow from a two-party asymmetric randomized communication complexity near-optimal lower bound for this problem, where we show that either Alice has to send {\textohm}(d log n) bits or Bob has to send {\textohm}(n1 - o(1)) bits. When applied to the cell-probe model, it means that if the number of cells is restricted to be poly(n, d) where each cell is of size poly(log n, d), then {\textohm}(d/log2 n) probes are needed. This is an exponential improvement over the previously known lower bounds for this problem[16, 4].},
  isbn = {978-1-58113-674-6},
  keywords = {cell probe,communication,communication complexity,lower bound,query},
  annotation = {D. Knuth . The Art of Computer Programming: Sorting and Searching . Addison-Wesley , 1973 . D. Knuth. The Art of Computer Programming: Sorting and Searching. Addison-Wesley, 1973.\\
\\
\\
\\
P. B. Miltersen . Cell probe complexity - a survey . In Pre-Conference Workshop on Advances in Data Structures at the 19th Conference on Foundations of Software Technology and Theoretical Computer Science , 1999 . P. B. Miltersen. Cell probe complexity - a survey. In Pre-Conference Workshop on Advances in Data Structures at the 19th Conference on Foundations of Software Technology and Theoretical Computer Science, 1999.\\
\\
\\
R. Rivest . Analysis of Associative Retrieval Algorithms. PhD thesis , Stanford University , 1974 . R. Rivest. Analysis of Associative Retrieval Algorithms. PhD thesis, Stanford University, 1974.},
  file = {/Users/tulasi/Zotero/storage/RSACAPLU/Jayram et al. - 2003 - Cell-probe lower bounds for the partial match problem.pdf}
}

@article{jazayeriTheIntrinsicallyExponential1975,
  title = {The Intrinsically Exponential Complexity of the Circularity Problem for Attribute Grammars},
  author = {Jazayeri, M. and Ogden, W. and Rounds, W.},
  year = {1975},
  doi = {10.1145/361227.361231},
  abstract = {Attribute grammars are an extension of context-free grammars devised by Knuth as a mechanism for including the semantics of a context-free language with the syntax of the language. The circularity problem for a grammar is to determine whether the semantics for all possible sentences (programs) in fact will be well defined. It is proved that this problem is, in general, computationally intractable. Specifically, it is shown that any deterministic algorithm which solves the problem must for infinitely many cases use an exponential amount of time. An improved version of Knuth's circularity testing algorithm is also given, which actually solves the problem within exponential time.},
  citationcount = {124},
  venue = {CACM}
}

@article{jean-philippejasienskiComputationalImplementationVectorbased2024,
  title = {A Computational Implementation of {{Vector-based 3D Graphic Statics}} ({{VGS}}) for Interactive and Real-Time Structural Design},
  author = {{Jean-Philippe Jasienski} and Yuchi Shen and P. Ohlbrock and Denis Zastavni and P. D'Acunto},
  year = {2024},
  journal = {Comput. Aided Des.},
  doi = {10.1016/j.cad.2024.103695},
  keywords = {static},
  annotation = {Citation Count: 6}
}

@article{jeffericksonSpacetimeTradeoffsEmptiness1997,
  title = {Space-Time Tradeoffs for Emptiness Queries (Extended Abstract)},
  author = {Jeff Erickson},
  year = {1997},
  journal = {SCG '97},
  doi = {10.1145/262839.262988},
  abstract = {We present the fist nontrivial spat-time tradeoff lower bounds for hyperplane and halfspaceemptinessqueries. Our lower bounds apply to a general class of geometric range query data structures called partition graphs. Informally, a partition graph is a directed acyclic graph that describes a recursive decomposition of space. We show that any partition graph that supports hyperplane emptiness queries implicitly deiines a halfspace range query data structure in the Fredman/Yao semigroup arithmetic model, with the same space and time bounds. Thus, results of Bronnimann, Chazelle, and Path imply that any partition graph of size s that supports hyperplane emptiness queries in time t must satisfy the inequality st d = {\textasciitilde}((n/logn)*--{\textasciitilde}d--Tl/ld+l)]. Using difterent techniques, we show that fl(nd/ polylog n) preprocessing time is required to achieve polylogarithmic query time, and that Cl(n('--1 " d/ polylogn) query time is required if only O(n polylog n) preprocessing time is used. These two lower bounds are optimal up to polylogarithrnic factors. For twcdimensional queries, we obtain an optimal continuous tradeoff between these two extremes. Finally, using a reduction argument, we show that the same lower bounds hold for halfspace emptiness queries in R.d('+3)'2 on a restricted class of partition graphs.},
  keywords = {data structure,lower bound,query,query time,reduction},
  annotation = {Citation Count: 2}
}

@article{jeffericksonSpaceTimeTradeoffsEmptiness2000,
  title = {Space-{{Time Tradeoffs}} for {{Emptiness Queries}}},
  author = {Jeff Erickson},
  year = {2000},
  journal = {SIAM journal on computing (Print)},
  doi = {10.1137/S0097539798337212},
  abstract = {We develop the first nontrivial lower bounds on the complexity of online hyperplane and halfspace emptiness queries. Our lower bounds apply to a general class of geometric range query data structures called partition graphs. Informally, a partition graph is a directed acyclic graph that describes a recursive decomposition of space. We show that any partition graph that supports hyperplane emptiness queries implicitly defines a halfspace range query data structure in the Fredman/Yao semigroup arithmetic model, with the same asymptotic space and time bounds. Thus, results of Bronnimann, Chazelle, and Pach imply that any partition graph of size s that supports hyperplane emptiness queries in time t satisfies the inequality \$st{\textasciicircum}d = {\textbackslash}Omega((n/{\textbackslash}log n){\textasciicircum}\{d - (d-1)/(d+1)\})\$. Using different techniques, we improve previous lower bounds for Hopcroft's problem---Given a set of points and hyperplanes, does any hyperplane contain a point?---in dimensions four and higher. Using this offline result, we show that for online hyperplane emptiness queries, \${\textbackslash}Omega(n{\textasciicircum}d/\{{\textbackslash}mbox\{ polylog \}\} n)\$ space is required to achieve polylogarithmic query time, and \${\textbackslash}Omega(n{\textasciicircum}\{(d-1)/d\}/\{{\textbackslash}mbox\{ polylog \}\} n)\$ query time is required if only O(n polylog n) space is available. These two lower bounds are optimal up to polylogarithmic factors. For two-dimensional queries, we obtain an optimal continuous tradeoff \$st{\textasciicircum}2={\textbackslash}Omega(n{\textasciicircum}2)\$ between these two extremes. Finally, using a lifting argument, we show that the same lower bounds hold for both offline and online halfspace emptiness queries in \$\{{\textbackslash}mathbb\{R\}\}{\textasciicircum}\{d(d+3)/2\}\$.},
  keywords = {data structure,lower bound,query,query time},
  annotation = {Citation Count: 28}
}

@article{jellissConcreteMathematicsA1991,
  title = {Concrete Mathematics, a Foundation for Computer Science},
  author = {Jelliss, G. P. and Graham, R. and Knuth, D. and Patashnik, Oren},
  year = {1991},
  doi = {10.2307/3619021},
  abstract = {No abstract available},
  citationcount = {654},
  venue = {Mathematical Gazette}
}

@article{jerzakBloomFilterBased2008,
  title = {Bloom Filter Based Routing for Content-Based Publish/Subscribe},
  author = {Jerzak, Zbigniew and Fetzer, C.},
  year = {2008},
  doi = {10.1145/1385989.1385999},
  abstract = {Achieving expressive and efficient content-based routing in publish/subscribe systems is a difficult problem. Traditional approaches prove to be either inefficient or severely limited in their expressiveness and flexibility. We present a novel routing method, based on Bloom filters, which shows high efficiency while simultaneously preserving the flexibility of content-based schemes. The resulting implementation is a fast, flexible and fully decoupled content-based publish/subscribe system.},
  citationcount = {76},
  venue = {Distributed Event-Based Systems}
}

@article{jerzakPrefixForwardingFor2007,
  title = {Prefix Forwarding for Publish/Subscribe},
  author = {Jerzak, Zbigniew and Fetzer, C.},
  year = {2007},
  doi = {10.1145/1266894.1266939},
  abstract = {We present a prefix forwarding algorithm for content-based publish/subscribe systems. Our algorithm performs only one content-based match per message regardless of the number of routers (hops) traversed from the source to the destination. Moreover, prefix forwarding preserves the decoupling properties of publish/subscribe system. Prefix forwarding does not put any restriction on the content of the messages. The presented algorithm does not introduce any false negatives and allows to tune the false positive rate to balance the bandwidth and processing overheads. We provide experimental results confirming the properties of the proposed approach.},
  citationcount = {11},
  venue = {Distributed Event-Based Systems}
}

@article{jiangAFasterAlgorithm2021,
  title = {A Faster Algorithm for Solving General {{LPs}}},
  author = {Jiang, Shunhua and Song, Zhao and Weinstein, Omri and Zhang, Hengjie},
  year = {2021},
  doi = {10.1145/3406325.3451058},
  abstract = {The fastest known LP solver for general (dense) linear programs is due to [Cohen, Lee and Song'19] and runs in O*(n{$\omega$} +n2.5-{$\alpha$}/2 + n2+1/6) time. A number of follow-up works [Lee, Song and Zhang'19, Brand'20, Song and Yu'20] obtain the same complexity through different techniques, but none of them can go below n2+1/6, even if {$\omega$}=2. This leaves a polynomial gap between the cost of solving linear systems (n{$\omega$}) and the cost of solving linear programs, and as such, improving the n2+1/6 term is crucial toward establishing an equivalence between these two fundamental problems. In this paper, we reduce the running time to O*(n{$\omega$} +n2.5-{$\alpha$}/2 + n2+1/18) where {$\omega$} and {$\alpha$} are the fast matrix multiplication exponent and its dual. Hence, under the common belief that {$\omega$} {$\approx$} 2 and {$\alpha$} {$\approx$} 1, our LP solver runs in O*(n2.055) time instead of O*(n2.16).},
  citationcount = {57},
  venue = {Symposium on the Theory of Computing}
}

@article{jiangAnImprovedCutting2020,
  title = {An Improved Cutting Plane Method for Convex Optimization, Convex-Concave Games, and Its Applications},
  author = {Jiang, Haotian and Lee, Y. and Song, Zhao and Wong, Sam Chiu-wai},
  year = {2020},
  doi = {10.1145/3357713.3384284},
  abstract = {Given a separation oracle for a convex set K {$\subset$} {$\mathbb{R}$} n that is contained in a box of radius R, the goal is to either compute a point in K or prove that K does not contain a ball of radius {\cyrchar\cyrie}. We propose a new cutting plane algorithm that uses an optimal O(n log({$\kappa$})) evaluations of the oracle and an additional O(n 2) time per evaluation, where {$\kappa$} = nR/{\cyrchar\cyrie}. This improves upon Vaidya's O( SO {$\cdot$} n log({$\kappa$}) + n {$\omega$}+1 log({$\kappa$})) time algorithm [Vaidya, FOCS 1989a] in terms of polynomial dependence on n, where {$\omega$} {\textexclamdown} 2.373 is the exponent of matrix multiplication and SO is the time for oracle evaluation. This improves upon Lee-Sidford-Wong's O( SO {$\cdot$} n log({$\kappa$}) + n 3 log O(1) ({$\kappa$})) time algorithm [Lee, Sidford and Wong, FOCS 2015] in terms of dependence on {$\kappa$}. For many important applications in economics, {$\kappa$} = {\textohm}(exp(n)) and this leads to a significant difference between log({$\kappa$}) and (log({$\kappa$})). We also provide evidence that the n 2 time per evaluation cannot be improved and thus our running time is optimal. A bottleneck of previous cutting plane methods is to compute leverage scores, a measure of the relative importance of past constraints. Our result is achieved by a novel multi-layered data structure for leverage score maintenance, which is a sophisticated combination of diverse techniques such as random projection, batched low-rank update, inverse maintenance, polynomial interpolation, and fast rectangular matrix multiplication. Interestingly, our method requires a combination of different fast rectangular matrix multiplication algorithms. Our algorithm not only works for the classical convex optimization setting, but also generalizes to convex-concave games. We apply our algorithm to improve the runtimes of many interesting problems, e.g., Linear Arrow-Debreu Markets, Fisher Markets, and Walrasian equilibrium.},
  citationcount = {104},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,update}
}

@article{jiangTheComplexityOf2022,
  title = {The Complexity of Dynamic Least-Squares Regression},
  author = {Jiang, Shunhua and Peng, Binghui and Weinstein, Omri},
  year = {2022},
  doi = {10.1109/FOCS57990.2023.00097},
  abstract = {We settle the complexity of dynamic least-squares regression (LSR), where rows and labels (\{A\}\textsuperscript{\{\vphantom\}}(t)\vphantom\{\},\{b\}\textsuperscript{\{\vphantom\}}(t)\vphantom\{\}) can be adaptively inserted and/or deleted, and the goal is to efficiently maintain an {$\epsilon$}-approximate solution to \textsubscript{\{\vphantom\}}\{x\}\textsuperscript{\{\vphantom\}}(t)\vphantom\{\}\vphantom\{\}\{A\}\textsuperscript{\{\vphantom\}}(t)\vphantom\{\}\{x\}\textsuperscript{\{\vphantom\}}(t)\vphantom\{\}-\{b\}\textsuperscript{\{\vphantom\}}(t)\vphantom\{\}\textsubscript{\{\vphantom\}}2\vphantom\{\} for all t{$\in$}[T]. We prove sharp separations (d\textsuperscript{\{\vphantom\}}2-o(1)\vphantom\{\}. vs. .{$\sim$}d) between the amortized update time of: (i) Fully vs. Partially dynamic 0.01-LSR; (ii) High vs. low-accuracy LSR in the partially-dynamic (insertion-only) setting.Our lower bounds follow from a gap-amplification reduction--reminiscent of iterative refinement-from the exact version of the Online Matrix Vector Conjecture (OMv) [HKNS15], to constant approximate OMv over the reals, where the i-th online product \{Hv\}\textsuperscript{\{\vphantom\}}(i)\vphantom\{\} only needs to be computed to 0.1 -relative error. All previous fine-grained reductions from OMv to its approximate versions only show hardness for inverse polynomial approximation {$\epsilon$}= n\textsuperscript{\{\vphantom\}}-{$\omega$}(1)\vphantom\{\} (additive or multiplicative). This result is of independent interest in fine-grained complexity and for the investigation of the OMv Conjecture, which is still widely open.},
  citationcount = {4},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {adaptive,dynamic,lower bound,reduction,update,update time}
}

@article{jiaoOnTheK2004,
  title = {On the K-{{Closest}} Substring and k-{{Consensus}} Pattern Problems},
  author = {Jiao, Yishan and Xu, Jingyi and Li, Ming},
  year = {2004},
  doi = {10.1007/978-3-540-27801-6_10},
  abstract = {No abstract available},
  citationcount = {25},
  venue = {Annual Symposium on Combinatorial Pattern Matching}
}

@article{jiazhencaiUsingMultisetDiscrimination1995,
  title = {Using {{Multiset Discrimination}} to {{Solve Language Processing Problems Without Hashing}}},
  author = {Jiazhen Cai and R. Paige},
  year = {1995},
  journal = {Theoretical Computer Science},
  doi = {10.1016/0304-3975(94)00183-J},
  annotation = {Citation Count: 53}
}

@article{jinghuixueEffectiveBranchandBoundAlgorithm2024,
  title = {An {{Effective Branch-and-Bound Algorithm}} with {{New Bounding Methods}} for the {{Maximum}} s-{{Bundle Problem}}},
  author = {Jinghui Xue and Jiongzhi Zheng and Mingming Jin and Kun He},
  year = {2024},
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2402.03736},
  abstract = {The Maximum s-Bundle Problem (MBP) addresses the task of identifying a maximum s-bundle in a given graph. A graph G=(V, E) is called an s-bundle if its vertex connectivity is at least {\textbar}V{\textbar}-s, where the vertex connectivity equals the minimum number of vertices whose deletion yields a disconnected or trivial graph. MBP is NP-hard and holds relevance in numerous realworld scenarios emphasizing the vertex connectivity. Exact algorithms for MBP mainly follow the branch-and-bound (BnB) framework, whose performance heavily depends on the quality of the upper bound on the cardinality of a maximum s-bundle and the initial lower bound with graph reduction. In this work, we introduce a novel Partition-based Upper Bound (PUB) that leverages the graph partitioning technique to achieve a tighter upper bound compared to existing ones. To increase the lower bound, we propose to do short random walks on a clique to generate larger initial solutions. Then, we propose a new BnB algorithm that uses the initial lower bound and PUB in preprocessing for graph reduction, and uses PUB in the BnB search process for branch pruning. Extensive experiments with diverse s values demonstrate the significant progress of our algorithm over state-of-the-art BnB MBP algorithms. Moreover, our initial lower bound can also be generalized to other relaxation clique problems.},
  keywords = {lower bound,reduction},
  annotation = {Citation Count: 0}
}

@article{jinListing6Cycles2023,
  title = {Listing 6-Cycles},
  author = {Jin, Ce and Williams, Virginia Vassilevska and Zhou, Renfei},
  year = {2023},
  doi = {10.48550/arXiv.2310.14575},
  abstract = {Listing copies of small subgraphs (such as triangles, 4-cycles, small cliques) in the input graph is an important and well-studied problem in algorithmic graph theory. In this paper, we give a simple algorithm that lists t (non-induced) 6-cycles in an n-node undirected graph in (n{$^2$}+t) time. This nearly matches the fastest known algorithm for detecting a 6-cycle in O(n{$^2$}) time by Yuster and Zwick (1997). Previously, a folklore O(n{$^2$}+t)-time algorithm was known for the task of listing 4-cycles.},
  citationcount = {3},
  venue = {SIAM Symposium on Simplicity in Algorithms}
}

@article{jinRemovingAdditiveStructure2022,
  title = {Removing Additive Structure in {{3SUM-based}} Reductions},
  author = {Jin, Ce and Xu, Yinzhan},
  year = {2022},
  doi = {10.1145/3564246.3585157},
  abstract = {Our work explores the hardness of 3SUM instances without certain additive structures, and its applications. As our main technical result, we show that solving 3SUM on a size-n integer set that avoids solutions to a+b=c+d for \{a, b\} {$\neq$} \{c, d\} still requires n2-o(1) time, under the 3SUM hypothesis. Such sets are called Sidon sets and are well-studied in the field of additive combinatorics. Combined with previous reductions, this implies that the All-Edges Sparse Triangle problem on n-vertex graphs with maximum degree {\textsurd}n and at most nk/2 k-cycles for every k {$\geq$} 3 requires n2-o(1) time, under the 3SUM hypothesis. This can be used to strengthen the previous conditional lower bounds by Abboud, Bringmann, Khoury, and Zamir [STOC'22] of 4-Cycle Enumeration, Offline Approximate Distance Oracle and Approximate Dynamic Shortest Path. In particular, we show that no algorithm for the 4-Cycle Enumeration problem on n-vertex m-edge graphs with no(1) delays has O(n2-{$\varepsilon$}) or O(m4/3-{$\varepsilon$}) pre-processing time for {$\varepsilon$} {\textquestiondown}0. We also present a matching upper bound via simple modifications of the known algorithms for 4-Cycle Detection. A slight generalization of the main result also extends the result of Dudek, Gawrychowski, and Starikovskaya [STOC'20] on the 3SUM hardness of nontrivial 3-Variate Linear Degeneracy Testing (3-LDTs): we show 3SUM hardness for all nontrivial 4-LDTs. The proof of our main technical result combines a wide range of tools: Balog-Szemer{\'e}di-Gowers theorem, sparse convolution algorithm, and a new almost-linear hash function with almost 3-universal guarantee for integers that do not have small-coefficient linear relations.},
  citationcount = {24},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,lower bound,reduction}
}

@article{jinTightDynamicProblem2022,
  title = {Tight Dynamic Problem Lower Bounds from Generalized {{BMM}} and {{OMv}}},
  author = {Jin, Ce and Xu, Yinzhan},
  year = {2022},
  doi = {10.1145/3519935.3520036},
  abstract = {Popular fine-grained hypotheses have been successful in proving conditional lower bounds for many dynamic problems. Two of the most widely applicable hypotheses in this context are the combinatorial Boolean Matrix Multiplication (BMM) hypothesis and the closely-related Online Matrix Vector Multiplication (OMv) hypothesis. The main theme of this paper is using k-dimensional generalizations of these two hypotheses to prove new tight conditional lower bounds for dynamic problems. The combinatorial k-Clique hypothesis, which is a standard hypothesis in the literature, naturally generalizes the combinatorial BMM hypothesis. In this paper, we prove tight lower bounds for several dynamic problems under the combinatorial k-Clique hypothesis. For instance, we show that the Dynamic Range Mode problem has no combinatorial algorithms with poly(n) pre-processing time, O(n2/3-{\cyrchar\cyrie}) update time and O(n2/3-{\cyrchar\cyrie}) query time for any {\cyrchar\cyrie} {\textquestiondown} 0, matching the known upper bounds for this problem. Previous lower bounds only ruled out algorithms with O(n1/2-{\cyrchar\cyrie}) update and query time under the OMv hypothesis. We also show that the Dynamic Subgraph Connectivity problem on undirected graphs with m edges has no combinatorial algorithms with poly(m) pre-processing time, O(m2/3-{\cyrchar\cyrie}) update time and O(m1-{\cyrchar\cyrie}) query time for {\cyrchar\cyrie} {\textquestiondown} 0, matching the upper bound given by Chan, P{\u a}tra{\c s}cu, and Roditty [SICOMP'11], and improving the previous update time lower bound (based on OMv) with exponent 1/2. Other examples include tight combinatorial lower bounds for Dynamic 2D Orthogonal Range Color Counting, Dynamic 2-Pattern Document Retrieval, and Dynamic Range Mode in higher dimensions. Furthermore, we propose the OuMvk hypothesis as a natural generalization of the OMv hypothesis. Under this hypothesis, we prove tight lower bounds for various dynamic problems. For instance, we show that the Dynamic Skyline Points Counting problem in (2k-1)-dimensional space has no algorithm with poly(n) pre-processing time and O(n1-1/k-{\cyrchar\cyrie}) update and query time for {\cyrchar\cyrie} {\textquestiondown} 0, even if the updates are semi-online. Other examples include tight conditional lower bounds for (semi-online) Dynamic Klee's measure for unit cubes, and high-dimensional generalizations of Erickson's problem and Langerman's problem.},
  citationcount = {14},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,lower bound,query,query time,update,update time}
}

@article{joannouDynamizingSuccinctTree2012,
  title = {Dynamizing Succinct Tree Representations},
  author = {Joannou, S. and Raman, R.},
  year = {2012},
  doi = {10.1007/978-3-642-30850-5_20},
  abstract = {No abstract available},
  citationcount = {20},
  venue = {The Sea}
}

@article{joCompressedBitVectors2014,
  title = {Compressed Bit Vectors Based on Variable-to-Fixed Encodings},
  author = {Jo, Seungbum and Joannou, S. and Okanohara, Daisuke and Raman, R. and Rao, S.},
  year = {2014},
  doi = {10.1109/DCC.2014.85},
  abstract = {Summary form only given. We consider practical implementations of compressed bit vectors, which support rank and select operations on a given bit-string, while storing the bit-string in compressed form. Our approach relies on variable-to-fixed (V2F) encodings of the bit-string, an approach that has not yet been considered systematically for practical encodings of bit-vectors. This approach leads to fast practical implementations with low redundancy (i.e., the space used by the bit vector in addition to the compressed representation of the bit-string),and is a flexible and promising solution to the problem of supporting rank and select on moderately compressible bit-strings, such as those frequently found in real-world applications.},
  citationcount = {4},
  venue = {Data Compression Conference}
}

@article{joCompressedRangeMinimum2018,
  title = {Compressed Range Minimum Queries},
  author = {Jo, Seungbum and Mozes, S. and Weimann, Oren},
  year = {2018},
  doi = {10.1007/978-3-030-00479-8_17},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {SPIRE}
}

@article{johannsenDepthLowerBounds2001,
  title = {Depth Lower Bounds for Monotone Semi-Unbounded Fan-in Circuits},
  author = {Johannsen, Jan},
  year = {2001},
  doi = {10.1051/ita:2001120},
  abstract = {The depth hierarchy results for monotone circuits of Raz and McKenzie [5] are extended to the case of monotone circuits of semi-unbounded fan-in. It follows that the inclusions NC i {$\subseteq$} SAC i {$\subseteq$} AC i are proper in the monotone setting, for every i {$\geq$} 1.},
  citationcount = {11},
  venue = {RAIRO - Theoretical Informatics and Applications}
}

@article{johnsonANewUpper1962,
  title = {A New Upper Bound for Error-Correcting Codes},
  author = {Johnson, Selmer M.},
  year = {1962},
  doi = {10.1109/TIT.1962.1057714},
  abstract = {By refining Hamming's geometric sphere-packing model a new upper bound for nonsystematic binary error-correcting codes is found. Only combinatorial arguments are used. Whereas Hamming's upper bound estimate for e -error-correcting codes involved a count of all points {$\leq$}e Hamming distance from the set of code points, the model is extended here to include consideration of points which are {\textquestiondown}e distance away from the code set. The percentage improvement from Hamming's bounds is sometimes quite sizable for cases of two or more errors to be corrected. The new bound improves on Wax's bounds in all but four of the cases he lists.},
  citationcount = {450},
  venue = {IRE Transactions on Information Theory}
}

@article{johnsonExtensionsOfLipschitz1984,
  title = {Extensions of Lipschitz Mappings into Hilbert Space},
  author = {Johnson, W. and Lindenstrauss, J.},
  year = {1984},
  doi = {10.1090/conm/026/737400},
  abstract = {(Here ll\&lltip is the Lipschitz constant of the function g.) A classical result of Kirszbraun's [14, p. 48] states that L(t2, n) = 1 for all n, but it is easy to see that L(X, n)     as n     for many metric spaces X. Marcus and Pisier [10] initiated the study of L(X, n) for X = Lp. (For brevity, we will use hereafter the notation L(p, n) for L(Lp(O,l), n).) They prove that for each 1 {\textexclamdown} p {\textexclamdown} 2 there is a constant C(p) so that for n = 2, 3, 4, , , ,},
  citationcount = {2915},
  venue = {No venue available}
}

@inproceedings{jorgensenRangeSelectionMedian2011,
  title = {Range {{Selection}} and {{Median}}: {{Tight Cell Probe Lower Bounds}} and {{Adaptive Data Structures}}},
  shorttitle = {Range {{Selection}} and {{Median}}},
  booktitle = {Proceedings of the 2011 {{Annual ACM-SIAM Symposium}} on {{Discrete Algorithms}} ({{SODA}})},
  author = {J{\o}rgensen, Allan Gr{\o}nlund and Larsen, Kasper Green},
  year = {2011},
  month = jan,
  series = {Proceedings},
  pages = {805--813},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611973082.63},
  url = {https://epubs.siam.org/doi/10.1137/1.9781611973082.63},
  urldate = {2025-04-20},
  abstract = {Range selection is the problem of preprocessing an input array A of n unique integers, such that given a query (i, j, k), one can report the k'th smallest integer in the subarray A[i], A[i + 1], {\dots}, A[j]. In this paper we consider static data structures in the word-RAM for range selection and several natural special cases thereof.The first special case is known as range median, which arises when k is fixed to {$\lfloor$}(j - i + 1)/2{$\rfloor$}. The second case, denoted prefix selection, arises when i is fixed to 0. Finally, we also consider the bounded rank prefix selection problem and the fixed rank range selection problem. In the former, data structures must support prefix selection queries under the assumption that k {$\leq$} {$\kappa$} for some value {$\kappa$} {$\leq$} n given at construction time, while in the latter, data structures must support range selection queries where k is fixed beforehand for all queries. We prove cell probe lower bounds for range selection, prefix selection and range median, stating that any data structure that uses S words of space needs {\textohm}(log n/log(Sw/n)) time to answer a query. In particular, any data structure that uses n logO(1) n space needs {\textohm}(log n/ log log n) time to answer a query, and any data structure that supports queries in constant time, needs n1+{\textohm}(1) space. For data structures that uses n logO(1) n space this matches the best known upper bound.Additionally, we present a linear space data structure that supports range selection queries in O(log k/log log n + log log n) time. Finally, we prove that any data structure that uses S space, needs {\textohm}(log {$\kappa$}/log(Sw/n)) time to answer a bounded rank prefix selection query and {\textohm}(log k/log(Sw/n)) time to answer a fixed rank range selection query. This shows that our data structure is optimal except for small values of k.},
  isbn = {978-0-89871-993-2},
  keywords = {adaptive,cell probe,data structure,lower bound,query,static},
  file = {/Users/tulasi/Zotero/storage/STYF3QT4/Jrgensen and Larsen - 2011 - Range Selection and Median Tight Cell Probe Lower Bounds and Adaptive Data Structures.pdf}
}

@article{jrgensenThreesomesDegeneratesAnd2014,
  title = {Threesomes, Degenerates, and Love Triangles},
  author = {J{\o}rgensen, A. and Pettie, Seth},
  year = {2014},
  doi = {10.1145/3185378},
  abstract = {The 3SUM problem is to decide, given a set of n real numbers, whether any three sum to zero. It is widely conjectured that a trivial O(n{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown})-time algorithm is optimal and over the years the consequences of this conjecture have been revealed. This 3SUM conjecture implies {\textohm}(n{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}) lower bounds on numerous problems in computational geometry and a variant of the conjecture implies strong lower bounds on triangle enumeration, dynamic graph algorithms, and string matching data structures. In this paper we refute the 3SUM conjecture. We prove that the decision tree complexity of 3SUM is O(n{\textexclamdown}sup{\textquestiondown}3/2{\textexclamdown}/sup{\textquestiondown} {\textsurd}/log n) and give two subquadratic 3SUM algorithms, a deterministic one running in O(n{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}/(log n/ log log n){\textexclamdown}sup{\textquestiondown}2/3{\textexclamdown}/sup{\textquestiondown}) time and a randomized one running in O(n{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} (log log n){\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}/ log n) time with high probability. Our results lead directly to improved bounds for k-variate linear degeneracy testing for all odd k {$\geq$} 3. The problem is to decide, given a linear function f(x{\textexclamdown}sub{\textquestiondown}1{\textexclamdown}/sub{\textquestiondown}, ... , x{\textexclamdown}sub{\textquestiondown}k{\textexclamdown}/sub{\textquestiondown}) = {$\alpha$}{\textexclamdown}sub{\textquestiondown}0{\textexclamdown}/sub{\textquestiondown} + {$\Sigma$}{\textexclamdown}sub{\textquestiondown}1{$\leq$}1{$\leq$}k{\textexclamdown}/sub{\textquestiondown} {$\alpha$}{\textexclamdown}sub{\textquestiondown}i{\textexclamdown}/sub{\textquestiondown}x{\textexclamdown}sub{\textquestiondown}i{\textexclamdown}/sub{\textquestiondown} and a set A {$\subset$} {$\mathbb{R}$}, whether 0 {$\in$} f(A{\textexclamdown}sup{\textquestiondown}k{\textexclamdown}/sup{\textquestiondown}). We show the decision tree complexity of this problem is O(n{\textexclamdown}sup{\textquestiondown}k/2{\textexclamdown}/sup{\textquestiondown} {\textsurd}log n). Finally, we give a subcubic algorithm for a generalization of the (min, +)-product over real-valued matrices and apply it to the problem of finding zero-weight triangles in weighted graphs. We give a depth-O(n{\textexclamdown}sup{\textquestiondown}5/2{\textexclamdown}/sup{\textquestiondown} {\textsurd}log n) decision tree for this problem, as well as an algorithm running in time O(n{\textexclamdown}sup{\textquestiondown}3{\textexclamdown}/sup{\textquestiondown}(log log n)2/ log n).},
  citationcount = {88},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,dynamic,lower bound}
}

@article{jorgensenTightLowerBounds2014,
  title = {Towards Tight Lower Bounds for Range Reporting on the {{RAM}}},
  author = {J{\o}rgensen, A. and Larsen, Kasper Green},
  year = {2014},
  doi = {10.4230/LIPIcs.ICALP.2016.92},
  abstract = {In the orthogonal range reporting problem, we are to preprocess a set of n points with integer coordinates on a U{\texttimes}U grid. The goal is to support reporting all k points inside an axis-aligned query rectangle. This is one of the most fundamental data structure problems in databases and computational geometry. Despite the importance of the problem its complexity remains unresolved in the word-RAM. On the upper bound side, three best tradeoffs exists: (1.) Query time O(n+k) with O(nlg\textsuperscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}n) words of space for any constant {$\varepsilon>$}0. (2.) Query time O((1+k)n) with O(nn) words of space. (3.) Query time O((1+k)\textsuperscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}n) with optimal O(n) words of space. However, the only known query time lower bound is {\textohm}(n+k), even for linear space data structures. All three current best upper bound tradeoffs are derived by reducing range reporting to a ball-inheritance problem. Ball-inheritance is a problem that essentially encapsulates all previous attempts at solving range reporting in the word-RAM. In this paper we make progress towards closing the gap between the upper and lower bounds for range reporting by proving cell probe lower bounds for ball-inheritance. Our lower bounds are tight for a large range of parameters, excluding any further progress for range reporting using the ball-inheritance reduction.},
  citationcount = {3},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {cell probe,data structure,lower bound,query,query time,reduction}
}

@article{josephy.halpernTimeSpaceIII1986,
  title = {On Time versus Space {{III}}},
  author = {Joseph Y. Halpern and M. Loui and A. Meyer and D. Weise},
  year = {1986},
  journal = {Mathematical Systems Theory},
  doi = {10.1007/BF01704903},
  annotation = {Citation Count: 5}
}

@article{juganHighPerformanceDensityBased2017,
  title = {High {{Performance Density-Based Clustering}} on {{Massive Data}}},
  author = {Ju Gan},
  year = {2017},
  doi = {10.14264/UQL.2017.648},
  annotation = {Citation Count: 0}
}

@article{juknaBooleanFunctionComplexity2012,
  title = {Boolean Function Complexity Advances and Frontiers},
  author = {Jukna, S.},
  year = {2012},
  doi = {10.1007/978-3-642-24508-4},
  abstract = {No abstract available},
  citationcount = {442},
  venue = {Bull. EATCS}
}

@article{juknaEntropyOperatorsWhy2010,
  title = {Entropy of Operators or Why Matrix Multiplication Is Hard for Depth-Two Circuits},
  author = {Jukna, Stasys},
  year = {2010},
  journal = {Theory of Computing Systems},
  volume = {46},
  number = {2},
  pages = {301--310},
  doi = {10.1007/s00224-008-9133-y},
  annotation = {Alon, N., Karchmer, M., Wigderson, A.: Linear circuits over GF(2). SIAM. J. Comput. 19(6), 1064--1067 (1990)\\
Alon, N., Pudl{\'a}k, P.: Superconcentrators of depth 2 and 3; odd levels help (rarely). J. Comput. Syst. Sci. 48, 194--202 (1994)\\
Beigel, R., Tarui, J.: On ACC. Comput. Complex. 4, 350--366 (1994)\\
Bshouty, N.H.: A lower bound for matrix multiplication. SIAM J. Comput. 18, 759--765 (1982)\\
B{\"u}rgisser, P., Lotz, M.: Lower bounds on the bounded coefficient complexity of bilinear maps. J.~ACM 51(3), 464--482 (2004)\\
Cherukhin, D.Y.: The lower estimate of complexity in the class of schemes of depth 2 without restrictions on a basis. Moscow University Math. Bull. 60(4), 42--44 (2005)\\
Coppersmith, D., Winograd, S.: Matrix multiplications via arithmetic progressions. J. Symb. Comput. 9, 251--280 (1990)\\
Dolev, D., Dwork, C., Pippenger, N., Wigderson, A.: Superconcentrators, generalizer and generalized connectors with limited depth. In: Proc. of the 15th STOC, pp.~42--51 (1983)\\
Friedman, J.: A note on matrix rigidity. Combinatorica 13, 235--239 (1993)\\
Jukna, S.: On set intersection representations of graphs. J. Graph Theory (submitted)\\
Morgenstern, J.: Note on a lower bound on the linear complexity of fast Fourier transform. J.~ACM 20(2), 305--306 (1973)\\
Morgenstern, J.: The linear complexity of computation. J.~ACM 22(2), 184--194 (1975)\\
Nechiporuk, E.I.: On a Boolean function. Sov. Math. Dokl. 7(4), 999--1000 (1966)\\
Pippenger, N.: Superconcentrators. SIAM J. Comput. 6, 298--304 (1977)\\
Pippenger, N.: Superconcentrators of depth 2. J. Comput. Syst. Sci. 24, 82--90 (1982)\\
Pudl{\'a}k, P.: Communication in bounded depth circuits. Combinatorica 14(2), 203--216 (1994)\\
Pudl{\'a}k, P.: A note on the use of determinant for proving lower bounds on the size of linear circuits. Inf. Process. Lett. 74, 197--201 (2000)\\
Pudl{\'a}k, P., R{\"o}dl, V.: Some combinatorial-algebraic problems from complexity theory. Discrete Math. 136, 253--279 (1994)\\
Pudl{\'a}k, P., R{\"o}dl, V., Sgall, J.: Boolean circuits, tensor ranks, and communication complexity. SIAM J. Comput. 26(3), 605--633 (1997)\\
Pudl{\'a}k, P., Savick{\'y}, P.: On shifting networks. Theor. Comput. Sci. 116, 415--419 (1993)\\
Radhakrishnan, J., Ta-Shma, A.: Bounds for dispersers, extractors, and depth-two superconcentrators. SIAM J. Discrete Math. 13(1), 2--24 (2000)\\
Raz, R., Shpilka, A.: Lower bounds for matrix product in bounded depth circuits with arbitrary gates. SIAM J. Comput. 32(2), 488--513 (2003)\\
Shoup, V., Smolensky, R.: Lower bounds for polynomial evaluation and interpolation problems. Comput. Complex. 6(4), 301--311 (1997)\\
Strassen, V.: Die Berechnungskomplexit{\"a}t von elementarsymmetrischen Funktionen und von Interpoliationskoefizienten. Numer. Math. 20, 238--251 (1973)\\
Valiant, L.: Graph-theoretic methods in low-level complexity. In: Proc. of the 6th MFCS. Lecture Notes in Computer Science, vol.~53, pp.~162--176. Springer, Berlin (1977)\\
Yao, A.C.: On ACC and threshold circuits. In: Proc. of the 31th FOCS, pp.~619--627 (1990)},
  file = {/Users/tulasi/Zotero/storage/3TJJ6WFC/Jukna - 2010 - Entropy of operators or why matrix multiplication is hard for depth-two circuits.pdf}
}

@article{juknaOnSetIntersection2009,
  title = {On Set Intersection Representations of Graphs},
  author = {Jukna, S.},
  year = {2009},
  doi = {10.1002/jgt.20367},
  abstract = {The intersection dimension of a bipartite graph with respect to a type L is the smallest number t for which it is possible to assign sets Ax{$\subseteq$}\{1, {\dots}, t\} of labels to vertices x so that any two vertices x and y from different parts are adjacent if and only if {\textbar}Ax{$\cap$}Ay{\textbar}{$\in$}L. The weight of such a representation is the sum {$\Sigma$}x{\textbar}Ax{\textbar} over all vertices x. We exhibit explicit bipartite n {\texttimes} n graphs whose intersection dimension is (i) at least n1/{\textbar}L{\textbar} with respect to any type L, (ii) at least {\textsurd}\{n\} with respect to any type of the form L=\{k, k+ 1, {\dots}\}, and (iii) at least n1/{\textbar}R{\textbar} with respect to any type of the form L=\{k{\textbar}k\,modp{$\in$}R\}, where p is a prime number. We also show that any intersection representation of a Hadamard graph must have weight about n\,lnn/ln\,lnn, independent on the used type L. Finally, we formulate several problems about intersection dimensions of graphs related to some basic open problems in the complexity of boolean functions. {\copyright} 2009 Wiley Periodicals, Inc. J Graph Theory 61: 55-75, 2009},
  citationcount = {15},
  venue = {Journal of Graph Theory}
}

@article{juliachuzhoyNewConjectureHardness2023,
  title = {A {{New Conjecture}} on {{Hardness}} of 2-{{CSP}}'s with {{Implications}} to {{Hardness}} of {{Densest}} k-{{Subgraph}} and {{Other Problems}}},
  author = {Julia Chuzhoy and Mina Dalirrooyfard and Vadim Grinberg and Zihan Tan},
  year = {2023},
  journal = {Information Technology Convergence and Services},
  doi = {10.4230/LIPIcs.ITCS.2023.38},
  abstract = {We propose a new conjecture on hardness of 2-CSP's, and show that new hardness of approximation results for Densest k -Subgraph and several other problems, including a graph partitioning problem, and a variation of the Graph Crossing Number problem, follow from this conjecture. The conjecture can be viewed as occupying a middle ground between the d -to-1 conjecture, and hardness results for 2-CSP's that can be obtained via standard techniques, such as Parallel Repetition combined with standard 2-prover protocols for the 3SAT problem. We hope that this work will motivate further exploration of hardness of 2-CSP's in the regimes arising from the conjecture. We believe that a positive resolution of the conjecture will provide a good starting point for other hardness of approximation proofs. Another contribution of our work is proving that the problems that we consider are roughly equivalent from the approximation perspective. Some of these problems arose in previous work, from which it appeared that they may be related to each other. We formalize this relationship in this work.},
  annotation = {Citation Count: 1}
}

@article{junhaoganHardnessApproximationEuclidean2017,
  title = {On the {{Hardness}} and {{Approximation}} of {{Euclidean DBSCAN}}},
  author = {Junhao Gan and Yufei Tao},
  year = {2017},
  journal = {ACM Transactions on Database Systems},
  doi = {10.1145/3083897},
  abstract = {DBSCAN is a method proposed in 1996 for clustering multi-dimensional points, and has received extensive applications. Its computational hardness is still unsolved to this date. The original KDD{\quotesinglbase}96 paper claimed an algorithm of O(n log n) ''average runtime complexity,, (where n is the number of data points) without a rigorous proof. In 2013, a genuine O(n log n)-time algorithm was found in 2D space under Euclidean distance. The hardness of dimensionality d {$\geq$}3 has remained open ever since. This article considers the problem of computing DBSCAN clusters from scratch (assuming no existing indexes) under Euclidean distance. We prove that, for d {$\geq$}3, the problem requires {$\omega$}(n 4/3) time to solve, unless very significant breakthroughs---ones widely believed to be impossible---could be made in theoretical computer science. Motivated by this, we propose a relaxed version of the problem called {$\rho$}-approximate DBSCAN, which returns the same clusters as DBSCAN, unless the clusters are ''unstable,, (i.e., they change once the input parameters are slightly perturbed). The {$\rho$}-approximate problem can be settled in O(n) expected time regardless of the constant dimensionality d. The article also enhances the previous result on the exact DBSCAN problem in 2D space. We show that, if the n data points have been pre-sorted on each dimension (i.e., one sorted list per dimension), the problem can be settled in O(n) worst-case time. As a corollary, when all the coordinates are integers, the 2D DBSCAN problem can be solved in O(n log log n) time deterministically, improving the existing O(n log n) bound.},
  annotation = {Citation Count: 63}
}

@article{juniorFullyRetroactiveMinimum2020,
  title = {Fully Retroactive Minimum Spanning Tree Problem},
  author = {{de Andrade Junior}, JoseWagner and Seabra, R. D.},
  year = {2020},
  doi = {10.1093/comjnl/bxaa135},
  abstract = {This article describes an algorithm that solves a fully dynamic variant of the minimum spanning tree (MST) problem. The fully retroactive MST allows adding an edge to time t, or to obtain the current MST at time t. By using the square root technique and a data structure link-cut tree, it was possible to obtain an algorithm that runs each query in O({\textsurd}\{m\}\{{\textbar}V(G){\textbar}\}) amortized, in which {\textbar}V(G){\textbar} is the number of nodes in graph G and m is the size of the timeline. We use a different approach to solve the MST problem instead of the standard algorithms, such as Prim or Kruskal, and this allows using the square root technique to improve the final complexity of the algorithm. Our empirical analysis shows that the proposed algorithm runs faster than re-executing the standard algorithms, and this difference only increases when the number of nodes in these graphs is larger.},
  citationcount = {1},
  venue = {Computer/law journal},
  keywords = {data structure,dynamic,query}
}

@article{justesenClassOfConstructive1972,
  title = {Class of Constructive Asymptotically Good Algebraic Codes},
  author = {Justesen, J.},
  year = {1972},
  doi = {10.1109/TIT.1972.1054893},
  abstract = {For any rate R, 0 , a sequence of specific (n,k) binary codes with rate R\_n {\textquestiondown} R and minimum distance d is constructed such that \{equation\} \_\{n {$\rightarrow\infty$}\} \textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{d}\vphantom\{\}\{n\} {$\geq$}(1 - r {\textasciicircum}\{-1\} R)H{\textasciicircum}\{-1\} (1 - r){\textquestiondown} 0 \{equation\} (and hence the codes are asymptotically good), where r is the maximum of \textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{2\} and the solution of \{equation\} R = \textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{r}{\textasciicircum}2\vphantom\{\}\{1 + \_2 [1 - H{\textasciicircum}\{-1\}(1 - r)]\}. \{equation\} The codes are extensions of the Reed-Solomon codes over GF(2{\textasciicircum}m) With a simple algebraic description of the added digits. Alternatively, the codes are the concatenation of a Reed-Solomon outer code of length N = 2{\textasciicircum}m - 1 with N distinct inner codes, namely all the codes in Wozeneraft's ensemble of randomly shifted codes. A decoding procedure is given that corrects all errors guaranteed correctable by the asymptotic lower bound on d . This procedure can be carried out by a simple decoder which performs approximately n{\textasciicircum}2 n computations.},
  citationcount = {327},
  venue = {IEEE Transactions on Information Theory}
}

@article{k.chatterjeeOptimalDyckReachability2017,
  title = {Optimal {{Dyck}} Reachability for Data-Dependence and Alias Analysis},
  author = {K. Chatterjee and Bhavya Choudhary and Andreas Pavlogiannis},
  year = {2017},
  journal = {Proc. ACM Program. Lang.},
  doi = {10.1145/3158118},
  abstract = {A fundamental algorithmic problem at the heart of static analysis is Dyck reachability. The input is a graph where the edges are labeled with different types of opening and closing parentheses, and the reachability information is computed via paths whose parentheses are properly matched. We present new results for Dyck reachability problems with applications to alias analysis and data-dependence analysis. Our main contributions, that include improved upper bounds as well as lower bounds that establish optimality guarantees, are as follows: First, we consider Dyck reachability on bidirected graphs, which is the standard way of performing field-sensitive points-to analysis. Given a bidirected graph with n nodes and m edges, we present: (i) an algorithm with worst-case running time O(m + n {$\cdot$} {$\alpha$}(n)), where {$\alpha$}(n) is the inverse Ackermann function, improving the previously known O(n2) time bound; (ii) a matching lower bound that shows that our algorithm is optimal wrt to worst-case complexity; and (iii) an optimal average-case upper bound of O(m) time, improving the previously known O(m {$\cdot$} logn) bound. Second, we consider the problem of context-sensitive data-dependence analysis, where the task is to obtain analysis summaries of library code in the presence of callbacks. Our algorithm preprocesses libraries in almost linear time, after which the contribution of the library in the complexity of the client analysis is only linear, and only wrt the number of call sites. Third, we prove that combinatorial algorithms for Dyck reachability on general graphs with truly sub-cubic bounds cannot be obtained without obtaining sub-cubic combinatorial algorithms for Boolean Matrix Multiplication, which is a long-standing open problem. Thus we establish that the existing combinatorial algorithms for Dyck reachability are (conditionally) optimal for general graphs. We also show that the same hardness holds for graphs of constant treewidth. Finally, we provide a prototype implementation of our algorithms for both alias analysis and data-dependence analysis. Our experimental evaluation demonstrates that the new algorithms significantly outperform all existing methods on the two problems, over real-world benchmarks.},
  keywords = {lower bound,static},
  annotation = {Citation Count: 57}
}

@article{k.kanthOptimalDynamicRange1999,
  title = {Optimal {{Dynamic Range Searching}} in {{Non-replicating Index Structures}}},
  author = {K. Kanth and Ambuj K. Singh},
  year = {1999},
  journal = {International Conference on Database Theory},
  doi = {10.1007/3-540-49257-7_17},
  annotation = {Citation Count: 83}
}

@article{k.mehlhornCHAPTER6Data1990,
  title = {{{CHAPTER}} 6 -- {{Data Structures}}},
  author = {K. Mehlhorn},
  year = {1990},
  doi = {10.1016/B978-0-444-88071-0.50011-4},
  keywords = {data structure},
  annotation = {Citation Count: 13}
}

@article{k.mehlhornLowerBoundComplexity1987,
  title = {A {{Lower Bound}} for the {{Complexity}} of the {{Union-Split-Find Problem}}},
  author = {K. Mehlhorn and S. N{\"a}her and H. Alt},
  year = {1987},
  journal = {International Colloquium on Automata, Languages and Programming},
  doi = {10.1007/3-540-18088-5_41},
  annotation = {Citation Count: 33}
}

@article{k.mehlhornLowerBoundComplexity1988,
  title = {A {{Lower Bound}} on the {{Complexity}} of the {{Union-Split-Find Problem}}},
  author = {K. Mehlhorn and S. N{\"a}her and H. Alt},
  year = {1988},
  journal = {SIAM journal on computing (Print)},
  doi = {10.1137/0217070},
  abstract = {We prove a \${\textbackslash}Theta ({\textbackslash}log {\textbackslash}log n)\$ (i.e., matching upper and lower) bound on the complexity of the Union-Split-Find problem, a variant of the Union-Find problem. Our lower bound holds for all pointer machine algorithms and does not require the separation assumption used in the lower-bound arguments of Tarjan [J. Comput. Systems Sci., 18 (1979), pp. 110--127] and Blum [SIAM J. Comput., 15 (1986), pp. 1021--1024]. We complement this with a \${\textbackslash}Theta ({\textbackslash}log n)\$ bound for the Split-Find problem under the separation assumption. This shows that the separation assumption can imply an exponential loss in efficiency.},
  annotation = {Citation Count: 31}
}

@article{kabanetsDerandomizingPolynomialIdentity2003,
  title = {Derandomizing Polynomial Identity Tests Means Proving Circuit Lower Bounds},
  author = {Kabanets, Valentine and Impagliazzo, R.},
  year = {2003},
  doi = {10.1007/s00037-004-0182-6},
  abstract = {No abstract available},
  citationcount = {549},
  venue = {Symposium on the Theory of Computing}
}

@article{kahalEigenvaluesAndExpansion1995,
  title = {Eigenvalues and Expansion of Regular Graphs},
  author = {Kahal{\'e}, N.},
  year = {1995},
  doi = {10.1145/210118.210136},
  abstract = {The spectral method is the best currently known technique to prove lower bounds on expansion. Ramanujan graphs, which have asymptotically optimal second eigenvalue, are the best-known explicit expanders. The spectral method yielded a lower bound of {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}/4 on the expansion of linear-sized subsets of {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}-regular Ramanujan graphs. We improve the lower bound on the expansion of Ramanujan graphs to approximately {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}/2. Moreover, we construct a family of {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}-regular graphs with asymptotically optimal second eigenvalue and linear expansion equal to {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}/2. This shows that {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}/2 is the best bound one can obtain using the second eigenvalue method. We also show an upper bound of roughly {\textexclamdown}inline-equation{\textquestiondown} {\textexclamdown}f{\textquestiondown} 1+{\textexclamdown}rad{\textquestiondown}{\textexclamdown}rcd{\textquestiondown}k-1{\textexclamdown}/rcd{\textquestiondown}{\textexclamdown}/rad{\textquestiondown}{\textexclamdown}/f{\textquestiondown} {\textexclamdown}/inline-equation{\textquestiondown} on the average degree of linear-sized induced subgraphs of Ramanujan graphs. This compares positively with the classical bound {\textexclamdown}inline-equation{\textquestiondown} {\textexclamdown}f{\textquestiondown} 2{\textexclamdown}rad{\textquestiondown}{\textexclamdown}rcd{\textquestiondown}k-1{\textexclamdown}/rcd{\textquestiondown}{\textexclamdown}/rad{\textquestiondown}{\textexclamdown}/f{\textquestiondown} {\textexclamdown}/inline-equation{\textquestiondown}. As a byproduct, we obtain improved results on random walks on expanders and construct selection networks (respectively, extrovert graphs) of smaller size (respectively, degree) than was previously known.},
  citationcount = {128},
  venue = {JACM}
}

@article{kaiduhrkopHeuristicAlgorithmsMaximum2018,
  title = {Heuristic Algorithms for the {{Maximum Colorful Subtree}} Problem},
  author = {Kai D{\"u}hrkop and Marie Lataretu and W. White and Sebastian B{\"o}cker},
  year = {2018},
  journal = {Workshop on Algorithms in Bioinformatics},
  doi = {10.4230/LIPIcs.WABI.2018.23},
  abstract = {In metabolomics, small molecules are structurally elucidated using tandem mass spectrometry (MS/MS); this resulted in the computational Maximum Colorful Subtree problem, which is NP-hard. Unfortunately, data from a single metabolite requires us to solve hundreds or thousands of instances of this problem; and in a single Liquid Chromatography MS/MS run, hundreds or thousands of metabolites are measured.  Here, we comprehensively evaluate the performance of several heuristic algorithms for the problem against an exact algorithm. We put particular emphasis on whether a heuristic is able to rank candidates such that the correct solution is ranked highly. We propose this "intermediate" evaluation because evaluating the approximating quality of heuristics is misleading: Even a slightly suboptimal solution can be structurally very different from the true solution. On the other hand, we cannot structurally evaluate against the ground truth, as this is unknown. We find that particularly one of the heuristics consistently ranks the correct solution in a favorable position. Integrating the heuristic into the analysis pipeline results in a speedup of 10-fold or more, without sacrificing accuracy.},
  annotation = {Citation Count: 7}
}

@article{kaleAnalysisAndAlgorithms2005,
  title = {Analysis and Algorithms for Content-Based Event Matching},
  author = {Kale, Satyen and Hazan, Elad and Cao, Fengyun and Singh, J.},
  year = {2005},
  doi = {10.1109/ICDCSW.2005.40},
  abstract = {Content-based event matching is an important problem in large-scale event-based publish/subscribe systems. However, open questions remain in analysis of its difficulty and evaluation of its solutions. This paper makes a few contributions toward analysis, evaluation and development of matching algorithms. First, based on a simplified yet generic model, we give a formal proof of hardness of matching problem by showing its equivalence to the notoriously hard Partial Match problem. Second, we compare two major existing matching approaches and show that counting-based algorithms are likely to be more computationally expensive than tree-based algorithms in this model. Third, we observe an important, prevalent characteristic of real-world publish/subscribe events, and develop a new matching algorithm called RAPIDMatch to exploit it. Finally, we propose a new metric for evaluation of matching algorithms. We analyze and evaluate RAPIDMatch using both the traditional and new metrics proposed. Results show that RAPIDMatch achieves large performance improvement over the tree-based algorithm under certain publish/subscribe scenarios.},
  citationcount = {49},
  venue = {25th IEEE International Conference on Distributed Computing Systems Workshops}
}

@article{kalorkotiAlgebraicComplexityTheory1999,
  title = {{{ALGEBRAIC COMPLEXITY THEORY}} (Grundlehren Der Mathematischen Wissenschaften 315)},
  author = {Kalorkoti, Kyriakos},
  year = {1999},
  doi = {10.1112/S0024609397323959},
  abstract = {No abstract available},
  citationcount = {150},
  venue = {No venue available}
}

@article{kaltofenSubquadraticTimeFactoring1995,
  title = {Subquadratic-Time Factoring of Polynomials over Finite Fields},
  author = {Kaltofen, E. and Shoup, V.},
  year = {1995},
  doi = {10.1145/225058.225166},
  abstract = {New probabilistic algorithms are presented for factoring univariate polynomials over finite fields. The algorithms factor a polynomial of degree n over a finite field of constant cardinality in time O(n 1.815 ). Previous algorithms required time {$\Theta$}(n 2+o(1) ). The new algorithms rely on fast matrix multiplication techniques. More generally, to factor a polynomial of degree n over the finite field F q with q elements, the algorithms use O(n 1.815 log q) arithmetic operations in F q . The new baby step/giant step techniques used in our algorithms also yield new fast practical algorithms at super-quadratic asymptotic running time, and subquadratic-time methods for manipulating normal bases of finite fields.},
  citationcount = {193},
  venue = {Symposium on the Theory of Computing}
}

@article{kalyanasundaramTheProbabilisticCommunication1992,
  title = {The Probabilistic Communication Complexity of Set Intersection},
  author = {Kalyanasundaram, B. and Schnitger, G.},
  year = {1992},
  doi = {10.1137/0405044},
  abstract = {It is shown that, for inputs of length n, the probabilistic (bounded error) communication complexity of set intersection is {$\Theta$}(n). Since set intersection can be recognized nondeterministic...},
  citationcount = {508},
  venue = {SIAM Journal on Discrete Mathematics},
  keywords = {communication,communication complexity}
}

@article{kanellakisIndexingForData1993,
  title = {Indexing for Data Models with Constraints and Classes (Extended Abstract)},
  author = {Kanellakis, P. and Ramaswamy, S. and Vengroff, Darren Erik and Vitter, J.},
  year = {1993},
  doi = {10.1145/153850.153884},
  abstract = {We examine I/O-efficient data structures that provide indexing support for new data models. The database languages of these models include concepts from constraint programming (e.g., relational tuples are generalized to conjunctions of constraints) and from object-oriented programming (e.g., objects are organized in class hierarchies). Let n be the size of the database, c the number of classes, B the secondary storage page size, and t the size of the output of a query. Indexing by one attribute in the constraint data model (for a fairly general type of constraints) is equivalent to external dynamic interval management, which is a special case of external dynamic 2-dimensional range searching. We present a semi-dynamic data structure for this problem which has optimal worst-case space O(n/B) pages and optimal query I/O time O(logBn+t/B) and has O(logBn+(log2Bn)/B) amortized insert I/O time. If the order of the insertions is random then the expected number of I/O operations needed to perform insertions is reduced to O(logBn). Indexing by one attribute and by class name in an object-oriented model, where objects are organized as a forest hierarchy of classes, is also a special case of external dynamic 2-dimensional range searching. Based on this observation we first identify a simple algorithm with good worst-case performance for the class indexing problem. Using the forest structure of the class hierarchy and techniques from the constraint indexing problem, we improve its query I/O time from O(log2c logBn + t/B) to O(logB + log2B).},
  citationcount = {15},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems}
}

@article{kanellakisIndexingForData1996,
  title = {Indexing for Data Models with Constraints and Classes},
  author = {Kanellakis, P. and Ramaswamy, S. and Vengroff, Darren Erik and Vitter, J.},
  year = {1996},
  doi = {10.1006/jcss.1996.0043},
  abstract = {We examine I/O-efficient data structures that provide indexing support for new data models. The database languages of these models include concepts from constraint programming (e.g., relational tuples are generalized to conjunctions of constraints) and from object-oriented programming (e.g., objects are organized in class hierarchies). Let n be the size of the database, c the number of classes, B the secondary storage page size, and t the size of the output of a query. Indexing by one attribute in the constraint data model (for a fairly general type of constraints) is equivalent to external dynamic interval management, which is a special case of external dynamic 2-dimensional range searching. We present a semi-dynamic data structure for this problem which has optimal worst-case space O(n/B) pages and optimal query I/O time O(\textsubscript{B}n+t/B) and has O(\textsubscript{B}n+({$^{2}$}\textsubscript{B}n)/B) amortized insert I/O time. If the order of the insertions is random then the expected number of I/O operations needed to perform insertions is reduced to O(\textsubscript{B}n). Indexing by one attribute and by class name in an object-oriented model, where objects are organized as a forest hierarchy of classes, is also a special case of external dynamic 2-dimensional range searching. Based on this observation we first identify a simple algorithm with good worst-case performance for the class indexing problem. Using the forest structure of the class hierarchy and techniques from the constraint indexing problem, we improve its query I/O time from O({$_2$}c\textsubscript{B}n+t/B) to O(\textsubscript{B}n+t/B+{$_2$}B).},
  citationcount = {173},
  venue = {Journal of computer and system sciences (Print)}
}

@article{kaneLocalityBoundsFor2024,
  title = {Locality Bounds for Sampling Hamming Slices},
  author = {Kane, Daniel M. and Ostuni, Anthony and Wu, Kewen},
  year = {2024},
  doi = {10.1145/3618260.3649670},
  abstract = {Spurred by the influential work of Viola (Journal of Computing 2012), the past decade has witnessed an active line of research into the complexity of (approximately) sampling distributions, in contrast to the traditional focus on the complexity of computing functions. We build upon and make explicit earlier implicit results of Viola to provide superconstant lower bounds on the locality of Boolean functions approximately sampling the uniform distribution over binary strings of particular Hamming weights, both exactly and modulo an integer, answering questions of Viola (Journal of Computing 2012) and Filmus, Leigh, Riazanov, and Sokolov (RANDOM 2023). Applications to data structure lower bounds and quantum-classical separations are discussed. This is an extended abstract. The full paper can be found at https://arxiv.org/abs/2402.14278.},
  citationcount = {1},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {data structure,lower bound}
}

@article{kaneNearOptimalLinear2019,
  title = {Near-Optimal Linear Decision Trees for k-{{SUM}} and Related Problems},
  author = {Kane, D. and Lovett, Shachar and Moran, S.},
  year = {2019},
  doi = {10.1145/3285953},
  abstract = {We construct near-optimal linear decision trees for a variety of decision problems in combinatorics and discrete geometry. For example, for any constant k, we construct linear decision trees that solve the k-SUM problem on n elements using O(n log2 n) linear queries. Moreover, the queries we use are comparison queries, which compare the sums of two k-subsets; when viewed as linear queries, comparison queries are 2k-sparse and have only \{ -1,0,1\} coefficients. We give similar constructions for sorting sumsets A+B and for solving the SUBSET-SUM problem, both with optimal number of queries, up to poly-logarithmic terms. Our constructions are based on the notion of ``inference dimension,'' recently introduced by the authors in the context of active classification with comparison queries. This can be viewed as another contribution to the fruitful link between machine learning and discrete geometry, which goes back to the discovery of the VC dimension.},
  citationcount = {14},
  venue = {Journal of the ACM},
  keywords = {query}
}

@article{kannanGamesOfFixed2005,
  title = {Games of Fixed Rank: A Hierarchy of Bimatrix Games},
  author = {Kannan, R. and Theobald, T.},
  year = {2005},
  doi = {10.1007/S00199-009-0436-2},
  abstract = {No abstract available},
  citationcount = {79},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{kanstantsinpashkovichLinearContractsSupermodular2024,
  title = {Linear {{Contracts}} for {{Supermodular Functions Based}} on {{Graphs}}},
  author = {Kanstantsin Pashkovich and Jacob Skitsko},
  year = {2024},
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2412.13290},
  abstract = {We study linear contracts for combinatorial problems in multi-agent settings. In this problem, a principal designs a linear contract with several agents, each of whom can decide to take a costly action or not. The principal observes only the outcome of the agents' collective actions, not the actions themselves, and obtains a reward from this outcome. Agents that take an action incur a cost, and so naturally agents require a fraction of the principal's reward as an incentive for taking their action. The principal needs to decide what fraction of their reward to give to each agent so that the principal's expected utility is maximized. Our focus is on the case when the agents are vertices in a graph and the principal's reward corresponds to the number of edges between agents who take their costly action. This case represents the natural scenario when an action of each agent complements actions of other agents though collaborations. Recently, Deo-Campo Vuong et.al. showed that for this problem it is impossible to provide any finite multiplicative approximation or additive FPTAS unless \${\textbackslash}mathcal\{P\} = {\textbackslash}mathcal\{NP\}\$. On a positive note, the authors provided an additive PTAS for the case when all agents have the same cost. They asked whether an additive PTAS can be obtained for the general case, i.e when agents potentially have different costs. We answer this open question in positive.},
  annotation = {Citation Count: 0}
}

@article{kanthIndexingMediumDimensionality1999,
  title = {Indexing Medium-Dimensionality Data in {{Oracle}}},
  author = {Kanth, K. and Ravada, S. and Sharma, J. and Banerjee, J.},
  year = {1999},
  doi = {10.1145/304182.304240},
  abstract = {With the advent of GIS, multi-media, and warehousing technologies, database systems have started focusing on storage and access of multi-dimensional data such as spatial, OLAP, image, audio, and video attributes. As a step in this direction, Oracle},
  citationcount = {48},
  venue = {ACM SIGMOD Conference}
}

@inproceedings{kaplanMeldableHeapsBoolean2002,
  title = {Meldable Heaps and Boolean Union-Find},
  booktitle = {Proceedings of the Thiry-Fourth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Kaplan, Haim and Shafrir, Nira and Tarjan, Robert E.},
  year = {2002},
  month = may,
  series = {{{STOC}} '02},
  pages = {573--582},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/509907.509990},
  url = {https://dl.acm.org/doi/10.1145/509907.509990},
  urldate = {2025-04-21},
  abstract = {In the classical meldable heap data type we maintain an item-disjoint collection of heaps under the operations find-min, insert, delete, decrease-key, and meld. In the usual definition decrease-key and delete get the item and the heap containing it as parameters. We consider the modified problem where decrease-key and delete get only the item but not the heap containing it. We show that for this problem one of the operations find-min, decrease-key, or meld must take non-constant time. This is in contrast with the original data type in which data structures supporting all these three operations in constant time are known (both in an amortized and a worst-case setting).To establish our results for meldable heaps we consider a weaker version of the union-find problem that is of independent interest, which we call Boolean union-find. In the Boolean union-find problem the find operation is a binary predicate that gets an item x and a set A and answers positively if and only if {$\chi$} {$\varepsilon$} A. We prove that the lower bounds which hold for union-find in the cell probe model hold for Boolean union-find as well.We also suggest new heap data structures implementing the modified meldable heap data type that are based on redundant binary counters. Our data structures have good worst-case bounds. The best of our data structures matches the worst-case lower bounds which we establish for the problem. The simplest of our data structures is an interesting generalization of binomial queues.},
  isbn = {978-1-58113-495-7},
  keywords = {cell probe,data structure,lower bound,sorted},
  file = {/Users/tulasi/Zotero/storage/S42GEV7N/Kaplan et al. - 2002 - Meldable heaps and boolean union-find.pdf}
}

@article{kaplanReachabilityOraclesFor2016,
  title = {Reachability Oracles for Directed Transmission Graphs},
  author = {Kaplan, Haim and Mulzer, Wolfgang and Roditty, L. and Seiferth, Paul},
  year = {2016},
  doi = {10.1007/s00453-019-00641-1},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Algorithmica}
}

@article{kaplanTheAmortizedCost2015,
  title = {The Amortized Cost of Finding the Minimum},
  author = {Kaplan, Haim and Zamir, Or and Zwick, Uri},
  year = {2015},
  doi = {10.1137/1.9781611973730.51},
  abstract = {We obtain an essentially optimal tradeoff between the amortized cost of the three basic priority queue operations insert, delete and find-min in the comparison model. More specifically we show that [EQUATION] for any fixed e {\textquestiondown} 0, where n is the number of items in the priority queue and A(insert), A(delete) and A(find-min) are the amortized costs of the insert, delete and find-min operations, respectively. In particular, if A(insert) + A(delete) = O(1), then A(find-min) = {\textohm}(n), and A(find-min) = O(n{$\alpha$}), for some {$\alpha$} {\textexclamdown} 1, only if A(insert) + A(delete) = {\textohm}(log n). (We can, of course, have A(insert) = O(1), A(delete) = O(log n), or vice versa, and A(find-min) = O(1).) Our lower bound holds even if randomization is allowed. Surprisingly, such fundamental bounds on the amortized cost of the operations were not known before. Brodal, Chaudhuri and Radhakrishnan, obtained similar bounds for the worst-case complexity of find-min.},
  citationcount = {3},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {lower bound}
}

@article{kapralovNnsLowerBounds2012,
  title = {{{NNS}} Lower Bounds via Metric Expansion for l {$\infty$} and {{EMD}}},
  author = {Kapralov, M. and Panigrahy, R.},
  year = {2012},
  doi = {10.1007/978-3-642-31594-7_46},
  abstract = {No abstract available},
  citationcount = {14},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {lower bound}
}

@article{kapralovSmoothTradeoffsBetween2015,
  title = {Smooth Tradeoffs between Insert and Query Complexity in Nearest Neighbor Search},
  author = {Kapralov, M.},
  year = {2015},
  doi = {10.1145/2745754.2745761},
  abstract = {Locality Sensitive Hashing (LSH) has emerged as the method of choice for high dimensional similarity search, a classical problem of interest in numerous applications. LSH-based solutions require that each data point be inserted into a number A of hash tables, after which a query can be answered by performing B lookups. The original LSH solution of [IM98] showed for the first time that both A and B can be made sublinear in the number of data points. Unfortunately, the classical LSH solution does not provide any tradeoff between insert and query complexity, whereas for data (respectively, query) intensive applications one would like to minimize insert time by choosing a smaller A (respectively, minimize query time by choosing a smaller B). A partial remedy for this is provided by Entropy LSH [Pan06], which allows to make either inserts or queries essentially constant time at the expense of a loss in the other parameter, but no algorithm that achieves a smooth tradeoff is known. In this paper, we present an algorithm for performing similarity search under the Euclidean metric that resolves the problem above. Our solution is inspired by Entropy LSH, but uses a very different analysis to achieve a smooth tradeoff between insert and query complexity. Our results improve upon or match, up to lower order terms in the exponent, best known data-oblivious algorithms for the Euclidean metric.},
  citationcount = {30},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {query,query complexity,query time}
}

@article{kapronDynamicGraphConnectivity2013,
  title = {Dynamic Graph Connectivity in Polylogarithmic Worst Case Time},
  author = {Kapron, B. and King, Valerie and Mountjoy, Ben},
  year = {2013},
  doi = {10.1137/1.9781611973105.81},
  abstract = {The dynamic graph connectivity problem is the following: given a graph on a fixed set of n nodes which is undergoing a sequence of edge insertions and deletions, answer queries of the form q(a, b): "Is there a path between nodes a and b?" While data structures for this problem with polylogarithmic amortized time per operation have been known since the mid-1990's, these data structures have {$\Theta$}(n) worst case time. In fact, no previously known solution has worst case time per operation which is o({\textsurd}n). We present a solution with worst case times O(log4 n) per edge insertion, O(log5 n) per edge deletion, and O(log n/log log n) per query. The answer to each query is correct if the answer is "yes" and is correct with high probability if the answer is "no". The data structure is based on a simple novel idea which can be used to quickly identify an edge in a cutset. Our technique can be used to simplify and significantly speed up the preprocessing time for the emergency planning problem while matching previous bounds for an update, and to approximate the sizes of cutsets of dynamic graphs in time O(min\{{\textbar}S{\textbar}, {\textbar}V{\S}{\textbar}\}) for an oblivious adversary.},
  citationcount = {194},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,query,update}
}

@article{karaConjunctiveQueriesWith2022,
  title = {Conjunctive Queries with Free Access Patterns under Updates},
  author = {Kara, A. and Nikolic, Milos and Olteanu, Dan and Zhang, Haozhe},
  year = {2022},
  doi = {10.4230/LIPIcs.ICDT.2023.17},
  abstract = {We study the problem of answering conjunctive queries with free access patterns (CQAPs) under updates. A free access pattern is a partition of the free variables of the query into input and output. The query returns tuples over the output variables given a tuple of values over the input variables. We introduce a fully dynamic evaluation approach that works for all CQAPs and is optimal for two classes of CQAPs. This approach recovers prior work on the dynamic evaluation of conjunctive queries without access patterns. We first give a syntactic characterisation of all CQAPs that admit constant time per single-tuple update and whose output tuples can be enumerated with constant delay given a tuple of values over the input variables. We further chart the complexity trade-off between the preprocessing time, update time and enumeration delay for a class of CQAPs. For some of these CQAPs, our approach achieves optimal, albeit non-constant, update time and delay. This optimality is predicated on the Online Matrix-Vector Multiplication conjecture. We finally adapt our approach to the dynamic evaluation of tractable CQAPs over probabilistic databases under updates.},
  citationcount = {7},
  venue = {International Conference on Database Theory},
  keywords = {dynamic,query,update,update time}
}

@article{karaConjunctiveQueriesWith2022,
  title = {Conjunctive Queries with Output Access Patterns under Updates},
  author = {Kara, A. and Nikolic, Milos and Olteanu, Dan and Zhang, Haozhe},
  year = {2022},
  doi = {10.48550/arXiv.2206.09032},
  abstract = {We study the dynamic evaluation of conjunctive queries with output access patterns. An access pattern is a partition of the free variables of the query into input and output. The query returns tuples over the output variables given a tuple over the input variables. Our contribution is threefold. First, we give a syntactic characterisation of queries that admit constant time per single-tuple update and whose output tuples can be enumerated with constant delay given an input tuple. Second, we define a class of queries that admit optimal, albeit non-constant, update time and delay. Their optimality is predicated on the Online Matrix-Vector Multiplication conjecture. Third, we chart the complexity trade-off between preprocessing, update time and enumeration delay for such queries. Our results recover prior work on the dynamic evaluation of conjunctive queries without access patterns.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {dynamic,query,update,update time}
}

@article{karaCountingTrianglesUnder2018,
  title = {Counting Triangles under Updates in Worst-Case Optimal Time},
  author = {Kara, A. and Ngo, H. and Nikolic, Milos and Olteanu, Dan and Zhang, Haozhe},
  year = {2018},
  doi = {10.4230/LIPIcs.ICDT.2019.4},
  abstract = {We consider the problem of incrementally maintaining the triangle count query under single-tuple updates to the input relations. We introduce an approach that exhibits a space-time tradeoff such that the space-time product is quadratic in the size of the input database and the update time can be as low as the square root of this size. This lowest update time is worst-case optimal conditioned on the Online Matrix-Vector Multiplication conjecture. The classical and factorized incremental view maintenance approaches are recovered as special cases of our approach within the space-time tradeoff. In particular, they require linear-time update maintenance, which is suboptimal. Our approach also recovers the worst-case optimal time complexity for computing the triangle count in the non-incremental setting.},
  citationcount = {32},
  venue = {International Conference on Database Theory},
  keywords = {query,update,update time}
}

@article{karaMaintainingTriangleQueries2020,
  title = {Maintaining Triangle Queries under Updates},
  author = {Kara, A. and Nikolic, Milos and Ngo, H. and Olteanu, Dan and Zhang, Haozhe},
  year = {2020},
  doi = {10.1145/3396375},
  abstract = {We consider the problem of incrementally maintaining the triangle queries with arbitrary free variables under single-tuple updates to the input relations. We introduce an approach called IVM{$\epsilon$} that exhibits a trade-off between the update time, the space, and the delay for the enumeration of the query result, such that the update time ranges from the square root to linear in the database size while the delay ranges from constant to linear time. IVM{$\epsilon$} achieves Pareto worst-case optimality in the update-delay space conditioned on the Online Matrix-Vector Multiplication conjecture. It is strongly Pareto optimal for the triangle queries with no or three free variables and weakly Pareto optimal for the remaining triangle queries with one or two free variables. IVM{$\epsilon$} recovers prior work such as the suboptimal classical view maintenance approach that uses delta query processing and the worst-case optimal approach that computes all triangles in a static database.},
  citationcount = {21},
  venue = {ACM Transactions on Database Systems},
  keywords = {query,static,update,update time}
}

@article{karaTractableConjunctiveQueries2024,
  title = {Tractable Conjunctive Queries over Static and Dynamic Relations},
  author = {Kara, Ahmet and Luo, Zheng and Nikolic, Milos and Olteanu, Dan and Zhang, Haozhe},
  year = {2024},
  doi = {10.48550/arXiv.2404.16224},
  abstract = {We investigate the evaluation of conjunctive queries over static and dynamic relations. While static relations are given as input and do not change, dynamic relations are subject to inserts and deletes. We characterise syntactically three classes of queries that admit constant update time and constant enumeration delay. We call such queries tractable. Depending on the class, the preprocessing time is linear, polynomial, or exponential (under data complexity, so the query size is constant). To decide whether a query is tractable, it does not suffice to analyse separately the sub-query over the static relations and the sub-query over the dynamic relations. Instead, we need to take the interaction between the static and the dynamic relations into account. Even when the sub-query over the dynamic relations is not tractable, the overall query can become tractable if the dynamic relations are sufficiently constrained by the static ones.},
  citationcount = {1},
  venue = {arXiv.org},
  keywords = {dynamic,query,static,update,update time}
}

@article{karaTradeOffsIn2019,
  title = {Trade-Offs in Static and Dynamic Evaluation of Hierarchical Queries},
  author = {Kara, A. and Nikolic, Milos and Olteanu, Dan and Zhang, Haozhe},
  year = {2019},
  doi = {10.1145/3375395.3387646},
  abstract = {We investigate trade-offs in static and dynamic evaluation of hierarchical queries with arbitrary free variables. In the static setting, the trade-off is between the time to partially compute the query result and the delay needed to enumerate its tuples. In the dynamic setting, we additionally consider the time needed to update the query result under single-tuple inserts or deletes to the database. Our approach observes the degree of values in the database and uses different computation and maintenance strategies for high-degree (heavy) and low-degree (light) values. For the latter it partially computes the result, while for the former it computes enough information to allow for on-the-fly enumeration. The main result of this work defines the preprocessing time, the update time, and the enumeration delay as functions of the light/heavy threshold. By conveniently choosing this threshold, our approach recovers a number of prior results when restricted to hierarchical queries. For a restricted class of hierarchical queries, our approach can achieve worst-case optimal update time and enumeration delay conditioned on the Online Matrix-Vector Multiplication Conjecture.},
  citationcount = {36},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {dynamic,query,static,update,update time}
}

@article{karchmerFractionalCoversAnd1992,
  title = {Fractional Covers and Communication Complexity},
  author = {Karchmer, M. and Kushilevitz, E. and Nisan, N.},
  year = {1992},
  doi = {10.1109/SCT.1992.215401},
  abstract = {It is possible to view communication complexity as the solution of an integer programming problem. The authors relax this integer programming problem to a linear programming problem, and try to deduce from it information regarding the original communication complexity question. This approach works well for nondeterministic communication complexity. In this case the authors get a special case of Lovasz's fractional cover measure and use it to completely characterize the amortized nondeterministic communication complexity. In the case of deterministic complexity the situation is more complicated. The authors discuss two attempts, and obtain some results using each of them.<<ETX>>},
  citationcount = {96},
  venue = {[1992] Proceedings of the Seventh Annual Structure in Complexity Theory Conference}
}

@article{karchmerMonotoneCircuitsFor1990,
  title = {Monotone Circuits for Connectivity Require Super-Logarithmic Depth},
  author = {Karchmer, M. and Wigderson, A.},
  year = {1990},
  doi = {10.1145/62212.62265},
  abstract = {We prove that every monotone circuit which tests {\textexclamdown}italic{\textquestiondown}st{\textexclamdown}/italic{\textquestiondown}-connectivity of an undirected graph on {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} nodes has depth \&OHgr;(log{\textexclamdown}supscrpt{\textquestiondown}2{\textexclamdown}/supscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}). This implies a superpolynomial ({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}\&OHgr;(log n){\textexclamdown}/supscrpt{\textquestiondown}) lower bound on the size of any monotone formula for {\textexclamdown}italic{\textquestiondown}st{\textexclamdown}/italic{\textquestiondown}-connectivity. The proof draws intuition from a new characterization of circuit depth in terms of communication complexity. It uses counting arguments and Extremal Set Theory. Within the same framework, we also give a very simple and intuitive proof of a depth analogue of a theorem of Krapchenko concerning formula size lower bounds.},
  citationcount = {490},
  venue = {Symposium on the Theory of Computing}
}

@article{karczmarzASimpleMergeable2016,
  title = {A Simple Mergeable Dictionary},
  author = {Karczmarz, Adam},
  year = {2016},
  doi = {10.4230/LIPIcs.SWAT.2016.7},
  abstract = {A mergeable dictionary is a data structure storing a dynamic subset S of a totally ordered set U and supporting predecessor searches in S. Apart from insertions and deletions to S, we can both merge two arbitrarily interleaved dictionaries and split a given dictionary around some pivot x. We present an implementation of a mergeable dictionary matching the optimal amortized logarithmic bounds of Iacono and Okzan [Iacono/Okzan,ICALP'10]. However, our solution is significantly simpler. The proposed data structure can also be generalized to the case when the universe U is dynamic or infinite, thus addressing one issue of [Iacono/Okzan,ICALP'10].},
  citationcount = {5},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {data structure,dynamic}
}

@article{karczmarzFastAndSimple2015,
  title = {Fast and Simple Connectivity in Graph Timelines},
  author = {Karczmarz, Adam and Lacki, Jakub},
  year = {2015},
  doi = {10.1007/978-3-319-21840-3_38},
  abstract = {No abstract available},
  citationcount = {9},
  venue = {Workshop on Algorithms and Data Structures}
}

@article{karczmarzFullyDynamicShortest2024,
  title = {Fully Dynamic Shortest Paths and Reachability in Sparse Digraphs},
  author = {Karczmarz, Adam and Sankowski, P.},
  year = {2024},
  doi = {10.4230/LIPICS.ICALP.2023.84},
  abstract = {We study the exact fully dynamic shortest paths problem. For real-weighted directed graphs, we show a deterministic fully dynamic data structure with O\vphantom\{\}(mn\textsuperscript{\{\vphantom\}}4/5\vphantom\{\}) worst-case update time processing arbitrary s,t-distance queries in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}4/5\vphantom\{\}) time. This constitutes the first non-trivial update/query tradeoff for this problem in the regime of sparse weighted directed graphs.},
  citationcount = {3},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,dynamic,query,update,update time}
}

@article{karczmarzFullyDynamicStrongly2024,
  title = {Fully Dynamic Strongly Connected Components in Planar Digraphs},
  author = {Karczmarz, Adam and Smulewicz, Marcin},
  year = {2024},
  doi = {10.4230/LIPIcs.ICALP.2024.95},
  abstract = {In this paper, we consider maintaining strongly connected components (SCCs) of a directed planar graph subject to edge insertions and deletions. We show a data structure maintaining an implicit representation of the SCCs within O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}6/7\vphantom\{\}) worst-case time per update. The data structure supports, in O({$^2$}\{n\}) time, reporting vertices of any specified SCC (with constant overhead per reported vertex) and aggregating vertex information (e.g., computing the maximum label) over all the vertices of that SCC. Furthermore, it can maintain global information about the structure of SCCs, such as the number of SCCs or the size of the largest SCC. To the best of our knowledge, no fully dynamic SCCs data structures with sublinear update time have been previously known for any major subclass of digraphs. Our result should be contrasted with the known n\textsuperscript{\{\vphantom\}}1-o(1)\vphantom\{\} amortized update time lower bound conditional on SETH, which holds even for dynamically maintaining whether a general digraph has more than two SCCs.},
  citationcount = {1},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,dynamic,lower bound,update,update time}
}

@article{karczmarzOnFullyDynamic2023,
  title = {On Fully Dynamic Strongly Connected Components},
  author = {Karczmarz, Adam and Smulewicz, Marcin},
  year = {2023},
  doi = {10.4230/LIPIcs.ESA.2023.68},
  abstract = {We consider maintaining strongly connected components (SCCs) of a directed graph subject to edge insertions and deletions. For this problem, we show a randomized algebraic data structure with conditionally tight O ( n 1 . 529 ) worst-case update time. The only previously described subquadratic update bound for this problem [Karczmarz, Mukherjee, and Sankowski, STOC'22] holds exclusively in the amortized sense. For the less general dynamic strong connectivity problem, where one is only interested in maintaining whether the graph is strongly connected, we give an efficient deterministic black-box reduction to (arbitrary-pair) dynamic reachability. Consequently, for dynamic strong connectivity we match the best-known O ( n 1 . 407 ) worst-case upper bound for dynamic reachability [van den Brand, Nanongkai, and Saranurak FOCS'19]. This is also conditionally optimal and improves upon the previous O ( n 1 . 529 ) bound. Our reduction also yields the first fully dynamic algorithms for maintaining the minimum strong connectivity augmentation of a digraph.},
  citationcount = {3},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,dynamic,reduction,update,update time}
}

@article{karczmarzSubquadraticAlgorithmsIn2024,
  title = {Subquadratic Algorithms in Minor-Free Digraphs: (Weighted) Distance Oracles, Decremental Reachability, and More},
  author = {Karczmarz, Adam and Zheng, Da Wei},
  year = {2024},
  doi = {10.48550/arXiv.2410.12003},
  abstract = {Le and Wulff-Nilsen [SODA '24] initiated a systematic study of VC set systems to unweighted K\textsubscript{h}-minor-free directed graphs. We extend their results in the following ways: {$\bullet$} We present the first application of VC set systems for real-weighted minor-free digraphs to build the first exact subquadratic-space distance oracle with O(n) query time. Prior work using VC set systems only applied in unweighted and integer weighted digraphs. {$\bullet$} We describe a unified system for analyzing the VC dimension of balls and the LP set system (based on Li--Parter [STOC '19]) of Le--Wulff-Nilsen [SODA '24] using pseudodimension. This is a major conceptual contribution that allows for both improving our understanding of set systems in digraphs as well as improving the bound of the LP set system in directed graphs to h-1. {$\bullet$} We present the first application of these set systems in a dynamic setting. Specifically, we construct decremental reachability oracles with subquadratic total update time and constant query time. Prior to this work, it was not known if this was possible to construct oracles with subquadratic total update time and polylogarithmic query time, even in planar digraphs. {$\bullet$} We describe subquadratic time algorithms for unweighted digraphs including (1) constructions of exact distance oracles, (2) computation of vertex eccentricities and Wiener index. The main innovation in obtaining these results is the use of dynamic string data structures.},
  citationcount = {1},
  venue = {arXiv.org},
  keywords = {data structure,dynamic,query,query time,update,update time}
}

@article{karima.adiprasitoTheoremsCaratheodoryHelly2018,
  title = {Theorems of {{Carath{\'e}odory}}, {{Helly}}, and {{Tverberg Without Dimension}}},
  author = {Karim A. Adiprasito and I. B{\'a}r{\'a}ny and Nabil H. Mustafa},
  year = {2018},
  journal = {Discrete \& Computational Geometry},
  doi = {10.1007/s00454-020-00172-5},
  annotation = {Citation Count: 16}
}

@article{karpDeferredDataStructuring1988,
  title = {Deferred Data Structuring},
  author = {Karp, R. and Motwani, R. and Raghavan, P.},
  year = {1988},
  doi = {10.1137/0217055},
  abstract = {We consider the problem of answering a series of on-line queries on a static data set. The conventional approach to such problems involves a preprocessing phase which constructs a data structure with good search behavior. The data structure representing the data set then remains fixed throughout the processing of the queries. Our approach involves dynamic or query driven structuring of the data set; our algorithm processes the data set only when doing so is required for answering a query. A data structure constructed progressively in this fashion is called a deferred data structure. We develop the notion of deferred data structures by solving the problem of answering membership queries on an ordered set. We obtain a randomized algorithm which achieves asymptotically optimal performance with high probability. We then present optimal deferred data structures for the following problems in the plane: testing convex hull membership, half-plane intersection queries and fixed-constraint multi-objective linear programming. We also apply the deferred data structuring technique to multidimensional dominance query problems.},
  citationcount = {29},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,dynamic,query,static}
}

@article{karpinskiPredecessorQueriesIn2005,
  title = {Predecessor Queries in Constant Time?},
  author = {Karpinski, Marek and Nekrich, Yakov},
  year = {2005},
  doi = {10.1007/11561071_23},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Embedded Systems and Applications},
  keywords = {query}
}

@article{karpinskiRandomizedComplexityOf1999,
  title = {Randomized Complexity of Linear Arrangements and Polyhedra},
  author = {Karpinski, Marek},
  year = {1999},
  doi = {10.1007/3-540-48321-7_1},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {International Symposium on Fundamentals of Computation Theory}
}

@article{karpinskiSpaceEfficientMulti2008,
  title = {Space Efficient Multi-Dimensional Range Reporting},
  author = {Karpinski, Marek and Nekrich, Yakov},
  year = {2008},
  doi = {10.1007/978-3-642-02882-3_22},
  abstract = {No abstract available},
  citationcount = {20},
  venue = {International Computing and Combinatorics Conference}
}

@article{karstenschmidtHowCalculateSymmetries2000,
  title = {How to Calculate Symmetries of {{Petri}} Nets},
  author = {Karsten Schmidt},
  year = {2000},
  journal = {Acta Informatica},
  doi = {10.1007/s002360050002},
  annotation = {Citation Count: 24}
}

@article{karthikc.OnTheParameterized2017,
  title = {On the Parameterized Complexity of Approximating Dominating Set},
  author = {KarthikC., S. and Laekhanukit, Bundit and Manurangsi, Pasin},
  year = {2017},
  doi = {10.1145/3188745.3188896},
  abstract = {We study the parameterized complexity of approximating the k-Dominating Set (domset) problem where an integer k and a graph G on n vertices are given as input, and the goal is to find a dominating set of size at most F(k) {$\cdot$} k whenever the graph G has a dominating set of size k. When such an algorithm runs in time T(k)poly(n) (i.e., FPT-time) for some computable function T, it is said to be an F(k)-FPT-approximation algorithm for k-domset. Whether such an algorithm exists is listed in the seminal book of Downey and Fellows (2013) as one of the ''most infamous'' open problems in Parameterized Complexity. This work gives an almost complete answer to this question by showing the non-existence of such an algorithm under W[1]{$\neq$}FPT and further providing tighter running time lower bounds under stronger hypotheses. Specifically, we prove the following for every computable functions T, F and every constant {$\varepsilon$} {\textquestiondown} 0: (i) Assuming W[1]{$\neq$}FPT, there is no F(k)-FPT-approximation algorithm for k-domset, (ii) Assuming the Exponential Time Hypothesis (ETH), there is no F(k)-approximation algorithm for k-domset that runs in T(k)no(k) time, (iii) Assuming the Strong Exponential Time Hypothesis (SETH), for every integer k {$\geq$} 2, there is no F(k)-approximation algorithm for k-domset that runs in T(k)nk - {$\varepsilon$} time, (iv) Assuming the k-sum Hypothesis, for every integer k {$\geq$} 3, there is no F(k)-approximation algorithm for k-domset that runs in T(k) n{$\lceil$} k/2 {$\rceil$} - {$\varepsilon$} time. Previously, only constant ratio FPT-approximation algorithms were ruled out under W[1]{$\neq$}FPT and (log1/4 - {$\varepsilon$} k)-FPT-approximation algorithms were ruled out under ETH [Chen and Lin, FOCS 2016]. Recently, the non-existence of an F(k)-FPT-approximation algorithm for any function F was shown under gapETH [Chalermsook et al., FOCS 2017]. Note that, to the best of our knowledge, no running time lower bound of the form n{$\delta$} k for any absolute constant {$\delta$} {\textquestiondown} 0 was known before even for any constant factor inapproximation ratio. Our results are obtained by establishing a connection between communication complexity and hardness of approximation, generalizing the ideas from a recent breakthrough work of Abboud et al. [FOCS 2017]. Specifically, we show that to prove hardness of approximation of a certain parameterized variant of the label cover problem, it suffices to devise a specific protocol for a communication problem that depends on which hypothesis we rely on. Each of these communication problems turns out to be either a well studied problem or a variant of one; this allows us to easily apply known techniques to solve them.},
  citationcount = {99},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {communication,communication complexity,lower bound}
}

@article{karthikc.OnTheParameterized2019,
  title = {On the Parameterized Complexity of Approximating Dominating Set},
  author = {KarthikC., S. and Laekhanukit, Bundit and Manurangsi, Pasin},
  year = {2019},
  doi = {10.1145/3325116},
  abstract = {We study the parameterized complexity of approximating the k-Dominating Set (DomSet) problem where an integer k and a graph G on n vertices are given as input, and the goal is to find a dominating set of size at most F(k) {$\cdot$} k whenever the graph G has a dominating set of size k. When such an algorithm runs in time T(k) {$\cdot$} poly (n) (i.e., FPT-time) for some computable function T, it is said to be an F(k)-FPT-approximation algorithm for k-DomSet. Whether such an algorithm exists is listed in the seminal book of Downey and Fellows (2013) as one of the ``most infamous'' open problems in parameterized complexity. This work gives an almost complete answer to this question by showing the non-existence of such an algorithm under W[1] {$\neq$} FPT and further providing tighter running time lower bounds under stronger hypotheses. Specifically, we prove the following for every computable functions T, F and every constant {$\varepsilon$} {\textquestiondown} 0: {$\bullet$} Assuming W[1] {$\neq$} FPT, there is no F(k)-FPT-approximation algorithm for k-DomSet. {$\bullet$} Assuming the Exponential Time Hypothesis (ETH), there is no F(k)-approximation algorithm for k-DomSet that runs in T(k) {$\cdot$} no(k) time. {$\bullet$} Assuming the Strong Exponential Time Hypothesis (SETH), for every integer k {$\geq$} 2, there is no F(k)-approximation algorithm for k-DomSet that runs in T(k) {$\cdot$} nk - {$\varepsilon$} time. {$\bullet$} Assuming the k-SUM Hypothesis, for every integer k {$\geq$} 3, there is no F(k)-approximation algorithm for k-DomSet that runs in T(k) {$\cdot$} n{$\lceil$} k/2 {$\rceil$} - {$\varepsilon$} time. Previously, only constant ratio FPT-approximation algorithms were ruled out under sf W[1] {$\neq$} FPT and (log1/4 \&minus {$\varepsilon$} k)-FPT-approximation algorithms were ruled out under ETH [Chen and Lin, FOCS 2016]. Recently, the non-existence of an F(k)-FPT-approximation algorithm for any function F was shown under Gap-ETH [Chalermsook et al., FOCS 2017]. Note that, to the best of our knowledge, no running time lower bound of the form n\&delta k for any absolute constant {$\delta$} {\textquestiondown} 0 was known before even for any constant factor inapproximation ratio. Our results are obtained by establishing a connection between communication complexity and hardness of approximation, generalizing the ideas from a recent breakthrough work of Abboud et al. [FOCS 2017]. Specifically, we show that to prove hardness of approximation of a certain parameterized variant of the label cover problem, it suffices to devise a specific protocol for a communication problem that depends on which hypothesis we rely on. Each of these communication problems turns out to be either a well-studied problem or a variant of one; this allows us to easily apply known techniques to solve them.},
  citationcount = {17},
  venue = {Journal of the ACM},
  keywords = {communication,communication complexity,lower bound}
}

@article{karthikc.RangeLongestIncreasing2024,
  title = {Range Longest Increasing Subsequence and Its Relatives: {{Beating}} Quadratic Barrier and Approaching Optimality},
  author = {KarthikC., S. and Rahul, Saladi},
  year = {2024},
  doi = {10.48550/arXiv.2404.04795},
  abstract = {In this work, we present a plethora of results for the range longest increasing subsequence problem (Range-LIS) and its variants. The input to Range-LIS is a sequence \{S\} of n real numbers and a collection \{Q\} of m query ranges and for each query in \{Q\}, the goal is to report the LIS of the sequence \{S\} restricted to that query. Our two main results are for the following generalizations of the Range-LIS problem: {$\bullet$} 2D Range Queries: In this variant of the Range-LIS problem, each query is a pair of ranges, one of indices and the other of values, and we provide an algorithm with running time O\vphantom\{\}(mn\textsuperscript{\{\vphantom\}}1/2\vphantom\{\}+n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}+k), where k is the cumulative length of the m output subsequences. This breaks the quadratic barrier of O\vphantom\{\}(mn) when m={\textohm}({\textsurd}\{n\}). Previously, the only known result breaking the quadratic barrier was of Tiskin [SODA'10] which could only handle 1D range queries (i.e., each query was a range of indices) and also just outputted the length of the LIS (instead of reporting the subsequence achieving that length). {$\bullet$} Colored Sequences: In this variant of the Range-LIS problem, each element in \{S\} is colored and for each query in \{Q\}, the goal is to report a monochromatic LIS contained in the sequence \{S\} restricted to that query. For 2D queries, we provide an algorithm for this colored version with running time O\vphantom\{\}(mn\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}+n\textsuperscript{\{\vphantom\}}5/3\vphantom\{\}+k). Moreover, for 1D queries, we provide an improved algorithm with running time O\vphantom\{\}(mn\textsuperscript{\{\vphantom\}}1/2\vphantom\{\}+n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}+k). Thus, we again break the quadratic barrier of O\vphantom\{\}(mn). Additionally, we prove that assuming the well-known Combinatorial Boolean Matrix Multiplication Hypothesis, that the runtime for 1D queries is essentially tight for combinatorial algorithms.},
  citationcount = {1},
  venue = {arXiv.org},
  keywords = {query}
}

@article{kashyopAnInvitationTo2022,
  title = {An Invitation to Dynamic Graph Problems: {{Lower}} Bounds --- {{III}}},
  author = {Kashyop, Manas Jyoti and Narayanaswamy, N.},
  year = {2022},
  doi = {10.1007/s12045-022-1471-6},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Resonance},
  keywords = {dynamic,lower bound}
}

@article{kashyopTradeOffsIn2019,
  title = {Trade-Offs in Dynamic Coloring for Bipartite and General Graphs},
  author = {Kashyop, Manas Jyoti and Narayanaswamy, N. and Nasre, Meghana and Potluri, Sai Mohith},
  year = {2019},
  doi = {10.1007/s00453-022-01050-7},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Algorithmica},
  keywords = {dynamic}
}

@article{kaspergreenlarsenImprovedRangeSearching2012,
  title = {Improved Range Searching Lower Bounds},
  author = {Kasper Green Larsen and Huy L. Nguyen},
  year = {2012},
  journal = {International Symposium on Computational Geometry},
  doi = {10.1145/2261250.2261275},
  abstract = {In this paper we present a number of improved lower bounds for range searching in the pointer machine and the group model. In the pointer machine, we prove lower bounds for the approximate simplex range reporting problem. In approximate simplex range reporting, points that lie within a distance of {$\varepsilon$} {$\cdot$} Diam(s) from the border of a query simplex s, are free to be included or excluded from the output, where {$\varepsilon$} {$\geq$} 0 is an input parameter to the range searching problem. We prove our lower bounds by constructing a hard input set and query set, and then invoking Chazelle and Rosenberg's [CGTA'96] general theorem on the complexity of navigation in the pointer machine. For the group model, we show that input sets and query sets that are hard for range reporting in the pointer machine (i.e. by Chazelle and Rosenberg's theorem), are also hard for dynamic range searching in the group model. This theorem allows us to reuse decades of research on range reporting lower bounds to immediately obtain a range of new group model lower bounds. Amongst others, this includes an improved lower bound for the fundamental problem of dynamic d-dimensional orthogonal range searching, stating that tqtu = {\textohm}((lg n/lg lg n)d-1). Here tq denotes the query time and tu the update time of the data structure. This is an improvement of a lg1-{$\delta$}n factor over the recent lower bound of Larsen [FOCS'11], where {$\delta>$}0 is a small constant depending on the dimension.},
  keywords = {data structure,dynamic,lower bound,query,query time,update,update time},
  annotation = {Citation Count: 3}
}

@article{katrinhesslerExactAlgorithmsMulticompartment2021,
  title = {Exact Algorithms for the Multi-Compartment Vehicle Routing Problem with Flexible Compartment Sizes},
  author = {Katrin He{\ss}ler},
  year = {2021},
  journal = {European Journal of Operational Research},
  doi = {10.1016/J.EJOR.2021.01.037},
  annotation = {Citation Count: 20}
}

@article{katzOnTheEfficiency2000,
  title = {On the Efficiency of Local Decoding Procedures for Error-Correcting Codes},
  author = {Katz, J. and Trevisan, L.},
  year = {2000},
  doi = {10.1145/335305.335315},
  abstract = {jkatz s. olumbia.edu. Department of Computer S ien e, Columbia University. Work supported in part by a DoD NDSEG Fellowship. y lu a s. olumbia.edu. Department of Computer S ien e, Columbia University. time (or whose reading devi e is subje t to errors). For example, the data stored in musi CDs and in CD-ROMs is en oded using Reed-Solomon odes. In appli ations to data storage (and also in appli ations to data transmission) a message is typi ally divided into small blo ks, and then ea h blo k is en oded separately. This allows e{\AE} ient retrieval of},
  citationcount = {317},
  venue = {Symposium on the Theory of Computing}
}

@article{kaurReviewOnComputational2024,
  title = {Review on Computational Biology and Computational Chemistry Applications},
  author = {Kaur, Manpreet and Kaur, Amandeep and Sharma, Ankita},
  year = {2024},
  doi = {10.36948/ijfmr.2024.v06i03.21967},
  abstract = {Using the most recent methods in computer science, informatics, statistics, and applied mathematics to address important biological issues, the review appropriately summarises the biological sciences, the chemical sciences, and their computer applications. Sequence alignment, gene discovery, the Human Genome Project, protein structural alignment, protein structure prediction, gene expression prediction, protein-protein interactions, and evolution modelling are some of the major research projects in this subject. The Human Genome Project, which identified the entire human genetic sequence---roughly three billion base pairs---used bioinformatics extensively in its research. Its primary contribution to the field was the understanding of diseases and the development of new, effective medications. Nearly identical content is covered by the three phrases bioinformatics, computational biology, and bioinformatics infrastructure. The use of computers to learn molecular structures and interactions is known as computational chemistry, genetics, and computational medicine. This field has expanded over the past few decades as a result of the incredible advancements in computers and software, which have increased their efficiency and allowed for the calculation of molecular properties for a wide range of chemistry-related applications.},
  citationcount = {Unknown},
  venue = {International Journal For Multidisciplinary Research}
}

@article{kayalSeparationBetweenRead2020,
  title = {Separation between Read-Once Oblivious Algebraic Branching Programs (Roabps) and Multilinear Depth-Three Circuits},
  author = {Kayal, N. and Nair, Vineet and Saha, Chandan},
  year = {2020},
  doi = {10.1145/3369928},
  abstract = {We show an exponential separation between two well-studied models of algebraic computation, namely, read-once oblivious algebraic branching programs (ROABPs) and multilinear depth-three circuits. In particular, we show the following: (1) There exists an explicit n-variate polynomial computable by linear sized multilinear depth-three circuits (with only two product gates) such that every ROABP computing it requires 2{\textohm}(n) size. (2) Any multilinear depth-three circuit computing IMMn,d (the iterated matrix multiplication polynomial formed by multiplying d, n {\texttimes} n symbolic matrices) has n{\textohm}(d) size. IMMn,d can be easily computed by a poly(n,d) sized ROABP. (3) Further, the proof of (2) yields an exponential separation between multilinear depth-four and multilinear depth-three circuits: There is an explicit n-variate, degree d polynomial computable by a poly(n) sized multilinear depth-four circuit such that any multilinear depth-three circuit computing it has size n{\textohm}(d). This improves upon the quasi-polynomial separation of Reference [36] between these two models. The hard polynomial in (1) is constructed using a novel application of expander graphs in conjunction with the evaluation dimension measure [15, 33, 34, 36], while (2) is proved via a new adaptation of the dimension of the partial derivatives measure of Reference [32]. Our lower bounds hold over any field.},
  citationcount = {31},
  venue = {Symposium on Theoretical Aspects of Computer Science}
}

@article{kearnsAnIntroductionTo1994,
  title = {An Introduction to Computational Learning Theory},
  author = {Kearns, Michael and Vazirani, U.},
  year = {1994},
  doi = {10.7551/mitpress/3897.001.0001},
  abstract = {The probably approximately correct learning model Occam's razor the Vapnik-Chervonenkis dimension weak and strong learning learning in the presence of noise inherent unpredictability reducibility in PAC learning learning finite automata by experimentation appendix - some tools for probabilistic analysis.},
  citationcount = {1171},
  venue = {No venue available}
}

@article{kedlayaFastModularComposition2008,
  title = {Fast Modular Composition in Any Characteristic},
  author = {Kedlaya, K. and Umans, C.},
  year = {2008},
  doi = {10.1109/FOCS.2008.13},
  abstract = {We give an algorithm for modular composition of degree n univariate polynomials over a finite field F{\textexclamdown}sub{\textquestiondown}q{\textexclamdown}/sub{\textquestiondown} requiring n {\textexclamdown}sup{\textquestiondown}1{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}+{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}o(1){\textexclamdown}/sup{\textquestiondown} log{\textexclamdown}sup{\textquestiondown}1{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}+{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}o(1){\textexclamdown}/sup{\textquestiondown} q bit operations; this had earlier been achieved in characteristic n{\textexclamdown}sup{\textquestiondown}o(1){\textexclamdown}/sup{\textquestiondown} by Umans (2008). As an application, we obtain a randomized algorithm for factoring degree n polynomials over F{\textexclamdown}sub{\textquestiondown}q{\textexclamdown}/sub{\textquestiondown} requiring (n{\textexclamdown}sup{\textquestiondown}1.5{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}+{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}o(1){\textexclamdown}/sup{\textquestiondown} + n {\textexclamdown}sup{\textquestiondown}1{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}+{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}o(1){\textexclamdown}/sup{\textquestiondown} log q) log{\textexclamdown}sup{\textquestiondown}1{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}+{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}o(1){\textexclamdown}/sup{\textquestiondown} q bit operations, improving upon the methods of von zur Gathen \& Shoup (1992) and Kaltofen \& Shoup (1998). Our results also imply algorithms for irreducibility testing and computing minimal polynomials whose running times are best-possible, up to lower order terms.As in Umans (2008), we reduce modular composition to certain instances of multipoint evaluation of multivariate polynomials. We then give an algorithm that solves this problem optimally (up to lower order terms), in arbitrary characteristic. The main idea is to lift to characteristic 0, apply a small number of rounds of multimodular reduction, and finish with a small number of multidimensional FFTs. The final evaluations are then reconstructed using the Chinese Remainder Theorem. As a bonus, we obtain a very efficient data structure supporting polynomial evaluation queries, which is of independent interest. Our algorithm uses techniques which are commonly employed in practice, so it may be competitive for real problem sizes. This contrasts with previous asymptotically fast methods relying on fast matrix multiplication.},
  citationcount = {73},
  venue = {2008 49th Annual IEEE Symposium on Foundations of Computer Science},
  keywords = {data structure,query,reduction}
}

@inproceedings{kedlayaFastModularComposition2008a,
  title = {Fast {{Modular Composition}} in Any {{Characteristic}}},
  booktitle = {2008 49th {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Kedlaya, Kiran S. and Umans, Christopher},
  year = {2008},
  month = oct,
  pages = {146--155},
  issn = {0272-5428},
  doi = {10.1109/FOCS.2008.13},
  url = {https://ieeexplore.ieee.org/document/4690949},
  urldate = {2024-09-05},
  abstract = {We give an algorithm for modular composition of degree n univariate polynomials over a finite field Fq requiring n 1 + o(1) log1 + o(1) q bit operations; this had earlier been achieved in characteristic no(1) by Umans (2008). As an application, we obtain a randomized algorithm for factoring degree n polynomials over Fq requiring (n1.5 + o(1) + n 1 + o(1) log q) log1 + o(1) q bit operations, improving upon the methods of von zur Gathen \& Shoup (1992) and Kaltofen \& Shoup (1998). Our results also imply algorithms for irreducibility testing and computing minimal polynomials whose running times are best-possible, up to lower order terms.As in Umans (2008), we reduce modular composition to certain instances of multipoint evaluation of multivariate polynomials. We then give an algorithm that solves this problem optimally (up to lower order terms), in arbitrary characteristic. The main idea is to lift to characteristic 0, apply a small number of rounds of multimodular reduction, and finish with a small number of multidimensional FFTs. The final evaluations are then reconstructed using the Chinese Remainder Theorem. As a bonus, we obtain a very efficient data structure supporting polynomial evaluation queries, which is of independent interest. Our algorithm uses techniques which are commonly employed in practice, so it may be competitive for real problem sizes. This contrasts with previous asymptotically fast methods relying on fast matrix multiplication.},
  keywords = {Application software,Computer science,data structure,Data structures,Engineering profession,Flexible printed circuits,Galois fields,Mathematics,modular composition,Multidimensional systems,multimodular reduction,multipoint evaluation,polynomial factorization,Polynomials,query,reduction,Testing},
  file = {/Users/tulasi/Zotero/storage/A7Z499QQ/Kedlaya and Umans - 2008 - Fast Modular Composition in any Characteristic.pdf}
}

@article{kedlayaFastPolynomialFactorization2011,
  title = {Fast Polynomial Factorization and Modular Composition},
  author = {Kedlaya, K. and Umans, C.},
  year = {2011},
  doi = {10.1137/08073408X},
  abstract = {We obtain randomized algorithms for factoring degree n univariate polynomials over F\textsubscript{q} requiring O(n\textsuperscript{\{\vphantom\}}1.5+o(1)\vphantom\{\}log\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}q+n\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}log\textsuperscript{\{\vphantom\}}2+o(1)\vphantom\{\}q) bit operations. When logq\{(omega+1)/2\}) field operations, where omega is the exponent of matrix multiplication (Brent \& Kung (1978)), with a slight improvement in the exponent achieved by employing fast rectangular matrix multiplication (Huang \& Pan (1997)). We show that modular composition and multipoint evaluation of multivariate polynomials are essentially equivalent, in the sense that an algorithm for one achieving exponent alpha implies an algorithm for the other with exponent alpha+o(1), and vice versa. We then give two new algorithms that solve the problem optimally (up to lower order terms): an algebraic algorithm for fields of characteristic at most n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\}, and a nonalgebraic algorithm that works in arbitrary characteristic. The latter algorithm works by lifting to characteristic 0, applying a small number of rounds of \{em multimodular reduction\}, and finishing with a small number of multidimensional FFTs. The final evaluations are reconstructed using the Chinese Remainder Theorem. As a bonus, this algorithm produces a very efficient data structure supporting polynomial evaluation queries, which is of independent interest. Our algorithms use techniques which are commonly employed in practice, so they may be competitive for real problem sizes. This contrasts with all previous subquadratic algorithsm for these problems, which rely on fast matrix multiplication. This is joint work with Kiran Kedlaya.},
  citationcount = {231},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,query,reduction}
}

@article{kejlberg-rasmussenFasterWorstCase2015,
  title = {Faster Worst Case Deterministic Dynamic Connectivity},
  author = {{Kejlberg-Rasmussen}, Casper and Kopelowitz, T. and Pettie, Seth and Thorup, M.},
  year = {2015},
  doi = {10.4230/LIPIcs.ESA.2016.53},
  abstract = {We present a deterministic dynamic connectivity data structure for undirected graphs with worst case update time O({\textsurd}\{\vphantom\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}(n){$^{2}$}\vphantom\{\}\{n\}\vphantom\{\}) and constant query time. This improves on the previous best deterministic worst case algorithm of Frederickson (STOC 1983) and Eppstein Galil, Italiano, and Nissenzweig (J. ACM 1997), which had update time O({\textsurd}\{n\}). All other algorithms for dynamic connectivity are either randomized (Monte Carlo) or have only amortized performance guarantees.},
  citationcount = {31},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,dynamic,query,query time,update,update time}
}

@article{kejlberg-rasmussenIOEfficient2013,
  title = {I/{{O-efficient}} Planar Range Skyline and Attrition Priority Queues},
  author = {{Kejlberg-Rasmussen}, Casper and Tao, Yufei and Tsakalidis, Konstantinos and Tsichlas, K. and Yoon, Jeonghun},
  year = {2013},
  doi = {10.1145/2463664.2465225},
  abstract = {We study the static and dynamic planar range skyline reporting problem in the external memory model with block size B, under a linear space budget. The problem asks for an O(n/B) space data structure that stores n points in the plane, and supports reporting the k maximal input points (a.k.a.skyline) among the points that lie within a given query rectangle Q = [{$\alpha$}1[{$\alpha$}2] {\texttimes} [{$\beta$}1{$\beta$}2. When Q is 3-sided, i.e. one of its edges is grounded, two variants arise: top-open for {$\beta$}2 = {$\infty$} and left-open for {$\alpha$}1 = - {$\infty$} (symmetrically bottom-open and right-open) queries. We present optimal static data structures for top-open queries, for the cases where the universe is R2, a U {\texttimes} U grid, and rank space [O(n)]2. We also show that left-open queries are harder, as they require {\textohm}((n/B){$\varepsilon$} + k/B) I/Os for {$\varepsilon$} {\textquestiondown} 0, when only linear space is allowed. We show that the lower bound is tight, by a structure that supports 4-sided queries in matching complexities. Interestingly, these lower and upper bounds coincide with those of the planar orthogonal range reporting problem, i.e., the skyline requirement does not alter the problem difficulty at all! Finally, we present the first dynamic linear space data structure that supports top-open queries in O(log2B{$\varepsilon$} n + k/B1 {$\varepsilon$} {\textquestiondown} and updates in O(log2B{$\varepsilon$} n) worst case I/Os, for {$\varepsilon$} {$\in$} [0, 1]. This also yields a linear space data structure for 4-sided queries with optimal query I/Os and O(log(n/B)) amortized update I/Os. We consider of independent interest the main component of our dynamic structures, a new real-time I/O-efficient and catenable variant of the fundamental structure priority queue with attrition by Sundar.},
  citationcount = {14},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {data structure,dynamic,lower bound,query,static,update}
}

@article{kejlberg-rasmussenIOEfficient2021,
  title = {I/{{O-efficient}} 2-d Orthogonal Range Skyline and Attrition Priority Queues},
  author = {{Kejlberg-Rasmussen}, Casper and Tao, Yufei and Tsakalidis, Konstantinos and Tsichlas, K. and Yoon, Jeonghun},
  year = {2021},
  doi = {10.1016/j.comgeo.2020.101689},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Computational geometry}
}

@article{kempaDynamicSuffixArray2022,
  title = {Dynamic Suffix Array with Polylogarithmic Queries and Updates},
  author = {Kempa, Dominik and Kociumaka, Tomasz},
  year = {2022},
  doi = {10.1145/3519935.3520061},
  abstract = {The suffix array SA[1..n] of a text T of length n is a permutation of \{1, {\dots}, n\} describing the lexicographical ordering of suffixes of T and is considered to be one of the most important data structures for string processing, with dozens of applications in data compression, bioinformatics, and information retrieval. One of the biggest drawbacks of the suffix array is that it is very difficult to maintain under text updates: even a single character substitution can completely change the contents of the suffix array. Thus, the suffix array of a dynamic text is modelled using suffix array queries, which return the value SA[i] given any i {$\in$} [1..n]. Prior to this work, the fastest dynamic suffix array implementations were by Amir and Boneh, who showed how to answer suffix array queries in {\~O}(k) time, where k {$\in$} [1..n] is a trade-off parameter, with {\~O}(n/k)-time text updates [ISAAC 2020]. In a very recent preprint, they also provided a solution with O(log5 n)-time queries and {\~O}(n2/3)-time updates [arXiv 2021]. We propose the first data structure that supports both suffix array queries and text updates in O(polylog n) time (achieving O(log4 n) and O(log3+o(1) n) time, respectively). Our data structure is deterministic and the running times for all operations are worst-case. In addition to the standard single-character edits (character insertions, deletions, and substitutions), we support (also in O(log3+o(1) n) time) the ''cut-paste'' operation that moves any (arbitrarily long) substring of T to any place in T. To achieve our result, we develop a number of new techniques which are of independent interest. This includes a new flavor of dynamic locally consistent parsing, as well as a dynamic construction of string synchronizing sets with an extra local sparsity property; this significantly generalizes the sampling technique introduced at STOC 2019. We complement our structure by a hardness result: unless the Online Matrix-Vector Multiplication (OMv) Conjecture fails, no data structure with O(polylog n)-time suffix array queries can support the ''copy-paste'' operation in O(n1-{\cyrchar\cyrie}) time for any {\cyrchar\cyrie} {\textquestiondown} 0.},
  citationcount = {22},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,dynamic,query,update}
}

@article{kempaLzEndParsing2017,
  title = {{{LZ-end}} Parsing in Linear Time},
  author = {Kempa, Dominik and Kosolobov, D.},
  year = {2017},
  doi = {10.4230/LIPIcs.ESA.2017.53},
  abstract = {We present a deterministic algorithm that constructs in linear time and space the LZ-End parsing (a variation of LZ77) of a given string over an integer polynomially bounded alphabet.},
  citationcount = {11},
  venue = {Embedded Systems and Applications}
}

@article{khamisInsertOnlyVersus2023,
  title = {Insert-Only versus Insert-Delete in Dynamic Query Evaluation},
  author = {Khamis, Mahmoud Abo and Kara, Ahmet and Olteanu, Dan and Suciu, Dan},
  year = {2023},
  doi = {10.48550/arXiv.2312.09331},
  abstract = {We study the dynamic query evaluation problem: Given a full conjunctive query Q and a sequence of updates to the input database, we construct a data structure that supports constant-delay enumeration of the tuples in the query output after each update. We show that a sequence of N insert-only updates to an initially empty database can be executed in total time O(N w(Q) ), where w(Q) is the fractional hypertree width of Q. This matches the complexity of the static query evaluation problem for Q and a database of size N. One corollary is that the amortized time per single-tuple insert is constant for acyclic full conjunctive queries. In contrast, we show that a sequence of N inserts and deletes can be executed in total time {\~O}(N w(Q') ), where Q' is obtained from Q by extending every relational atom with extra variables that represent the "lifespans" of tuples in the database. We show that this reduction is optimal in the sense that the static evaluation runtime of Q' provides a lower bound on the total update time for the output of Q. Our approach achieves amortized optimal update times for the hierarchical and Loomis-Whitney join queries.},
  citationcount = {2},
  venue = {Proc. ACM Manag. Data},
  keywords = {data structure,dynamic,lower bound,query,reduction,static,update,update time}
}

@article{khamisJoinsViaGeometric2014,
  title = {Joins via Geometric Resolutions: {{Worst-case}} and Beyond},
  author = {Khamis, Mahmoud Abo and Ngo, H. and R{\'e}, C. and Rudra, A.},
  year = {2014},
  doi = {10.1145/2745754.2745776},
  abstract = {We present a simple geometric framework for the relational join. Using this framework, we design an algorithm that achieves the fractional hypertree-width bound, which generalizes classical and recent worst-case algorithmic results on computing joins. In addition, we use our framework and the same algorithm to show a series of what are colloquially known as beyond worst-case results. The framework allows us to prove results for data stored in Btrees, multidimensional data structures, and even multiple indices per table. A key idea in our framework is formalizing the inference one does with an index as a type of geometric resolution; transforming the algorithmic problem of computing joins to a geometric problem. Our notion of geometric resolution can be viewed as a geometric analog of logical resolution. In addition to the geometry and logic connections, our algorithm can also be thought of as backtracking search with memoization.},
  citationcount = {63},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {data structure}
}

@article{khamisJoinsViaGeometric2016,
  title = {Joins via Geometric Resolutions},
  author = {Khamis, Mahmoud Abo and Ngo, H. and R{\'e}, Christopher and Rudra, A.},
  year = {2016},
  doi = {10.1145/2967101},
  abstract = {We present a simple geometric framework for the relational join. Using this framework, we design an algorithm that achieves the fractional hypertree-width bound, which generalizes classical and recent worst-case algorithmic results on computing joins. In addition, we use our framework and the same algorithm to show a series of what are colloquially known as beyond worst-case results. The framework allows us to prove results for data stored in BTrees, multidimensional data structures, and even multiple indices per table. A key idea in our framework is formalizing the inference one does with an index as a type of geometric resolution, transforming the algorithmic problem of computing joins to a geometric problem. Our notion of geometric resolution can be viewed as a geometric analog of logical resolution. In addition to the geometry and logic connections, our algorithm can also be thought of as backtracking search with memoization.},
  citationcount = {21},
  venue = {ACM Transactions on Database Systems},
  keywords = {data structure}
}

@article{khamisTheComplexityOf2021,
  title = {The Complexity of Boolean Conjunctive Queries with Intersection Joins},
  author = {Khamis, Mahmoud Abo and Chichirim, George and Kormpa, Antonia and Olteanu, Dan},
  year = {2021},
  doi = {10.1145/3517804.3524156},
  abstract = {Intersection joins over interval data are relevant in spatial and temporal data settings. A set of intervals join if their intersection is non-empty. In case of point intervals, the intersection join becomes the standard equality join. We establish the complexity of Boolean conjunctive queries with intersection joins by a many-one equivalence to disjunctions of Boolean conjunctive queries with equality joins. The complexity of any query with intersection joins is that of the hardest query with equality joins in the disjunction exhibited by our equivalence. This is captured by a new width measure called the ij-width. We also introduce a new syntactic notion of acyclicity called iota-acyclicity to characterise the class of Boolean queries with intersection joins that admit linear time computation modulo a poly-logarithmic factor in the data size. Iota-acyclicity is for intersection joins what alpha-acyclicity is for equality joins. It strictly sits between gamma-acyclicity and Berge-acyclicity. The intersection join queries that are not iota-acyclic are at least as hard as the Boolean triangle query with equality joins, which is widely considered not computable in linear time.},
  citationcount = {5},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {query}
}

@article{kightleyOnePassSparsified2019,
  title = {One-Pass Sparsified Gaussian Mixtures},
  author = {Kightley, E. and Becker, Stephen},
  year = {2019},
  doi = {10.1109/BigData47090.2019.9006543},
  abstract = {We present a one-pass sparsified Gaussian mixture model (SGMM). Given N data points in P dimensions X, the model fits K Gaussian distributions to X and (softly) classifies each point to these clusters. After paying an up-front cost of O(NPlogP) to precondition the data, we subsample Q entries of each data point and discard the full P-dimensional data. SGMM operates in O(KNQ) time per iteration for diagonal or spherical covariances, independent of P, while estimating the model parameters in the full P-dimensional space, making it one-pass and hence suitable for streaming data. We derive the maximum likelihood estimators for the parameters in the sparsified regime, demonstrate clustering on synthetic and real data, and show that SGMM is faster than GMM while preserving accuracy.},
  citationcount = {1},
  venue = {2019 IEEE International Conference on Big Data (Big Data)}
}

@article{kingAFullyDynamic1999,
  title = {A Fully Dynamic Algorithm for Maintaining the Transitive Closure},
  author = {King, Valerie and Sagert, Garry},
  year = {1999},
  doi = {10.1145/301250.301380},
  abstract = {This paper presents an efficient fully dynamic graph algorithm for maintaining the transitive closure of a directed graph. The algorithm updates the adjacency matrix of the transitive closure with each update to the graph; hence, each reachability query of the form "Is there a directed path from i to j?" can be answered in O(1) time. The algorithm is randomized and has a one-sided error; it is correct when answering yes, but has O(1/nc) probability of error when answering no, for any constant c. In acyclic graphs, worst case update time is O(n2). In general graphs, the update time is O(n2.26). The space complexity of the algorithm is O(n2).},
  citationcount = {101},
  venue = {Symposium on the Theory of Computing}
}

@article{kingFullyDynamicAlgorithms1999,
  title = {Fully Dynamic Algorithms for Maintaining All-Pairs Shortest Paths and Transitive Closure in Digraphs},
  author = {King, Valerie},
  year = {1999},
  doi = {10.1109/SFFCS.1999.814580},
  abstract = {This paper presents the first fully dynamic algorithms for maintaining all-pairs shortest paths in digraphs with positive integer weights less than b. For approximate shortest paths with an error factor of (2+/spl epsiv/), for any positive constant /spl epsiv/, the amortized update time is O(n/sup 2/ log/sup 2/ n/log log n); for an error factor of (1+/spl epsiv/) the amortized update time is O(n/sup 2/ log/sup 3/ (bn)//spl epsiv//sup 2/). For exact shortest paths the amortized update time is O(n/sup 2.5/ /spl radic/(b log n)). Query time for exact and approximate shortest distances is O(1); exact time and approximate paths can be generated in time proportional to their lengths. Also presented is a fully dynamic transitive closure algorithm with update time O(n/sup 2/ log n) and query time O(1). The previously known fully dynamic transitive closure algorithm with fast query time has one-sided error and update time O(n/sup 2.28/). The algorithms use simple data structures, and are deterministic.},
  citationcount = {261},
  venue = {40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039)}
}

@article{kingFullyDynamicConnectivity2008,
  title = {Fully Dynamic Connectivity},
  author = {King, Valerie},
  year = {2008},
  doi = {10.1007/978-0-387-30162-4_152},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Encyclopedia of Algorithms},
  keywords = {dynamic}
}

@article{kinoshitaPowerSeriesComposition2024,
  title = {Power Series Composition in Near-Linear Time},
  author = {Kinoshita, Yasunori and Li, Baitian},
  year = {2024},
  doi = {10.1109/FOCS61266.2024.00127},
  abstract = {We present an algebraic algorithm that computes the composition of two power series in softly linear time complexity. The previous best algorithms are \{O\}(n\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}) non-alzebraic algorithm by Kedlaya and Umans (FOCS 2008) and an \{O\}(n\textsuperscript{\{\vphantom\}}1.43\vphantom\{\}) algebraic algorithm by Neiger, Salvy, Schost and Villard (JACM 2023). Our algorithm builds upon the recent Graeffe iteration approach to manipulate rational power series introduced by Bostan and Mori (SOSA 2021).},
  citationcount = {Unknown},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{kissDeterministicDynamicMatching2021,
  title = {Deterministic Dynamic Matching in Worst-Case Update Time},
  author = {Kiss, P.},
  year = {2021},
  doi = {10.1007/s00453-023-01151-x},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {Algorithmica},
  keywords = {dynamic,update,update time}
}

@article{kittlerExperimentalAlgorithms2016,
  title = {Experimental Algorithms},
  author = {Kittler, J. and Mitchell, J.C.},
  year = {2016},
  doi = {10.1007/978-3-319-38851-9},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {Lecture Notes in Computer Science}
}

@article{klauckAStrongDirect2009,
  title = {A Strong Direct Product Theorem for Disjointness},
  author = {Klauck, H.},
  year = {2009},
  doi = {10.1145/1806689.1806702},
  abstract = {A strong direct product theorem states that if we want to compute k independent instances of a function, using less than k times the resources needed for one instance, then the overall success probability will be exponentially small in k. We establish such a theorem for the randomized communication complexity of the Disjointness problem, i.e., with communication const{$\bullet$} kn the success probability of solving k instances of size n can only be exponentially small in k. This solves an open problem of [KSW07, LSS08]. We also show that this bound even holds for AM-communication protocols with limited ambiguity. The main result implies a new lower bound for Disjointness in a restricted 3-player NOF protocol, and optimal communication-space tradeoffs for Boolean matrix product. Our main result follows from a solution to the dual of a linear programming problem, whose feasibility comes from a so-called Intersection Sampling Lemma that generalizes a result by Razborov [Raz92].},
  citationcount = {69},
  venue = {Symposium on the Theory of Computing}
}

@article{klauckInteractionInQuantum2001,
  title = {Interaction in Quantum Communication and the Complexity of Set Disjointness},
  author = {Klauck, H. and Nayak, A. and {Ta-Shma}, A. and Zuckerman, David},
  year = {2001},
  doi = {10.1145/380752.380786},
  abstract = {One of the most intriguing facts about communication using quantum states is that these states cannot be used to transmit more classical bits than the number of qubits used, yet in some scenarios there are ways of conveying information with exponentially fewer qubits than possible classically [3, 26]. Moreover, these methods have a very simple structure---they involve only few message exchanges between the communicating parties. We consider the question as to whether every classical protocol may be transformed to a ``simpler'' quantum protocol---one that has similar efficiency, but uses fewer message exchanges. We show that for any constant {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}, there is a problem such that its {\textexclamdown}italic{\textquestiondown}k+1{\textexclamdown}/italic{\textquestiondown} message classical communication complexity is exponentially smaller than its {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown} message quantum communication complexity, thus answering the above question in the negative. This in particular proves a round hierarchy theorem for quantum communication complexity, and implies via a simple reduction, an {\textexclamdown}italic{\textquestiondown}{\textohm}(N{\textasciicircum}\{1/k\}){\textexclamdown}/italic{\textquestiondown} lower bound for {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown} message protocols for Set Disjointness for constant {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}. Our result builds on two primitives, {\textexclamdown}italic{\textquestiondown}local transitions in bi-partite states{\textexclamdown}/italic{\textquestiondown} (based on previous work) and {\textexclamdown}italic{\textquestiondown}average encoding{\textexclamdown}/italic{\textquestiondown} which may be of significance in other contexts as well.},
  citationcount = {71},
  venue = {Symposium on the Theory of Computing}
}

@article{klauckOnArthurMerlin2011,
  title = {On Arthur Merlin Games in Communication Complexity},
  author = {Klauck, H.},
  year = {2011},
  doi = {10.1109/CCC.2011.33},
  abstract = {We show several results related to interactive proof modes of communication complexity. First we show lower bounds for the QMA-communication complexity of the functions Inner Product and Disjointness. We describe a general method to prove lower bounds for QMA-communication complexity, and show how one can 'transfer' hardness under an analogous measure in the query complexity model to the communication model using Sherstov's pattern matrix method.Combining a result by Vereshchagin and the pattern matrix method we find a partial function with AM-communication complexity O(n), PP-communication complexity {\textohm}(n{\textasciicircum}\{1/3\}), and QMA-communication complexity {\textohm}(n{\textasciicircum}\{1/6\}). Hence in the world of communication complexity noninteractive quantum proof systems are not able to efficiently simulate co-nondeterminism or interaction. These results imply that the related questions in Turing machine complexity theory cannot be resolved by 'algebrizing' techniques. Finally we show that in MA-protocols there is an exponential gap between one-way protocols and two-way protocols for a partial function (this refers to the interaction between Alice and Bob). This is in contrast to nondeterministic, AM-, and QMA-protocols, where one-way communication is essentially optimal.},
  citationcount = {43},
  venue = {2011 IEEE 26th Annual Conference on Computational Complexity}
}

@article{kleinbergTwoAlgorithmsFor1997,
  title = {Two Algorithms for Nearest-Neighbor Search in High Dimensions},
  author = {Kleinberg, J.},
  year = {1997},
  doi = {10.1145/258533.258653},
  abstract = {Representing data as points in a high-dimensional space, so as to use geometric methods for indexing, is an algorithmic technique with a wide array of uses. It is central to a number of areas such as information retrieval, pattern recognition, and statistical data analysis; many of the problems arising in these applications can involve several hundred or several thousand dimensions. We consider the nearest-neighbor problem for d-dimensional Euclidean space: we wish to pre-process a database of n points so that given a query point, one can efficiently determine its nearest neighbors in the database. There is a large literature on algorithms for this problem, in both the exact and approximate cases. The more sophisticated algorithms typically achieve a query time that is logarithmic in n at the expense of an exponential dependence on the dimension d; indeed, even the averagecase analysis of heuristics such as k-d trees reveals an exponential dependence on d in the query time. In this work, we develop a new approach to the nearest-neighbor problem, based on a method for combining randomly chosen one-dimensional projections of the underlying point set. From this, we obtain the following two results. (i) An algorithm for finding e-approximate nearest neighbors with a query time of O((d log d)(d + log n)). (ii) An e-approximate nearest-neighbor algorithm with near-linear storage and a query time that improves asymptotically on linear search in all dimensions. {$\ast$}Department of Computer Science, Cornell University, Ithaca NY 14853. Email: kleinber@cs.cornell.edu. This work was performed in large part while on leave at the IBM Almaden Research Center, San Jose CA 95120. The author is currently supported by an Alfred P. Sloan Research Fellowship and by NSF Faculty Early Career Development Award CCR-9701399.},
  citationcount = {437},
  venue = {Symposium on the Theory of Computing},
  keywords = {query,query time}
}

@article{kleitmanFamiliesOfK1973,
  title = {Families of K-Independent Sets},
  author = {Kleitman, D. and Spencer, J.},
  year = {1973},
  doi = {10.1016/0012-365X(73)90098-8},
  abstract = {No abstract available},
  citationcount = {238},
  venue = {Discrete Mathematics}
}

@article{klitzkeAGeneralFramework2016,
  title = {A General Framework for Dynamic Succinct and Compressed Data Structures},
  author = {Klitzke, Patrick and Nicholson, Patrick K.},
  year = {2016},
  doi = {10.1137/1.9781611974317.14},
  abstract = {Succinct data structures are becoming increasingly popular in big data processing applications due to their low memory consumption. However, a feature that is currently lacking from most implementations of succinct data structures is dynamism. In this paper we design, implement, and test a general framework that allows for practical dynamic succinct structures. Firstly, a key component of our approach is careful memory management, which is often overlooked in the succinct data structures literature. Most succinct data structures allocate and deallocate relatively small data blocks each time a modify, insert, or delete operation occurs. We demonstrate experimentally that the space cost of neglecting memory management can be over 25},
  citationcount = {9},
  venue = {Workshop on Algorithm Engineering and Experimentation},
  keywords = {data structure,dynamic}
}

@article{klockerALowerBound2020,
  title = {A Lower Bound for the Smallest Uniquely Hamiltonian Planar Graph with Minimum Degree Three},
  author = {Klocker, Benedikt and Fleischner, H. and Raidl, G.},
  year = {2020},
  doi = {10.1016/j.amc.2020.125233},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Applied Mathematics and Computation},
  keywords = {lower bound}
}

@article{klockerFindingUniquelyHamiltonian2016,
  title = {Finding Uniquely Hamiltonian Graphs of Minimum Degree Three with Small Crossing Numbers},
  author = {Klocker, Benedikt and Fleischner, H. and Raidl, G.},
  year = {2016},
  doi = {10.1007/978-3-319-39636-1_1},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {International Workshop on Hybrid Metaheuristics}
}

@article{kneserAbschtzungDerAsymptotischen1953,
  title = {Absch{\"a}tzung Der Asymptotischen {{Dichte}} von {{Summenmengen}}},
  author = {Kneser, M.},
  year = {1953},
  doi = {10.1007/BF01174162},
  abstract = {No abstract available},
  citationcount = {239},
  venue = {No venue available}
}

@article{kniesburgesHashedPatriciaTrie2011,
  title = {Hashed Patricia Trie: {{Efficient}} Longest Prefix Matching in Peer-to-Peer Systems},
  author = {Kniesburges, Sebastian and Scheideler, C.},
  year = {2011},
  doi = {10.1007/978-3-642-19094-0_18},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {Workshop on Algorithms and Computation}
}

@article{knuthTheArtOf1970,
  title = {The Art of Computer Programming, Volume {{II}}: {{Seminumerical}} Algorithms},
  author = {Knuth, D.},
  year = {1970},
  doi = {10.2307/2004500},
  abstract = {No abstract available},
  citationcount = {2736},
  venue = {No venue available}
}

@article{knuthTheExpectedLinearity1978,
  title = {The Expected Linearity of a Simple Equivalence Algorithm},
  author = {Knuth, D. and Sch{\"o}nhage, A.},
  year = {1978},
  doi = {10.1016/0304-3975(78)90009-9},
  abstract = {No abstract available},
  citationcount = {96},
  venue = {Theoretical Computer Science}
}

@article{kociumakaImprovedDynamicAlgorithms2020,
  title = {Improved Dynamic Algorithms for Longest Increasing Subsequence},
  author = {Kociumaka, Tomasz and Seddighin, Saeed},
  year = {2020},
  doi = {10.1145/3406325.3451026},
  abstract = {We study dynamic algorithms for the longest increasing subsequence (LIS) problem. A dynamic LIS algorithm maintains a sequence subject to operations of the following form arriving one by one: insert an element, delete an element, or substitute an element for another. After each update, the algorithm must report the length of the longest increasing subsequence of the current sequence. Our main contribution is the first exact dynamic LIS algorithm with sublinear update time. More precisely, we present a randomized algorithm that performs each operation in time {\~O}(n4/5) and, after each update, reports the answer to the LIS problem correctly with high probability. We use several novel techniques and observations for this algorithm that may find applications in future work. In the second part of the paper, we study approximate dynamic LIS algorithms, which are allowed to underestimate the solution size within a bounded multiplicative factor. In this setting, we give a deterministic (1-o(1))-approximation algorithm with update time O(no(1)). This result improves upon the previous work of Mitzenmacher and Seddighin (STOC'20) that provides an {\textohm}({\cyrchar\cyrie}O(1/{\cyrchar\cyrie}))-approximation algorithm with update time {\~O}(n{\cyrchar\cyrie}) for any {\cyrchar\cyrie} {\textquestiondown} 0.},
  citationcount = {16},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,update,update time}
}

@article{kociumakaOnTheString2014,
  title = {On the String Consensus Problem and the Manhattan Sequence Consensus Problem},
  author = {Kociumaka, Tomasz and Pachocki, J. and Radoszewski, J. and Rytter, W. and Wale{\'n}, Tomasz},
  year = {2014},
  doi = {10.1007/978-3-319-11918-2_24},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {SPIRE}
}

@article{kohlenbachBoundsOnIterations2003,
  title = {Bounds on Iterations of Asymptotically Quasi-Nonexpansive Mappings},
  author = {Kohlenbach, U. and Lambov, Branimir},
  year = {2003},
  doi = {10.7146/BRICS.V10I51.21823},
  abstract = {This paper establishes explicit quantitative bounds on the computation of approximate fixed points of asymptotically (quasi-) nonexpansive mappings f by means of iterative processes. Here f is a selfmapping of a convex subset C of a uniformly convex normed space X. We consider general Krasnoselski-Mann iterations with and without error terms. As a consequence of our quantitative analysis we also get new qualitative results which show that the assumption on the existence of fixed points of f can be replaced by the existence of approximate fixed points only. We explain how the existence of effective uniform bounds in this context can be inferred already a-priorily by a logical metatheorem recently proved by the first author. Our bounds were in fact found with the help of the general logical machinery behind the proof of this metatheorem. The proofs we present here are, however, completely self-contained and do not require any tools from logic.},
  citationcount = {41},
  venue = {No venue available}
}

@article{koldaCountingTrianglesIn2013,
  title = {Counting Triangles in Massive Graphs with {{MapReduce}}},
  author = {Kolda, T. and Pinar, Ali and Plantenga, T. and Comandur, Seshadhri and Task, Christine},
  year = {2013},
  doi = {10.1137/13090729X},
  abstract = {Graphs and networks are used to model interactions in a variety of contexts. There is a growing need to quickly assess the characteristics of a graph in order to understand its underlying structure. Some of the most useful metrics are triangle-based and give a measure of the connectedness of mutual friends. This is often summarized in terms of clustering coefficients, which measure the likelihood that two neighbors of a node are themselves connected. Computing these measures exactly for large-scale networks is prohibitively expensive in both memory and time. However, a recent wedge-sampling algorithm has proved successful in efficiently and accurately estimating clustering coefficients. In this paper, we describe how to implement this approach in MapReduce to deal with massive graphs. We show results on publicly available networks, the largest of which is 132M nodes and 4.7B edges, as well as artificially generated networks (using the Graph500 benchmark), the largest of which has 240M nodes and 8.5B edges...},
  citationcount = {83},
  venue = {SIAM Journal on Scientific Computing}
}

@article{kollIntegrationIndexierungUnd2009,
  title = {Integration, Indexierung Und Interaktion Hochdimensionaler Datenobjekte},
  author = {Koll, K.},
  year = {2009},
  doi = {10.17877/DE290R-377},
  abstract = {Dissertation zur Erlangung des Grades eines D o k t o r s d e r N a t u r w i s s e n s c h a f t e n},
  citationcount = {Unknown},
  venue = {No venue available}
}

@article{komargodskiALogarithmicLower2021,
  title = {A Logarithmic Lower Bound for Oblivious {{RAM}} (for All Parameters)},
  author = {Komargodski, Ilan and Lin, Wei-Kai},
  year = {2021},
  doi = {10.1007/978-3-030-84259-8_20},
  abstract = {No abstract available},
  citationcount = {12},
  venue = {Annual International Cryptology Conference},
  keywords = {lower bound}
}

@article{komlosALowerBound1982,
  title = {A Lower Bound for {{Heilbronn}}'{{S}} Problem},
  author = {Komlos, J. and Pintz, J. and Szemer{\'e}di, E.},
  year = {1982},
  doi = {10.1112/JLMS/S2-25.1.13},
  abstract = {We disprove Heilbronn's conjecture---that N points lying in the unit disc necessarily contain a triangle of area less than c/N. Introduction We think that the best account of the problem can be achieved by simply copying the corresponding paragraphs from Roth's paper [6]. "Let Pv, P2,..., Pn (where n {\textasciicircum} 3) be a distribution of n points in a (closed) disc of unit area, such that the minimum of the areas of the triangles PiPjPk (taken over 1 {\textexclamdown} i {\textexclamdown} j {\textexclamdown} k {\textasciicircum} n) assumes its maximum possible value A = A(n). "Heilbronn conjectured that A(n) {\textexclamdown}t n  and Paul Erdos (see [3]) showed that this result, if true, would be best possible. "The first improvement on the trivial A(n) {\^{\textexclamdown}} n' was due to K. F. Roth, who in 1950 proved that A(n) {\textexclamdown} n-\{)  . There was no further improvement until about 20 years later, when Wolfgang M. Schmidt [7], using a different method, proved that A(n) {\textexclamdown} n-'ilogn)-''. (Actually Schmidt obtained a result containing an explicit constant.) In [4] Roth proved that A(n) {\textexclamdown} n ", where {\i}= 2-(0-8) = 1-105..., and in [5] refined his method to obtain A(n) {\textexclamdown} n "', where n' = {\textbar} (17-(65) / 2 ) = 1-117...". All efforts have been directed so far toward improving the upper bounds (the more interesting and much harder part of the problem). In this paper we are going to improve slightly Erdos' lower bound 1/N. We are thankful to Professor Roth for suggesting that Heilbronn's conjecture might be false. THEOREM. There exists a configuration of N points in the unit disc that contains no triangle of area less than Cl(logiV)/N . The proof will use a probabilistic construction. The main tool is a combinatorial lemma of independent interest, which will be formulated in terms of hypergraphs. Received 15 December, 1980. [J. LONDON MATH. SOC. (2), 25 (1982), 13-24] 14 JANOS KOMLOS, JANOS PINTZ AND ENDRE SZEMERED1 In {\S}1 we recall the necessary definitions about hypergraphs, and state the lemma. In {\S}2 we prove the theorem. In {\S}3 we present an essay that explains the ideas behind the proof of the basic lemma, while in {\S}4 we give the precise details. This essay was written by Joel Spencer. After having read the formal proof, both he and Professor Roth have been urging the authors to give a more understandable and enjoyable version based on first principles. Finally Joel Spencer decided to shoulder the task, and the authors were happy to incorporate his writing into this paper. It was also he who suggested the language of hypergraphs. The authors are much indebted for his invaluable help. 1. Hypergraphs: definitions and notation A hypergraph on n vertices (or points) is a pair (V, G) where V is a non-empty set of n elements, and G is a family of subsets of V. We shall also write graph for a hypergraph. The elements of G are called edges (hyperedges), and an edge is called an r-edge if it is a set of cardinality r. We assume that all edges have cardinality at least 2. The collection of all r-edges in G is denoted by G; thus G = G. We shall also r use the (somewhat dubious) notation G and G for the hypergraphs (V, G) and (V, G). We call G an r-graph if it is r-uniform, that is, all its edges are r-sets (G = G). Thus a 2-graph is an ordinary edge-graph, and a 3-graph is sometimes called a triangle-graph. A path of l e n g t h d, d {\textasciicircum} 1, c o n s i s t s of d + i different ve r t i ces vt,..., !;{\textexclamdown},+ , a n d d different e d g e s A x , . . . , A d s u c h t h a t V j , v j + 1 e A j , 1 {\textasciicircum} j {\textasciicircum} d . If v1 = vd+l and d {\textasciicircum} 2, we speak of a cycle of length d. In particular, a cycle of length 2 consists of 2 edges that have at least two vertices in common. The girth of G is the length of its shortest cycle. We say that a cycle is simple if non-consecutive edges (in the order of the cycle) are disjoint and consecutive edges have only one common vertex. It is easy to see that the girth of G is greater than g if and only if there is no cycle of length 2 and no simple cycle of length /, 3 {\textasciicircum} /{\^ } g, in G. The r-valency (or r-degree) of a vertex v is the number of r-edges containing v (adjacent to v). It is denoted by deg(u). The valency (or degree) of v is the sequence deg>>,deg>>,.... A set A of vertices is called an independent set if no subset of A is an edge. A set A of vertices is called an r-clique if all subsets of A of cardinality r are edges. Two points are neighbours if they are contained in the same edge. The neighbourhood of a point and that of a set of vertices are defined in the natural way. Further notation that we shall use is n = n(G) for the number of vertices of the graph G, e = e\{G) for the number of edges of G, t = {\pounds} deg(i;) = rn for n veV the average /--degree in G, and a = a(G) for the maximal size of independent sets in G (this is the independence number or stability number). We shall write t0, cx, c2,. . . for absolute constants. For a (hyper)graph with girth greater than 4 we shall also use the word uncrowded. A LOWER BOUND FOR HEILBRONN'S PROBLEM 15 We are now in a position to formulate the basic lemma. LEMMA 1. Let G be a 3-graph with n = n(G), t = t(G), and with girth greater than 4. If for some t0 {\textexclamdown} t {\textexclamdown} n 01 we have t {\textexclamdown} t, then 0L{\textquestiondown}c2(n/t)() . Remark 1. For any 3-graph one has the Turan-type estimate a {\textquestiondown} n/(3t), since a random (spanned) subgraph of size n/(2t) expectedly contains n/(24t) edges; delete all vertices in edges (see Spencer [8]). This is sharp up to constant multiple, for the Turan 3-graph (n/t disjoint cliques of size t) has t   t/2 and a = 2n/t. Thus the condition that G is uncrowded improves the bound on a by a factor (log t). This is again sharp, as is shown by random 3-graphs (choose nt triples at random and delete the few short cycles). Remark 2. Lemma 1 is an analogue of Lemma 1 in [1] or Theorem 2 in [2], which state that for a 2-graph with average valency t = 2e/n the Turan bound a {\textasciicircum} n(t+ can be improved to a {\textquestiondown} (i/0)(n/t)if only the graph is trianglefree. The proof will also be analogous to the (complicated) one in [1], which uses random methods, rather than to the simple inductive one in [2], which was thoroughly rewritten and simplified by Joel Spencer. The reader is challenged to give a simple, non-probabilistic proof for Lemma 1. 2. The proof of the theorem If we drop N points to the unit disc at random, then (as will be seen shortly) we can select half of them with smallest triangle c/N (an alternative proof for Erdos' lower bound). We shall improve on this method by dropping N points and then selecting an appropriate subset of N points. Define the numbers t and n by the implicit equations t = n / 1 0 0 , N = c2(n/t)() 112 . Set A = (1/200) t/n; then n = -Nt(' 2 and A = c3(log0/iV 2 = Cl()/N 2 . Let us drop n points to the unit disc at random, independently of each other, each with a uniform distribution. We define a 3-graph G on these n points (as vertices) by \{a, b, c\} {\texteuro} G if the points a, b, c form a triangle of area less than A. Now the probability that three random points form a triangle of area less than A is less than 2 2 --- d\{rn) = --- Inrdr = 32TIA {\textexclamdown} t/n r J r o 16 JANOS KOML6S, JANOS PINTZ AND ENDRE SZEMERED1 (fix two points at a distance r, and then average over r). Hence the expected number of triangles of area less than A is less than nt/6. Thus the expected value of f is less than t/2. Hence, by Markov's inequality (see {\S}4), with probability greater than 1/2, weget a 3-graph with t {\textexclamdown} t. Now we show that, with large probability, only o(n) short cycles occur in this 3-graph. All calculations will be based on the simple remark (already used above) that once two vertices have been chosen at a distance r, in order to get a triangle of area less than A, the third point has to belong to a strip of area less than 8A/r. The number of pairs of points at a distance less than d = n {$^\circ$} 6 is, with large probability, less than We discard these points. The number of 2-cycles is, with large probability, less than 2 f A c 4 n 4 ---Inrdr {\textexclamdown} c5t {\textexclamdown} n . The number of simple 3-cycles is, with large probability, less than\vphantom{\}\}\}}},
  citationcount = {125},
  venue = {No venue available}
}

@article{komlosOnHeilbronnS1981,
  title = {On Heilbronn's Triangle Problem},
  author = {Komlos, J. and Pintz, J. and Szemer{\'e}di, E.},
  year = {1981},
  doi = {10.1112/JLMS/S2-24.3.385},
  abstract = {Let PuP2,...,Pn be a distribution of n points (where n {\textasciicircum} 3) in a closed convex region K of unit area such that the minimum of the areas of the triangles PjPjPk (taken over all selections of three out of n points) assumes its maximum possible value A(/C ; n). We write A(n) = A(D ; n) if D is a disc. Heilbronn conjectured over 30 years ago that A(n) << n , where A << B means A {\textasciicircum} cB with an absolute constant c. It was proved by Erdos (see [3; Appendix]) that A ( n ) >> n " 2 , (1.1)},
  citationcount = {43},
  venue = {No venue available}
}

@article{kommAdviceAndRandomization2012,
  title = {Advice and Randomization in Online Computation},
  author = {Komm, D.},
  year = {2012},
  doi = {10.3929/ETHZ-A-007089360},
  abstract = {Online computation is both of theoretical interest and practical relevance as numerous computational problems require a model in which algorithms do not know the whole input at every time step during runtime. The established measurement for the output quality of these online algorithms is the so-called competitive analysis, introduced by Sleator and Tarjan in 1985. Similar to the decrease in accuracy we have to accept when efficiently (i. e., in polynomial time) solving NP-hard problems, the competitive ratio describes what we have to pay for not knowing the future. In this thesis, we want to measure how much additional information is both necessary and sufficient to escape from this dilemma, i. e., we want to understand what causes this, for the majority of problems, vast amount of precision we lose due to facing an online scenario. More specifically, we want to study the advice complexity of online problems, which describes the amount of information online algorithms lack, causing them to fail (compared to hypothetical offline algorithms that know all yet unrevealed parts of the input from the start). In the model used throughout this thesis, we equip online algorithms with an additional advice tape onto which an oracle, which sees the whole input before the algorithm is executed, may write binary information. The algorithm can then use these advice bits during computation. We call the minimum number of advice bits needed to compute an optimal solution for some online problem the information content of this problem. This information is what needs to be extracted from the instance in order to overcome the drawback of not completely knowing it in advance. We know that there exist well-studied online problems for which any solution computed by a deterministic online algorithm is (asymptotically) half as good as the optimal solution, because it does not know future input parts. However, a single bit of advice suffices to perform optimally. For many other problems, measuring the information content proves to be a more complicated task. Moreover, generalizing this idea, we study the tradeoff between obtaining highquality results (i. e., creating online algorithms with a reasonable competitive ratio) and the number of advice bits both necessary and sufficient for this. We study five online problems within the framework described above, the job shop scheduling problem with two jobs and unit-length tasks, the disjoint path allocation problem, the k-server problem, the set cover problem, and the knapsack problem. It turns out that online problems may behave very differently in terms of advice complexity. There are, for instance, problems that achieve very good results with a constant number of advice bits and other ones that, if given less advice than linear in the input size, are doomed to fail. Specifically, we ask how many advice bits are necessary and sufficient to (i) be optimal, (ii) to improve over purely deterministic strategies, or (iii) to be on par with (or better than) randomized strategies. Since we may look at computing with advice as supplying the best possible random string for any input, we are particularly interested in the last point and the further rela-},
  citationcount = {14},
  venue = {No venue available}
}

@article{kommOnTheAdvice2012,
  title = {On the Advice Complexity of the Set Cover Problem},
  author = {Komm, D. and Kr{\'a}lovic, Richard and M{\"o}mke, Tobias},
  year = {2012},
  doi = {10.1007/978-3-642-30642-6_23},
  abstract = {No abstract available},
  citationcount = {43},
  venue = {Computer Science Symposium in Russia}
}

@article{knigClickThroughPrediction2009,
  title = {Click-through Prediction for News Queries},
  author = {K{\"o}nig, A. and Gamon, Michael and Wu, Qiang},
  year = {2009},
  doi = {10.1145/1571941.1572002},
  abstract = {A growing trend in commercial search engines is the display of specialized content such as news, products, etc. interleaved with web search results. Ideally, this content should be displayed only when it is highly relevant to the search query, as it competes for space with "regular" results and advertisements. One measure of the relevance to the search query is the click-through rate the specialized content achieves when displayed; hence, if we can predict this click-through rate accurately, we can use this as the basis for selecting when to show specialized content. In this paper, we consider the problem of estimating the click-through rate for dedicated news search results. For queries for which news results have been displayed repeatedly before, the click-through rate can be tracked online; however, the key challenge for which previously unseen queries to display news results remains. In this paper we propose a supervised model that offers accurate prediction of news click-through rates and satisfies the requirement of adapting quickly to emerging news events.},
  citationcount = {72},
  venue = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  keywords = {query}
}

@article{kontogiannisEfficientAlgorithmsFor2007,
  title = {Efficient Algorithms for Constant Well Supported Approximate Equilibria in Bimatrix Games},
  author = {Kontogiannis, S. and Spirakis, P.},
  year = {2007},
  doi = {10.1007/978-3-540-73420-8_52},
  abstract = {No abstract available},
  citationcount = {40},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{kontogiannisPolynomialAlgorithmsFor2006,
  title = {Polynomial Algorithms for Approximating {{Nash}} Equilibria of Bimatrix Games},
  author = {Kontogiannis, S. and Panagopoulou, Panagiota N. and Spirakis, P.},
  year = {2006},
  doi = {10.1016/j.tcs.2008.12.033},
  abstract = {No abstract available},
  citationcount = {106},
  venue = {Theoretical Computer Science}
}

@article{kopelowitzColorDistanceOracles2016,
  title = {Color-Distance Oracles and Snippets},
  author = {Kopelowitz, T. and Krauthgamer, Robert},
  year = {2016},
  doi = {10.4230/LIPIcs.CPM.2016.24},
  abstract = {In the snippets problem we are interested in preprocessing a text T so that given two pattern queries P\_1 and P\_2, one can quickly locate the occurrences of the patterns in T that are the closest to each other. A closely related problem is that of constructing a color-distance oracle, where the goal is to preprocess a set of points from some metric space, in which every point is associated with a set of colors, so that given two colors one can quickly locate two points associated with those colors, that are as close as possible to each other. We introduce efficient data structures for both color-distance oracles and the snippets problem. Moreover, we prove conditional lower bounds for these problems from both the 3SUM conjecture and the Combinatorial Boolean Matrix Multiplication conjecture.},
  citationcount = {9},
  venue = {Annual Symposium on Combinatorial Pattern Matching},
  keywords = {data structure,lower bound,query}
}

@article{kopelowitzDynamicSetIntersection2014,
  title = {Dynamic Set Intersection},
  author = {Kopelowitz, T. and Pettie, Seth and Porat, E.},
  year = {2014},
  doi = {10.1007/978-3-319-21840-3_39},
  abstract = {No abstract available},
  citationcount = {24},
  venue = {Workshop on Algorithms and Data Structures},
  keywords = {dynamic}
}

@article{kopelowitzDynamicWeightedAncestors2007,
  title = {Dynamic Weighted Ancestors},
  author = {Kopelowitz, T. and Lewenstein, Moshe},
  year = {2007},
  doi = {10.5555/1283383.1283444},
  abstract = {In the weighted ancestor problem one preprocesses a weighted tree (the weights are on the nodes and increase with tree depth) to support predecessor queries, which are called weighted ancestors queries, on the paths from the query node to the root. Since, the weighted ancestor problem appears in numerous applications, the problem has been studied and solutions for static trees are well known. However, it has been an open question whether this can be solved optimally for the dynamic version of the problem, where node insertions are supported. Node insertions are leaf insertions or edge splittings. In this paper we present a solution for the dynamic weighted ancestors problem which supports queries and update operations in the same time bounds as those for dynamic predecessor structures.},
  citationcount = {36},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,query,static,update}
}

@article{kopelowitzHigherLowerBounds2014,
  title = {Higher Lower Bounds from the {{3SUM}} Conjecture},
  author = {Kopelowitz, T. and Pettie, Seth and Porat, E.},
  year = {2014},
  doi = {10.1137/1.9781611974331.CH89},
  abstract = {The 3SUM conjecture has proven to be a valuable tool for proving conditional lower bounds on dynamic data structures and graph problems. This line of work was initiated by P{\u a}trascu (STOC 2010) who reduced 3SUM to an offline SetDisjointness problem. However, the reduction introduced by P{\u a}trascu suffers from several inefficiencies, making it difficult to obtain tight conditional lower bounds from the 3SUM conjecture. In this paper we address many of the deficiencies of P{\u a}trascu's framework. We give new and efficient reductions from 3SUM to offline SetDisjointness and offline SetIntersection (the reporting version of SetDisjointness) which leads to polynomially higher lower bounds on several problems. Using our reductions, we are able to show the essential optimality of several algorithms, assuming the 3SUM conjecture. {$\bullet$} Chiba and Nishizeki's O(m{$\alpha$})-time algorithm (SICOMP 1985) for enumerating all triangles in a graph with arboricity/degeneracy {$\alpha$} is essentially optimal, for any {$\alpha$}. {$\bullet$} Bjorklund, Pagh, Williams, and Zwick's algorithm (ICALP 2014) for listing t triangles is essentially optimal (assuming the matrix multiplication exponent is {$\omega$} = 2). {$\bullet$} Any static data structure for SetDisjointness that answers queries in constant time must spend {\textohm}(N2--o(1)) time in preprocessing, where N is the size of the set system. These statements were unattainable via P{\u a}trascu's reductions. We also introduce several new reductions from 3SUM to pattern matching problems and dynamic graph problems. Of particular interest are new conditional lower bounds for dynamic versions of Maximum Cardinality Matching, which introduce a new technique for obtaining amortized lower bounds.},
  citationcount = {154},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,lower bound,query,reduction,static}
}

@article{kopelowitzImprovedWorstCase2018,
  title = {Improved Worst-Case Deterministic Parallel Dynamic Minimum Spanning Forest},
  author = {Kopelowitz, T. and Porat, E. and Rosenmutter, Yair},
  year = {2018},
  doi = {10.1145/3210377.3210403},
  abstract = {This paper gives a new deterministic algorithm for the dynamic Minimum Spanning Forest (MSF) problem in the EREW PRAM model, where the goal is to maintain a MSF of a weighted graph with n vertices and m edges while supporting edge insertions and deletions. We show that one can solve the dynamic MSF problem using O({\textsurd}n) processors and O({\l}ogn) worst-case update time, for a total of O({\textsurd}n{\l}ogn) work. This improves on the work of Ferragina [IPPS 1995] which costs O({\l}ogn) worst-case update time and O(n{$^2$}/3{\l}ogn) work.},
  citationcount = {8},
  venue = {ACM Symposium on Parallelism in Algorithms and Architectures},
  keywords = {dynamic,update,update time}
}

@article{kopelowitzOnLineIndexing2012,
  title = {On-Line Indexing for General Alphabets via Predecessor Queries on Subsets of an Ordered List},
  author = {Kopelowitz, T.},
  year = {2012},
  doi = {10.1109/FOCS.2012.79},
  abstract = {The problem of Text Indexing is a fundamental algorithmic problem in which one wishes to preprocess a text in order to quickly locate pattern queries within the text. In the ever evolving world of dynamic and on-line data, there is also a need for developing solutions to index texts which arrive online, i.e. a character at a time, and still be able to quickly locate said patterns. In this paper, a new solution for on-line indexing is presented by providing an on-line suffix tree construction in O(log log n + log log {\textbar}{$\Sigma\vert$}) worst-case expected time per character, where n is the size of the string, and {$\Sigma$} is the alphabet. This improves upon all previously known on-line suffix tree constructions for general alphabets, at the cost of having the run time in expectation. The main idea is to reduce the problem of constructing a suffix tree on-line to an interesting variant of the order maintenance problem, which may be of independent interest. In the famous order maintenance problem, one wishes to maintain a dynamic list L of size n under insertions, deletions, and order queries. In an order query, one is given two nodes from L and must determine which node precedes the other in L. In an extension to this problem, named the Predecessor search on Dynamic Subsets of an Ordered Dynamic List problem (POLP for short), it is also necessary to maintain dynamic subsets S1, {$\cdot$} {$\cdot$} {$\cdot$} , Sk {$\subseteq$} L, such that given some u {$\in$} L it will be possible to quickly locate the predecessor of u in Si, for any integer 1 {$\leq$} i {$\leq$} k. This paper provides an efficient data structure capable of locating the predecessor of u in Si in O(log log n) worst-case time and answering order queries on L in O(1) worst-case time, while allowing updates to L in O(1) worst-case expected time and updates to the subsets in O(log log n) worst-case expected time. This improves over a previous data structure which may be implicitly obtained from Dietz [8], in which the updates to the sets and L are done in O(log log n) amortized expected time. In addition, the bounds shown here match the currently best known bounds for predecessor search in the RAM model. Furthermore, this paper improves or simplifies bounds for several additional applications, including fully-persistent arrays, the monotonic list labeling problem, and the Order-Maintenance Problem.},
  citationcount = {35},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,dynamic,query,update}
}

@article{kopelowitzOnTheSpace2023,
  title = {On the Space Usage of Approximate Distance Oracles with Sub-2 Stretch},
  author = {Kopelowitz, T. and Korin, Ariel and Roditty, L.},
  year = {2023},
  doi = {10.48550/arXiv.2310.12239},
  abstract = {For an undirected unweighted graph G = (V, E) with n vertices and m edges, let d(u, v) denote the distance from u in V to v in V in G. An (alpha, beta)-stretch approximate distance oracle (ADO) for G is a data structure that, given u, v in V, returns in constant time a value d-hat (u, v) such that d(u, v){\textexclamdown}= d-hat (u, v){\textexclamdown}= alpha * d(u, v) + beta, for some reals alpha{\textquestiondown}1, beta. If beta = 0, we say that the ADO has stretch alpha. Thorup and Zwick (2005) showed that one cannot beat stretch 3 with subquadratic space (in terms of n) for general graphs. Patrascu and Roditty (2010) showed that one can obtain stretch 2 using O(m{\textasciicircum}(1/3)n{\textasciicircum}(4/3)) space, and so if m is subquadratic in n, then the space usage is also subquadratic. Moreover, Patrascu and Roditty (2010) showed that one cannot beat stretch 2 with subquadratic space even for graphs where m = O-tilde(n), based on the set-intersection hypothesis. In this paper, we investigate the minimum possible stretch achievable by an ADO as a function of the graph's maximum degree, a study motivated by the question of identifying the conditions under which an ADO can be stored with subquadratic space while still ensuring a sub-2 stretch. In particular, we show that if the maximum degree in G is Delta\_G{\textexclamdown}= O(n{\textasciicircum}(1/k - epsilon)) for some 0{\textexclamdown}epsilon{\textexclamdown}= 1/k, then there exists a (2, 1 - k)-stretch ADO for G that uses O-tilde(n{\textasciicircum}(2 - (k * epsilon) / 3)) space. For k = 2, this result implies a subquadratic sub-2 stretch ADO for graphs with Delta\_G{\textexclamdown}= O(n{\textasciicircum}(1/2 - epsilon)). We provide tight lower bounds for the upper bound under the same set intersection hypothesis, showing that if Delta\_G = Theta(n{\textasciicircum}(1/k)), a (2, 1 - k)-stretch ADO requires Omega-tilde(n{\textasciicircum}2) space. Moreover, we show that for constants epsilon, c{\textquestiondown}0, a (2 - epsilon, c)-stretch ADO requires Omega-tilde(n{\textasciicircum}2) space even for graphs with Delta\_G = Theta-tilde(1).},
  citationcount = {1},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,lower bound}
}

@article{kopelowitzPersistencyInSuffix2011,
  title = {Persistency in Suffix Trees with Applications to String Interval Problems},
  author = {Kopelowitz, T. and Lewenstein, Moshe and Porat, E.},
  year = {2011},
  doi = {10.1007/978-3-642-24583-1_8},
  abstract = {No abstract available},
  citationcount = {9},
  venue = {SPIRE}
}

@article{kopelowitzTowardsOptimalSet2020,
  title = {Towards Optimal Set-Disjointness and Set-Intersection Data Structures},
  author = {Kopelowitz, T. and Williams, V. V.},
  year = {2020},
  doi = {10.4230/LIPIcs.ICALP.2020.74},
  abstract = {In the online set-disjointness problem the goal is to preprocess a family of sets F , so that given two sets S, S 0 {$\in$} F , one can quickly establish whether the two sets are disjoint or not. If N = P S {$\in$}F {\textbar} S {\textbar} , then let N p be the preprocessing time and let N q be the query time. The most efficient known combinatorial algorithm is a generalization of an algorithm by Cohen and Porat [TCS'10] which has a tradeoff curve of p + q = 2. Kopelowitz, Pettie, and Porat [SODA'16] showed that, based on the 3SUM hypothesis, there is a conditional lower bound curve of p + 2 q {$\geq$} 2. Thus, the current state-of-the-art exhibits a large gap. The online set-intersection problem is the reporting version of the online set-disjointness problem, and given a query, the goal is to report all of the elements in the intersection. When considering algorithms with N p preprocessing time and N q + O ( op ) query time, where op is the size of the output, the combinatorial algorithm for online set-disjointess can be extended to solve online set-intersection with a tradeoff curve of p + q = 2. Kopelowitz, Pettie, and Porat [SODA'16] showed that, assuming the 3SUM hypothesis, for 0 {$\leq$} q {$\leq$} 2 / 3 this curve is tight. However, for 2 / 3 {$\leq$} q {\textexclamdown} 1 there is no known lower bound. In this paper we close both gaps by showing the following:},
  citationcount = {6},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,lower bound,query,query time}
}

@article{korhonenMinorContainmentAnd2024,
  title = {Minor Containment and Disjoint Paths in Almost-Linear Time},
  author = {Korhonen, T. and Pilipczuk, Micha{\l} and Stamoulis, Giannos},
  year = {2024},
  doi = {10.1109/FOCS61266.2024.00014},
  abstract = {We give an algorithm that, given graphs {\textexclamdown}tex{\textquestiondown}G{\textexclamdown}/tex{\textquestiondown} and {\textexclamdown}tex{\textquestiondown}H{\textexclamdown}/tex{\textquestiondown}, tests whether {\textexclamdown}tex{\textquestiondown}H{\textexclamdown}/tex{\textquestiondown} is a minor of {\textexclamdown}tex{\textquestiondown}G{\textexclamdown}/tex{\textquestiondown} in time {\textexclamdown}tex{\textquestiondown}\{O\}\textsubscript{\{\vphantom\}}H\vphantom\{\}(\{n\}\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}){\textexclamdown}/tex{\textquestiondown}; here, {\textexclamdown}tex{\textquestiondown}n{\textexclamdown}/tex{\textquestiondown} is the number of vertices of {\textexclamdown}tex{\textquestiondown}G{\textexclamdown}/tex{\textquestiondown} and the {\textexclamdown}tex{\textquestiondown}\{O\}\textsubscript{\{\vphantom\}}H\vphantom\{\}(.){\textexclamdown}/tex{\textquestiondown} -notation hides factors that depend on {\textexclamdown}tex{\textquestiondown}H{\textexclamdown}/tex{\textquestiondown} and are computable. By the Graph Minor Theorem, this implies the existence of an {\textexclamdown}tex{\textquestiondown}n\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}{\textexclamdown}/tex{\textquestiondown} -time membership test for every minor-closed class of graphs. More generally, we give an {\textexclamdown}tex{\textquestiondown}\{O\}\textsubscript{\{\vphantom\}}H,{\textbar}X{\textbar}\vphantom\{\}(m\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\}){\textexclamdown}/tex{\textquestiondown} -time algorithm for the rooted version of the problem, in which {\textexclamdown}tex{\textquestiondown}G{\textexclamdown}/tex{\textquestiondown} comes with a set of roots {\textexclamdown}tex{\textquestiondown}X{$\subseteq$}V(G){\textexclamdown}/tex{\textquestiondown} and some of the branch sets of the sought minor model of {\textexclamdown}tex{\textquestiondown}H{\textexclamdown}/tex{\textquestiondown} are required to contain prescribed subsets of {\textexclamdown}tex{\textquestiondown}X{\textexclamdown}/tex{\textquestiondown}; here, {\textexclamdown}tex{\textquestiondown}m{\textexclamdown}/tex{\textquestiondown} is the total number of vertices and edges of {\textexclamdown}tex{\textquestiondown}G{\textexclamdown}/tex{\textquestiondown}. This captures the Disjoint Pathsproblem, for which we obtain an {\textexclamdown}tex{\textquestiondown}\{O\}\textsubscript{\{\vphantom\}}k\vphantom\{\}(m\textsuperscript{\{\vphantom\}}1+o(1){\textbackslash}\vphantom\{\}{\textexclamdown}/tex{\textquestiondown} -time algorithm, where {\textexclamdown}tex{\textquestiondown}k{\textexclamdown}/tex{\textquestiondown} is the number of terminal pairs. For all the mentioned problems, the fastest algorithms known before are due to Kawarabayashi, Kobayashi, and Reed [JCTB 2012], and have a time complexity that is quadratic in the number of vertices of {\textexclamdown}tex{\textquestiondown}G{\textexclamdown}/tex{\textquestiondown}. Our algorithm has two main ingredients: First, we show that by using the dynamic treewidth data structure of Korhonen, Majewski, Nadara, Pilipczuk, and Sokolowski [FOCS 2023], the irrelevant vertex technique of Robertson and Seymour can be implemented in almost-linear time on apex-minor-free graphs. Then, we apply the recent advances in almost-linear time flow/cut algorithms to give an almost-linear time implementation of the recursive understanding technique, which effectively reduces the problem to apex-minor-free graphs.},
  citationcount = {7},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,dynamic}
}

@article{kornackerConcurrencyAndRecovery1997,
  title = {Concurrency and Recovery in Generalized Search Trees},
  author = {Kornacker, Marcel and Mohan, C. and Hellerstein, J.},
  year = {1997},
  doi = {10.1145/253260.253272},
  abstract = {This paper presents general algorithms for concurrency control in tree-based access methods as well as a recovery protocol and a mechanism for ensuring repeatable read. The algorithms are developed in the context of the Generalized Search Tree (GiST) data structure, an index structure supporting an extensible set of queries and data types. Although developed in a GiST context, the algorithms are generally applicable to many tree-based access methods. The concurrency control protocol is based on an extension of the link technique originally developed for B-trees, and completely avoids holding node locks during I/Os. Repeatable read isolation is achieved with a novel combination of predicate locks and two-phase locking of data records. To our knowledge, this is the first time that isolation issues have been addressed outside the context of B-trees. A discussion of the fundamental structural differences between B-trees and more general tree structures like GiSTs explains why the algorithms developed here deviate from their B-tree counterparts. An implementation of GiSTs emulating B-trees in DB2/Common Server is underway.},
  citationcount = {173},
  venue = {ACM SIGMOD Conference}
}

@article{kornOnTheDimensionality2001,
  title = {On the 'dimensionality Curse' and the 'Self-Similarity Blessing'},
  author = {Korn, Flip and Pagel, Bernd-Uwe and Faloutsos, C.},
  year = {2001},
  doi = {10.1109/69.908983},
  abstract = {Spatial queries in high-dimensional spaces have been studied extensively. Among them, nearest neighbor queries are important in many settings, including spatial databases (Find the k closest cities) and multimedia databases (Find the k most similar images). Previous analyses have concluded that nearest-neighbor search is hopeless in high dimensions due to the notorious "curse of dimensionality". We show that this may be overpessimistic. We show that what determines the search performance (at least for R-tree-like structures) is the intrinsic dimensionality of the data set and not the dimensionality of the address space (referred to as the embedding dimensionality). The typical (and often implicit) assumption in many previous studies is that the data is uniformly distributed, with independence between attributes. However, real data sets overwhelmingly disobey these assumptions; rather, they typically are skewed and exhibit intrinsic ("fractal") dimensionalities that are much lower than their embedding dimension, e.g. due to subtle dependencies between attributes. We show how the Hausdorff and Correlation fractal dimensions of a data set can yield extremely accurate formulas that can predict the I/O performance to within one standard deviation on multiple real and synthetic data sets.},
  citationcount = {215},
  venue = {IEEE Transactions on Knowledge and Data Engineering},
  keywords = {query}
}

@misc{kortenStrongerCellProbe2025,
  title = {Stronger {{Cell Probe Lower Bounds}} via {{Local PRGs}}},
  author = {Korten, Oliver and Pitassi, Toniann and Impagliazzo, Russell},
  year = {2025},
  month = mar,
  number = {TR25-030},
  eprint = {TR25-030},
  publisher = {Electronic Colloquium on Computational Complexity},
  issn = {1433-8092},
  url = {https://eccc.weizmann.ac.il/report/2025/030/},
  urldate = {2025-03-16},
  abstract = {In this work we observe a tight connection between three topics: NC0 cryptography, NC0 range avoidance, and static data structure lower bounds. Using this connection, we leverage techniques from the cryptanalysis of NC0 PRGs to prove state-of-the-art results in the latter two subjects. Our main result is a quadratic improvement to the best known static data structure lower bounds, breaking a barrier which has stood for several decades. Prior to our work, the best known lower bound for any explicit problem with M inputs and N queries was SNt1(logM)1-t1 for any setting of the word length w (where S= space and t= time) (Siegel `89). We prove, for the same class of explicit problems considered by Siegel, a quadratically stronger lower bound of the form SNt2(logM)1-t22-O(w)~ for all even t0. Second, for the restricted class of nonadaptive bit probe data structures, we improve on this lower bound polynomially: for all odd constants t1 we give an explicit problem with N queries and MNO(1) inputs and prove a lower bound S(Nt2+t) for some constant t0. Our results build off of an exciting body of work on refuting semi-random CSPs. We then utilize our explicit cell probe lower bounds to obtain the best known unconditional algorithms for NC0 range avoidance: we can solve any instance with stretch nm in polynomial time once mnt2 when t is even; with the aid of an NP oracle we can solve any instance with mnt2-t for t0 when t is odd. Finally, using our main correspondence we establish novel barrier results for obtaining significant improvements to our cell probe lower bounds: (i) near-optimal space lower bounds for an explicit problem with t=4w=1~ implies EXPNPNC1~; (ii) under the widely-believed assumption that polynomial-stretch NC0 PRGs exist, there is no natural proof of a lower bound of the form SN(1) when t=(1), w=1.},
  archiveprefix = {Electronic Colloquium on Computational Complexity},
  langid = {english},
  keywords = {adaptive,cell probe,cell probe model,CSP refutation,data structure,data structures,lower bound,NC0 Cryptography,non-adaptive,query,range avoidance,static},
  file = {/Users/tulasi/Zotero/storage/WZNR625P/Korten et al. - 2025 - Stronger Cell Probe Lower Bounds via Local PRGs.pdf}
}

@article{koscheAbsentSubsequencesIn2021,
  title = {Absent Subsequences in Words},
  author = {Kosche, Maria and Koss, Tore and Manea, F. and Siemer, Stefan},
  year = {2021},
  doi = {10.1007/978-3-030-89716-1_8},
  abstract = {No abstract available},
  citationcount = {16},
  venue = {Reachability Problems}
}

@article{koutrisTheFineGrained2023,
  title = {The Fine-Grained Complexity of {{CFL}} Reachability},
  author = {Koutris, Paraschos and Deep, Shaleen},
  year = {2023},
  doi = {10.1145/3571252},
  abstract = {Many problems in static program analysis can be modeled as the context-free language (CFL) reachability problem on directed labeled graphs. The CFL reachability problem can be generally solved in time O(n3), where n is the number of vertices in the graph, with some specific cases that can be solved faster. In this work, we ask the following question: given a specific CFL, what is the exact exponent in the monomial of the running time? In other words, for which cases do we have linear, quadratic or cubic algorithms, and are there problems with intermediate runtimes? This question is inspired by recent efforts to classify classic problems in terms of their exact polynomial complexity, known as fine-grained complexity. Although recent efforts have shown some conditional lower bounds (mostly for the class of combinatorial algorithms), a general picture of the fine-grained complexity landscape for CFL reachability is missing. Our main contribution is lower bound results that pinpoint the exact running time of several classes of CFLs or specific CFLs under widely believed lower bound conjectures (e.g., Boolean Matrix Multiplication, k-Clique, APSP, 3SUM). We particularly focus on the family of Dyck-k languages (which are strings with well-matched parentheses), a fundamental class of CFL reachability problems. Remarkably, we are able to show a {\textohm}(n2.5) lower bound for Dyck-2 reachability, which to the best of our knowledge is the first super-quadratic lower bound that applies to all algorithms, and shows that CFL reachability is strictly harder that Boolean Matrix Multiplication. We also present new lower bounds for the case of sparse input graphs where the number of edges m is the input parameter, a common setting in the database literature. For this setting, we show a cubic lower bound for Andersen's Pointer Analysis which significantly strengthens prior known results.},
  citationcount = {4},
  venue = {Proc. ACM Program. Lang.},
  keywords = {lower bound,static}
}

@article{koutsoupiasTightBoundsFor1998,
  title = {Tight Bounds for 2-Dimensional Indexing Schemes},
  author = {Koutsoupias, E. and Taylor, David Scot},
  year = {1998},
  doi = {10.1145/275487.275494},
  abstract = {We study the trade-off between storage redundancy and access overhead for range queries, using the framework of [6]. We show that the Fibonacci workload of size n, which is the regular 2-dimensional grid rotated by the golden ratio, does not admit an indexing scheme with access overhead less than the block size B (the worst possible access overhead), even for storage redundancy as high as clogn, for some constant c. We also show that this bound is tight (up to a constant factor) by providing an indexing scheme with storage redundancy O(logn) and constant access overhead, for any 2-dimensional workload. We extend the lower bound to random point sets and show that if the maximum storage redundancy is less than cloglogn, the access overhead is 13, Finally, we explore the relation between indexability and fractal (Hausdorff) dimension of point sets.},
  citationcount = {27},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems}
}

@article{kowalikAdjacencyQueriesIn2007,
  title = {Adjacency Queries in Dynamic Sparse Graphs},
  author = {Kowalik, Lukasz},
  year = {2007},
  doi = {10.1016/j.ipl.2006.12.006},
  abstract = {No abstract available},
  citationcount = {31},
  venue = {Information Processing Letters},
  keywords = {dynamic,query}
}

@article{kozikFullyDynamicEvaluation2013,
  title = {Fully Dynamic Evaluation of Sequence Pair},
  author = {Kozik, A.},
  year = {2013},
  doi = {10.1109/TCAD.2013.2244642},
  abstract = {In the electronic design automation field, as well as in other areas, problem instances and solutions are often subject to discrete changes. The foundational significance of efficient updates of the criterion value after dynamic updates, instead of recomputing it from scratch each time, has attracted a lot of research. In this paper, motivated by the significance of the sequence pair (SP) representation for floorplanning, we develop a fully dynamic algorithm of SP evaluation, that efficiently updates a criterion value after insertions and deletions of SP elements and after modifications of element weights. Our result is based on a new data structure for the predecessor problem, which maintains the whole history of its dynamic modifications, and the path-compression technique from the union-find problem, which efficiently support predecessor queries. Numerical experiments showed that our algorithm exhibits linear-time behavior and considerably reduces the time of SP evaluation, compared to other approaches.},
  citationcount = {6},
  venue = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  keywords = {data structure,dynamic,query,update}
}

@article{krauthgamerMeasuredDescentA2004,
  title = {Measured Descent: A New Embedding Method for Finite Metrics},
  author = {Krauthgamer, Robert and Lee, James R. and Mendel, M. and Naor, A.},
  year = {2004},
  doi = {10.1007/s00039-005-0527-6},
  abstract = {No abstract available},
  citationcount = {177},
  venue = {45th Annual IEEE Symposium on Foundations of Computer Science}
}

@article{krauthgamerTheBlackBox2005,
  title = {The Black-Box Complexity of Nearest-Neighbor Search},
  author = {Krauthgamer, Robert and Lee, James R.},
  year = {2005},
  doi = {10.1016/j.tcs.2005.09.017},
  abstract = {No abstract available},
  citationcount = {64},
  venue = {Theoretical Computer Science}
}

@article{kremerOnRandomizedOne1995,
  title = {On Randomized One-Round Communication Complexity},
  author = {Kremer, I. and Nisan, N. and Ron, D.},
  year = {1995},
  doi = {10.1007/s000370050018},
  abstract = {No abstract available},
  citationcount = {250},
  venue = {Symposium on the Theory of Computing}
}

@article{kreveldRangeSearching2016,
  title = {Range Searching},
  author = {Kreveld, M. V. and L{\"o}ffler, M.},
  year = {2016},
  doi = {10.1007/978-1-4939-2864-4_510},
  abstract = {No abstract available},
  citationcount = {55},
  venue = {Encyclopedia of Algorithms}
}

@article{krizancRangeModeAnd2003,
  title = {Range Mode and Range Median Queries on Lists and Trees},
  author = {Krizanc, D. and Morin, Pat and Smid, M.},
  year = {2003},
  doi = {10.1007/978-3-540-24587-2_53},
  abstract = {No abstract available},
  citationcount = {79},
  venue = {Nordic Journal of Computing},
  keywords = {query}
}

@article{krohnAlgebraicTheoryOf1965,
  title = {Algebraic Theory of Machines. {{I}}. {{Prime}} Decomposition Theorem for Finite Semigroups and Machines},
  author = {Krohn, K. and Rhodes, J.},
  year = {1965},
  doi = {10.1090/S0002-9947-1965-0188316-1},
  abstract = {Introduction. In the following all semigroups are of finite order. One semigroup S, is said to divide another semigroup S2, written SlIS2, if S, is a homomorphic image of a subsemigroup of S2. The semidirect product of S2 by Sl, with connecting homomorphism Y, is written S2 X y Sl. See Definition 1.6. A semigroup S is called irreducible if for all finite semigroups S2 and Si and all connecting homomorphisms Y, S I (S2 X Y SJ) implies S I S2 or S I S1. It is shown that S is irreducible if and only if either:},
  citationcount = {341},
  venue = {No venue available}
}

@article{kronmalOnTheAlias1979,
  title = {On the Alias Method for Generating Random Variables from a Discrete Distribution},
  author = {Kronmal, R. and Peterson, A. V.},
  year = {1979},
  doi = {10.1080/00031305.1979.10482697},
  abstract = {Abstract The alias method of Walker is a clever, new, fast method for generating random variables from an arbitrary, specified discrete distribution. A simple probabilistic proof is given, in terms of mixtures, that the method works for any discrete distribution with a finite number of outcomes. A more efficient version of the table-generating portion of the method is described. Finally, a brief discussion on efficiency of the method is given. We believe that the generality, speed, and simplicity of the method make it attractive for use in generating discrete random variables.},
  citationcount = {166},
  venue = {No venue available}
}

@article{kuelbsMetricEntropyAnd1993,
  title = {Metric Entropy and the Small Ball Problem for {{Gaussian}} Measures},
  author = {Kuelbs, J. and Li, Wenbo V.},
  year = {1993},
  doi = {10.1006/JFAN.1993.1107},
  abstract = {Abstract We establish a precise link between the small ball problem for a Gaussian measure {$\mu$} on a separable Banach space and the metric entropy of the unit ball of the Hubert space H {$\mu$} generating {$\mu$}. This link allows us to compute small ball probabilities from metric entropy results, and vice versa.},
  citationcount = {197},
  venue = {No venue available}
}

@article{kulikovPolynomialFormulationsAs2022,
  title = {Polynomial Formulations as a Barrier for Reduction-Based Hardness Proofs},
  author = {Kulikov, A. and Mihajlin, Ivan},
  year = {2022},
  doi = {10.48550/arXiv.2205.07709},
  abstract = {The Strong Exponential Time Hypothesis (SETH) asserts that for every {$\varepsilon>$}0 there exists k such that k -SAT requires time (2-{$\varepsilon$})\textsuperscript{\{\vphantom\}}n\vphantom\{\} . The field of fine-grained complexity has leveraged SETH to prove quite tight conditional lower bounds for dozens of problems in various domains and complexity classes, including Edit Distance, Graph Diameter, Hitting Set, Independent Set, and Orthogonal Vectors. Yet, it has been repeatedly asked in the literature whether SETH-hardness results can be proven for other fundamental problems such as Hamiltonian Path, Independent Set, Chromatic Number, MAX- k -SAT, and Set Cover. In this paper, we show that fine-grained reductions implying even {$\lambda$}\textsuperscript{\{\vphantom\}}n\vphantom\{\} -hardness of these problems from SETH for any {$\lambda>$}1 , would imply new circuit lower bounds: super-linear lower bounds for Boolean series-parallel circuits or polynomial lower bounds for arithmetic circuits (each of which is a four-decade open question). We also extend this barrier result to the class of parameterized problems. Namely, for every {$\lambda>$}1 we conditionally rule out fine-grained reductions implying SETH-based lower bounds of {$\lambda$}\textsuperscript{\{\vphantom\}}k\vphantom\{\} for a number of problems parameterized by the solution size k . Our main technical tool is a new concept called polynomial formulations. In particular, we show that many problems can be represented by relatively succinct low-degree polynomials, and that any problem with such a representation cannot be proven SETH-hard (without proving new circuit lower bounds).},
  citationcount = {7},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {lower bound,reduction}
}

@article{kumarLowerBoundsFor2019,
  title = {Lower Bounds for Matrix Factorization},
  author = {Kumar, Mrinal and {lee Volk}, Ben},
  year = {2019},
  doi = {10.1007/s00037-021-00205-2},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Computational Complexity},
  keywords = {lower bound}
}

@article{kumarTheLimitsOf2013,
  title = {The Limits of Depth Reduction for Arithmetic Formulas: It's All about the Top Fan-In},
  author = {Kumar, Mrinal and Saraf, Shubhangi},
  year = {2013},
  doi = {10.1145/2591796.2591827},
  abstract = {In recent years, a very exciting and promising method for proving lower bounds for arithmetic circuits has been proposed. This method combines the method of depth reduction developed in the works of Agrawal and Vinay[1], Koiran [11] and Tavenas [16], and the use of the shifted partial derivative complexity measure developed in the works of Kayal [9] and Gupta et al [5]. These results inspired a flurry of other beautiful results and strong lower bounds for various classes of arithmetic circuits, in particular a recent work of Kayal et al [10] showing superpolynomial lower bounds for regular arithmetic formulas via an improved depth reduction for these formulas. It was left as an intriguing question if these methods could prove superpolynomial lower bounds for general (homogeneous) arithmetic formulas, and if so this would indeed be a breakthrough in arithmetic circuit complexity. In this paper we study the power and limitations of depth reduction and shifted partial derivatives for arithmetic formulas. We do it via studying the class of depth 4 homogeneous arithmetic circuits. We show: (1) the first superpolynomial lower bounds for the class of homogeneous depth 4 circuits with top fan-in o(log n). The core of our result is to show improved depth reduction for these circuits. This class of circuits has received much attention for the problem of polynomial identity testing. We give the first nontrivial lower bounds for these circuits for any top fan-in {$\geq$} 2. (2) We show that improved depth reduction is not possible when the top fan-in is {\textohm}(log n). In particular this shows that the depth reduction procedure of Koiran and Tavenas [11, 16] cannot be improved even for homogeneous formulas, thus strengthening the results of Fournier et al [3] who showed that depth reduction is tight for circuits, and answering some of the main open questions of [10, 3]. Our results in particular suggest that the method of improved depth reduction and shifted partial derivatives may not be powerful enough to prove superpolynomial lower bounds for (even homogeneous) arithmetic formulas.},
  citationcount = {37},
  venue = {SIAM journal on computing (Print)}
}

@article{kun-koAnAdaptiveStep2019,
  title = {An Adaptive Step toward the Multiphase Conjecture},
  author = {{Kun-Ko}, Young and Weinstein, Omri},
  year = {2019},
  doi = {10.1109/FOCS46700.2020.00075},
  abstract = {In 2010, P{\u a}tra{\c s}cu proposed a dynamic set-disjointness problem, known as the Multiphase problem, as a candidate for proving polynomial lower bounds on the operational time of dynamic data structures. He conjectured that any data structure for the Multiphase problem must make n\textsuperscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\} cell-probes in either update or query phases, and showed that this would imply similar unconditional lower bounds on many important dynamic data structure problems. There has been almost no progress on this conjecture in the past decade since its introduction. We show an {\textohm}\vphantom\{\}({\textsurd}\{n\}) cell-probe lower bound on the Multiphase problem for data structures with general (adaptive) updates, and queries with unbounded but ``layered'' adaptivity. This result captures all known set-intersection data structures and significantly strengthens previous Multiphase lower bounds, which only captured non-adaptive data structures. Our main technical result is a communication lower bound on a 4-party variant of P{\u a}tra{\c s}cu's Number-On-Forehead Multiphase game, using information complexity techniques. We then use this result to make progress on understanding the power of nonlinear gates in networks computing linear operators, a long-standing open problem in circuit complexity and network design: We show that any depth-d circuit that computes a random m{\texttimes}n linear operator x{$\mapsto$}Ax using gates of degree k (width-k DNFs) must have {\textohm}(m{$\cdot$}n\textsuperscript{\{\vphantom\}}1/2(d+k)\vphantom\{\}) wires. Finally, we show that a lower bound on P{\u a}tra{\c s}cu's original NOF game would imply a polynomial wire lower bound (n\textsuperscript{\{\vphantom\}}1+{\textohm}(1/d)\vphantom\{\}) for circuits with arbitrary gates computing a random linear operator. This suggests that the NOF conjecture is much stronger than its data structure counterpart.},
  citationcount = {2},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {cell probe,communication,data structure,dynamic,lower bound,non-adaptive,query,update}
}

@article{kushilevitzCommunicationComplexity1997,
  title = {Communication Complexity},
  author = {Kushilevitz, Eyal},
  year = {1997},
  doi = {10.1016/S0065-2458(08)60342-3},
  abstract = {No abstract available},
  citationcount = {647},
  venue = {Advances in Computing},
  keywords = {communication,communication complexity}
}

@article{kushilevitzEfficientSearchFor1998,
  title = {Efficient Search for Approximate Nearest Neighbor in High Dimensional Spaces},
  author = {Kushilevitz, E. and Ostrovsky, R. and Rabani, Y.},
  year = {1998},
  doi = {10.1145/276698.276877},
  abstract = {We address the problem ofdesigning data structures that allow efficient search f or approximate nearest neighbors. More specifically, given a database consisting ofa set ofvectors in some high dimensional Euclidean space, we want to construct a space-efficient data structure that would allow us to search, given a query vector, for the closest or nearly closest vector in the database. We also address this problem when distances are measured by the L1 norm and in the Hamming cube. Significantly improving and extending recent results ofKleinberg, we construct data structures whose size is polynomial in the size ofthe database and search algorithms that run in time nearly linear or nearly quadratic in the dimension. (Depending on the case, the extra factors are polylogarithmic in the size ofthe database.)},
  citationcount = {523},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,query}
}

@article{kushNearNeighborSearch2021,
  title = {Near Neighbor Search via Efficient Average Distortion Embeddings},
  author = {Kush, D. and Nikolov, Aleksandar and Tang, Haohua},
  year = {2021},
  doi = {10.4230/LIPIcs.SoCG.2021.50},
  abstract = {A recent series of papers by Andoni, Naor, Nikolov, Razenshteyn, and Waingarten (STOC 2018, FOCS 2018) has given approximate near neighbour search (NNS) data structures for a wide class of distance metrics, including all norms. In particular, these data structures achieve approximation on the order of p for {$\ell_{p}^d$} norms with space complexity nearly linear in the dataset size n and polynomial in the dimension d, and query time sub-linear in n and polynomial in d. The main shortcoming is the exponential in d pre-processing time required for their construction. In this paper, we describe a more direct framework for constructing NNS data structures for general norms. More specifically, we show via an algorithmic reduction that an efficient NNS data structure for a given metric is implied by an efficient average distortion embedding of it into {$\ell_1$} or into Euclidean space. In particular, the resulting data structures require only polynomial pre-processing time, as long as the embedding can be computed in polynomial time. As a concrete instantiation of this framework, we give an NNS data structure for {$\ell_p$} with efficient pre-processing that matches the approximation factor, space and query complexity of the aforementioned data structure of Andoni et al. On the way, we resolve a question of Naor (Analysis and Geometry in Metric Spaces, 2014) and provide an explicit, efficiently computable embedding of {$\ell_p$}, for p{$\geq$}2, into {$\ell_2$} with (quadratic) average distortion on the order of p. We expect our approach to pave the way for constructing efficient NNS data structures for all norms.},
  citationcount = {2},
  venue = {International Symposium on Computational Geometry},
  keywords = {data structure,query,query complexity,query time,reduction}
}

@article{kwanTheIO1985,
  title = {The {{I}}/{{O}} Performance of Multiway Mergesort and Tag Sort},
  author = {Kwan, S. C. and Baer, J.},
  year = {1985},
  doi = {10.1109/TC.1985.5009392},
  abstract = {We develop models of secondary storage to evaluate external sorting and use them to analyze the average I/O access time of mergesort and tag sort on files with uniform key distribution. The k-way mergesort takes [logk R] merge passes to sort a file with R initial sorted runs. Choosing k as large as possible reduces the number of merge passes, but we show that under the assumptions of our models, the I/O access time of the merge phase in mergesort increases as a function of k. For large files with short keys, tag sort provides a promising alternative to mergesort. We analyze the I/O access time of tag sort with a ``sequential scan'' distribution method. We show that for large files tag sort takes asymptotically less I/O time than mergesort.},
  citationcount = {23},
  venue = {IEEE transactions on computers}
}

@article{kwokTwoDimensionalPacket2007,
  title = {Two-Dimensional Packet Classification and Filter Conflict Resolution in the Internet},
  author = {Kwok, Andy and Poon, C.},
  year = {2007},
  doi = {10.1007/s00224-007-9050-5},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Theory of Computing Systems}
}

@article{kyngIncrementalSsspFor2021,
  title = {Incremental {{SSSP}} for Sparse Digraphs beyond the Hopset Barrier},
  author = {Kyng, Rasmus and Meierhans, Simon and Gutenberg, Maximilian Probst},
  year = {2021},
  doi = {10.1137/1.9781611977073.137},
  abstract = {Given a directed, weighted graph G=(V,E) undergoing edge insertions, the incremental single-source shortest paths (SSSP) problem asks for the maintenance of approximate distances from a dedicated source s while optimizing the total time required to process the insertion sequence of m edges. Recently, Gutenberg, Williams and Wein [STOC'20] introduced a deterministic O\vphantom\{\}(n{$^2$}) algorithm for this problem, achieving near linear time for very dense graphs. For sparse graphs, Chechik and Zhang [SODA'21] recently presented a deterministic O\vphantom\{\}(m\textsuperscript{\{\vphantom\}}5/3\vphantom\{\}) algorithm, and an adaptive randomized algorithm with run-time O\vphantom\{\}(m{\textsurd}\{n\}+m\textsuperscript{\{\vphantom\}}7/5\vphantom\{\}). This algorithm is remarkable for two reasons: 1) in very spare graphs it reaches the directed hopset barrier of {\textohm}\vphantom\{\}(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}) that applied to all previous approaches for partially-dynamic SSSP [STOC'14, SODA'20, FOCS'20] \{and\} 2) it does not resort to a directed hopset technique itself. In this article we introduce \{propagation synchronization\}, a new technique for controlling the error build-up on paths throughout batches of insertions. This leads us to a significant improvement of the approach in [SODA'21] yielding a \{deterministic\} O\vphantom\{\}(m\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}) algorithm for the problem. By a very careful combination of our new technique with the sampling approach from [SODA'21], we further obtain an adaptive randomized algorithm with total update time O\vphantom\{\}(m\textsuperscript{\{\vphantom\}}4/3\vphantom\{\}). This is the first partially-dynamic SSSP algorithm in sparse graphs to bypass the notorious directed hopset barrier which is often seen as the fundamental challenge towards achieving truly near-linear time algorithms.},
  citationcount = {6},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {adaptive,dynamic,update,update time}
}

@article{l.banachowskiComplementTarjansResult1980,
  title = {A {{Complement}} to {{Tarjan}}'s {{Result}} about the {{Lower Bound}} on the {{Complexity}} of the {{Set Union Problem}}},
  author = {L. Banachowski},
  year = {1980},
  journal = {Information Processing Letters},
  doi = {10.1016/0020-0190(80)90001-0},
  keywords = {lower bound},
  annotation = {Citation Count: 25}
}

@article{l.georgiadisComputing4EdgeConnectedComponents2021,
  title = {Computing the 4-{{Edge-Connected Components}} of a {{Graph}} in {{Linear Time}}},
  author = {L. Georgiadis and G. Italiano and E. Kosinas},
  year = {2021},
  journal = {Embedded Systems and Applications},
  doi = {10.4230/LIPIcs.ESA.2021.47},
  abstract = {We present the first linear-time algorithm that computes the 4-edge-connected components of an undirected graph. Hence, we also obtain the first linear-time algorithm for testing 4-edge connectivity. Our results are based on a linear-time algorithm that computes the 3-edge cuts of a 3-edge-connected graph G, and a linear-time procedure that, given the collection of all 3-edge cuts, partitions the vertices of G into the 4-edge-connected components.},
  annotation = {Citation Count: 6}
}

@article{laceyOnTheSmall2006,
  title = {On the Small Ball Inequality in Three Dimensions},
  author = {Lacey, M. and Bilyk, D.},
  year = {2006},
  doi = {10.1215/00127094-2008-016},
  abstract = {We prove an inequality related to questions in Approximation Theory, Probability Theory, and to Irregularities of Distribution. Let h\textsubscript{R} denote an L\textsuperscript{\{\vphantom\}}{$\infty$}\vphantom\{\} normalized Haar function adapted to a dyadic rectangle R{$\subset$}[0,1]\textsuperscript{\{\vphantom\}}3\vphantom\{\}. We show that there is a postive {$\eta$} so that for all integers n, and coefficients {$\alpha$}(R) we have 2 {\textasciicircum}\{-n\} {$\sum\_$}\{\{R\}=2 {\textasciicircum}\{-n\}\} \{{$\alpha$}(R)\} \{\}{$\lessequivlnt$}\{\} n {\textasciicircum}\{1 - {$\eta$}\} {$\sum\_$}\{\{R\}=2 {\textasciicircum}\{-n\}\} {$\alpha$}(R) h\_R {\textquestiondown}.{$\infty$}. This is an improvement over the `trivial' estimate by an amount of n\textsuperscript{\{\vphantom\}}-{$\eta$}\vphantom\{\}, and the optimal value of {$\eta$} (which we do not prove) would be {$\eta$}={$\frac{1}{2}$}. There is a corresponding lower bound on the L\textsuperscript{\{\vphantom\}}{$\infty$}\vphantom\{\} norm of the Discrepancy function of an arbitary distribution of a finite number of points in the unit cube in three dimensions. The prior result, in dimension 3, is that of J\{o\}zsef Beck \{MR1032337\}, in which the improvement over the trivial estimate was logarithmic in n. We find several simplifications and extensions of Beck's argument to prove the result above.},
  citationcount = {72},
  venue = {No venue available}
}

@article{lackiImprovedDeterministicAlgorithms2013,
  title = {Improved Deterministic Algorithms for Decremental Reachability and Strongly Connected Components},
  author = {Lacki, Jakub},
  year = {2013},
  doi = {10.1145/2483699.2483707},
  abstract = {This article presents a new deterministic algorithm for decremental maintenance of the transitive closure in a directed graph. The algorithm processes any sequence of edge deletions in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}mn{\textexclamdown}/i{\textquestiondown}) time and answers queries in constant time. Previously, such time bound has only been achieved by a randomized Las Vegas algorithm. In addition to that, a few decremental algorithms for maintaining strongly connected components are shown, whose time complexity is {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}1.5{\textexclamdown}/sup{\textquestiondown}) for planar graphs, {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) for graphs with bounded treewidth and {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}mn{\textexclamdown}/i{\textquestiondown}) for general digraphs.},
  citationcount = {51},
  venue = {TALG}
}

@article{lackiThePowerOf2013,
  title = {The Power of Dynamic Distance Oracles: {{Efficient}} Dynamic Algorithms for the Steiner Tree},
  author = {Lacki, Jakub and Ocwieja, Jakub and Pilipczuk, Marcin and Sankowski, P. and Zych, Anna},
  year = {2013},
  doi = {10.1145/2746539.2746615},
  abstract = {In this paper we study the Steiner tree problem over a dynamic set of terminals. We consider the model where we are given an n-vertex graph G=(V,E,w) with positive real edge weights, and our goal is to maintain a tree which is a good approximation of the minimum Steiner tree spanning a terminal set S {$\subseteq$} V, which changes over time. The changes applied to the terminal set are either terminal additions (incremental scenario), terminal removals (decremental scenario), or both (fully dynamic scenario). Our task here is twofold. We want to support updates in sublinear o(n) time, and keep the approximation factor of the algorithm as small as possible. We show that we can maintain a (6+{$\varepsilon$})-approximate Steiner tree of a general graph in  O({\textsurd}n log D) time per terminal addition or removal. Here, strecz denotes the stretch of the metric induced by G. For planar graphs we achieve the same running time and the approximation ratio of (2+{$\varepsilon$}). Moreover, we show faster algorithms for incremental and decremental scenarios. Finally, we show that if we allow higher approximation ratio, even more efficient algorithms are possible. In particular we show a polylogarithmic time (4+{$\varepsilon$})-approximate algorithm for planar graphs. One of the main building blocks of our algorithms are dynamic distance oracles for vertex-labeled graphs, which are of independent interest. We also improve and use the online algorithms for the Steiner tree problem.},
  citationcount = {48},
  venue = {Symposium on the Theory of Computing}
}

@article{lagogiannisDynamicConnectivitySome2021,
  title = {Dynamic Connectivity: {{Some}} Graphs of Interest},
  author = {Lagogiannis, G.},
  year = {2021},
  doi = {10.33965/ijcsis_2021160101},
  abstract = {In this paper we deal with the dynamic connectivity problem, targeting deterministic worst-case poly-logarithmic time-complexities. First we show that instead of solving the dynamic connectivity problem on a general graph G , it suffices to solve it on a graph we name aligned double-forest that has only 2 n -1 edges where n is the number of vertices. Then we present an algorithm that achieves all the operations in logarithmic worst-case time on a graph we name star-tied forest that consists of a star and a forest (of trees), both defined on the same set of vertices. The star-tied forest which can be seen as a special case of an aligned double-forest is more complicated than a forest on which deterministic worst-case logarithmic time-complexities have already been obtained by means of the Dynamic Trees algorithm, introduced by Sleator and Tarjan (1983). For implementing the operations we build upon Dynamic Trees.},
  citationcount = {Unknown},
  venue = {IADIS INTERNATIONAL JOURNAL ON COMPUTER SCIENCE AND INFORMATION SYSTEMS},
  keywords = {dynamic}
}

@article{lagogiannisDynamicConnectivityThe2020,
  title = {Dynamic Connectivity: {{The}} Star and the Tree Story},
  author = {Lagogiannis, G.},
  year = {2020},
  doi = {10.33965/ac2020_202013l001},
  abstract = {In this paper we deal with the dynamic connectivity problem, targeting deterministic worst-case logarithmic time complexities. We present an algorithm that achieves all the operations in logarithmic worst-case time, on a graph that consists of a star and a forest (of trees), both defined on the same set of vertices. This graph is more complicated than a forest on which deterministic worst-case logarithmic time complexities have already been obtained by means of the Dynamic Trees algorithm, introduced by Sleator and Tarjan (1983). For implementing the operations we build upon Dynamic Trees.},
  citationcount = {Unknown},
  venue = {Advanced Courses},
  keywords = {dynamic}
}

@article{lagogiannisReducingStructuralChanges2006,
  title = {Reducing Structural Changes in van {{Emde Boas}}' Data Structure to the Lower Bound for the Dynamic Predecessor Problem},
  author = {Lagogiannis, G. and Makris, C. and Tsakalidis, A.},
  year = {2006},
  doi = {10.1016/j.jda.2005.01.006},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {J. Discrete Algorithms},
  keywords = {data structure,dynamic,lower bound}
}

@article{landauerLatentSemanticAnalysis2008,
  title = {Latent Semantic Analysis},
  author = {Landauer, T. and Dumais, S.},
  year = {2008},
  doi = {10.4249/scholarpedia.4356},
  abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents ("semantic structure") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising.},
  citationcount = {12058},
  venue = {Scholarpedia}
}

@article{landsbergTheComplexityOf2011,
  title = {The Complexity of Matrix Multiplication},
  author = {Landsberg, J.},
  year = {2011},
  doi = {10.1090/GSM/128/11},
  abstract = {No abstract available},
  citationcount = {228},
  venue = {No venue available}
}

@article{langbergTopologyDependentBounds2019,
  title = {Topology Dependent Bounds for Faqs},
  author = {Langberg, M. and Li, Shi and Jayaraman, Sai Vikneshwar Mani and Rudra, A.},
  year = {2019},
  doi = {10.1145/3294052.3319686},
  abstract = {In this paper, we prove topology dependent bounds on the number of rounds needed to compute Functional Aggregate Queries (s) studied by Abo Khamis et al. [PODS 2016] in a synchronous distributed network under the model considered by Chattopadhyay et al. [FOCS 2014, SODA 2017]. Unlike the recent work on computing database queries in the Massively Parallel Computation model, in the model of Chattopadhyay et al., nodes can communicate only via private point-to-point channels and we are interested in bounds that work over an \emph{arbitrary communication topology. This model, which is closer to the well-studied  model in distributed computing and generalizes Yao's two party communication complexity model, has so far only been studied for problems that are common in the two-party communication complexity literature. This is the first work to consider more practically motivated problems in this distributed model. For the sake of exposition, we focus on two specific problems in this paper: Boolean Conjunctive Query () and computing variable/factor marginals in Probabilistic Graphical Models (PGMs). We obtain tight bounds on the number of rounds needed to compute such queries as long as the underlying hypergraph of the query is O(1)-degenerate and has O(1)-arity. In particular, the O(1)-degeneracy condition covers most well-studied queries that are efficiently computable in the centralized computation model like queries with constant treewidth. These tight bounds depend on a new notion of 'width' (namely internal-node-width ) for Generalized Hypertree Decompositions (GHDs) of acyclic hypergraphs, which minimizes the number of internal nodes in a sub-class of GHDs. To the best of our knowledge, this width has not been studied explicitly in the theoretical database literature. Finally, we consider the problem of computing the product of a vector with a chain of matrices and prove tight bounds on its round complexity (over a finite field of two elements) using a novel min-entropy based argument.}},
  citationcount = {3},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {communication,communication complexity,query}
}

@article{larkinNestedSetUnion2014,
  title = {Nested Set Union},
  author = {Larkin, Daniel H. and Tarjan, R.},
  year = {2014},
  doi = {10.1007/978-3-662-44777-2_51},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Embedded Systems and Applications}
}

@article{larsenCellProbeComplexity2011,
  title = {The Cell Probe Complexity of Dynamic Range Counting},
  author = {Larsen, Kasper Green},
  year = {2011},
  doi = {10.1145/2213977.2213987},
  abstract = {In this paper we develop a new technique for proving lower bounds on the update time and query time of dynamic data structures in the cell probe model. With this technique, we prove the highest lower bound to date for any explicit problem, namely a lower bound of tq={\textohm}((lg n/lg(wtu))2). Here n is the number of update operations, w the cell size, tq the query time and tu the update time. In the most natural setting of cell size w={$\Theta$}(lg n), this gives a lower bound of tq={\textohm}((lg n/lg lg n)2) for any polylogarithmic update time. This bound is almost a quadratic improvement over the highest previous lower bound of {\textohm}(lg n), due to Patrascu and Demaine [SICOMP'06]. We prove our lower bound for the fundamental problem of weighted orthogonal range counting. In this problem, we are to support insertions of two-dimensional points, each assigned a {$\Theta$}(lg n)-bit integer weight. A query to this problem is specified by a point q=(x,y), and the goal is to report the sum of the weights assigned to the points dominated by q, where a point (x',y') is dominated by q if x' {$\leq$} x and y' {$\leq$} y. In addition to being the highest cell probe lower bound to date, our lower bound is also tight for data structures with update time tu = {\textohm}(lg2+{$\varepsilon$}n), where {$\varepsilon$}{\textquestiondown}0 is an arbitrarily small constant.},
  citationcount = {76},
  venue = {Symposium on the Theory of Computing},
  keywords = {cell probe,data structure,dynamic,lower bound,query,query time,update,update time}
}

@inproceedings{larsenCellProbeComplexity2012,
  title = {The Cell Probe Complexity of Dynamic Range Counting},
  booktitle = {Proceedings of the Forty-Fourth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Larsen, Kasper Green},
  year = {2012},
  month = may,
  series = {{{STOC}} '12},
  pages = {85--94},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2213977.2213987},
  url = {https://dl.acm.org/doi/10.1145/2213977.2213987},
  urldate = {2024-09-02},
  abstract = {In this paper we develop a new technique for proving lower bounds on the update time and query time of dynamic data structures in the cell probe model. With this technique, we prove the highest lower bound to date for any explicit problem, namely a lower bound of tq={\textohm}((lg n/lg(wtu))2). Here n is the number of update operations, w the cell size, tq the query time and tu the update time. In the most natural setting of cell size w={$\Theta$}(lg n), this gives a lower bound of tq={\textohm}((lg n/lg lg n)2) for any polylogarithmic update time. This bound is almost a quadratic improvement over the highest previous lower bound of {\textohm}(lg n), due to Patrascu and Demaine [SICOMP'06].We prove our lower bound for the fundamental problem of weighted orthogonal range counting. In this problem, we are to support insertions of two-dimensional points, each assigned a {$\Theta$}(lg n)-bit integer weight. A query to this problem is specified by a point q=(x,y), and the goal is to report the sum of the weights assigned to the points dominated by q, where a point (x',y') is dominated by q if x' {$\leq$} x and y' {$\leq$} y. In addition to being the highest cell probe lower bound to date, our lower bound is also tight for data structures with update time tu = {\textohm}(lg2+{$\varepsilon$}n), where {$\varepsilon\&$}gt;0 is an arbitrarily small constant.},
  isbn = {978-1-4503-1245-5},
  keywords = {cell probe,data structure,dynamic,lower bound,query,query time,update,update time},
  annotation = {J. Matousek . Geometric Discrepancy . Springer , 1999 . J. Matousek. Geometric Discrepancy. Springer, 1999.},
  file = {/Users/tulasi/Zotero/storage/PJHGGWJN/Larsen - 2012 - The cell probe complexity of dynamic range counting.pdf}
}

@article{larsenCrossingLogarithmicBarrier2017,
  title = {Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure Lower Bounds},
  author = {Larsen, Kasper Green and Weinstein, Omri and Yu, Huacheng},
  year = {2017},
  doi = {10.1145/3188745.3188790},
  abstract = {This paper proves the first super-logarithmic lower bounds on the cell probe complexity of dynamic boolean (a.k.a. decision) data structure problems, a long-standing milestone in data structure lower bounds. We introduce a new method for proving dynamic cell probe lower bounds and use it to prove a {\textohm}\vphantom\{\}(\{lg\}\textsuperscript{\{\vphantom\}}1.5\vphantom\{\}n) lower bound on the operational time of a wide range of boolean data structure problems, most notably, on the query time of dynamic range counting over \{F\}\textsubscript{\{\vphantom\}}2\vphantom\{\} ([Pat07]). Proving an {$\omega$}(\{lg\}n) lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai P{\u a}tra{\c s}cu's obituary [Tho13]. This result also implies the first {$\omega$}(\{lg\}n) lower bound for the classical 2D range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-boolean) problems of range selection and range median. Our technical centerpiece is a new way of ``weakly'' simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebychev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the ``cell sampling'' method of Panigrahy et al. [PTW10].},
  citationcount = {38},
  venue = {Information Theory and Applications Workshop},
  keywords = {cell probe,cell sampling,communication,data structure,dynamic,lower bound,query,query time}
}

@inproceedings{larsenCrossingLogarithmicBarrier2018,
  title = {Crossing the Logarithmic Barrier for Dynamic {{Boolean}} Data Structure Lower Bounds},
  booktitle = {Proceedings of the 50th {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}},
  author = {Larsen, Kasper Green and Weinstein, Omri and Yu, Huacheng},
  year = {2018},
  month = jun,
  series = {{{STOC}} 2018},
  pages = {978--989},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3188745.3188790},
  url = {https://dl.acm.org/doi/10.1145/3188745.3188790},
  urldate = {2024-09-03},
  abstract = {This paper proves the first super-logarithmic lower bounds on the cell probe complexity of dynamic boolean (a.k.a. decision) data structure problems, a long-standing milestone in data structure lower bounds. We introduce a new approach and use it to prove a {\textohm}(log1.5 n) lower bound on the operational time of a wide range of boolean data structure problems, most notably, on the query time of dynamic range counting over F2. Proving an {$\omega$}(lgn) lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai P{\v a}tra{\c s}cu's obituary\&nbsp;. This result also implies the first {$\omega$}(lgn) lower bound for the classical 2D range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-boolean) problems of range selection and range median. Our technical centerpiece is a new way of ``weakly'' simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebyshev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the ``cell sampling'' method of Panigrahy et al.\&nbsp;.},
  isbn = {978-1-4503-5559-9},
  keywords = {cell probe,cell sampling,communication,data structure,dynamic,lower bound,query,query time},
  annotation = {\{Aga04\} Pankaj K. Agarwal . Range searching . In Handbook of Discrete and Computational Geometry , Second Edition., pages 809-- 837 . 2004. \{Aga04\} Pankaj K. Agarwal. Range searching. In Handbook of Discrete and Computational Geometry, Second Edition., pages 809--837. 2004.\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
STOC'18, June 25--29 , 2018 , Los Angeles, CA, USA Kasper Green Larsen, Omri Weinstein, and Huacheng Yu \{P {\textasciicaron} at08\} Mihai P {\textasciicaron} atra{\c s}cu. Unifying the landscape of cell-probe lower bounds . In Proc. 49th IEEE Symposium on Foundations of Computer Science , pages 434-- 443 , 2008. STOC'18, June 25--29, 2018, Los Angeles, CA, USA Kasper Green Larsen, Omri Weinstein, and Huacheng Yu \{P {\textasciicaron} at08\} Mihai P {\textasciicaron} atra{\c s}cu. Unifying the landscape of cell-probe lower bounds. In Proc. 49th IEEE Symposium on Foundations of Computer Science, pages 434--443, 2008.\\
\{PD04\} Mihai P {\textasciicaron} atra {\c s}cu and Erik D. Demaine . Tight bounds for the partial-sums problem . In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2004 , pages 20-- 29 , 2004 . \{PD04\} Mihai P {\textasciicaron} atra{\c s}cu and Erik D. Demaine. Tight bounds for the partial-sums problem. In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2004, pages 20--29, 2004.\\
\\
\\
\{Tho13\} Mikkel Thorup . Mihai P {\textasciicaron} atra {\c s}cu : Obituary and open problems . Bulletin of the EATCS , 109 : 7 -- 13 , 2013 . \{Tho13\} Mikkel Thorup. Mihai P {\textasciicaron} atra{\c s}cu: Obituary and open problems. Bulletin of the EATCS, 109:7--13, 2013.},
  file = {/Users/tulasi/Zotero/storage/TLIKEFLU/Larsen et al. - 2018 - Crossing the logarithmic barrier for dynamic Boolean data structure lower bounds.pdf}
}

@article{larsenDanfossEkcTrial2003,
  title = {Danfoss {{EKC}} Trial Project Deliverables},
  author = {Larsen, K. and Larsen, U. and Nielsen, B. and Skou, A. and W{\k a}sowski, A.},
  year = {2003},
  doi = {10.7146/brics.v10i48.21820},
  abstract = {This report documents the results of the Danfoss EKC trial project on model based development using IAR visualState. We present a formal state-model of a refrigeration controller based on a specification given by Danfoss. We report results on modeling, verification, simulation, and code-generation. It is found that the IAR visualState is a promising tool for this application domain, but that improvements must be done to code-generation and automatic test generation.},
  citationcount = {1},
  venue = {No venue available}
}

@article{larsenFasterOnlineMatrix2017,
  title = {Faster Online Matrix-Vector Multiplication},
  author = {Larsen, Kasper Green and Williams, Ryan},
  year = {2017},
  doi = {10.1137/1.9781611974782},
  abstract = {We consider the Online Boolean Matrix-Vector Multiplication (OMV) problem studied by Henzinger et al. [STOC'15]: given an n {\texttimes} n Boolean matrix M, we receive n Boolean vectors v1,...,vn one at a time, and are required to output Mvi (over the Boolean semiring) before seeing the vector vi+1, for all i. Previous known algorithms for this problem are combinatorial, running in O(n3 /log2 n) time. Henzinger et al. conjecture there is no O(n3-e) time algorithm for OMV, for all e {\textquestiondown} 0; their OMV conjecture is shown to imply strong hardness results for many basic dynamic problems. We give a substantially faster method for computing OMV, running in [EQUATION] randomized time. In fact, after seeing [EQUATION] vectors, we already achieve [EQUATION] amortized time for matrix-vector multiplication. Our approach gives a way to reduce matrix-vector multiplication to solving a version of the Orthogonal Vectors problem, which in turn reduces to "small" algebraic matrix-matrix multiplication. Applications include faster independent set detection, partial match retrieval, and 2-CNF evaluation. We also show how a modification of our method gives a cell probe data structure for OMV with worst case [EQUATION] time per query vector, where w is the word size. This result rules out an unconditional proof of the OMV conjecture using purely information-theoretic arguments.},
  citationcount = {5},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{larsenFasterOnlineMatrixvector2016,
  title = {Faster Online Matrix-Vector Multiplication},
  author = {Larsen, Kasper Green and Williams, Ryan},
  year = {2016},
  doi = {10.1137/1.9781611974782.142},
  abstract = {We consider the Online Boolean Matrix-Vector Multiplication (OMV) problem studied by Henzinger et al. [STOC'15]: given an n{\texttimes}n Boolean matrix M, we receive n Boolean vectors v{$_1$},{\dots},v\textsubscript{n} one at a time, and are required to output Mv\textsubscript{i} (over the Boolean semiring) before seeing the vector v\textsubscript{\{\vphantom\}}i+1\vphantom\{\}, for all i. Previous known algorithms for this problem are combinatorial, running in O(n{$^3$}/{$^2$}n) time. Henzinger et al. conjecture there is no O(n\textsuperscript{\{\vphantom\}}3-{$\varepsilon$}\vphantom\{\}) time algorithm for OMV, for all {$\varepsilon>$}0; their OMV conjecture is shown to imply strong hardness results for many basic dynamic problems. We give a substantially faster method for computing OMV, running in n{$^3$}/2\textsuperscript{\{\vphantom\}}{\textohm}({\textsurd}\{n\})\vphantom\{\} randomized time. In fact, after seeing 2\textsuperscript{\{\vphantom\}}{$\omega$}({\textsurd}\{n\})\vphantom\{\} vectors, we already achieve n{$^2$}/2\textsuperscript{\{\vphantom\}}{\textohm}({\textsurd}\{n\})\vphantom\{\} amortized time for matrix-vector multiplication. Our approach gives a way to reduce matrix-vector multiplication to solving a version of the Orthogonal Vectors problem, which in turn reduces to "small" algebraic matrix-matrix multiplication. Applications include faster independent set detection, partial match retrieval, and 2-CNF evaluation. We also show how a modification of our method gives a cell probe data structure for OMV with worst case O(n\textsuperscript{\{\vphantom\}}7/4\vphantom\{\}/{\textsurd}\{w\}) time per query vector, where w is the word size. This result rules out an unconditional proof of the OMV conjecture using purely information-theoretic arguments.},
  citationcount = {61},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,data structure,dynamic,information theoretic,query}
}

@article{larsenFurtherUnifyingThe2020,
  title = {Further Unifying the Landscape of Cell Probe Lower Bounds},
  author = {Larsen, Kasper Green and Starup, Jonathan Lindegaard and Steensgaard, J.},
  year = {2020},
  doi = {10.1137/1.9781611976496.25},
  abstract = {In a landmark paper, P{\v a}trascu demonstrated how a single lower bound for the static data structure problem of reachability in the butterfly graph, could be used to derive a wealth of new and previous lower bounds via reductions. These lower bounds are tight for numerous static data structure problems. Moreover, he also showed that reachability in the butterfly graph reduces to dynamic marked ancestor, a classic problem used to prove lower bounds for dynamic data structures. Unfortunately, P{\v a}trascu's reduction to marked ancestor loses a n factor and therefore falls short of fully recovering all the previous dynamic data structure lower bounds that follow from marked ancestor. In this paper, we revisit P{\v a}trascu's work and give a new lossless reduction to dynamic marked ancestor, thereby establishing reachability in the butterfly graph as a single seed problem from which a range of tight static and dynamic data structure lower bounds follow.},
  citationcount = {2},
  venue = {SIAM Symposium on Simplicity in Algorithms},
  keywords = {cell probe,data structure,dynamic,lower bound,reduction,static}
}

@article{larsenHigherCellProbe2012,
  title = {Higher Cell Probe Lower Bounds for Evaluating Polynomials},
  author = {Larsen, Kasper Green},
  year = {2012},
  doi = {10.1109/FOCS.2012.21},
  abstract = {In this paper, we study the cell probe complexity of evaluating an n-degree polynomial P over a finite field F of size at least n1+{\textohm}(1). More specifically, we show that any static data structure for evaluating P(x), where x {$\in$} F, must use {\textohm}(lg {\textbar}F{\textbar}/ lg(Sw/n lg {\textbar}F{\textbar})) cell probes to answer a query, where S denotes the space of the data structure in number of cells and w the cell size in bits. This bound holds in expectation for randomized data structures with any constant error probability {$\delta$} {\textexclamdown}; 1/2. Our lower bound not only improves over the {\textohm}(lg {\textbar}F{\textbar}/ lg S) lower bound of Miltersen [TCS'95], but is in fact the highest static cell probe lower bound to date: For linear space (i.e. S = O(n lg {\textbar}F{\textbar}/w)), our query time lower bound simplifies to {\textohm}(lg {\textbar}F{\textbar}), whereas the highest previous lower bound for any static data structure problem having d different queries is {\textohm}(lg d/ lg lg d), which was first achieved by P{\'a}trascu and Thorup [SICOMP'10]. We also use the recent technique of Larsen [STOC'12] to show a lower bound of tq = {\textohm}(lg {\textbar}F{\textbar} lg n/lg(wtu/ lg {\textbar}F{\textbar}) lg(wtu)) for dynamic data structures for polynomial evaluation over a finite field F of size {\textohm}(n2). Here tq denotes the expected query time and tu the worst case update time. This lower bound holds for randomized data structures with any constant error probability {$\delta$} {\textexclamdown}; 1/2. This is only the second time a lower bound beyond max\{tu, tq\} = {\textohm}(max\{lg n, lg d/ lg lg d\}) has been achieved for dynamic data structures, where d denotes the number of different queries and updates to the problem. Furthermore, it is the first such lower bound that holds for randomized data structures with a constant probability of error.},
  citationcount = {51},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {cell probe,data structure,dynamic,lower bound,query,query time,static,update,update time}
}

@inproceedings{larsenHigherCellProbe2012a,
  title = {Higher {{Cell Probe Lower Bounds}} for {{Evaluating Polynomials}}},
  booktitle = {2012 {{IEEE}} 53rd {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Larsen, Kasper Green},
  year = {2012},
  month = oct,
  pages = {293--301},
  issn = {0272-5428},
  doi = {10.1109/FOCS.2012.21},
  url = {https://ieeexplore.ieee.org/document/6375307},
  urldate = {2024-09-03},
  abstract = {In this paper, we study the cell probe complexity of evaluating an n-degree polynomial P over a finite field F of size at least n1+{\textohm}(1). More specifically, we show that any static data structure for evaluating P(x), where x {$\in$} F, must use {\textohm}(lg {\textbar}F{\textbar}/ lg(Sw/n lg {\textbar}F{\textbar})) cell probes to answer a query, where S denotes the space of the data structure in number of cells and w the cell size in bits. This bound holds in expectation for randomized data structures with any constant error probability {$\delta$} {$<$}; 1/2. Our lower bound not only improves over the {\textohm}(lg {\textbar}F{\textbar}/ lg S) lower bound of Miltersen [TCS'95], but is in fact the highest static cell probe lower bound to date: For linear space (i.e. S = O(n lg {\textbar}F{\textbar}/w)), our query time lower bound simplifies to {\textohm}(lg {\textbar}F{\textbar}), whereas the highest previous lower bound for any static data structure problem having d different queries is {\textohm}(lg d/ lg lg d), which was first achieved by P{\'a}trascu and Thorup [SICOMP'10]. We also use the recent technique of Larsen [STOC'12] to show a lower bound of tq = {\textohm}(lg {\textbar}F{\textbar} lg n/lg(wtu/ lg {\textbar}F{\textbar}) lg(wtu)) for dynamic data structures for polynomial evaluation over a finite field F of size {\textohm}(n2). Here tq denotes the expected query time and tu the worst case update time. This lower bound holds for randomized data structures with any constant error probability {$\delta$} {$<$}; 1/2. This is only the second time a lower bound beyond maxtu, tq = {\textohm}(maxlg n, lg d/ lg lg d) has been achieved for dynamic data structures, where d denotes the number of different queries and updates to the problem. Furthermore, it is the first such lower bound that holds for randomized data structures with a constant probability of error.},
  keywords = {cell probe,Complexity theory,Data models,data structures,dynamic,Encoding,Error probability,lower bound,lower bounds,polynomials,Polynomials,Probes,query,query time,static,update,update time},
  file = {/Users/tulasi/Zotero/storage/I745NMZE/Larsen - 2012 - Higher Cell Probe Lower Bounds for Evaluating Polynomials.pdf}
}

@inproceedings{larsenImprovedRangeSearching2012a,
  title = {Improved Range Searching Lower Bounds},
  booktitle = {Proc. 28th {{ACM Symposium}} on {{Computational Geometry}}},
  author = {Larsen, K. G. and Nguyen, H. L.},
  year = {2012},
  pages = {171--178},
  doi = {10.1145/2261250.2261275},
  keywords = {lower bound},
  annotation = {J. Pach and P. K. Agarwal . Combinatorial geometry . Wiley-Interscience series in discrete mathematics and optimization. Wiley , 1995 . J. Pach and P. K. Agarwal. Combinatorial geometry. Wiley-Interscience series in discrete mathematics and optimization. Wiley, 1995.},
  file = {/Users/tulasi/Zotero/storage/FETJNSGB/Larsen and Nguyen - 2012 - Improved range searching lower bounds.pdf}
}

@article{larsenIOEfficient2011,
  title = {I/{{O-efficient}} Data Structures for Colored Range and Prefix Reporting},
  author = {Larsen, Kasper Green and Pagh, R.},
  year = {2011},
  doi = {10.1137/1.9781611973099.49},
  abstract = {Motivated by information retrieval applications, we consider the one-dimensional colored range reporting problem in rank space. The goal is to build a static data structure for sets C1,...,Cm {$\subseteq$} \{1,...,{$\sigma$}\} that supports queries of the kind: Given indices a, b, report the set [EQUATION]. We study the problem in the I/O model, and show that there exists an optimal linear-space data structure that answers queries in O(1 + k/B) I/Os, where k denotes the output size and B the disk block size in words. In fact, we obtain the same bound for the harder problem of three-sided orthogonal range reporting. In this problem, we are to preprocess a set of n two-dimensional points in rank space, such that all points inside a query rectangle of the form [x1, x2] x (-{$\infty$}, y] can be reported. The best previous bounds for this problem is either O(n lg2B n) space and O(1 + k/B) query I/Os, or O(n) space and O(lg(h)B n + k/B) query I/Os, where lg(h)B n is the base B logarithm iterated h times, for any constant integer h. The previous bounds are both achieved under the indivisibility assumption, while our solution exploits the full capabilities of the underlying machine. Breaking the indivisibility assumption thus provides us with cleaner and optimal bounds. Our results also imply an optimal solution to the following colored prefix reporting problem. Given a set S of strings, each O(1) disk blocks in length, and a function c: S {$\rightarrow$} 2{\textquestiondown}\{1,...,{$\sigma$}\}, support queries of the kind: Given a string p, report the set [EQUATION], where p* denotes the set of strings with prefix p. Finally, we consider the possibility of top-k extensions of this result, and present a simple solution in a model that allows non-blocked I/O.},
  citationcount = {28},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{larsenLowerBoundsFor2019,
  title = {Lower Bounds for Oblivious Near-Neighbor Search},
  author = {Larsen, Kasper Green and Malkin, T. and Weinstein, Omri and Yeo, Kevin},
  year = {2019},
  doi = {10.1137/1.9781611975994.68},
  abstract = {We prove an {\textohm}(dn/(n){$^2$}) lower bound on the dynamic cell-probe complexity of statistically \{oblivious\} approximate-near-neighbor search (\{ANN\}) over the d-dimensional Hamming cube. For the natural setting of d={$\Theta$}(n), our result implies an {\textohm}\vphantom\{\}({$^2$}n) lower bound, which is a quadratic improvement over the highest (non-oblivious) cell-probe lower bound for \{ANN\}. This is the first super-logarithmic \{unconditional\} lower bound for \{ANN\} against general (non black-box) data structures. We also show that any oblivious \{static\} data structure for decomposable search problems (like \{ANN\}) can be obliviously dynamized with O(n) overhead in update and query time, strengthening a classic result of Bentley and Saxe (Algorithmica, 1980).},
  citationcount = {20},
  venue = {IACR Cryptology ePrint Archive},
  keywords = {cell probe,data structure,dynamic,lower bound,query,query time,static,update}
}

@article{larsenLowerBoundsFor2019,
  title = {Lower Bounds for Multi-Server Oblivious Rams},
  author = {Larsen, Kasper Green and Simkin, Mark and Yeo, Kevin},
  year = {2019},
  doi = {10.1007/978-3-030-64375-1_17},
  abstract = {No abstract available},
  citationcount = {19},
  venue = {IACR Cryptology ePrint Archive},
  keywords = {lower bound}
}

@article{larsenModelsTechniquesProving,
  title = {Models and {{Techniques}} for {{Proving Data Structure Lower Bounds}}},
  author = {Larsen, Kasper Green},
  journal = {Lecture Notes in Computer Science},
  doi = {10.1007/978-3-030-26176-4_15},
  abstract = {In this dissertation, we present a number of new techniques and tools for proving lower bounds on the operational time of data structures. These techniques provide new lines of attack for proving lower bounds in both the cell probe model, the group model, the pointer machine model and the I/O-model. In all cases, we push the frontiers further by proving lower bounds higher than what could possibly be proved using previously known techniques. For the cell probe model, our results have the following consequences:  {$\bullet$} The first {\textohm}(lg n) query time lower bound for linear space static data structures. The highest previous lower bound for any static data structure problem peaked at {\textohm}(lg n/ lg lg n).  {$\bullet$} An {\textohm}((lg n/ lg lg n)2) lower bound on the maximum of the update time and the query time of dynamic data structures. This is almost a quadratic improvement over the highest previous lower bound of {\textohm}(lg n).  In the group model, we establish a number of intimate connections to the fields of combinatorial discrepancy and range reporting in the pointer machine model. These connections immediately allow us to translate decades of research in discrepancy and range reporting to very high lower bounds on the update time tu and query time tq of dynamic group model data structures. We have listed a few in the following:  {$\bullet$} For d-dimensional halfspace range searching, we get a lower bound of tutq = {\textohm}(n1-1/d). This comes within a lg lg n factor of the best known upper bound.  {$\bullet$} For orthogonal range searching, we get a lower bound of tutq = {\textohm}(lgd-1 n).  {$\bullet$} For ball range searching, we get a lower bound of tutq = {\textohm}(n1-1/d).  The highest previous lower bound proved in the group model does not exceed {\textohm}((lg n/ lg lg n)2) on the maximum of tu and tq. Finally, we present a new technique for proving lower bounds for range reporting problems in the pointer machine and the I/O-model. With this technique, we tighten the gap between the known upper bound and lower bound for the most fundamental range reporting problem, orthogonal range reporting.},
  langid = {english},
  keywords = {cell probe,data structure,dynamic,lower bound,query,query time,static,update,update time},
  annotation = {Amini, O., Fomin, F.V., Saurabh, S.: Counting subgraphs via homomorphisms. SIAM J. Discrete Math. 26(2), 695--717 (2012)\\
Arvind, V., Raja, S.: Some lower bound results for set-multilinear arithmetic computations. Chicago J. Theor. Comput. Sci. (2016)\\
Baur, W., Strassen, V.: The complexity of partial derivatives. Theoret. Comput. Sci. 22(3), 317--330 (1983)\\
Bj{\"o}rklund, A.: Exact covers via determinants. In: STACS, pp. 95--106 (2010)\\
Bj{\"o}rklund, A., Husfeldt, T., Taslaman, N.: Shortest cycle through specified elements. In: SODA, pp. 1747--1753 (2012)\\
Chauhan, A., Rao, B.V.R.: Parameterized analogues of probabilistic computation. In: Ganguly, S., Krishnamurti, R. (eds.) CALDAM 2015. LNCS, vol. 8959, pp. 181--192. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-14974-5\_18\\
Chillara, S., Engels, C., Limaye, N., Srinivasan, S.: A near-optimal depth-hierarchy theorem for small-depth multilinear circuits. In: FOCS (2018)\\
Downey, R.G., Fellows, M.R.: Fundamentals of Parameterized Complexity. Texts in Computer Science. Springer, London (2013). https://doi.org/10.1007/978-1-4471-5559-1\\
Engels, C.: Why are certain polynomials hard? A look at non-commutative, parameterized and homomorphism polynomials. Ph.D. thesis, Saarland University (2016)\\
Fomin, F.V., Lokshtanov, D., Raman, V., Saurabh, S., Rao, B.V.R.: Faster algorithms for finding and counting subgraphs. J. Comput. Syst. Sci. 78(3), 698--706 (2012)\\
Fournier, H., Limaye, N., Malod, G., Srinivasan, S.: Lower bounds for depth 4 formulas computing iterated matrix multiplication. In: STOC, pp. 128--135 (2014)\\
Ghosal, P., Prakash, O., Rao, B.V.R.: On constant depth circuits parameterized by degree: identity testing and depth reduction. In: Cao, Y., Chen, J. (eds.) COCOON 2017. LNCS, vol. 10392, pp. 250--261. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-62389-4\_21\\
Gupta, A., Kamath, P., Kayal, N., Saptharishi, R.: Approaching the chasm at depth four. J. ACM (JACM) 61(6), 33 (2014)\\
Hoory, S., Linial, N., Wigderson, A.: Expander graphs and their applications. Bull. Am. Math. Soc. 43(4), 439--561 (2006)\\
Kabanets, V., Impagliazzo, R.: Derandomizing polynomial identity tests means proving circuit lower bounds. Comput. Complex. 13(1--2), 1--46 (2004)\\
Kayal, N., Nair, V., Saha, C.: Separation between read-once oblivious algebraic branching programs (ROABPs) and multilinear depth three circuits. In: STACS, pp. 46:1--46:15 (2016)\\
Kumar, M., Saraf, S.: The limits of depth reduction for arithmetic formulas: it's all about the top fan-in. SIAM J. Comput. 44(6), 1601--1625 (2015)\\
M{\"u}ller, M.: Parameterized randomization. Ph.D. thesis, Albert-Ludwigs-Universit{\"a}t Freiburg im Breisgau (2008)\\
Nisan, N.: Lower bounds for non-commutative computation. In: STOC, pp. 410--418. ACM (1991)\\
Raz, R.: Separation of multilinear circuit and formula size. Theory Comput. 2(6), 121--135 (2006)\\
Raz, R.: Multi-linear formulas for permanent and determinant are of super-polynomial size. J. ACM 56(2), 8:1--8:17 (2009)\\
Raz, R., Yehudayoff, A.: Lower bounds and separations for constant depth multilinear circuits. Comput. Complex. 18(2), 171--207 (2009)\\
Saptharishi, R.: A survey of lower bounds in arithmetic circuit complexity. Technical report (2019)\\
Shpilka, A., Yehudayoff, A.: Arithmetic circuits: a survey of recent results and open questions. Found. Trends Theor. Comput. Sci. 5(3--4), 207--388 (2010)\\
Valiant, L.G.: The complexity of computing the permanent. Theor. Comput. Sci. 8, 189--201 (1979)\\
Amini, O., Fomin, F.V., Saurabh, S.: Counting subgraphs via homomorphisms. SIAM J. Discrete Math. 26(2), 695--717 (2012)\\
Arvind, V., Raja, S.: Some lower bound results for set-multilinear arithmetic computations. Chicago J. Theor. Comput. Sci. (2016)\\
Baur, W., Strassen, V.: The complexity of partial derivatives. Theoret. Comput. Sci. 22(3), 317--330 (1983)\\
Bj{\"o}rklund, A.: Exact covers via determinants. In: STACS, pp. 95--106 (2010)\\
Bj{\"o}rklund, A., Husfeldt, T., Taslaman, N.: Shortest cycle through specified elements. In: SODA, pp. 1747--1753 (2012)\\
Chauhan, A., Rao, B.V.R.: Parameterized analogues of probabilistic computation. In: Ganguly, S., Krishnamurti, R. (eds.) CALDAM 2015. LNCS, vol. 8959, pp. 181--192. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-14974-5\_18\\
Chillara, S., Engels, C., Limaye, N., Srinivasan, S.: A near-optimal depth-hierarchy theorem for small-depth multilinear circuits. In: FOCS (2018)\\
Downey, R.G., Fellows, M.R.: Fundamentals of Parameterized Complexity. Texts in Computer Science. Springer, London (2013). https://doi.org/10.1007/978-1-4471-5559-1\\
Engels, C.: Why are certain polynomials hard? A look at non-commutative, parameterized and homomorphism polynomials. Ph.D. thesis, Saarland University (2016)\\
Fomin, F.V., Lokshtanov, D., Raman, V., Saurabh, S., Rao, B.V.R.: Faster algorithms for finding and counting subgraphs. J. Comput. Syst. Sci. 78(3), 698--706 (2012)\\
Fournier, H., Limaye, N., Malod, G., Srinivasan, S.: Lower bounds for depth 4 formulas computing iterated matrix multiplication. In: STOC, pp. 128--135 (2014)\\
Ghosal, P., Prakash, O., Rao, B.V.R.: On constant depth circuits parameterized by degree: identity testing and depth reduction. In: Cao, Y., Chen, J. (eds.) COCOON 2017. LNCS, vol. 10392, pp. 250--261. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-62389-4\_21\\
Gupta, A., Kamath, P., Kayal, N., Saptharishi, R.: Approaching the chasm at depth four. J. ACM (JACM) 61(6), 33 (2014)\\
Hoory, S., Linial, N., Wigderson, A.: Expander graphs and their applications. Bull. Am. Math. Soc. 43(4), 439--561 (2006)\\
Kabanets, V., Impagliazzo, R.: Derandomizing polynomial identity tests means proving circuit lower bounds. Comput. Complex. 13(1--2), 1--46 (2004)\\
Kayal, N., Nair, V., Saha, C.: Separation between read-once oblivious algebraic branching programs (ROABPs) and multilinear depth three circuits. In: STACS, pp. 46:1--46:15 (2016)\\
Kumar, M., Saraf, S.: The limits of depth reduction for arithmetic formulas: it's all about the top fan-in. SIAM J. Comput. 44(6), 1601--1625 (2015)\\
M{\"u}ller, M.: Parameterized randomization. Ph.D. thesis, Albert-Ludwigs-Universit{\"a}t Freiburg im Breisgau (2008)\\
Nisan, N.: Lower bounds for non-commutative computation. In: STOC, pp. 410--418. ACM (1991)\\
Raz, R.: Separation of multilinear circuit and formula size. Theory Comput. 2(6), 121--135 (2006)\\
Raz, R.: Multi-linear formulas for permanent and determinant are of super-polynomial size. J. ACM 56(2), 8:1--8:17 (2009)\\
Raz, R., Yehudayoff, A.: Lower bounds and separations for constant depth multilinear circuits. Comput. Complex. 18(2), 171--207 (2009)\\
Saptharishi, R.: A survey of lower bounds in arithmetic circuit complexity. Technical report (2019)\\
Shpilka, A., Yehudayoff, A.: Arithmetic circuits: a survey of recent results and open questions. Found. Trends Theor. Comput. Sci. 5(3--4), 207--388 (2010)\\
Valiant, L.G.: The complexity of computing the permanent. Theor. Comput. Sci. 8, 189--201 (1979)},
  file = {/Users/tulasi/Zotero/storage/QZ8SUMNZ/Larsen - Models and Techniques for Proving Data Structure Lower Bounds.pdf}
}

@article{larsenNearOptimalRange2013,
  title = {Near-Optimal Range Reporting Structures for Categorical Data},
  author = {Larsen, Kasper Green and Walderveen, F. V.},
  year = {2013},
  doi = {10.1137/1.9781611973105.20},
  abstract = {Range reporting on categorical (or colored) data is a well-studied generalization of the classical range reporting problem in which each of the N input points has an associated color (category). A query then asks to report the set of colors of the points in a given rectangular query range, which may be far smaller than the set of all points in the query range. We study two-dimensional categorical range reporting in both the word-RAM and I/O-model. For the I/O-model, we present two alternative data structures for three-sided queries. The first answers queries in optimal O(lgB N + K/B) I/Os using O(N lg* N) space, where K is the number of distinct colors in the output, B is the disk block size, and lg* N is the iterated logarithm of N. Our second data structure uses linear space and answers queries in O(lgB N + lg(h) N + K/B) I/Os for any constant integer h {$\geq$} 1. Here lg(1) N = lg N and lg(h) N = lg(lg(h-1) N) when h {\textquestiondown} 1. Both solutions use only comparisons on the coordinates. We also show that the lgB N terms in the query costs can be reduced to optimal lg lgB U when the input points lie on a U x U grid and we allow word-level manipulations of the coordinates. We further reduce the query time to just O(1) if the points are given on an N x N grid. Both solutions also lead to improved data structures for four-sided queries. For the word-RAM, we obtain optimal data structures for three-sided range reporting, as well as improved upper bounds for four-sided range reporting. Finally, we show a tight lower bound on one-dimensional categorical range counting using an elegant reduction from (standard) two-dimensional range counting.},
  citationcount = {31},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,lower bound,query,query time,reduction}
}

@inproceedings{larsenNearOptimalRangeReporting2013,
  title = {Near-{{Optimal Range Reporting Structures}} for {{Categorical Data}}},
  booktitle = {Proceedings of the {{Twenty-Fourth Annual ACM-SIAM Symposium}} on {{Discrete Algorithms}}},
  author = {Larsen, Kasper Green and Van Walderveen, Freek},
  year = {2013},
  month = jan,
  pages = {265--276},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611973105.20},
  url = {https://epubs.siam.org/doi/10.1137/1.9781611973105.20},
  urldate = {2024-11-20},
  isbn = {978-1-61197-251-1 978-1-61197-310-5},
  langid = {english},
  file = {/Users/tulasi/Zotero/storage/K378B7BR/Larsen and Van Walderveen - 2013 - Near-Optimal Range Reporting Structures for Categorical Data.pdf}
}

@article{larsenOptimalNonAdaptive2023,
  title = {Optimal Non-Adaptive Cell Probe Dictionaries and Hashing},
  author = {Larsen, Kasper Green and Pagh, R. and Pitassi, T. and Zamir, Or},
  year = {2023},
  doi = {10.48550/arXiv.2308.16042},
  abstract = {We present a simple and provably optimal non-adaptive cell probe data structure for the static dictionary problem. Our data structure supports storing a set of n key-value pairs from [u]x[u] using s words of space and answering key lookup queries in t = O(lg(u/n)/ lg(s/n)) nonadaptive probes. This generalizes a solution to the membership problem (i.e., where no values are associated with keys) due to Buhrman et al. We also present matching lower bounds for the non-adaptive static membership problem in the deterministic setting. Our lower bound implies that both our dictionary algorithm and the preceding membership algorithm are optimal, and in particular that there is an inherent complexity gap in these problems between no adaptivity and one round of adaptivity (with which hashing-based algorithms solve these problems in constant time). Using the ideas underlying our data structure, we also obtain the first implementation of a n-wise independent family of hash functions with optimal evaluation time in the cell probe model.},
  citationcount = {4},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {cell probe,data structure,lower bound,non-adaptive,query,static}
}

@article{larsenRangeSearchingGroup2011,
  title = {On Range Searching in the Group Model and Combinatorial Discrepancy},
  author = {Larsen, Kasper Green},
  year = {2011},
  doi = {10.1137/120865240},
  abstract = {In this paper we establish an intimate connection between dynamic range searching in the group model and combinatorial discrepancy. Our result states that, for a broad class of range searching data structures (including all known upper bounds), it must hold that t\textsubscript{u}t\textsubscript{q}={\textohm}({$^2$}/n) where t\textsubscript{u} is the worst case update time, t\textsubscript{q} the worst case query time and  is the combinatorial discrepancy of the range searching problem in question. This relation immediately implies a whole range of exceptionally high and near-tight lower bounds for all of the basic range searching problems. We list a few of them in the following:\{itemize\} For half space range searching in d-dimensional space, we get a lower bound of t\textsubscript{u}t\textsubscript{q}={\textohm}(n\textsuperscript{\{\vphantom\}}1-1/d\vphantom\{\}/n). This comes within a nn factor of the best known upper bound.  For orthogonal range searching in d-dimensional space, we get a lower bound of t\textsubscript{u}t\textsubscript{q}={\textohm}(\textsuperscript{\{\vphantom\}}d-2+{$\mu$}(d)\vphantom\{\}n), where {$\mu$}(d){$>$}0 is some small but strictly positive function of d. For ball range searching in d-dimensional space, we get a lower bound of t\textsubscript{u}t\textsubscript{q}={\textohm}(n\textsuperscript{\{\vphantom\}}1-1/d\vphantom\{\}/n).\{itemize\}We note that the previous highest lower bound for any explicit problem, due to P\{{\v a}\}tra\{{\c s}\}cu [STOC'07], states that t\textsubscript{q}={\textohm}((n/(n+t\textsubscript{u})){$^2$}), which does however hold for a less restrictive class of data structures. Our result also has implications for the field of combinatorial discrepancy. Using textbook range searching solutions, we improve on the best known discrepancy upper bound for axis-aligned rectangles in dimensions d{$\geq$}3.},
  citationcount = {36},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,dynamic,lower bound,query,query time,update,update time}
}

@article{larsenSecretSharingLower2020,
  title = {Secret Sharing Lower Bound: {{Either}} Reconstruction Is Hard or Shares Are Long},
  author = {Larsen, Kasper Green and Simkin, Mark},
  year = {2020},
  doi = {10.1007/978-3-030-57990-6_28},
  abstract = {No abstract available},
  citationcount = {12},
  venue = {International Conference on Security and Cryptography for Networks},
  keywords = {lower bound}
}

@article{larsenSuperLogarithmicLower2023,
  title = {Super-Logarithmic Lower Bounds for Dynamic Graph Problems},
  author = {Larsen, Kasper Green and Yu, Huacheng},
  year = {2023},
  doi = {10.1109/FOCS57990.2023.00096},
  abstract = {In this work, we prove a {\textohm}\vphantom\{\}(\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}n) unconditional lower bound on the maximum of the query time and update time for dynamic data structures supporting reachability queries in n-node directed acyclic graphs under edge insertions. This is the first super-logarithmic lower bound for any natural graph problem. In proving the lower bound, we also make novel contributions to the state-of-the-art data structure lower bound techniques that we hope may lead to further progress in proving lower bounds.},
  citationcount = {Unknown},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,dynamic,lower bound,query,query time,update,update time}
}

@inproceedings{larsenSuperLogarithmicLowerBounds2023,
  title = {Super-{{Logarithmic Lower Bounds}} for {{Dynamic Graph Problems}}},
  booktitle = {2023 {{IEEE}} 64th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}})},
  author = {Larsen, Kasper Green and Yu, Huacheng},
  year = {2023},
  month = nov,
  pages = {1589--1604},
  issn = {2575-8454},
  doi = {10.1109/FOCS57990.2023.00096},
  url = {https://ieeexplore.ieee.org/document/10353102},
  urldate = {2024-09-02},
  abstract = {In this work, we prove a {\textbackslash}tilde{\O}mega({\l}g{\textasciicircum}3/2 n) unconditional lower bound on the maximum of the query time and update time for dynamic data structures supporting reachability queries in n-node directed acyclic graphs under edge insertions. This is the first super-logarithmic lower bound for any natural graph problem. In proving the lower bound, we also make novel contributions to the state-of-the-art data structure lower bound techniques that we hope may lead to further progress in proving lower bounds.},
  keywords = {Computational modeling,Computer science,data structure,Data structures,Directed acyclic graph,dynamic,dynamic graph algorithms,dynamic reachability,Heuristic algorithms,lower bound,query,query time,update,update time},
  file = {/Users/tulasi/Zotero/storage/B55UA5J3/Larsen and Yu - 2023 - Super-Logarithmic Lower Bounds for Dynamic Graph Problems.pdf}
}

@article{larsenTimeLowerBounds2014,
  title = {Time Lower Bounds for Nonadaptive Turnstile Streaming Algorithms},
  author = {Larsen, Kasper Green and Nelson, Jelani and Nguyen, Huy L.},
  year = {2014},
  doi = {10.1145/2746539.2746542},
  abstract = {We say a turnstile streaming algorithm is \{\vphantom\}\emph{non-adaptive}\vphantom\{\}\emph{ if, during updates, the memory cells written and read depend only on the index being updated and random coins tossed at the beginning of the stream (and not on the memory contents of the algorithm). Memory cells read during queries may be decided upon adaptively. All known turnstile streaming algorithms in the literature, except a single recent example for a particular promise problem [7], are non-adaptive. In fact, even more specifically, they are all linear sketches. We prove the first non-trivial update time lower bounds for both randomized and deterministic turnstile streaming algorithms, which hold when the algorithms are non-adaptive. While there has been abundant success in proving space lower bounds, there have been no non-trivial turnstile update time lower bounds. Our lower bounds hold against classically studied problems such as heavy hitters, point query, entropy estimation, and moment estimation. In some cases of deterministic algorithms, our lower bounds nearly match known upper bounds.}},
  citationcount = {18},
  venue = {Symposium on the Theory of Computing},
  keywords = {lower bound,non-adaptive,query,update,update time}
}

@article{larsenYesThereIs2018,
  title = {Yes, There Is an Oblivious {{RAM}} Lower Bound!},
  author = {Larsen, Kasper Green and Nielsen, J.},
  year = {2018},
  doi = {10.1007/978-3-319-96881-0_18},
  abstract = {No abstract available},
  citationcount = {109},
  venue = {IACR Cryptology ePrint Archive},
  keywords = {lower bound}
}

@article{lauAlgorithmsAndHardness2021,
  title = {Algorithms and Hardness for Multidimensional Range Updates and Queries},
  author = {Lau, Joshua and Ritossa, Angus},
  year = {2021},
  doi = {10.4230/LIPIcs.ITCS.2021.37},
  abstract = {Traditional orthogonal range problems allow queries over a static set of points, each with some value. Dynamic variants allow points to be added or removed, one at a time. To support more powerful updates, we introduce the Grid Range class of data structure problems over integer arrays in one or more dimensions. These problems allow range updates (such as filling all cells in a range with a constant) and queries (such as finding the sum or maximum of values in a range). In this work, we consider these operations along with updates that replace each cell in a range with the minimum, maximum, or sum of its existing value, and a constant. In one dimension, it is known that segment trees can be leveraged to facilitate any n of these operations in O\vphantom\{\}(n) time overall. Other than a few specific cases, until now, higher dimensional variants have been largely unexplored. We show that no truly subquadratic time algorithm can support certain pairs of these updates simultaneously without falsifying several popular conjectures. On the positive side, we show that truly subquadratic algorithms can be obtained for variants induced by other subsets. We provide two approaches to designing such algorithms that can be generalised to online and higher dimensional settings. First, we give almost-tight O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}) time algorithms for single-update variants where the update operation distributes over the query operation. Second, for other variants, we provide a general framework for reducing to instances with a special geometry. Using this, we show that O(m\textsuperscript{\{\vphantom\}}3/2-{$\epsilon$}\vphantom\{\}) time algorithms for counting paths and walks of length 2 and 3 between vertex pairs in sparse graphs imply truly subquadratic data structures for certain variants; to this end, we give an O\vphantom\{\}(m\textsuperscript{\{\vphantom\}}(4{$\omega$}-1)/(2{$\omega$}+1)\vphantom\{\})=O(m\textsuperscript{\{\vphantom\}}1.478\vphantom\{\}) time algorithm for counting simple 3-paths between vertex pairs.},
  citationcount = {13},
  venue = {Information Technology Convergence and Services},
  keywords = {data structure,dynamic,query,static,update}
}

@article{lavignePublicKeyCryptography2019,
  title = {Public-Key Cryptography in the Fine-Grained Setting},
  author = {LaVigne, Rio and Lincoln, Andrea and Williams, V. V.},
  year = {2019},
  doi = {10.1007/978-3-030-26954-8_20},
  abstract = {No abstract available},
  citationcount = {22},
  venue = {IACR Cryptology ePrint Archive}
}

@article{lecerfFactorisationDesPolynmes2013,
  title = {Factorisation Des Polyn{\^o}mes {\`a} Plusieurs Variables},
  author = {Lecerf, G.},
  year = {2013},
  doi = {10.5802/CCIRM.18},
  abstract = {. --- We give an algorithm that generalizes the algorithm of Musser and Wang to the multivariate polynomials over K where IK = Z, R or C. The given polynomial P(x lt . . ., x n ) isfirst reduced to a monic polynomial P\{x lt .. ., x n ), then wefactorize P(x lt 0, . .., 0); we use the factors q t ofP\{x lt 0, ..., 0) to construct polynomials Q t eK[x lt . . ., x,,]such that each factor Q ofP isequal to a product Q ii ... Q im . The searchfor the factors ofP is simplified by a theorem that allows to test if Qit {$\bullet$} {$\bullet$} {$\bullet$} Qi n is a factor of P without executing the division.\vphantom{\}\}}},
  citationcount = {2},
  venue = {No venue available}
}

@article{leDynamicMatchingAlgorithms2022,
  title = {Dynamic Matching Algorithms under Vertex Updates},
  author = {Le, Hung and Milenkovi{\'c}, Lazar and Solomon, Shay and Williams, V. V.},
  year = {2022},
  doi = {10.4230/LIPIcs.ITCS.2022.96},
  abstract = {Dynamic graph matching algorithms have been extensively studied, but mostly under edge updates . This paper concerns dynamic matching algorithms under vertex updates, where in each update step a single vertex is either inserted or deleted along with its incident edges. A basic setting arising in online algorithms and studied by Bosek et al. [FOCS'14] and Bernstein et al. [SODA'18] is that of dynamic approximate maximum cardinality matching (MCM) in bipartite graphs in which one side is fixed and vertices on the other side either arrive or depart via vertex updates. In the BASIC-incremental setting, vertices only arrive, while in the BASIC-decremental setting vertices only depart. When vertices can both arrive and depart, we have the BASIC-dynamic setting. In this paper we also consider the setting in which both sides of the bipartite graph are dynamic. We call this the MEDIUM-dynamic setting, and MEDIUM-decremental is the restriction when vertices can only depart. The GENERAL-dynamic setting is when the graph is not necessarily bipartite and the vertices can both depart and arrive.},
  citationcount = {6},
  venue = {Information Technology Convergence and Services},
  keywords = {dynamic,update}
}

@article{leeADirectProduct2008,
  title = {A Direct Product Theorem for Discrepancy},
  author = {Lee, Troy and Shraibman, A. and Spalek, R.},
  year = {2008},
  doi = {10.1109/CCC.2008.25},
  abstract = {Discrepancy is a versatile bound in communication complexity which can be used to show lower bounds in randomized, quantum, and even weakly-unbounded error models of communication. We show an optimal product theorem for discrepancy, namely that for any two Boolean functions f, g, disc(f odot g)=thetas(disc(f) disc(g)). As a consequence we obtain a strong direct product theorem for distributional complexity, and direct sum theorems for worst-case complexity, for bounds shown by the discrepancy method. Our results resolve an open problem of Shaltiel (2003) who showed a weaker product theorem for discrepancy with respect to the uniform distribution, discUodot(fodotk)=O(discU(f))k/3. The main tool for our results is semidefinite programming, in particular a recent characterization of discrepancy in terms of a semidefinite programming quantity by Linial and Shraibman (2006).},
  citationcount = {98},
  venue = {2008 23rd Annual IEEE Conference on Computational Complexity}
}

@article{leeReplacementPathsVia2013,
  title = {Replacement Paths via Row Minima of Concise Matrices},
  author = {Lee, Cheng-Wei and Lu, Hsueh-I},
  year = {2013},
  doi = {10.1137/120897146},
  abstract = {Matrix M is k-concise if the finite entries of each column of M consist of k or fewer intervals of identical numbers. We give an O(n+m)-time algorithm to compute the row minima of any O(1)-concise n{\texttimes}m matrix. Our algorithm yields the first O(n+m)-time reductions from the replacement-paths problem on an n-node m-edge undirected graph (respectively, directed acyclic graph) to the single-source shortest-paths problem on an O(n)-node O(m)-edge undirected graph (respectively, directed acyclic graph). That is, we prove that the replacement-paths problem is no harder than the single-source shortest-paths problem on undirected graphs and directed acyclic graphs. Moreover, our linear-time reductions lead to the first O(n+m)-time algorithms for the replacement-paths problem on the following classes of n-node m-edge graphs: (1) undirected graphs in the word-RAM model of computation, (2) undirected planar graphs, (3) undirected minor-closed graphs, and (4) directed acyclic graphs.},
  citationcount = {4},
  venue = {SIAM Journal on Discrete Mathematics},
  keywords = {reduction}
}

@article{leeuwenGraphAlgorithms1991,
  title = {Graph Algorithms},
  author = {Leeuwen, J.},
  year = {1991},
  doi = {10.1016/b978-0-444-88071-0.50015-1},
  abstract = {No abstract available},
  citationcount = {794},
  venue = {Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity}
}

@article{leightonTightBoundsOn1984,
  title = {Tight Bounds on the Complexity of Parallel Sorting},
  author = {Leighton, F.},
  year = {1984},
  doi = {10.1145/800057.808667},
  abstract = {In this paper, we prove tight upper and lower bounds on the number of processors, information transfer, wire area, and time needed to sort N numbers in a bounded-degree fixed-connection network. Our most important new results are: 1) the construction of an N-node degree-3 network capable of sorting N numbers in O(log N) word steps; 2) a proof that any network capable of sorting N (7 log N)-bit numbers in T bit steps requires area A where AT2 = {\textquestiondown}(N2 log2 N); and 3) the construction of a ``small-constant-factor'' bounded-degree network that sorts N {\textquestiondown}(log N)-bit numbers in T = {\textquestiondown}(log N) bit steps with A = {\textquestiondown}(N2) area.},
  citationcount = {504},
  venue = {IEEE transactions on computers}
}

@article{lemireReorderingRowsFor2012,
  title = {Reordering Rows for Better Compression: {{Beyond}} the Lexicographic Order},
  author = {Lemire, D. and Kaser, Owen and Gutarra, E.},
  year = {2012},
  doi = {10.1145/2338626.2338633},
  abstract = {Sorting database tables before compressing them improves the compression rate. Can we do better than the lexicographical order? For minimizing the number of runs in a run-length encoding compression scheme, the best approaches to row-ordering are derived from traveling salesman heuristics, although there is a significant trade-off between running time and compression. A new heuristic, Multiple Lists, which is a variant on Nearest Neighbor that trades off compression for a major running-time speedup, is a good option for very large tables. However, for some compression schemes, it is more important to generate long runs rather than few runs. For this case, another novel heuristic, Vortex, is promising. We find that we can improve run-length encoding up to a factor of 3 whereas we can improve prefix coding by up to 80},
  citationcount = {34},
  venue = {TODS}
}

@article{leNearOptimalSpanners2021,
  title = {Near-Optimal Spanners for General Graphs in (Nearly) Linear Time},
  author = {Le, Hung and Solomon, Shay},
  year = {2021},
  doi = {10.1137/1.9781611977073.132},
  abstract = {Let G=(V,E,w) be a weighted undirected graph on {\textbar}V{\textbar}=n vertices and {\textbar}E{\textbar}=m edges, let k{$\geq$}1 be any integer, and let {$\epsilon<$}1 be any parameter. We present the following results on fast constructions of spanners with near-optimal sparsity and lightness, which culminate a long line of work in this area. (By near-optimal we mean optimal under Erd{\H \{}o\vphantom\{\}s' girth conjecture and disregarding the {$\epsilon$}-dependencies.) - There are (deterministic) algorithms for constructing (2k-1)(1+{$\epsilon$})-spanners for G with a near-optimal sparsity of O(n\textsuperscript{\{\vphantom\}}1/k\vphantom\{\}(1/{$\epsilon$})/{$\epsilon$})). The first algorithm can be implemented in the pointer-machine model within time O(m{$\alpha$}(m,n)(1/{$\epsilon$})/{$\epsilon$})+SORT(m)), where {$\alpha$}(,) is the two-parameter inverse-Ackermann function and SORT(m) is the time needed to sort m integers. The second algorithm can be implemented in the WORD RAM model within time O(m(1/{$\epsilon$})/{$\epsilon$})). - There is a (deterministic) algorithm for constructing a (2k-1)(1+{$\epsilon$})-spanner for G that achieves a near-optimal bound of O(n\textsuperscript{\{\vphantom\}}1/k\vphantom\{\}\{poly\}(1/{$\epsilon$})) on both sparsity and lightness. This algorithm can be implemented in the pointer-machine model within time O(m{$\alpha$}(m,n)\{poly\}(1/{$\epsilon$})+SORT(m)) and in the WORD RAM model within time O(m{$\alpha$}(m,n)\{poly\}(1/{$\epsilon$})). The previous fastest constructions of (2k-1)(1+{$\epsilon$})-spanners with near-optimal sparsity incur a runtime of is O( m(n{\textasciicircum}\{1+1/k\})+nn,kn{\textasciicircum}\{2+1/k\} ), even regardless of the lightness. Importantly, the greedy spanner for stretch 2k-1 has sparsity O(n\textsuperscript{\{\vphantom\}}1/k\vphantom\{\}) -- with no {$\epsilon$}-dependence whatsoever, but its runtime is O(m(n\textsuperscript{\{\vphantom\}}1+1/k\vphantom\{\}+nn)). Moreover, the state-of-the-art lightness bound of any (2k-1)-spanner is poor, even regardless of the sparsity and runtime.},
  citationcount = {5},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{lenglerWhenDoesHillclimbing2018,
  title = {When Does Hillclimbing Fail on Monotone Functions: {{An}} Entropy Compression Argument},
  author = {Lengler, J. and Martinsson, A. and Steger, A.},
  year = {2018},
  doi = {10.1137/1.9781611975505.10},
  abstract = {Hillclimbing is an essential part of any optimization algorithm. An important benchmark for hillclimbing algorithms on pseudo-Boolean functions f: 0,1 {$^{n}\rightarrow$}\{R\} are (strictly) montone functions, on which a surprising number of hillclimbers fail to be efficient. For example, the (1+1)-Evolutionary Algorithm is a standard hillclimber which flips each bit independently with probability c/n in each round. Perhaps surprisingly, this algorithm shows a phase transition: it optimizes any monotone pseudo-boolean function in quasilinear time if c2.2. But so far it was unclear whether the threshold is at c=1. In this paper we show how Moser's entropy compression argument can be adapted to this situation, that is, we show that a long runtime would allow us to encode the random steps of the algorithm with less bits than their entropy. Thus there exists a c{$_{0}>$}1 such that for all 0},
  citationcount = {19},
  venue = {Workshop on Analytic Algorithmics and Combinatorics}
}

@article{leOptimalApproximateDistance2021,
  title = {Optimal Approximate Distance Oracle for Planar Graphs},
  author = {Le, Hung and {Wulff-Nilsen}, Christian},
  year = {2021},
  doi = {10.1109/FOCS52979.2021.00044},
  abstract = {A ({\textexclamdown}tex{\textquestiondown}1+{$\epsilon$}{\textexclamdown}/tex{\textquestiondown}) -approximate distance oracle of an edge-weighted graph is a data structure that returns an approximate shortest path distance between any two query vertices up to a ({\textexclamdown}tex{\textquestiondown}1+{$\epsilon$}{\textexclamdown}/tex{\textquestiondown}) factor. Thorup (FOCS 2001, JACM 2004) and Klein (SODA 2002) independently constructed a ({\textexclamdown}tex{\textquestiondown}1+{$\epsilon$}{\textexclamdown}/tex{\textquestiondown}) -approximate distance oracle with {\textexclamdown}tex{\textquestiondown}O(nn){\textexclamdown}/tex{\textquestiondown} space, measured in number of words, and {\textexclamdown}tex{\textquestiondown}O(1){\textexclamdown}/tex{\textquestiondown} query time when {\textexclamdown}tex{\textquestiondown}G{\textexclamdown}/tex{\textquestiondown} is an undirected planar graph with {\textexclamdown}tex{\textquestiondown}n{\textexclamdown}/tex{\textquestiondown} vertices and {\textexclamdown}tex{\textquestiondown}{$\epsilon$}{\textexclamdown}/tex{\textquestiondown} is a fixed constant. Many follow-up works gave ({\textexclamdown}tex{\textquestiondown}1+{$\epsilon$}{\textexclamdown}/tex{\textquestiondown}) -approximate distance oracles with various trade-offs between space and query time. However, improving {\textexclamdown}tex{\textquestiondown}O(nn){\textexclamdown}/tex{\textquestiondown} space bound without sacrificing query time remains an open problem for almost two decades. In this work, we resolve this problem affirmatively by constructing a ({\textexclamdown}tex{\textquestiondown}1+{$\epsilon$}{\textexclamdown}/tex{\textquestiondown}) approximate distance oracle with optimal {\textexclamdown}tex{\textquestiondown}O(n){\textexclamdown}/tex{\textquestiondown} space and {\textexclamdown}tex{\textquestiondown}O(1){\textexclamdown}/tex{\textquestiondown} query time for undirected planar graphs and fixed {\textexclamdown}tex{\textquestiondown}{$\epsilon$}{\textexclamdown}/tex{\textquestiondown}. We also make substantial progress for planar digraphs with non-negative edge weights. For fixed {\textexclamdown}tex{\textquestiondown}{$\epsilon>$}0{\textexclamdown}/tex{\textquestiondown}, we give a ({\textexclamdown}tex{\textquestiondown}1+{$\epsilon$}{\textexclamdown}/tex{\textquestiondown}) -approximate distance oracle with space {\textexclamdown}tex{\textquestiondown}o(n(Nn)){\textexclamdown}/tex{\textquestiondown} and {\textexclamdown}tex{\textquestiondown}O((Nn){\textexclamdown}/tex{\textquestiondown} query time; here {\textexclamdown}tex{\textquestiondown}N{\textexclamdown}/tex{\textquestiondown} is the ratio between the largest and smallest positive edge weight. This improves Thorup's (FOCS 2001, JACM 2004) {\textexclamdown}tex{\textquestiondown}O(n(Nn)n){\textexclamdown}/tex{\textquestiondown} space bound by more than a logarithmic factor while matching the query time of his structure. This is the first improvement for planar digraphs in two decades, both in the weighted and unweighted setting.},
  citationcount = {15},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,query,query time}
}

@article{leSparseEuclideanSpanners2023,
  title = {Sparse Euclidean Spanners with Optimal Diameter: A General and Robust Lower Bound via a Concave Inverse-Ackermann Function},
  author = {Le, Hung and Milenkovi{\'c}, Lazar and Solomon, Shay},
  year = {2023},
  doi = {10.4230/LIPIcs.SoCG.2023.47},
  abstract = {In STOC'95 [6] Arya et al. showed that any set of n points in R d admits a (1 + {$\epsilon$} )-spanner with hop-diameter at most 2 (respectively, 3) and O ( n log n ) edges (resp., O ( n log log n ) edges). They also gave a general upper bound tradeoff of hop-diameter k with O ( n{$\alpha$} k ( n )) edges, for any k {$\geq$} 2. The function {$\alpha$} k is the inverse of a certain Ackermann-style function, where {$\alpha$} 0 ( n ) = {$\lceil$} n/ 2 {$\rceil$} , {$\alpha$} 1 ( n ) = (cid:6) {\textsurd} n (cid:7) , {$\alpha$} 2 ( n ) = {$\lceil$} log n {$\rceil$} , {$\alpha$} 3 ( n ) = {$\lceil$} log log n {$\rceil$} , {$\alpha$} 4 ( n ) = log {$\ast$} n , {$\alpha$} 5 ( n ) = {$\lfloor$} 12 log {$\ast$} n {$\rfloor$} , . . . . Roughly speaking, for k {$\geq$} 2 the function {$\alpha$} k is close to {$\lfloor$} k - 2 2 {$\rfloor$} -iterated log-star function, i.e., log with {$\lfloor$} k - 2 2 {$\rfloor$} stars. Despite a large body of work on spanners of bounded hop-diameter, the fundamental question of whether this tradeoff between size and hop-diameter of Euclidean (1 + {$\epsilon$} )-spanners is optimal has remained open, even in one-dimensional spaces. Three lower bound tradeoffs are known: An optimal k},
  citationcount = {1},
  venue = {International Symposium on Computational Geometry},
  keywords = {lower bound}
}

@article{levyAComparativeStudy2021,
  title = {A Comparative Study of Dictionary Matching with Gaps: {{Limitations}}, Techniques and Challenges},
  author = {Levy, Avivit and Shalom, B. R.},
  year = {2021},
  doi = {10.1007/s00453-021-00851-6},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Algorithmica}
}

@article{lewensteinSpaceEfficientString2014,
  title = {Space-Efficient String Indexing for Wildcard Pattern Matching},
  author = {Lewenstein, Moshe and Nekrich, Yakov and Vitter, J.},
  year = {2014},
  doi = {10.4230/LIPIcs.STACS.2014.506},
  abstract = {In this paper we describe compressed indexes that support pattern matching queries for strings with wildcards. For a constant size alphabet our data structure uses O(n\textsuperscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}n) bits for any {$\varepsilon>$}0 and reports all \{occ\} occurrences of a wildcard string in O(m+{$\sigma^{g}\cdot\mu$}(n)+\{occ\}) time, where {$\mu$}(n)=o(n), {$\sigma$} is the alphabet size, m is the number of alphabet symbols and g is the number of wildcard symbols in the query string. We also present an O(n)-bit index with O((m+{$\sigma^g$}+\{occ\})\textsuperscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}n) query time and an O(n(n){$^2$})-bit index with O((m+{$\sigma^g$}+\{occ\})n) query time. These are the first non-trivial data structures for this problem that need o(nn) bits of space.},
  citationcount = {21},
  venue = {Symposium on Theoretical Aspects of Computer Science},
  keywords = {data structure,query,query time}
}

@article{liangOptimalCollapsingProtocol2013,
  title = {Optimal Collapsing Protocol for Multiparty Pointer Jumping},
  author = {Liang, Hongyu},
  year = {2013},
  doi = {10.1007/s00224-013-9476-x},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Theory of Computing Systems}
}

@article{libertyTheMailmanAlgorithm2009,
  title = {The {{Mailman}} Algorithm: {{A}} Note on Matrix-Vector Multiplication},
  author = {Liberty, Edo and Zucker, S.},
  year = {2009},
  doi = {10.1016/j.ipl.2008.09.028},
  abstract = {No abstract available},
  citationcount = {62},
  venue = {Information Processing Letters}
}

@article{liDynamicSuccincter2023,
  title = {Dynamic ``Succincter''},
  author = {Li, Tianxiao and Liang, Jingxun and Yu, Huacheng and Zhou, Renfei},
  year = {2023},
  doi = {10.1109/FOCS57990.2023.00104},
  abstract = {Augmented B-trees (aB-trees) are a broad class of data structures. The seminal work ``succincter'' by P{\v a}tra{\c s}cu [1] showed that any aB-tree can be stored using only two bits of redundancy, while supporting queries to the tree in time proportional to its depth. It has been a versatile building block for constructing succinct data structures, including rank/select data structures, dictionaries, locally decodable arithmetic coding, storing balanced parenthesis, etc.In this paper, we show how to ``dynamize'' an aB-tree. Our main result is the design of dynamic aB-trees (daB-trees) with branching factor two using only three bits of redundancy (with the help of lookup tables that are of negligible size in applications), while supporting updates and queries in time polynomial in its depth. As an application, we present a dynamic rank/select data structure for n-bit arrays, also known as a dynamic fully indexable dictionary (FID) [2]. It supports updates and queries in O(n/n) time, and when the array has m ones, the \{equation*\}\{pmatrix\}n  m\{pmatrix\}+On / 2{\textasciicircum}\{0.199\} n\{equation*\}bits. Note that the update and query times are optimal even without space constraints due to a lower bound by Fredman and Saks [3]. Prior to our work, no dynamic FID with near-optimal update and query times and redundancy o(n/n) was known. We further show that a dynamic sequence supporting insertions, deletions and rank/select queries can be maintained in (optimal) O(n/n) time and with O(n{$\cdot$}\{poly\}n/\textsuperscript{\{\vphantom\}}2\vphantom\{\}n) bits of redundancy.},
  citationcount = {3},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,dynamic,lower bound,query,query time,update}
}

@article{liEfficientAlgorithmsFor2022,
  title = {Efficient Algorithms for Dynamic Bidirected {{Dyck-reachability}}},
  author = {Li, Yuanbo and Satya, K. and Zhang, Qirun},
  year = {2022},
  doi = {10.1145/3498724},
  abstract = {Dyck-reachability is a fundamental formulation for program analysis, which has been widely used to capture properly-matched-parenthesis program properties such as function calls/returns and field writes/reads. Bidirected Dyck-reachability is a relaxation of Dyck-reachability on bidirected graphs where each edge u{$\rightarrow$}(iv labeled by an open parenthesis ``(i'' is accompanied with an inverse edge v{$\rightarrow$})iu labeled by the corresponding close parenthesis ``)i'', and vice versa. In practice, many client analyses such as alias analysis adopt the bidirected Dyck-reachability formulation. Bidirected Dyck-reachability admits an optimal reachability algorithm. Specifically, given a graph with n nodes and m edges, the optimal bidirected Dyck-reachability algorithm computes all-pairs reachability information in O(m) time. This paper focuses on the dynamic version of bidirected Dyck-reachability. In particular, we consider the problem of maintaining all-pairs Dyck-reachability information in bidirected graphs under a sequence of edge insertions and deletions. Dynamic bidirected Dyck-reachability can formulate many program analysis problems in the presence of code changes. Unfortunately, solving dynamic graph reachability problems is challenging. For example, even for maintaining transitive closure, the fastest deterministic dynamic algorithm requires O(n2) update time to achieve O(1) query time. All-pairs Dyck-reachability is a generalization of transitive closure. Despite extensive research on incremental computation, there is no algorithmic development on dynamic graph algorithms for program analysis with worst-case guarantees. Our work fills the gap and proposes the first dynamic algorithm for Dyck reachability on bidirected graphs. Our dynamic algorithms can handle each graph update (i.e., edge insertion and deletion) in O(n{$\cdot\alpha$}(n)) time and support any all-pairs reachability query in O(1) time, where {$\alpha$}(n) is the inverse Ackermann function. We have implemented and evaluated our dynamic algorithm on an alias analysis and a context-sensitive data-dependence analysis for Java. We compare our dynamic algorithms against a straightforward approach based on the O(m)-time optimal bidirected Dyck-reachability algorithm and a recent incremental Datalog solver. Experimental results show that our algorithm achieves orders of magnitude speedup over both approaches.},
  citationcount = {5},
  venue = {Proc. ACM Program. Lang.},
  keywords = {dynamic,query,query time,update,update time}
}

@article{lifshitsCombinatorialFrameworkFor2009,
  title = {Combinatorial Framework for Similarity Search},
  author = {Lifshits, Y.},
  year = {2009},
  doi = {10.1109/SISAP.2009.31},
  abstract = {We present an overview of combinatorial framework for similarity search. An algorithm is combinatorial if only direct comparisons between two pairwise similarity values are allowed. Namely, the input dataset is represented by a comparison oracle that given any three points X,Y,Z answers whether Y or Z is closer to X. We assume that the similarity order of the dataset satisfies the four variations of the following disorder inequality: if X is the A'th most similar object to Y and Y is the B'th most similar object to Z, then X is among the D(A+B) most similar objects to Z, where D is a relatively small disorder constant. Combinatorial algorithms for nearest neighbor search have two important advantages: (1) they do not map similarity values to artificial distance values and do not use triangle inequality for the latter, and (2) they work for arbitrarily complicated data representations and similarity functions. Ranwalk, the first known combinatorial solution for nearest neighbors, is randomized, exact, zero-error algorithm with query time that is logarithmic in number of objects. But Ranwalk preprocessing time is quadratic. Later on, another solution, called combinatorial nets, was discovered. It is deterministic and exact algorithm with almost linear time and space complexity of preprocessing, and near-logarithmic time complexity of search. Combinatorial nets also have a number of side applications. For near-duplicate detection they lead to the first known deterministic algorithm that requires just near-linear time + time proportional to the size of output. For any dataset with small disorder combinatorial nets can be used to construct a visibility graph: the one in which greedy routing deterministically converges to the nearest neighbor of a target in logarithmic number of steps. The later result is the first known work-around for Navarro's impossibility of generalizing Delaunay graphs.},
  citationcount = {5},
  venue = {2009 Second International Workshop on Similarity Search and Applications},
  keywords = {query,query time}
}

@article{liGaussianProcessesInequalities2001,
  title = {Gaussian Processes: {{Inequalities}}, Small Ball Probabilities and Applications},
  author = {Li, Wenbo V. and Shao, Q.},
  year = {2001},
  doi = {10.1016/S0169-7161(01)19019-X},
  abstract = {No abstract available},
  citationcount = {455},
  venue = {No venue available}
}

@article{liHighQualityHypergraph2021,
  title = {High Quality Hypergraph Partitioning for Logic Emulation},
  author = {Li, B. and Qi, Zhongdong and Tang, Zhengguang and He, Xiyi and You, Hailong},
  year = {2021},
  doi = {10.1016/j.vlsi.2021.11.005},
  abstract = {No abstract available},
  citationcount = {5},
  venue = {Integr.}
}

@article{linBinarySearchAlgorithm2019,
  title = {Binary Search Algorithm},
  author = {Lin, Anthony},
  year = {2019},
  doi = {10.15347/wjs/2019.005},
  abstract = {In In computer science, binary search , also known as half-interval search , [1] logarithmic search , [2] or binary chop , [3] is a search algorithm that finds a position of a target value within a sorted array. [4] Binary search compares the target value to an element in the middle of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value, and repeating this until the target value is found. If the search ends with the remaining half being empty, the target is not in the array. Binary search runs in logarithmic time in the worst case, making {$O$} ( log {$n$} ) comparisons, where {$n$} is the number of elements in the array, the {$O$} is ` Big O ' notation, and {$log$} is the logarithm. [5] Binary search is faster than linear search except for small arrays. However, the array must be sorted first to be able to apply binary search. There are specialized data structures designed for fast searching, such as hash tables, that can be searched more efficiently than binary search. However, binary search can be used to solve a wider range of problems, such as finding the next-smallest or next-largest element in the array relative to the target even if it is absent from the array. There are numerous variations of binary search. In particular, fractional cascading speeds up binary searches for the same value in multiple arrays. Fractional cascading efficiently solves a number of search problems in computational geometry and in numerous other fields. Exponential search extends binary search to unbounded lists. The binary search tree and B-tree data structures are based on binary search.},
  citationcount = {45},
  venue = {WikiJournal of Science},
  keywords = {data structure}
}

@article{lincolnDeterministicTimeSpace2016,
  title = {Deterministic Time-Space Tradeoffs for k-{{SUM}}},
  author = {Lincoln, Andrea and Williams, V. V. and Wang, Joshua R. and Williams, Richard Ryan},
  year = {2016},
  doi = {10.4230/LIPIcs.ICALP.2016.58},
  abstract = {Given a set of numbers, the k-SUM problem asks for a subset of k numbers that sums to zero. When the numbers are integers, the time and space complexity of k-SUM is generally studied in the word-RAM model; when the numbers are reals, the complexity is studied in the real-RAM model, and space is measured by the number of reals held in memory at any point. We present a time and space efficient deterministic self-reduction for the k-SUM problem which holds for both models, and has many interesting consequences. To illustrate: * 3-SUM is in deterministic time O(n{$^2$}(n)/(n)) and space O({\textsurd}\{\vphantom\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}(n)\vphantom\{\}\{(n)\}\vphantom\{\}). In general, any polylogarithmic-time improvement over quadratic time for 3-SUM can be converted into an algorithm with an identical time improvement but low space complexity as well. * 3-SUM is in deterministic time O(n{$^2$}) and space O({\textsurd}n), derandomizing an algorithm of Wang. * A popular conjecture states that 3-SUM requires n\textsuperscript{\{\vphantom\}}2-o(1)\vphantom\{\} time on the word-RAM. We show that the 3-SUM Conjecture is in fact equivalent to the (seemingly weaker) conjecture that every O(n\textsuperscript{\{\vphantom\}}.51\vphantom\{\})-space algorithm for 3-SUM requires at least n\textsuperscript{\{\vphantom\}}2-o(1)\vphantom\{\} time on the word-RAM. * For k{$\geq$}4, k-SUM is in deterministic O(n\textsuperscript{\{\vphantom\}}k-2+2/k\vphantom\{\}) time and O({\textsurd}\{n\}) space.},
  citationcount = {34},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {reduction,time-space}
}

@article{lincolnMonochromaticTrianglesIntermediate2020,
  title = {Monochromatic Triangles, Intermediate Matrix Products, and Convolutions},
  author = {Lincoln, Andrea and Polak, Adam and Williams, V. V.},
  year = {2020},
  doi = {10.4230/LIPIcs.ITCS.2020.53},
  abstract = {The most studied linear algebraic operation, matrix multiplication, has surprisingly fast O(n\textsuperscript{{$\omega$}}) time algorithms for {$\omega<$}2.373. On the other hand, the (,+) matrix product which is at the heart of many fundamental graph problems such as APSP, has received only minor improvements over its brute-force cubic running time and is widely conjectured to require n\textsuperscript{\{\vphantom\}}3-o(1)\vphantom\{\} time. There is a plethora of matrix products and graph problems whose complexity seems to lie in the middle of these two problems. For instance, the Min-Max matrix product, the Minimum Witness matrix product, APSP in directed unweighted graphs and determining whether an edge-colored graph contains a monochromatic triangle, can all be solved in (n\textsuperscript{\{\vphantom\}}(3+{$\omega$})/2\vphantom\{\}) time. A similar phenomenon occurs for convolution problems, where analogous intermediate problems can be solved in (n\textsuperscript{\{\vphantom\}}1.5\vphantom\{\}) time. Can one improve upon the running times for these intermediate problems, in either the matrix product or the convolution world? Or, alternatively, can one relate these problems to each other and to other key problems in a meaningful way? This paper makes progress on these questions by providing a network of fine-grained reductions. We show for instance that APSP in directed unweighted graphs and Minimum Witness product can be reduced to both the Min-Max product and a variant of the monochromatic triangle problem. We also show that a natural convolution variant of monochromatic triangle is fine-grained equivalent to the famous 3SUM problem. As this variant is solvable in O(n\textsuperscript{\{\vphantom\}}1.5\vphantom\{\}) time and 3SUM is in O(n{$^2$}) time (and is conjectured to require n\textsuperscript{\{\vphantom\}}2-o(1)\vphantom\{\} time), our result gives the first fine-grained equivalence between natural problems of different running times.},
  citationcount = {29},
  venue = {Information Technology Convergence and Services},
  keywords = {reduction}
}

@article{lincolnTightHardnessFor2017,
  title = {Tight Hardness for Shortest Cycles and Paths in Sparse Graphs},
  author = {Lincoln, Andrea and Williams, V. V. and Williams, Richard Ryan},
  year = {2017},
  doi = {10.1137/1.9781611975031.80},
  abstract = {Fine-grained reductions have established equivalences between many core problems with O\vphantom\{\}(n{$^3$})-time algorithms on n-node weighted graphs, such as Shortest Cycle, All-Pairs Shortest Paths (APSP), Radius, Replacement Paths, Second Shortest Paths, and so on. These problems also have O\vphantom\{\}(mn)-time algorithms on m-edge n-node weighted graphs, and such algorithms have wider applicability. Are these mn bounds optimal when m{$\ll$}n{$^2$}? Starting from the hypothesis that the minimum weight (2{$\ell$}+1)-Clique problem in edge weighted graphs requires n\textsuperscript{\{\vphantom\}}2{$\ell$}+1-o(1)\vphantom\{\} time, we prove that for all sparsities of the form m={$\Theta$}(n\textsuperscript{\{\vphantom\}}1+1/{$\ell$}\vphantom\{\}), there is no O(n{$^2$}+mn\textsuperscript{\{\vphantom\}}1-{$\epsilon$}\vphantom\{\}) time algorithm for {$\epsilon>$}0 for \{any\} of the below problems: Minimum Weight (2{$\ell$}+1)-Cycle in a directed weighted graph, Shortest Cycle in a directed weighted graph, APSP in a directed or undirected weighted graph, Radius (or Eccentricities) in a directed or undirected weighted graph, Wiener index of a directed or undirected weighted graph, Replacement Paths in a directed weighted graph, Second Shortest Path in a directed weighted graph, Betweenness Centrality of a given node in a directed weighted graph. That is, we prove hardness for a variety of sparse graph problems from the hardness of a dense graph problem. Our results also lead to new conditional lower bounds from several related hypothesis for unweighted sparse graph problems including k-cycle, shortest cycle, Radius, Wiener index and APSP.},
  citationcount = {112},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {lower bound,reduction}
}

@article{lindenstraussClassicalBanachSpaces1973,
  title = {Classical Banach Spaces},
  author = {Lindenstrauss, J. and Tzafriri, L.},
  year = {1973},
  doi = {10.1142/9789814578554_0004},
  abstract = {Springer-Verlag began publishing books in higher mathematics in 1920, when the series Grundlehren der mathematischen Wissenschaften, initially conceived as a series of advanced textbooks, was founded by Richard Courant. A few years later a new series Ergebnisse der Mathematik und ihrer Grenzgebiete, survey reports of recent mathematical research, was added. Of over 400 books published in these series, many have become recognized classics and remain standard references for their subject. Springer is reissuing a selected few of these highly successful books in a new, inexpensive sofcover edition to make them easily accessible to younger generations of students and researchers.},
  citationcount = {3858},
  venue = {No venue available}
}

@article{linDoublyEfficientPrivate2023,
  title = {Doubly Efficient Private Information Retrieval and Fully Homomorphic {{RAM}} Computation from Ring {{LWE}}},
  author = {Lin, Wei-Kai and Mook, Ethan and Wichs, Daniel},
  year = {2023},
  doi = {10.1145/3564246.3585175},
  abstract = {A (single server) private information retrieval (PIR) allows a client to read data from a public database held on a remote server, without revealing to the server which locations she is reading. In a doubly efficient PIR (DEPIR), the database is first preprocessed, but the server can subsequently answer any client's query in time that is sub-linear in the database size. Prior work gave a plausible candidate for a public-key variant of DEPIR, where a trusted party is needed to securely preprocess the database and generate a corresponding public key for the clients; security relied on a new non-standard code-based assumption and a heuristic use of ideal obfuscation. In this work we construct the stronger unkeyed notion of DEPIR, where the preprocessing is a deterministic procedure that the server can execute on its own. Moreover, we prove security under just the standard ring learning-with-errors (RingLWE) assumption. For a database of size N and any constant {$\varepsilon$}{\textquestiondown}0, the preprocessing run-time and size is O(N1+{$\varepsilon$}), while the run-time and communication-complexity of each PIR query is polylog(N). We also show how to update the preprocessed database in time O(N{$\varepsilon$}). Our approach is to first construct a standard PIR where the server's computation consists of evaluating a multivariate polynomial; we then convert it to a DEPIR by preprocessing the polynomial to allow for fast evaluation, using the techniques of Kedlaya and Umans (STOC '08). Building on top of our DEPIR, we construct general fully homomorphic encryption for random-access machines (RAM-FHE), which allows a server to homomorphically evaluate an arbitrary RAM program P over a client's encrypted input x and the server's preprocessed plaintext input y to derive an encryption of the output P(x,y) in time that scales with the RAM run-time of the computation rather than its circuit size. Prior work only gave a heuristic candidate construction of a restricted notion of RAM-FHE. In this work, we construct RAM-FHE under the RingLWE assumption with circular security. For a RAM program P with worst-case run-time T, the homomorphic evaluation runs in time T1+{$\varepsilon$} {$\cdot$} ({\textbar}x{\textbar} + {\textbar}y{\textbar}).},
  citationcount = {48},
  venue = {IACR Cryptology ePrint Archive},
  keywords = {communication,communication complexity,query,update}
}

@article{lindseyAssignmentOfNumbers1964,
  title = {Assignment of Numbers to Vertices},
  author = {Lindsey, J.},
  year = {1964},
  doi = {10.1080/00029890.1964.11992272},
  abstract = {No abstract available},
  citationcount = {123},
  venue = {No venue available}
}

@article{lindstromTheDesignAnd1985,
  title = {The Design and Analysis of {{BucketSort}} for Bubble Memory Secondary Storage},
  author = {Lindstrom, E. E. and Vitter, J.},
  year = {1985},
  doi = {10.1109/TC.1985.1676565},
  abstract = {BucketSort is a new external sorting algorithm for very large files that is a substantial improvement over merge sorting with disks. BucketSort requires an associative secondary storage device, which can be realized by large disk drives with logic-per-track capabilities or by magnetic bubble memory (MBM). This paper describes and analyzes a hypothetical Bucket-Sort implementation that uses bubble memory. A new software marking technique is introduced that reduces the effective time for an associative search.},
  citationcount = {43},
  venue = {IEEE transactions on computers}
}

@article{linialConstantDepthCircuits1989,
  title = {Constant Depth Circuits, {{Fourier}} Transform, and Learnability},
  author = {Linial, N. and Mansour, Y. and Nisan, N.},
  year = {1989},
  doi = {10.1145/174130.174138},
  abstract = {Boolean functions in AC/sup O/ are studied using the harmonic analysis of the cube. The main result is that an AC/sup O/ Boolean function has almost all of its power spectrum on the low-order coefficients. This result implies the following properties of functions in AC/sup O/: functions in AC/sup O/ have low average sensitivity; they can be approximated well be a real polynomial of low degree; they cannot be pseudorandom function generators and their correlation with any polylog-wide independent probability distribution is small. An O(n/sup polylog(/ /sup sup)/ /sup (n)/)-time algorithm for learning functions in AC/sup O/ is obtained. The algorithm observed the behavior of an AC/sup O/ function on O(n/sup polylog/ /sup (n)/) randomly chosen inputs and derives a good approximation for the Fourier transform of the function. This allows it to predict with high probability the value of the function on other randomly chosen inputs.<<ETX>>},
  citationcount = {784},
  venue = {30th Annual Symposium on Foundations of Computer Science}
}

@article{linialExpanderGraphsAnd2006,
  title = {Expander Graphs and Their Applications},
  author = {Linial, N.},
  year = {2006},
  doi = {10.1090/S0273-0979-06-01126-8},
  abstract = {A major consideration we had in writing this survey was to make it accessible to mathematicians as well as to computer scientists, since expander graphs, the protagonists of our story, come up in numerous and often surprising contexts in both fields. But, perhaps, we should start with a few words about graphs in general. They are, of course, one of the prime objects of study in Discrete Mathematics. However, graphs are among the most ubiquitous models of both natural and human-made structures. In the natural and social sciences they model relations among species, societies, companies, etc. In computer science, they represent networks of communication, data organization, computational devices as well as the flow of computation, and more. In mathematics, Cayley graphs are useful in Group Theory. Graphs carry a natural metric and are therefore useful in Geometry, and though they are ``just'' one-dimensional complexes, they are useful in certain parts of Topology, e.g. Knot Theory. In statistical physics, graphs can represent local connections between interacting parts of a system, as well as the dynamics of a physical process on such systems. The study of these models calls, then, for the comprehension of the significant structural properties of the relevant graphs. But are there nontrivial structural properties which are universally important? Expansion of a graph requires that it is simultaneously sparse and highly connected. Expander graphs were first defined by Bassalygo and Pinsker, and their existence first proved by Pinsker in the early '70s. The property of being an expander seems significant in many of these mathematical, computational and physical contexts. It is not surprising that expanders are useful in the design and analysis of communication networks. What is less obvious is that expanders have surprising utility in other computational settings such as in the theory of error correcting codes and the theory of pseudorandomness. In mathematics, we will encounter e.g. their role in the study of metric embeddings, and in particular in work around the Baum-Connes Conjecture. Expansion is closely related to the convergence rates of Markov Chains, and so they play a key role in the study of Monte-Carlo algorithms in statistical mechanics and in a host of practical computational applications. The list of such interesting and fruitful connections goes on and on with so many applications we will not even},
  citationcount = {1859},
  venue = {No venue available}
}

@article{linialLearningComplexityVs2008,
  title = {Learning Complexity vs. Communication Complexity},
  author = {Linial, N. and Shraibman, A.},
  year = {2008},
  doi = {10.1017/S0963548308009656},
  abstract = {This paper has two main focal points. We first consider an important class of machine learning algorithms - large margin classifiers, such as support vector machines. The notion of margin complexity quantifies the extent to which a given class of functions can be learned by large margin classifiers. We prove that up to a small multiplicative constant, margin complexity is equal to the inverse of discrepancy. This establishes a strong tie between seemingly very different notions from two distinct areas. In the same way that matrix rigidity is related to rank, we introduce the notion of rigidity of margin complexity. We prove that sign matrices with small margin complexity rigidity are very rare. This leads to the question of proving lower bounds on the rigidity of margin complexity. Quite surprisingly, this question turns out to be closely related to basic open problems in communication complexity, e.g., whether PSPACE can be separated from the polynomial hierarchy in communication complexity. There are numerous known relations between the field of learning theory and that of communication complexity, as one might expect since communication is an inherent aspect of learning. The results of this paper constitute another link in this rich web of relations. This link has already proved significant as it was used in the solution of a few open problems in communication complexity.},
  citationcount = {78},
  venue = {2008 23rd Annual IEEE Conference on Computational Complexity}
}

@article{linialTheGeometryOf1994,
  title = {The Geometry of Graphs and Some of Its Algorithmic Applications},
  author = {Linial, N. and London, E. and Rabinovich, Yuri},
  year = {1994},
  doi = {10.1109/SFCS.1994.365733},
  abstract = {AbstractIn this paper we explore some implications of viewing graphs asgeometric objects. This approach offers a new perspective on a number of graph-theoretic and algorithmic problems. There are several ways to model graphs geometrically and our main concern here is with geometric representations that respect themetric of the (possibly weighted) graph. Given a graphG we map its vertices to a normed space in an attempt to (i) keep down the dimension of the host space, and (ii) guarantee a smalldistortion, i.e., make sure that distances between vertices inG closely match the distances between their geometric images.In this paper we develop efficient algorithms for embedding graphs low-dimensionally with a small distortion. Further algorithmic applications include:{$\bullet$}A simple, unified approach to a number of problems on multicommodity flows, including the Leighton-Rao Theorem [37] and some of its extensions. We solve an open question in this area, showing that the max-flow vs. min-cut gap in thek-commodities problem isO(logk). Our new deterministic polynomial-time algorithm finds a (nearly tight) cut meeting this bound.{$\bullet$}For graphs embeddable in low-dimensional spaces with a small distortion, we can find low-diameter decompositions (in the sense of [7] and [43]). The parameters of the decomposition depend only on the dimension and the distortion and not on the size of the graph.{$\bullet$}In graphs embedded this way, small balancedseparators can be found efficiently. Given faithful low-dimensional representations of statistical data, it is possible to obtain meaningful and efficientclustering. This is one of the most basic tasks in pattern-recognition. For the (mostly heuristic) methods used in the practice of pattern-recognition, see [20], especially chapter 6.Our studies of multicommodity flows also imply that every embedding of (the metric of) ann-vertex, constant-degree expander into a Euclidean space (of any dimension) has distortion {\textohm}(logn). This result is tight, and closes a gap left open by Bourgain [12].},
  citationcount = {1142},
  venue = {Proceedings 35th Annual Symposium on Foundations of Computer Science}
}

@article{liNonlinearEstimatorsAnd2006,
  title = {Nonlinear Estimators and Tail Bounds for Dimension Reduction in L1 Using Cauchy Random Projections},
  author = {Li, Ping and Hastie, Trevor J. and Church, Kenneth Ward},
  year = {2006},
  doi = {10.5555/1314498.1314579},
  abstract = {For dimension reduction in the l1 norm, the method of Cauchy random projections multiplies the original data matrix A {$\in$} {$\mathbb{R}$}n{\texttimes}D with a random matrix R {$\in$} {$\mathbb{R}$}D{\texttimes}k (k{$\ll$}D) whose entries are i.i.d. samples of the standard Cauchy C(0,1). Because of the impossibility result, one can not hope to recover the pairwise l1 distances in A from B=A{\texttimes}R{$\in$} {$\mathbb{R}$}n{\texttimes}k, using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O(log n / e2) suffices with the constants explicitly given. Asymptotically (as k{$\rightarrow\infty$}), both the sample median and the geometric mean estimators are about 80},
  citationcount = {152},
  venue = {Journal of machine learning research}
}

@article{liOnTheClosest2000,
  title = {On the Closest String and Substring Problems},
  author = {Li, Ming and Ma, B. and Wang, Lusheng},
  year = {2000},
  doi = {10.1145/506147.506150},
  abstract = {The problem of finding a center string that is "close" to everygiven string arises in computational molecular biology and codingtheory. This problem has two versions: the Closest String problemand the Closest Substring problem. Given a set of strings {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown}= \{{\textexclamdown}i{\textquestiondown}s{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}1{\textexclamdown}/sub{\textquestiondown}, {\textexclamdown}i{\textquestiondown}s{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}2{\textexclamdown}/sub{\textquestiondown}, ...,{\textexclamdown}i{\textquestiondown}s{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}n{\textexclamdown}/sub{\textquestiondown}\}, each of length {\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown}, the Closest Stringproblem is to find the smallest {\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown} and a string s of length{\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown} which is within Hamming distance d to each{\textexclamdown}i{\textquestiondown}s{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}i{\textexclamdown}/sub{\textquestiondown} {$\varepsilon$} {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown}. This problem comes fromcoding theory when we are looking for a code not too far away froma given set of codes. Closest Substring problem, with an additionalinput integer {\textexclamdown}i{\textquestiondown}L{\textexclamdown}/i{\textquestiondown}, asks for the smallest d and a string{\textexclamdown}i{\textquestiondown}s{\textexclamdown}/i{\textquestiondown}, of length {\textexclamdown}i{\textquestiondown}L{\textexclamdown}/i{\textquestiondown}, which is within Hamming distance daway from a substring, of length {\textexclamdown}i{\textquestiondown}L{\textexclamdown}/i{\textquestiondown}, of each si. This problemis much more elusive than the Closest String problem. The ClosestSubstring problem is formulated from applications in findingconserved regions, identifying genetic drug targets and generatinggenetic probes in molecular biology. Whether there are efficientapproximation algorithms for both problems are major open questionsin this area. We present two polynomial-time approximationalgorithms with approximation ratio 1 + {$\varepsilon$} for any small{$\varepsilon$} to settle both questions.},
  citationcount = {193},
  venue = {JACM}
}

@article{liptonPlayingLargeGames2003,
  title = {Playing Large Games Using Simple Strategies},
  author = {Lipton, R. and Markakis, E. and Mehta, Aranyak},
  year = {2003},
  doi = {10.1145/779928.779933},
  abstract = {We prove the existence of {$\varepsilon$}-Nash equilibrium strategies with support logarithmic in the number of pure strategies. We also show that the payoffs to all players in any (exact) Nash equilibrium can be {$\varepsilon$}-approximated by the payoffs to the players in some such logarithmic support {$\varepsilon$}-Nash equilibrium. These strategies are also uniform on a multiset of logarithmic size and therefore this leads to a quasi-polynomial algorithm for computing an {$\varepsilon$}-Nash equilibrium. To our knowledge this is the first subexponential algorithm for finding an {$\varepsilon$}-Nash equilibrium. Our results hold for any multiple-player game as long as the number of players is a constant (i.e., it is independent of the number of pure strategies). A similar argument also proves that for a fixed number of players m, the payoffs to all players in any m-tuple of mixed strategies can be {$\varepsilon$}-approximated by the payoffs in some m-tuple of constant support strategies.We also prove that if the payoff matrices of a two person game have low rank then the game has an exact Nash equilibrium with small support. This implies that if the payoff matrices can be well approximated by low rank matrices, the game has an {$\varepsilon$}-equilibrium with small support. It also implies that if the payoff matrices have constant rank we can compute an exact Nash equilibrium in polynomial time.},
  citationcount = {421},
  venue = {ACM Conference on Economics and Computation}
}

@article{liTightCellProbe2023,
  title = {Tight Cell-Probe Lower Bounds for Dynamic Succinct Dictionaries},
  author = {Li, Tianxiao and Liang, Jingxun and Yu, Huacheng and Zhou, Renfei},
  year = {2023},
  journal = {IEEE Annual Symposium on Foundations of Computer Science},
  doi = {10.1109/FOCS57990.2023.00112},
  abstract = {A dictionary data structure maintains a set of at most n keys from the universe [U] under key insertions and deletions, such that given a query x{$\in$}[U], it returns if x is in the set. Some variants also store values associated to the keys such that given a query x, the value associated to x is returned when x is in the set.This fundamental data structure problem has been studied for six decades since the introduction of hash tables in 1953. A hash table occupies O(nU) bits of space with constant time per operation in expectation. There has been a vast literature on improving its time and space usage. The state-of-the-art dictionary by Bender, Farach-Colton, Kuszmaul, Kuszmaul and Liu [1] has space consumption close to the information-theoretic optimum, using a total of \{equation*\}\{pmatrix\} U   n \{pmatrix\}+On{\textasciicircum}\{(k)\} n\{equation*\} bits, while supporting all operations in O(k) time, for any parameter k{$\leq$}\textsuperscript{\{\vphantom\}}*\vphantom\{\}n. The term O(\textsuperscript{\{\vphantom\}}(k)\vphantom\{\}n)=O(\{{$\cdots$}n\}) is referred to as the wasted bits per key.In this paper, we prove a matching cell-probe lower bound: For U=n\textsuperscript{\{\vphantom\}}1+{$\Theta$}(1)\vphantom\{\}, any dictionary with O(\textsuperscript{\{\vphantom\}}(k)\vphantom\{\}n) wasted bits per key must have expected operational time {\textohm}(k), in the cell-probe model with word-size w={$\Theta$}(U). Furthermore, if a dictionary stores values of {$\Theta$}(U) bits, we show that regardless of the query time, it must have {\textohm}(k) expected update time. It is worth noting that this is the first cell-probe lower bound on the trade-off between space and update time for general data structures.},
  citationcount = {4},
  keywords = {cell probe,data structure,dynamic,information theoretic,lower bound,query,query time,update,update time}
}

@article{liuAlgorithmsForFinding2008,
  title = {Algorithms for Finding the Weight-Constrained k Longest Paths in a Tree and the Length-Constrained k Maximum-Sum Segments of a Sequence},
  author = {Liu, Hsiao-Fei and Chao, K.},
  year = {2008},
  doi = {10.1016/j.tcs.2008.06.052},
  abstract = {No abstract available},
  citationcount = {13},
  venue = {Theoretical Computer Science}
}

@article{liuOnApproximateFully2024,
  title = {On Approximate Fully-Dynamic Matching and Online Matrix-Vector Multiplication},
  author = {Liu, Yang P.},
  year = {2024},
  doi = {10.1109/FOCS61266.2024.00006},
  abstract = {We study connections between the problem of fully dynamic (1-{$\epsilon$})-approximate maximum bipartite matching, and the dual (1+{$\epsilon$})-approximate vertex cover problem, with the online matrix-vector (OMv) conjecture which has recently been used in several fine-grained hardness reductions. We prove that there is an online algorithm that maintains a (1+{$\epsilon$})-approximate vertex cover in amortized n\textsuperscript{\{\vphantom\}}1-c\vphantom\{\}{$\epsilon$}\textsuperscript{\{\vphantom\}}-C\vphantom\{\} time for constants c,C{$>$}0 for fully dynamic updates if and only if the OMv conjecture is false. Similarly, we prove that there is an online algorithm that maintains a (1-{$\epsilon$})-approximate maximum matching in amortized n\textsuperscript{\{\vphantom\}}1-c\vphantom\{\}{$\epsilon$}\textsuperscript{\{\vphantom\}}-C\vphantom\{\} time if and only if there is a nontrivial algorithm for another dynamic problem, which we call dynamic approximate OMv, that has seemingly no matching structure. This provides some evidence against achieving amortized sublinear update times for approximate fully dynamic matching and vertex cover. Leveraging these connections, we obtain faster algorithms for approximate fully dynamic matching in both the online and offline settings. We give a randomized algorithm that with high probability maintains a (1-{$\epsilon$})-approximate bipartite matching and (1+{$\epsilon$})-approximate vertex cover in fully dynamic graphs, in amortized O({$\epsilon$}\textsuperscript{\{\vphantom\}}-O(1)\vphantom\{\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}\vphantom\{\}\{2\vphantom\}\textsuperscript{\{\vphantom\}}{\textohm}\vphantom\{\}({\textsurd}\{n\})\vphantom\{\}) up-date time. This improves over the previous fastest runtimes of O(n/(\textsuperscript{\{\vphantom\}}*\vphantom\{\}n)\textsuperscript{\{\vphantom\}}{\textohm}(1)\vphantom\{\}) due to Assadi-Behnezhad-Khanna-Li [STOC 2023], and O\textsubscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}(n\textsuperscript{\{\vphantom\}}1-{\textohm}\textsubscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}(1)\vphantom\{\}) due to Bhattacharya-Kiss-Saranurak [FOCS 2023] for small {$\epsilon$}. Our algorithm leverages fast algorithms for OMv due to Larsen and Williams [SODA 2017]. We give a randomized offline algorithm for (1 - {$\epsilon$})-approximate maximum matching with amortized runtime O(n\textsuperscript{\{\vphantom\}}.58\vphantom\{\}{$\epsilon$}\textsuperscript{\{\vphantom\}}-O(1)\vphantom\{\}) by using fast matrix multi-plication, significantly improving over the runtimes achieved via online algorithms mentioned above. This mirrors the situation with OMv, where an offline algorithm exactly corresponds to fast matrix mul-tiplication. We also give an offline algorithm that maintains a (1+{$\epsilon$})-approximate vertex cover in amortized O(n\textsuperscript{\{\vphantom\}}.723\vphantom\{\}{$\epsilon$}\textsuperscript{\{\vphantom\}}-O(1)\vphantom\{\}) time.},
  citationcount = {6},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,reduction,update,update time}
}

@article{liuOnTheFine2023,
  title = {On the Fine-Grained Complexity of Small-Size Geometric Set Cover and Discrete k-{{Center}} for Small k},
  author = {Liu, Shu and Xing, C. and Yuan, Chen},
  year = {2023},
  doi = {10.4230/LIPIcs.ICALP.2023.89},
  abstract = {We study the time complexity of the discrete k-center problem and related (exact) geometric set cover problems when k or the size of the cover is small. We obtain a plethora of new results: - We give the first subquadratic algorithm for rectilinear discrete 3-center in 2D, running in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}) time. - We prove a lower bound of {\textohm}(n\textsuperscript{\{\vphantom\}}4/3-{$\delta$}\vphantom\{\}) for rectilinear discrete 3-center in 4D, for any constant {$\delta>$}0, under a standard hypothesis about triangle detection in sparse graphs. - Given n points and n weighted axis-aligned unit squares in 2D, we give the first subquadratic algorithm for finding a minimum-weight cover of the points by 3 unit squares, running in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}8/5\vphantom\{\}) time. We also prove a lower bound of {\textohm}(n\textsuperscript{\{\vphantom\}}3/2-{$\delta$}\vphantom\{\}) for the same problem in 2D, under the well-known APSP Hypothesis. For arbitrary axis-aligned rectangles in 2D, our upper bound is O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}7/4\vphantom\{\}). - We prove a lower bound of {\textohm}(n\textsuperscript{\{\vphantom\}}2-{$\delta$}\vphantom\{\}) for Euclidean discrete 2-center in 13D, under the Hyperclique Hypothesis. This lower bound nearly matches the straightforward upper bound of O\vphantom\{\}(n\textsuperscript{{$\omega$}}), if the matrix multiplication exponent {$\omega$} is equal to 2. - We similarly prove an {\textohm}(n\textsuperscript{\{\vphantom\}}k-{$\delta$}\vphantom\{\}) lower bound for Euclidean discrete k-center in O(k) dimensions for any constant k{$\geq$}3, under the Hyperclique Hypothesis. This lower bound again nearly matches known upper bounds if {$\omega$}=2. - We also prove an {\textohm}(n\textsuperscript{\{\vphantom\}}2-{$\delta$}\vphantom\{\}) lower bound for the problem of finding 2 boxes to cover the largest number of points, given n points and n boxes in 12D. This matches the straightforward near-quadratic upper bound.},
  citationcount = {1},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {lower bound}
}

@article{liuRandomizedApproximateNearest2016,
  title = {Randomized Approximate Nearest Neighbor Search with Limited Adaptivity},
  author = {Liu, Mingmou and Pan, Xiaoyin and Yin, Yitong},
  year = {2016},
  journal = {ACM Symposium on Parallelism in Algorithms and Architectures},
  doi = {10.1145/2935764.2935776},
  abstract = {We study the problem of approximate nearest neighbor search in d-dimensional Hamming space \{0,1\}d. We study the complexity of the problem in the famous cell-probe model, a classic model for data structures. We consider algorithms in the cell-probe model with limited adaptivity, where the algorithm makes k rounds of parallel accesses to the data structure for a given k. For any k {$\geq$} 1, we give a simple randomized algorithm solving the approximate nearest neighbor search using k rounds of parallel memory accesses, with O(k(log d)1/k) accesses in total. We also give a more sophisticated randomized algorithm using O(k+(1/k log d)O(1/k)) memory accesses in k rounds for large enough k. Both algorithms use data structures of size polynomial in n, the number of points in the database. We prove an {\textohm}(1/k(log d)1/k) lower bound for the total number of memory accesses required by any randomized algorithm solving the approximate nearest neighbor search within k {$\leq$} (log log d)/(2 log log log d) rounds of parallel memory accesses on any data structures of polynomial size. This lower bound shows that our first algorithm is asymptotically optimal for any constant round k. And our second algorithm approaches the asymptotically optimal tradeoff between rounds and memory accesses, in a sense that the lower bound of memory accesses for any k1 rounds can be matched by the algorithm within k2=O(k1) rounds. In the extreme, for some large enough k={$\Theta$}((log log d)/(log log log d)), our second algorithm matches the {$\Theta$}((log log d)/(log log log d)) tight bound for fully adaptive algorithms for approximate nearest neighbor search due to Chakrabarti and Regev.},
  citationcount = {8},
  keywords = {adaptive,cell probe,data structure,lower bound}
}

@article{liuRandomizedApproximateNearest2018,
  title = {Randomized Approximate Nearest Neighbor Search with Limited Adaptivity},
  author = {Liu, Mingmou and Pan, Xiaoyin and Yin, Yitong},
  year = {2018},
  doi = {10.1145/3209884},
  abstract = {We study the complexity of parallel data structures for approximate nearest neighbor search in d-dimensional Hamming space \{0,1\}d. A classic model for static data structures is the cell-probe model [27]. We consider a cell-probe model with limited adaptivity, where given a k{$\geq$}1, a query is resolved by making at most k rounds of parallel memory accesses to the data structure. We give two randomized algorithms that solve the approximate nearest neighbor search using k rounds of parallel memory accesses: ---a simple algorithm with O(k(logd)1/k) total number of memory accesses for all k{$\geq$}1; ---an algorithm with O(k+(1/klogd)O(1/k)) total number of memory accesses for all sufficiently large k. Both algorithms use data structures of polynomial size. We prove an {\textohm}(1/k(logd)1/k) lower bound for the total number of memory accesses for any randomized algorithm solving the approximate nearest neighbor search within k {$\leq$}log logd/2log log log d rounds of parallel memory accesses on any data structures of polynomial size. This lower bound shows that our first algorithm is asymptotically optimal when k=O(1). And our second algorithm achieves the asymptotically optimal tradeoff between number of rounds and total number of memory accesses. In the extremal case, when k=O(log logd/log log log d) is big enough, our second algorithm matches the {$\Theta$}(log logd/log log log d) tight bound for fully adaptive algorithms for approximate nearest neighbor search in [11].},
  citationcount = {2},
  venue = {ACM Transactions on Parallel Computing},
  keywords = {adaptive,cell probe,data structure,lower bound,query,static}
}

@article{liuRelationAwareDistribution2023,
  title = {Relation-Aware Distribution Representation Network for Person Clustering with Multiple Modalities},
  author = {Liu, Kaijian and Tang, Shixiang and Li, Ziyue and Li, Zhishuai and Bai, Lei and Zhu, Feng and Zhao, Rui},
  year = {2023},
  doi = {10.1109/TMM.2023.3304454},
  abstract = {Person clustering with multi-modal clues, including faces, bodies, and voices, is critical for various tasks, such as movie parsing and identity-based movie editing. Related methods such as multi-view clustering mainly project multi-modal features into a joint feature space. However, multi-modal clue features are usually rather weakly correlated due to the semantic gap from the modality-specific uniqueness. As a result, these methods are not suitable for person clustering. In this article, we propose a Relation-Aware Distribution representation Network (RAD-Net) to generate a distribution representation for multi-modal clues. The distribution representation of a clue is a vector consisting of the relation between this clue and all other clues from all modalities, thus being modality agnostic and good for person clustering. Accordingly, we introduce a graph-based method to construct distribution representation and employ a cyclic update policy to refine distribution representation progressively. Our method achieves substantial improvements of +6},
  citationcount = {3},
  venue = {IEEE transactions on multimedia},
  keywords = {update}
}

@article{liuStrongLowerBound2004,
  title = {A Strong Lower Bound for Approximate Nearest Neighbor Searching},
  author = {Liu, Ding},
  year = {2004},
  month = oct,
  journal = {Information Processing Letters},
  volume = {92},
  number = {1},
  pages = {23--29},
  issn = {0020-0190},
  doi = {10.1016/j.ipl.2004.06.001},
  url = {https://www.sciencedirect.com/science/article/pii/S002001900400170X},
  urldate = {2024-11-20},
  abstract = {We prove a lower bound of d1-o(1) on the query time for any deterministic algorithms that solve approximate nearest neighbor searching in Yao's cell probe model. Our result greatly improves the best previous lower bound for this problem, which is {\textohm}(loglogdlogloglogd) [A. Chakrabarti et al., in: Proc. 31st Ann. ACM Symp. Theory of Computing, 1999, pp. 305--311]. Our proof is also much simpler than the proof of A. Chakrabarti et al.},
  keywords = {Approximate nearest neighbor searching,cell probe,Computational geometry,lower bound,query,query time},
  annotation = {A. Chakrabarti, Limitations of non-uniform computational models, Ph.D. Thesis, Computer Science Department, Princeton University, 2002},
  file = {/Users/tulasi/Zotero/storage/P3FLGRFD/Liu - 2004 - A strong lower bound for approximate nearest neighbor searching.pdf}
}

@article{liuVotingAlgorithmsFor2008,
  title = {Voting Algorithms for the Motif Finding Problem.},
  author = {Liu, Xiaowen and Ma, Bin and Wang, Lusheng},
  year = {2008},
  doi = {10.1142/9781848162648_0004},
  abstract = {UNLABELLED Finding motifs in many sequences is an important problem in computational biology, especially in identification of regulatory motifs in DNA sequences. Let c be a motif sequence. Given a set of sequences, each is planted with a mutated version of c at an unknown position, the motif finding problem is to find these planted motifs and the original c. In this paper, we study the VM model of the planted motif problem, which is proposed by Pevzner and Sze. We give a simple Selecting One Voting algorithm and a more powerful Selecting k Voting algorithm. When the length of motif and the number of input sequences are large enough, we prove that the two algorithms can find the unknown motif consensus with high probability. In the proof, we show why a large number of input sequences is so important for finding motifs, which is believed by most researchers. Experimental results on simulated data also support the claim. Selecting k Voting algorithm is powerful, but computational intensive. To speed up the algorithm, we propose a progressive filtering algorithm, which improves the running time significantly and has good accuracy in finding motifs. Our experimental results show that Selecting k Voting algorithm with progressive filtering performs very well in practice and it outperforms some best known algorithms. AVAILABILITY The software is available upon request.},
  citationcount = {Unknown},
  venue = {Computational systems bioinformatics. Computational Systems Bioinformatics Conference}
}

@article{lodatoOnPartialInformation2020,
  title = {On Partial Information Retrieval: The Unconstrained 100 Prisoner Problem},
  author = {Lodato, I. and Shekatkar, S. and Wong, T.},
  year = {2020},
  doi = {10.1007/s00236-022-00436-y},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Acta Informatica}
}

@article{loffLiftingTheoremsFor2018,
  title = {Lifting Theorems for Equality},
  author = {Loff, B. and Mukhopadhyay, Sagnik},
  year = {2018},
  doi = {10.4230/LIPIcs.STACS.2019.50},
  abstract = {We show a deterministic simulation (or lifting) theorem for composed problems f {\textopenbullet}Eqn where the9 inner function (the gadget) is Equality on n bits. When f is a total function on p bits, it is easy t ...},
  citationcount = {19},
  venue = {Electron. Colloquium Comput. Complex.}
}

@article{longBetterDecrementalAnd2024,
  title = {Better Decremental and Fully Dynamic Sensitivity Oracles for Subgraph Connectivity},
  author = {Long, Yaowei and Wang, Yunfan},
  year = {2024},
  doi = {10.48550/arXiv.2402.09150},
  abstract = {We study the \{sensitivity oracles problem for subgraph connectivity\} in the \{decremental\} and \{fully dynamic\} settings. In the fully dynamic setting, we preprocess an n-vertices m-edges undirected graph G with n\textsubscript{\{\vphantom\}}off\vphantom\{\} deactivated vertices initially and the others are activated. Then we receive a single update D{$\subseteq$}V(G) of size {\textbar}D{\textbar}=d{$\leq$}d\textsubscript{\{\vphantom\}}{$\star$}\vphantom\{\}, representing vertices whose states will be switched. Finally, we get a sequence of queries, each of which asks the connectivity of two given vertices u and v in the activated subgraph. The decremental setting is a special case when there is no deactivated vertex initially, and it is also known as the \{vertex-failure connectivity oracles\} problem. We present a better deterministic vertex-failure connectivity oracle with O\vphantom\{\}(d\textsubscript{\{\vphantom\}}{$\star$}\vphantom\{\}m) preprocessing time, O\vphantom\{\}(m) space, O\vphantom\{\}(d\textsuperscript{\{\vphantom\}}2\vphantom\{\}) update time and O(d) query time, which improves the update time of the previous almost-optimal oracle [Long-Saranurak, FOCS 2022] from O\vphantom\{\}(d\textsuperscript{\{\vphantom\}}2\vphantom\{\}) to O\vphantom\{\}(d\textsuperscript{\{\vphantom\}}2\vphantom\{\}). We also present a better deterministic fully dynamic sensitivity oracle for subgraph connectivity with O\vphantom\{\}( m(n\_\{off\}+d\_\{{$\star$}\}),n{\textasciicircum}\{{$\omega$}\} ) preprocessing time, O\vphantom\{\}( m(n\_\{off\}+d\_\{{$\star$}\}),n{\textasciicircum}\{2\} ) space, O\vphantom\{\}(d\textsuperscript{\{\vphantom\}}2\vphantom\{\}) update time and O(d) query time, which significantly improves the update time of the state of the art [Hu-Kosinas-Polak, 2023] from O\vphantom\{\}(d\textsuperscript{\{\vphantom\}}4\vphantom\{\}) to O\vphantom\{\}(d\textsuperscript{\{\vphantom\}}2\vphantom\{\}). Furthermore, our solution is even almost-optimal assuming popular fine-grained complexity conjectures.},
  citationcount = {1},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {dynamic,query,query time,update,update time}
}

@article{longNearOptimalDeterministic2022,
  title = {Near-Optimal Deterministic Vertex-Failure Connectivity Oracles},
  author = {Long, Yaowei and Saranurak, Thatchaphol},
  year = {2022},
  doi = {10.1109/FOCS54457.2022.00098},
  abstract = {We revisit the vertex-failure connectivity oracle problem. This is one of the most basic graph data structure problems under vertex updates, yet its complexity is still not well-understood. We essentially settle the complexity of this problem by showing a new data structure whose space, preprocessing time, update time, and query time are simultaneously optimal up to sub-polynomial factors assuming popular conjectures. Moreover, the data structure is deterministic.More precisely, for any integer d\textsubscript{\{\vphantom\}}{$\star$}\vphantom\{\}, the data structure preprocesses a graph G with n vertices and m edges in O\vphantom\{\}(md\textsubscript{\{\vphantom\}}{$\star$}\vphantom\{\}) time and uses O\vphantom\{\}( m,nd\_\{{$\star$}\} ) space. Then, given the vertex set D to be deleted where {\textbar}D{\textbar}=d{$\leq$}d\textsubscript{\{\vphantom\}}{$\star$}\vphantom\{\}, it takes O\vphantom\{\}(d\textsuperscript{\{\vphantom\}}2\vphantom\{\}) updates time. Finally, given any vertex pair (u,v), it checks if u and v are connected in G{\textbackslash}D in O(d) time. This improves the previously best deterministic algorithm by Duan and Pettie [SICOMP 2020] in both space and update time by a factor of d. It also significantly speeds up the {\textohm}( mn,n{\textasciicircum}\{{$\omega$}\} ) preprocessing time of all known (even randomized) algorithms with update time at most O\vphantom\{\}(d\textsuperscript{\{\vphantom\}}5\vphantom\{\}).},
  citationcount = {14},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,query,query time,update,update time}
}

@article{longPlanarDistanceOracles2020,
  title = {Planar Distance Oracles with Better Time-Space Tradeoffs},
  author = {Long, Yaowei and Pettie, Seth},
  year = {2020},
  doi = {10.1137/1.9781611976465.149},
  abstract = {In a recent breakthrough, Charalampopoulos, Gawrychowski, Mozes, and Weimann (STOC 2019) showed that exact distance queries on planar graphs could be answered in n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} time by a data structure occupying n\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\} space, i.e., up to o(1) terms, optimal exponents in time (0) and space (1) can be achieved simultaneously. Their distance query algorithm is recursive: it makes successive calls to a point-location algorithm for planar Voronoi diagrams, which involves many recursive distance queries. The depth of this recursion is non-constant and the branching factor logarithmic, leading to (n)\textsuperscript{\{\vphantom\}}{$\omega$}(1)\vphantom\{\}=n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} query times. In this paper we present a new way to do point-location in planar Voronoi diagrams, which leads to a new exact distance oracle. At the two extremes of our space-time tradeoff curve we can achieve either n\textsuperscript{\{\vphantom\}}1+o(1)\vphantom\{\} space and \textsuperscript{\{\vphantom\}}2+o(1)\vphantom\{\}n query time, or n\textsuperscript{\{\vphantom\}}2+o(1)\vphantom\{\}n space and n\textsuperscript{\{\vphantom\}}o(1)\vphantom\{\} query time. All previous oracles with O\vphantom\{\}(1) query time occupy space n\textsuperscript{\{\vphantom\}}1+{\textohm}(1)\vphantom\{\}, and all previous oracles with space O\vphantom\{\}(n) answer queries in n\textsuperscript{\{\vphantom\}}{\textohm}(1)\vphantom\{\} time.},
  citationcount = {25},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,query,query time,time-space}
}

@article{lovettBoundeddepthCircuitsCannot2011,
  title = {Bounded-Depth Circuits Cannot Sample Good Codes},
  author = {Lovett, Shachar and Viola, Emanuele},
  year = {2011},
  doi = {10.1007/s00037-012-0039-3},
  abstract = {No abstract available},
  citationcount = {46},
  venue = {2011 IEEE 26th Annual Conference on Computational Complexity}
}

@article{lovettStreamingLowerBounds2023,
  title = {Streaming Lower Bounds and Asymmetric Set-Disjointness},
  author = {Lovett, Shachar and Zhang, Jiapeng},
  year = {2023},
  doi = {10.1109/FOCS57990.2023.00056},
  abstract = {Frequency estimation in data streams is one of the classical problems in streaming algorithms. Following much research, there are now almost matching upper and lower bounds for the trade-off needed between the number of samples and the space complexity of the algorithm, when the data streams are adversarial. However, in the case where the data stream is given in a random order, or is stochastic, only weaker lower bounds exist. In this work we close this gap, up to logarithmic factors. In order to do so we consider the needle problem, which is a natural hard problem for frequency estimation studied in (Andoni et al. 2008, Crouch et al. 2016). Here, the goal is to distinguish between two distributions over data streams with t samples. The first is uniform over a large enough domain. The second is a planted model; a secret ``needle`` is uniformly chosen, and then each element in the stream equals the needle with probability p, and otherwise is uniformly chosen from the domain. It is simple to design streaming algorithms that distinguish the distributions using space s{$\approx$}1/(p\textsuperscript{\{\vphantom\}}2\vphantom\{\}t). It was unclear if this is tight, as the existing lower bounds are weaker. We close this gap and show that the trade-off is near optimal, up to a logarithmic factor. Our proof builds and extends classical connections between streaming algorithms and communication complexity, concretely multi-party unique set-disjointness. We introduce two new ingredients that allow us to prove sharp bounds. The first is a lower bound for an asymmetric version of multi-party unique set-disjointness, where players receive input sets of different sizes, and where the communication of each player is normalized relative to their input length. The second is a combinatorial technique that allows to sample needles in the planted model by first sampling intervals, and then sampling a uniform needle in each interval.},
  citationcount = {4},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {communication,communication complexity,lower bound}
}

@article{luefanSuperSparse3D2023,
  title = {Super {{Sparse 3D Object Detection}}},
  author = {Lue Fan and Y. Yang and Feng Wang and {Nai-long Wang} and Zhaoxiang Zhang},
  year = {2023},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/TPAMI.2023.3286409},
  abstract = {As the perception range of LiDAR expands, LiDAR-based 3D object detection contributes ever-increasingly to the long-range perception in autonomous driving. Mainstream 3D object detectors often build dense feature maps, where the cost is quadratic to the perception range, making them hardly scale up to the long-range settings. To enable efficient long-range detection, we first propose a fully sparse object detector termed FSD. FSD is built upon the general sparse voxel encoder and a novel sparse instance recognition (SIR) module. SIR groups the points into instances and applies highly-efficient instance-wise feature extraction. The instance-wise grouping sidesteps the issue of the center feature missing, which hinders the design of the fully sparse architecture. To further enjoy the benefit of fully sparse characteristic, we leverage temporal information to remove data redundancy and propose a super sparse detector named FSD++. FSD++ first generates residual points, which indicate the point changes between consecutive frames. The residual points, along with a few previous foreground points, form the super sparse input data, greatly reducing data redundancy and computational overhead. We comprehensively analyze our method on the large-scale Waymo Open Dataset, and state-of-the-art performance is reported. To showcase the superiority of our method in long-range detection, we also conduct experiments on Argoverse 2 Dataset, where the perception range ({$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$\backslash$}text\{200\}\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mtext{$>$}200{$<$}/mml:mtext{$><$}/mml:math{$><$}inline-graphic xlink:href="fan-ieq1-3286409.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$} m) is much larger than Waymo Open Dataset ({$<$}inline-formula{$><$}tex-math notation="LaTeX"{$>\$\backslash$}text\{75\}\${$<$}/tex-math{$><$}alternatives{$><$}mml:math{$><$}mml:mtext{$>$}75{$<$}/mml:mtext{$><$}/mml:math{$><$}inline-graphic xlink:href="fan-ieq2-3286409.gif"/{$><$}/alternatives{$><$}/inline-formula{$>$} m).},
  keywords = {information},
  annotation = {Citation Count: 28}
}

@article{luIndexingForKeyword2023,
  title = {Indexing for Keyword Search with Structured Constraints},
  author = {Lu, Shangqi and Tao, Yufei},
  year = {2023},
  doi = {10.1145/3584372.3588663},
  abstract = {Keyword search, which finds the documents containing all the keywords supplied by a user, has proved to be an effective approach for querying non-structured information that does not conform to any pre-set schemas. In the last two decades, a vast amount of research --- especially in the field of "spatial keyword search'' --- has been carried out to design indexes for answering queries that integrate keyword search with structured predicates imposed on pre-determined attributes (common examples of such predicates are range conditions, linear constraints, prioritization by distance, etc.). Although the past investigation has led to a plethora of empirical solutions, little progress has been made in theory. In fact, for most problems in the literature, the state of the art is still the naive method that answers a query purely based on the keyword conditions or the structured predicates. In this paper, we remedy the issue by developing new indexes with strong theoretical guarantees for a suite of problems with heavy importance in real applications. Many of our indexes are near-optimal, subject to widely-accepted conjectures.},
  citationcount = {Unknown},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {query}
}

@article{luRangeUpdatesAnd2023,
  title = {Range Updates and Range Sum Queries on Multidimensional Points with Monoid Weights},
  author = {Lu, Shangqi and Tao, Yufei},
  year = {2023},
  doi = {10.4230/LIPIcs.ISAAC.2022.57},
  abstract = {Let P be a set of n points in R d where each point p {$\in$} P carries a weight drawn from a commutative monoid ( M , + , 0). Given a d -rectangle r upd (i.e., an orthogonal rectangle in R d ) and a value  {$\in$} M , a range update adds  to the weight of every point p {$\in$} P {$\cap$} r upd ; given a d -rectangle r qry , a range sum query returns the total weight of the points in P {$\cap$} r qry . The goal is to store P in a structure to support updates and queries with attractive performance guarantees. We describe a structure of {\texttildelow} O ( n ) space that handles an update in {\texttildelow} O ( T upd ) time and a query in {\texttildelow} O ( T qry ) time for arbitrary functions T upd ( n ) and T qry ( n ) satisfying T upd {$\cdot$} T qry = n . The result holds for any fixed dimensionality d {$\geq$} 2. Our query-update tradeoff is tight up to a polylog factor subject to the OMv-conjecture. 2012 ACM Subject Classification Theory of computation {$\rightarrow$} Data structures design and analysis},
  citationcount = {Unknown},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {data structure,query,update}
}

@article{luTowardsOptimalDynamic2021,
  title = {Towards Optimal Dynamic Indexes for Approximate (and Exact) Triangle Counting},
  author = {Lu, Shangqi and Tao, Yufei},
  year = {2021},
  doi = {10.4230/LIPIcs.ICDT.2021.6},
  abstract = {Abstract},
  citationcount = {3},
  venue = {International Conference on Database Theory},
  keywords = {dynamic}
}

@article{m.a.benderBatchedPredecessorProblem2014,
  title = {The {{Batched Predecessor Problem}} in {{External Memory}}},
  author = {M. A. Bender and {Mart{\'i}n Farach-Colton} and Mayank Goswami and Dzejla Medjedovic and Pablo Montes and M. Tsai},
  year = {2014},
  journal = {Embedded Systems and Applications},
  doi = {10.1007/978-3-662-44777-2_10},
  annotation = {Citation Count: 6}
}

@article{m.atallahOptimalAlgorithmShortest1993,
  title = {An Optimal Algorithm for Shortest Paths on Weighted Interval and Circular-Arc Graphs, with Applications},
  author = {M. Atallah and D. Chen and {Der-Tsai Lee}},
  year = {1993},
  journal = {Algorithmica},
  doi = {10.1007/BF01192049},
  annotation = {Citation Count: 30}
}

@article{m.blumenstockConstructiveArboricityApproximation2018,
  title = {A {{Constructive Arboricity Approximation Scheme}}},
  author = {M. Blumenstock and F. Fischer},
  year = {2018},
  journal = {Conference on Current Trends in Theory and Practice of Informatics},
  doi = {10.1007/978-3-030-38919-2_5},
  annotation = {Citation Count: 12}
}

@article{m.bravermanETHHardnessDensestkSubgraph2015,
  title = {{{ETH Hardness}} for {{Densest-k-Subgraph}} with {{Perfect Completeness}}},
  author = {M. Braverman and {Young Kun-Ko} and A. Rubinstein and Omri Weinstein},
  year = {2015},
  journal = {ACM-SIAM Symposium on Discrete Algorithms},
  doi = {10.1137/1.9781611974782.86},
  abstract = {We show that, assuming the (deterministic) Exponential Time Hypothesis, distinguishing between a graph with an induced \$k\$-clique and a graph in which all k-subgraphs have density at most \$1-{\textbackslash}epsilon\$, requires \$n{\textasciicircum}\{{\textbackslash}tilde {\textbackslash}Omega(log n)\}\$ time. Our result essentially matches the quasi-polynomial algorithms of Feige and Seltser [FS97] and Barman [Bar15] for this problem, and is the first one to rule out an additive PTAS for Densest \$k\$-Subgraph. We further strengthen this result by showing that our lower bound continues to hold when, in the soundness case, even subgraphs smaller by a near-polynomial factor (\$k' = k 2{\textasciicircum}\{-{\textbackslash}tilde {\textbackslash}Omega (log n)\}\$) are assumed to be at most (\$1-{\textbackslash}epsilon\$)-dense.  Our reduction is inspired by recent applications of the "birthday repetition" technique [AIM14,BKW15]. Our analysis relies on information theoretical machinery and is similar in spirit to analyzing a parallel repetition of two-prover games in which the provers may choose to answer some challenges multiple times, while completely ignoring other challenges.},
  keywords = {information,information theoretic,lower bound,reduction},
  annotation = {Citation Count: 53}
}

@article{m.fredmanCellProbeComplexity1989,
  title = {The Cell Probe Complexity of Dynamic Data Structures},
  author = {M. Fredman and M. Saks},
  year = {1989},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/73007.73040},
  abstract = {Dynamic data structure problems involve the representation of data in memory in such a way as to permit certain types of modifications of the data (updates) and certain types of questions about the data (queries). This paradigm encompasses many fundamental problems in computer science. The purpose of this paper is to prove new lower and upper bounds on the time per operation to implement solutions to some familiar dynamic data structure problems including list representation, subset ranking, partial sums, and the set union problem. The main features of our lower bounds are:{$<$}list{$><$}item{$>$}They hold in the {$<$}italic{$>$}cell probe{$<$}/italic{$>$} model of computation (A. Yao [18]) in which the time complexity of a sequential computation is defined to be the number of words of memory that are accessed. (The number of bits {$<$}italic{$>$}b{$<$}/italic{$>$} in a single word of memory is a parameter of the model). All other computations are free. This model is at least as powerful as a random access machine and allows for unusual representation of data, indirect addressing etc. This contrasts with most previous lower bounds which are proved in models (e.g., algebraic, comparison, pointer manipulation) which require restrictions on the way data is represented and manipulated. {$<$}/item{$><$}item{$>$}The lower bound method presented here can be used to derive amortized complexities, worst case per operation complexities, and randomized complexities. {$<$}/item{$><$}item{$>$}The results occasionally provide (nearly tight) tradeoffs between the number {$<$}italic{$>$}R{$<$}/italic{$>$} of words of memory that are read per operation, the number {$<$}italic{$>$}W{$<$}/italic{$>$} of memory words rewritten per operation and the size {$<$}italic{$>$}b{$<$}/italic{$>$} of each word. For the problems considered here there is a parameter {$<$}italic{$>$}n{$<$}/italic{$>$} that represents the size of the data set being manipulated and for these problems {$<$}italic{$>$}b{$<$}/italic{$>$} = log{$<$}italic{$>$}n{$<$}/italic{$>$} is a natural register size to consider. By letting {$<$}italic{$>$}b{$<$}/italic{$>$} vary, our results illustrate the effect of register size on time complexity. For instance, one consequence of the results is that for some of the problems considered here, increasing the register size from log{$<$}italic{$>$}n{$<$}/italic{$>$} to polylog({$<$}italic{$>$}n{$<$}/italic{$>$}) only reduces the time complexity by a constant factor. On the other hand, decreasing the register size from log{$<$}italic{$>$}n{$<$}/italic{$>$} to 1 increases time complexity by a log{$<$}italic{$>$}n{$<$}/italic{$>$} factor for one of the problems we consider and only a loglog{$<$}italic{$>$}n{$<$}/italic{$>$} factor for some other problems. {$<$}/item{$><$}/list{$>$} The first two specific data structure problems for which we obtain bounds are:{$<$}list{$><$}item{$>$}List Representation. This problem concerns the representation of an ordered list of at most {$<$}italic{$>$}n{$<$}/italic{$>$} (not necessarily distinct) elements from the universe {$<$}italic{$>$}U{$<$}/italic{$>$} = \{1, 2,{\dots}, {$<$}italic{$>$}n{$<$}/italic{$>$}\}. The operations to be supported are report({$<$}italic{$>$}k{$<$}/italic{$>$}), which returns the {$<$}italic{$>$}k{$<$}supscrpt{$>$}th{$<$}/supscrpt{$><$}/italic{$>$} element of the list, insert({$<$}italic{$>$}k{$<$}/italic{$>$}, {$<$}italic{$>$}u{$<$}/italic{$>$}) which inserts element {$<$}italic{$>$}u{$<$}/italic{$>$} into the list between the elements in positions {$<$}italic{$>$}k{$<$}/italic{$>$} - 1 and {$<$}italic{$>$}k{$<$}/italic{$>$}, delete({$<$}italic{$>$}k{$<$}/italic{$>$}), which deletes the {$<$}italic{$>$}k{$<$}supscrpt{$>$}th{$<$}/supscrpt{$><$}/italic{$>$} item. {$<$}/item{$><$}item{$>$}Subset Rank. This problem concerns the representation of a subset {$<$}italic{$>$}S{$<$}/italic{$>$} of {$<$}italic{$>$}U{$<$}/italic{$>$} = \{1, 2,{\dots}, {$<$}italic{$>$}n{$<$}/italic{$>$}\}. The operations that must be supported are the updates ``insert item {$<$}italic{$>$}j{$<$}/italic{$>$} into the set'' and ``delete item {$<$}italic{$>$}j{$<$}/italic{$>$} from the set'' and the queries rank({$<$}italic{$>$}j{$<$}/italic{$>$}), which returns the number of elements in {$<$}italic{$>$}S{$<$}/italic{$>$} that are less than or equal to {$<$}italic{$>$}j{$<$}/italic{$>$}. {$<$}/item{$><$}/list{$>$} The natural word size for these problems is {$<$}italic{$>$}b{$<$}/italic{$>$} = log{$<$}italic{$>$}n{$<$}/italic{$>$}, which allows an item of {$<$}italic{$>$}U{$<$}/italic{$>$} or an index into the list to be stored in one register. One simple solution to the list representation problem is to maintain a vector {$<$}italic{$>$}v{$<$}/italic{$>$}, whose {$<$}italic{$>$}k{$<$}supscrpt{$>$}th{$<$}/supscrpt{$><$}/italic{$>$} entry contains the {$<$}italic{$>$}k{$<$}supscrpt{$>$}th{$<$}/supscrpt{$><$}/italic{$>$} item of the list. The report operation can be done in constant time, but the insert and delete operations may take time linear in the length of the list. Alternatively, one could store the items of the list with each element having a pointer to its predecessor and successor in the list. This allows for constant time updates (given a pointer to the appropriate location), but requires linear cost for queries. This problem can be solved must more efficiently by use of balanced trees (such as AVL trees). When {$<$}italic{$>$}b{$<$}/italic{$>$} = log{$<$}italic{$>$}n{$<$}/italic{$>$}, the worst case cost per operation using AVL trees is {$<$}italic{$>$}O{$<$}/italic{$>$}(log{$<$}italic{$>$}n{$<$}/italic{$>$}). If instead {$<$}italic{$>$}b{$<$}/italic{$>$} = 1, so that each bit access costs 1, then the AVL three solution requires {$<$}italic{$>$}O{$<$}/italic{$>$}(log{$<$}supscrpt{$>$}2{$<$}/supscrpt{$><$}italic{$>$}n{$<$}/italic{$>$}) per operation. It is not hard to find similar upper bounds for the subset rank problem (the algorithms for this problem are actually simpler than AVL trees). The question is: are these upper bounds bet possible? Our results show that the upper bounds for the case of log{$<$}italic{$>$}n{$<$}/italic{$>$} bit registers are within a loglog{$<$}italic{$>$}n{$<$}/italic{$>$} factor of optimal. On the other hand, somewhat surprisingly, for the case of single bit registers there are implementations for both of these problems that run in time significantly faster than {$<$}italic{$>$}O{$<$}/italic{$>$}(log{$<$}supscrpt{$>$}2{$<$}/supscrpt{$><$}italic{$>$}n{$<$}/italic{$>$}) per operation. Let CPROBE({$<$}italic{$>$}b{$<$}/italic{$>$}) denote the cell probe computational model with register size {$<$}italic{$>$}b{$<$}/italic{$>$}. Theorem 1. If {$<$}italic{$>$}b{$<$}/italic{$>$} {$\leq$} (log{$<$}italic{$>$}n{$<$}/italic{$>$}){$<$}supscrpt{$><$}italic{$>$}t{$<$}/italic{$><$}/supscrpt{$>$} for some {$<$}italic{$>$}t{$<$}/italic{$>$}, then any CPROBE({$<$}italic{$>$}b{$<$}/italic{$>$}) implementation of either list representation or the subset rank requires \&OHgr;(log{$<$}italic{$>$}n{$<$}/italic{$>$}/loglog{$<$}italic{$>$}n{$<$}/italic{$>$}) amortized time per operation. Theorem 2. Subset rank and list representation have CPROBE(1) implementations with respective complexities {$<$}italic{$>$}O{$<$}/italic{$>$}((log{$<$}italic{$>$}n{$<$}/italic{$>$})(loglog{$<$}italic{$>$}n{$<$}/italic{$>$})) and {$<$}italic{$>$}O{$<$}/italic{$>$}((log{$<$}italic{$>$}n{$<$}/italic{$>$})(loglog{$<$}italic{$>$}n{$<$}/italic{$>$}){$<$}supscrpt{$>$}2{$<$}/supscrpt{$>$}) per operation. Paul Dietz (personal communication) has found an implementation of list representation with log{$<$}italic{$>$}n{$<$}/italic{$>$} bit registers that requires only {$<$}italic{$>$}O{$<$}/italic{$>$}(log{$<$}italic{$>$}n{$<$}/italic{$>$}/loglog{$<$}italic{$>$}n{$<$}/italic{$>$}) time per operation, and thus the result of theorem 1 is best possible. The lower bounds of theorem 1 are derived from lower bounds for a third problem:{$<$}list{$><$}item{$>$}Partial sum mode k. An array {$<$}italic{$>$}A{$<$}/italic{$>$}[1],{\dots}, {$<$}italic{$>$}A{$<$}/italic{$>$}[{$<$}italic{$>$}N{$<$}/italic{$>$}] of integers mod {$<$}italic{$>$}k{$<$}/italic{$>$} is to be represented. Updates are add({$<$}italic{$>$}i{$<$}/italic{$>$}, {$\delta$}) which implements {$<$}italic{$>$}A{$<$}/italic{$>$}[{$<$}italic{$>$}i{$<$}/italic{$>$}] {\textleftarrow} {$<$}italic{$>$}A{$<$}/italic{$>$}[{$<$}italic{$>$}i{$<$}/italic{$>$}] + {$\delta$}; and queries are sum(j) which returns \&Sgr;{$<$}subscrpt{$><$}italic{$>$}i{$<$}/italic{$>\leq<$}italic{$>$}j{$<$}/italic{$><$}/subscrpt{$><$}italic{$>$}A{$<$}/italic{$>$}[{$<$}italic{$>$}i{$<$}/italic{$>$}] (mod {$<$}italic{$>$}k{$<$}/italic{$>$}). {$<$}/item{$><$}/list{$>$} This problem is demoted PS(n, k). Our main lower bound theorems provide tradeoffs between the number of register rewrites and register reads as a function of {$<$}italic{$>$}n{$<$}/italic{$>$}, {$<$}italic{$>$}k{$<$}/italic{$>$}, and {$<$}italic{$>$}b{$<$}/italic{$>$}. Two corollaries of these results are:{$<$}list{$><$}item{$>$}Theorem 3. Any CPROBE({$<$}italic{$>$}b{$<$}/italic{$>$}) implementation of PS(n, 2) (partial sums mod 2) requires \&OHgr;(log{$<$}italic{$>$}n{$<$}/italic{$>$}/(loglog{$<$}italic{$>$}n{$<$}/italic{$>$} + log{$<$}italic{$>$}b{$<$}/italic{$>$})) amortized time per operation, and for {$<$}italic{$>$}b{$<$}/italic{$>$} {$\geq$} log{$<$}italic{$>$}n{$<$}/italic{$>$}, there is an implementation that achieves this. In particular, if {$<$}italic{$>$}b{$<$}/italic{$>$} = \&THgr;((log{$<$}italic{$>$}n{$<$}/italic{$>$}){$<$}supscrpt{$>$}c{$<$}/supscrpt{$>$}) for some constant {$<$}italic{$>$}c{$<$}/italic{$>$}, then the optimal time complexity of PS(n, 2) is \&thgr;(log{$<$}italic{$>$}n{$<$}/italic{$>$}/loglog{$<$}italic{$>$}n{$<$}/italic{$>$}). {$<$}/item{$><$}item{$>$}Theorem 4. Any CPROBE(1) implementation of PS(n, n) with single bit registers requires \&OHgr;((log{$<$}italic{$>$}n{$<$}/italic{$>$}/loglog{$<$}italic{$>$}n{$<$}/italic{$>$}){$<$}supscrpt{$>$}2{$<$}/supscrpt{$>$}) amortized time per operation, and there is an implementation that achieves {$<$}italic{$>$}O{$<$}/italic{$>$}(log{$<$}supscrpt{$>$}2{$<$}/supscrpt{$><$}italic{$>$}n{$<$}/italic{$>$}) time per operation. {$<$}/item{$><$}/list{$>$} It can be shown that a lower bound of PS(n, 2) is also a lower bound for both list representation and subset rank (the details, which are not difficult, are omitted from this report), and thus theorem 1 follows from theorem 3. The results of theorem 4 make an interesting contrast with those of theorem 2. For the three problems, list representation, subset rank and PS(n, k), there are standard algorithms that can be implemented on a CPROBE(log{$<$}italic{$>$}n{$<$}/italic{$>$}) that use time {$<$}italic{$>$}O{$<$}/italic{$>$}(log{$<$}italic{$>$}n{$<$}/italic{$>$} per operation, and their implementations on CPROBE(1) require {$<$}italic{$>$}O{$<$}/italic{$>$}(log{$<$}supscrpt{$>$}2{$<$}/supscrpt{$><$}italic{$>$}n{$<$}/italic{$>$}) time. Theorem 4 says that for the problem PS(n, n) this algorithm is essentially best possible, while theorem 2 says that for list representation and rank, the algorithm can be significantly improved. In fact, the rank problem an be viewed as a special case of PS(n, n) where the variables take on values on \{0, 1\}, and apparently this specialization is enough to reduce the complexity on a CPROBE(1) by a factor of log{$<$}italic{$>$}n{$<$}/italic{$>$}/loglog{$<$}italic{$>$}n{$<$}/italic{$>$}, even though on a CPROBE(log{$<$}italic{$>$}n{$<$}/italic{$>$}) the complexities of the two problems differ by no more than a loglog{$<$}italic{$>$}n{$<$}/italic{$>$} factor. The third problem we consider is the set union problem. This problem concerns the design of a data structure for the on-line manipulation of sets in the following setting. Initially, there are {$<$}italic{$>$}n{$<$}/italic{$>$} singleton sets \{1\}, \{2\},{\dots}, \{{$<$}italic{$>$}n{$<$}/italic{$>$}\} with {$<$}italic{$>$}i{$<$}/italic{$>$} chosen as the name of the set \{{$<$}italic{$>$}i{$<$}/italic{$>$}\}. Our data structure is required to implement two operations, Find({$<$}italic{$>$}j{$<$}/italic{$>$}), and Union({$<$}italic{$>$}A},
  keywords = {cell probe,communication,data structure,dynamic,lower bound,query,update},
  annotation = {Citation Count: 386}
}

@article{m.fredmanFibonacciHeapsTheir1984,
  title = {Fibonacci Heaps and Their Uses in Improved Network Optimization Algorithms},
  author = {M. Fredman and R. Tarjan},
  year = {1984},
  journal = {JACM},
  doi = {10.1145/28869.28874},
  abstract = {In this paper we develop a new data structure for implementing heaps (priority queues). Our structure, {$<$}italic{$>$}Fibonacci heaps{$<$}/italic{$>$} (abbreviated {$<$}italic{$>$}F-heaps{$<$}/italic{$>$}), extends the binomial queues proposed by Vuillemin and studied further by Brown. F-heaps support arbitrary deletion from an {$<$}italic{$>$}n{$<$}/italic{$>$}-item heap in {$<$}italic{$>$}O{$<$}/italic{$>$}(log {$<$}italic{$>$}n{$<$}/italic{$>$}) amortized time and all other standard heap operations in {$<$}italic{$>$}O{$<$}/italic{$>$}(1) amortized time. Using F-heaps we are able to obtain improved running times for several network optimization algorithms. In particular, we obtain the following worst-case bounds, where {$<$}italic{$>$}n{$<$}/italic{$>$} is the number of vertices and {$<$}italic{$>$}m{$<$}/italic{$>$} the number of edges in the problem graph:{$<$}list{$><$}item{$><$}italic{$>$}O{$<$}/italic{$>$}({$<$}italic{$>$}n{$<$}/italic{$>$} log {$<$}italic{$>$}n{$<$}/italic{$>$} + {$<$}italic{$>$}m{$<$}/italic{$>$}) for the single-source shortest path problem with nonnegative edge lengths, improved from {$<$}italic{$>$}O{$<$}/italic{$>$}({$<$}italic{$>$}m{$<$}/italic{$>$}log{$<$}subscrpt{$>$}({$<$}italic{$>$}m/n{$<$}/italic{$>$}+2){$<$}/subscrpt{$><$}italic{$>$}n{$<$}/italic{$>$}); {$<$}/item{$><$}item{$><$}italic{$>$}O{$<$}/italic{$>$}({$<$}italic{$>$}n{$<$}/italic{$><$}supscrpt{$>$}2{$<$}/supscrpt{$>$}log {$<$}italic{$>$}n{$<$}/italic{$>$} + {$<$}italic{$>$}nm{$<$}/italic{$>$}) for the all-pairs shortest path problem, improved from {$<$}italic{$>$}O{$<$}/italic{$>$}({$<$}italic{$>$}nm{$<$}/italic{$>$} log{$<$}subscrpt{$>$}({$<$}italic{$>$}m/n{$<$}/italic{$>$}+2){$<$}/subscrpt{$><$}italic{$>$}n{$<$}/italic{$>$}); {$<$}/item{$><$}item{$><$}italic{$>$}O{$<$}/italic{$>$}({$<$}italic{$>$}n{$<$}/italic{$><$}supscrpt{$>$}2{$<$}/supscrpt{$>$}log {$<$}italic{$>$}n{$<$}/italic{$>$} + {$<$}italic{$>$}nm{$<$}/italic{$>$}) for the assignment problem (weighted bipartite matching), improved from {$<$}italic{$>$}O{$<$}/italic{$>$}({$<$}italic{$>$}nm{$<$}/italic{$>$}log{$<$}subscrpt{$>$}({$<$}italic{$>$}m/n{$<$}/italic{$>$}+2){$<$}/subscrpt{$><$}italic{$>$}n{$<$}/italic{$>$}); {$<$}/item{$><$}item{$><$}italic{$>$}O{$<$}/italic{$>$}({$<$}italic{$>$}m{$\beta<$}/italic{$>$}({$<$}italic{$>$}m, n{$<$}/italic{$>$})) for the minimum spanning tree problem, improved from {$<$}italic{$>$}O{$<$}/italic{$>$}({$<$}italic{$>$}m{$<$}/italic{$>$}log log{$<$}subscrpt{$>$}({$<$}italic{$>$}m/n{$<$}/italic{$>$}+2){$<$}/subscrpt{$><$}italic{$>$}n{$<$}/italic{$>$}); where {$<$}italic{$>\beta<$}/italic{$>$}({$<$}italic{$>$}m, n{$<$}/italic{$>$}) = min \{{$<$}italic{$>$}i{$<$}/italic{$>$}  log{$<$}supscrpt{$>$}({$<$}italic{$>$}i{$<$}/italic{$>$}){$<$}/supscrpt{$><$}italic{$>$}n{$<$}/italic{$>$} {$\leq$} {$<$}italic{$>$}m/n{$<$}/italic{$>$}\}. Note that {$<$}italic{$>\beta<$}/italic{$>$}({$<$}italic{$>$}m, n{$<$}/italic{$>$}) {$\leq$} log{$<$}supscrpt{$>$}*{$<$}/supscrpt{$><$}italic{$>$}n{$<$}/italic{$>$} if {$<$}italic{$>$}m{$<$}/italic{$>$} {$\geq$} {$<$}italic{$>$}n{$<$}/italic{$>$}. {$<$}/item{$><$}/list{$>$}Of these results, the improved bound for minimum spanning trees is the most striking, although all the results give asymptotic improvements for graphs of appropriate densities.},
  keywords = {data structure},
  annotation = {Citation Count: 3107}
}

@article{m.goodrichSortingParallelPointer1989,
  title = {Sorting on a Parallel Pointer Machine with Applications to Set Expression Evaluation},
  author = {M. Goodrich and S. Kosaraju},
  year = {1989},
  journal = {30th Annual Symposium on Foundations of Computer Science},
  doi = {10.1109/SFCS.1989.63477},
  abstract = {Optimal algorithms for sorting on parallel CREW (concurrent read, exclusive write) and EREW (exclusive read, exclusive write) versions of the pointer machine model are presented. Intuitively, these methods can be viewed as being based on the use of linked lists rather than arrays (the usual parallel data structure). It is shown how to exploit the 'locality' of the approach to solve a problem with applications to database querying and logic programming (set-expression evaluation) in O(log n) time using O(n) processors.{$<<$}ETX{$>>$}},
  keywords = {data structure,query},
  annotation = {Citation Count: 47}
}

@article{m.goodrichSortingParallelPointer1996,
  title = {Sorting on a Parallel Pointer Machine with Applications to Set Expression Evaluation},
  author = {M. Goodrich and S. Kosaraju},
  year = {1996},
  journal = {JACM},
  doi = {10.1145/226643.226670},
  abstract = {We present optimal algorithms for sorting on parallel CREW and EREW versions of the pointer machine model. Intuitively, one can view our methods as being based on a parallel mergesort using linked lists rather than arrays (the usual parallel data structure). We also show how to exploit the ``locality'' of our approach to solve the set expression evaluation problem, a problem with applications to database querying and logic-programming in {$<$}italic{$>$}O{$<$}/italic{$>$}(log {$<$}italic{$>$}n{$<$}/italic{$>$}) time using {$<$}italic{$>$}O{$<$}/italic{$>$}({$<$}italic{$>$}n{$<$}/italic{$>$}) processors. Interestingly, this is an asymptotic improvement over what seems possible using previous techniques.},
  keywords = {data structure,query},
  annotation = {Citation Count: 0}
}

@article{m.kaoSimpleEfficientGraph1994,
  title = {Simple and {{Efficient Graph Compression Schemes}} for {{Dense}} and {{Complement Graphs}}},
  author = {M. Kao and S. Teng},
  year = {1994},
  journal = {International Symposium on Algorithms and Computation},
  doi = {10.1007/3-540-58325-4_211},
  annotation = {Citation Count: 22}
}

@article{m.patrascuDontRushUnion2011,
  title = {Don't Rush into a Union: Take Time to Find Your Roots},
  author = {M. Patrascu and M. Thorup},
  year = {2011},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/1993636.1993711},
  abstract = {We present a new threshold phenomenon in data structure lower bounds where slightly reduced update times lead to exploding query times. Consider incremental connectivity, letting tu be the time to insert an edge and tq be the query time. For tu = Omega(tq), the problem is equivalent to the well-understood union-find problem: proc\{InsertEdge\}(s,t) can be implemented by Union(Find(s), Find(t)). This gives worst-case time tu = tq = O(lg n / lg lg n) and amortized tu = tq = O({$\alpha$}(n)). By contrast, we show that if tu = o(lg n / lg lg n), the query time explodes to tq {$\geq$} n1-o(1). In other words, if the data structure doesn't have time to find the roots of each disjoint set (tree) during edge insertion, there is no effective way to organize the information! For amortized complexity, we demonstrate a new inverse-Ackermann type trade-off in the regime tu = o(tq). A similar lower bound is given for fully dynamic connectivity, where an update time of o(lg n) forces the query time to be n1-o(1). This lower bound allows for amortization and Las Vegas randomization, and comes close to the known O(lg n {$\bullet$} (lg lg n)O(1)) upper bound.},
  keywords = {data structure,dynamic,lower bound,query,query time,update,update time},
  annotation = {Citation Count: 25}
}

@article{m.penttonenNotesComplexitySorting1985,
  title = {Notes on the Complexity of Sorting in Abstract Machines},
  author = {M. Penttonen and J. Katajainen},
  year = {1985},
  journal = {BIT},
  doi = {10.1007/BF01936140},
  annotation = {Citation Count: 4}
}

@article{m.shupletsovFloodingTopologyAlgorithms2022,
  title = {Flooding {{Topology Algorithms}} for {{Computer Networks}}},
  author = {M. Shupletsov and Dmitrii Romanov and E. Stepanov},
  year = {2022},
  journal = {Modern Network technologies},
  doi = {10.1109/MoNeTec55448.2022.9960759},
  abstract = {Flooding topology is a subset of initial network's links, which can be used by flooding algorithms to send broadcast messages. Use of such topology can significantly reduce network load. From mathematical viewpoint flooding topology is a spanning subgraph of the initial network's graph with specific properties and constraints, such as minimal or close to the minimal number of edges, bounded vertex degree, bounded diameter and additional connectivity properties. In this paper one of the possible mathematical formulations of the flooding topology problem is introduced using graph-theoretic approach. Different parameters of the flooding topology are investigated and examples of possible tradeoffs between them are presented. Based on the introduced mathematical problem formulation several flooding topology algorithms were designed and their prototypes were created and tested on the typical network topologies, such as fat trees, Clos networks, spine-leaf topology and VL2 topology. Simulation results showed that proposed algorithms significantly reduce flooding for all considered topologies, while respecting other properties of the flooding topology in most cases.},
  annotation = {Citation Count: 0}
}

@article{m.tsaiAlgorithmicAspectsTurans2017,
  title = {On the Algorithmic Aspects of {{Tur{\'a}n}}'s {{Theorem}}},
  author = {M. Tsai},
  year = {2017},
  doi = {10.7282/T3WS8XC7},
  abstract = {OF THE DISSERTATION On the Algorithmic Aspects of Tur{\'a}n's Theorem by Meng-Tsung Tsai Dissertation Director: Mar{\'t}{\i}n Farach-Colton Tur{\'a}n's Theorem gives an upper bound on the number of edges of n-node, Kr-free graphs, or equivalently it can be restated as that every n-node, m-edge graph has an independent set of size n2/(n+2m). We illustrate how to apply Tur{\'a}n's Theorem to algorithmic problems in several ways. The complexity of dictionary operations, insertion for example, in external memory is well studied. However, the complexity of a batch of n operations is less known, and is seldom as easy as summing up the complexity of individual operations. We obtain lower bounds for batched predecessors by showing the necessity of fetching a set of information that preserves some ``independence'', where Tur{\'a}n's Theorem applies. We also prove lower bounds for batched deletions in cross-referenced dictionaries based on the existence of an adversarial input that forbids some patterns, where Tur{\'a}n's Theorem again applies. In addition, we present an interesting class of problems that are NP-hard to approximate. Tur{\'a}n's theorem is useful in the proof that a large class of problems that can be defined in a simple framework are all NP-hard to approximate.},
  keywords = {lower bound},
  annotation = {Citation Count: 0}
}

@article{m.v.kreveldUnioncopyStructuresDynamic1993,
  title = {Union-Copy Structures and Dynamic Segment Trees},
  author = {M. V. Kreveld and M. Overmars},
  year = {1993},
  journal = {JACM},
  doi = {10.1145/174130.174140},
  abstract = {A new data structure---the union-copy structure ---is introduced, which generalizes the well-known union-find structure. Besides the usual union and find operations, the new structure also supports a copy operation, that generates a duplicate of a given set. The structure can enumerate a given set, find all sets that contain a given element, insert and delete elements, etc. All these operations can be performed very efficiently. The structure can be tuned as to obtain different trade-offs in the efficiency of the different operations. As an application of the union-copy structure, we give a dynamic version of the segment tree. Contrary to the classical semi-dynamic segment trees, the dynamic segment tree is not restricted to a fixed universe, from which the endpoints of the segments must be chosen. The tree allows for insertions, splits, and concatenations in O(log n)-time each. Deletions can be performed in slightly more time.},
  keywords = {data structure,dynamic},
  annotation = {Citation Count: 29}
}

@article{maaTextIndexingWith2005,
  title = {Text Indexing with Errors},
  author = {Maa{\ss}, Moritz G. and Nowak, Johannes},
  year = {2005},
  doi = {10.1016/j.jda.2006.11.001},
  abstract = {No abstract available},
  citationcount = {42},
  venue = {J. Discrete Algorithms}
}

@article{macbeathACompactnessTheorem1951,
  title = {A Compactness Theorem for Affine Equivalence-Classes of Convex Regions},
  author = {Macbeath, A.},
  year = {1951},
  doi = {10.4153/CJM-1951-008-7},
  abstract = {In some parts of the Geometry of Numbers it is convenient to know that certain affine invariants associated with convex regions attain their upper and lower bounds. A classical example is the quotient of the critical determinant by the content (if the region is symmetrical) for which Minkowski determined the exact lower bound 2--n. The object of this paper is to prove that for continuous functions of bounded regions the bounds are attained. The result is, of course, deduced from the selection theorem of Blaschke, and itself is a compactness theorem about the space of affine equivalence-classes.},
  citationcount = {73},
  venue = {Canadian Journal of Mathematics - Journal Canadien de Mathematiques}
}

@article{maggsAnAlgorithmFor1993,
  title = {An Algorithm for Finding Predecessors in Integer Sets},
  author = {Maggs, B. and Henzinger, Monika},
  year = {1993},
  doi = {10.1007/3-540-57155-8_273},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Workshop on Algorithms and Data Structures}
}

@article{mairsonAverageCaseLower1985,
  title = {Average Case Lower Bounds on the Construction and Searching of Partial Orders},
  author = {Mairson, Harry G.},
  year = {1985},
  doi = {10.1109/SFCS.1985.13},
  abstract = {It is very well known in computer science that partially ordered files are easier to search. In the worst case, for example, a totally unordered file requires no preprocessing, but {\textquestiondown}(n) time to search, while a totally ordered file requires {\textquestiondown}(n log n) preprocessing time to sort, but can be searched in O(log n) time. Behind the casual observation, then, lurks the notion of a computational tradeoff between sorting and searching. We analyze this tradeoff in the average case, using the decision tree model. Let P be a preprocessing algorithm that produces partial orders given a set U of n elements, and let S be a searching algorithm for these partial orders. Assuming any of the n! permutations of the elements of U are equally likely, and that we search for any y isin; U with equal probability (in unsuccessful search, all "gaps" are considered equally likely), the average costs P(n) of preprocessing and S(n) of searching may be computed. We demonstrate a tradeoff of the form P(n) + n log S(n) = {\textquestiondown}(n log n), for both successful and unsuccessful search. The bound is tight up to a constant factor. In proving this tradeoff, we show a lower bound on the average case of searching a partial order. Let A be a partial order on n elements consistent with {$\Pi$} permutations. We show S(n) = {\textquestiondown}({$\Pi$}3/n/n2) for successful search of A, and S(n) = {\textquestiondown}({$\Pi$}2/n/n) for unsuccessful search. These lower bounds show, for example, that heaps require linear time to search on the average.},
  citationcount = {5},
  venue = {26th Annual Symposium on Foundations of Computer Science (sfcs 1985)}
}

@article{majewskiMaintainingCmso2Properties2025,
  title = {Maintaining {{CMSO2}} Properties on Dynamic Structures with Bounded Feedback Vertex Number},
  author = {Majewski, Konrad and Pilipczuk, Michal and Soko{\l}owski, Marek},
  year = {2025},
  doi = {10.1145/3715884},
  abstract = {Let {$\varphi$} be a sentence of \{CMSO\}{$_2$} (monadic second-order logic with quantification over edge subsets and counting modular predicates) over the signature of graphs. We present a dynamic data structure that for a given graph G that is updated by edge insertions and edge deletions, maintains whether {$\varphi$} is satisfied in G . The data structure is required to correctly report the outcome only when the feedback vertex number of G does not exceed a fixed constant k , otherwise it reports that the feedback vertex number is too large. With this assumption, we guarantee amortized update time \{O\}\textsubscript{\{\vphantom\}}{$\varphi$},k\vphantom\{\}\{(n)\} . If we additionally assume that the feedback vertex number of G never exceeds k , this update time guarantee is worst-case. By combining this result with a classic theorem of Erd{\H o}s and P{\'o}sa, we give a fully dynamic data structure that maintains whether a graph contains a packing of k vertex-disjoint cycles with amortized update time \{O\}\textsubscript{\{\vphantom\}}k\vphantom\{\}\{(n)\} . Our data structure also works in a larger generality of relational structures over binary signatures.},
  citationcount = {5},
  venue = {ACM Transactions on Computation Theory},
  keywords = {data structure,dynamic,update,update time}
}

@article{majorOnTheInvariance1978,
  title = {On the Invariance Principle for Sums of Independent Identically Distributed Random Variables},
  author = {Major, P.},
  year = {1978},
  doi = {10.1016/0047-259X(78)90029-5},
  abstract = {No abstract available},
  citationcount = {87},
  venue = {No venue available}
}

@article{maksimovAlgebraicBayesianNetworks2021,
  title = {Algebraic Bayesian Networks: {{Checking}} Backbone Connectivity},
  author = {Maksimov, Anatolii G. and Tulupyev, A. L.},
  year = {2021},
  doi = {10.1134/S1063454121020059},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Vestnik St Petersburg University Mathematics}
}

@article{maLowLatencyPhoton2002,
  title = {Low Latency Photon Mapping Using Block Hashing},
  author = {Ma, Vincent C. H. and McCool, M.},
  year = {2002},
  doi = {10.2312/EGGH/EGGH02/089-098},
  abstract = {For hardware accelerated rendering, photon mapping is especially useful for simulating caustic lighting effects on non-Lambertian surfaces. However, an efficient hardware algorithm for the computation of the k nearest neighbours to a sample point is required.Existing algorithms are often based on recursive spatial subdivision techniques, such askd-trees. However, hardware implementation of a tree-based algorithm would have a high latency, or would require a large cache to avoid this latency on average.We present a neighbourhood-preserving hashing algorithm that is low-latency and has sub-linear access time. This algorithm is more amenable to fine-scale parallelism than tree-based recursive spatial subdivision, and maps well onto coherent block-oriented pipelined memory access. These properties make the algorithm suitable for implementation using future programmable fragment shaders with only one stage of dependent texturing.},
  citationcount = {58},
  venue = {SIGGRAPH/EUROGRAPHICS Conference On Graphics Hardware}
}

@article{maMoreEfficientAlgorithms2008,
  title = {More Efficient Algorithms for Closest String and Substring Problems},
  author = {Ma, Bin and Sun, Xiaoming},
  year = {2008},
  doi = {10.1137/080739069},
  abstract = {The closest string and substring problems find applications in PCR primer design, genetic probe design, motif finding, and antisense drug design. For their importance, the two problems have been extensively studied recently in computational biology. Unfortunately both problems are NP-complete. Researchers have developed both fixed-parameter algorithms and approximation algorithms for the two problems. In terms of fixed-parameter, when the radius d is the parameter, the best-known fixed-parameter algorithm for closest string has time complexity O(ndd+1), which is still superpolynomial even if d = O(log n). In this paper we provide an O(n{\textbar}{$\Sigma\vert$}O(d)) algorithm where {$\Sigma$} is the alphabet. This gives a polynomial time algorithm when d = O(log n) and {$\Sigma$} has constant size. Using the same technique, we additionally provide a more efficient subexponential time algorithm for the closest substring problem. In terms of approximation, both closest string and closest substring problems admit polynomial time approximation schemes (PTAS). The best known time complexity of the PTAS is O(nO(-2 log 1/)). In this paper we present a PTAS with time complexity O(nO(-2)). At last, we prove that a restricted version of the closest substring has the same parameterized complexity as closest substring, answering an open question in the literature.},
  citationcount = {87},
  venue = {SIAM journal on computing (Print)}
}

@article{mangasarianTwoPersonNonzero1964,
  title = {Two-Person Nonzero-Sum Games and Quadratic Programming},
  author = {Mangasarian, O. and Stone, H.},
  year = {1964},
  doi = {10.1016/0022-247X(64)90021-6},
  abstract = {No abstract available},
  citationcount = {162},
  venue = {No venue available}
}

@article{manorLiftingWithInner2024,
  title = {Lifting with Inner Functions of Polynomial Discrepancy},
  author = {Manor, Yahel and Meir, Or},
  year = {2024},
  doi = {10.4230/LIPIcs.APPROX/RANDOM.2022.26},
  abstract = {Lifting theorems are theorems that bound the communication complexity of a composed function f{$\circ$}g\textsuperscript{\{\vphantom\}}n\vphantom\{\} in terms of the query complexity of f and the communication complexity of g. Such theorems constitute a powerful generalization of direct-sum theorems for g, and have seen numerous applications in recent years. We prove a new lifting theorem that works for every two functions f,g such that the discrepancy of g is at most inverse polynomial in the input length of f. Our result is a significant generalization of the known direct-sum theorem for discrepancy, and extends the range of inner functions g for which lifting theorems hold.},
  citationcount = {1},
  venue = {International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques},
  keywords = {communication,communication complexity,query,query complexity}
}

@article{mansourTheComputationalComplexity1990,
  title = {The Computational Complexity of Universal Hashing},
  author = {Mansour, Y. and Nisan, N. and Tiwari, Prasoon},
  year = {1990},
  doi = {10.1145/100216.100246},
  abstract = {Summary form only given. Any implementation of Carter-Wegman universal hashing from n-b strings to m-b strings requires a time-space tradeoff of TS= Omega (nm). The bound holds in the general Boolean branching program model, and thus in essentially any model of computation. As a corollary, computing a+b*c in any field F requires a quadratic time-space tradeoff, and the bound holds for any representation of the elements of the field. Other lower bounds on the complexity of any implementation of universal hashing are given as well: quadratic AT/sup 2/ bound for VLSI implementation; Omega (log n) parallel time bound on a CREW PRAM; and exponential size for constant depth circuits. The results on VLSI implementation are proved using information transfer bounds derived from the definition of a universal family of hash functions.<<ETX>>},
  citationcount = {224},
  venue = {Proceedings Fifth Annual Structure in Complexity Theory Conference}
}

@article{marchiniCompactFenwickTrees2019,
  title = {Compact {{Fenwick}} Trees for Dynamic Ranking and Selection},
  author = {Marchini, Stefano and Vigna, S.},
  year = {2019},
  doi = {10.1002/spe.2791},
  abstract = {The Fenwick tree is a classical implicit data structure that stores an array in such a way that modifying an element, accessing an element, computing a prefix sum and performing a predecessor search on prefix sums all take logarithmic time. We introduce a number of variants which improve the classical implementation of the tree: in particular, we can reduce its size when an upper bound on the array element is known, and we can perform much faster predecessor searches. Our aim is to use our variants to implement an efficient dynamic bit vector: our structure is able to perform updates, ranking and selection in logarithmic time, with a space overhead in the order of a few percents, outperforming existing data structures with the same purpose. Along the way, we highlight the pernicious interplay between the arithmetic behind the Fenwick tree and the structure of current CPU caches, suggesting simple solutions that improve performance significantly.},
  citationcount = {5},
  venue = {Software, Practice \& Experience},
  keywords = {data structure,dynamic,update}
}

@article{marekj.laoClassTreeLikeUNIONFIND1981,
  title = {A {{Class}} of {{Tree-Like UNION-FIND Data Structures}} and the {{Nonlinearity}}},
  author = {Marek J. Lao},
  year = {1981},
  journal = {Colloquium on Trees in Algebra and Programming},
  doi = {10.1007/3-540-10828-9_67},
  keywords = {data structure},
  annotation = {Citation Count: 1}
}

@article{maresTheSagaOf2008,
  title = {The Saga of Minimum Spanning Trees},
  author = {Mares, Martin},
  year = {2008},
  doi = {10.1016/j.cosrev.2008.10.002},
  abstract = {No abstract available},
  citationcount = {40},
  venue = {Computer Science Review}
}

@article{margoAScalableDistributed2015,
  title = {A Scalable Distributed Graph Partitioner},
  author = {Margo, Daniel W. and Seltzer, M.},
  year = {2015},
  doi = {10.14778/2824032.2824046},
  abstract = {We present Scalable Host-tree Embeddings for Efficient Partitioning (Sheep), a distributed graph partitioning algorithm capable of handling graphs that far exceed main memory. Sheep produces high quality edge partitions an order of magnitude faster than both state of the art offline (e.g., METIS) and streaming partitioners (e.g., Fennel). Sheep's partitions are independent of the input graph distribution, which means that graph elements can be assigned to processing nodes arbitrarily without affecting the partition quality. Sheep transforms the input graph into a strictly smaller elimination tree via a distributed map-reduce operation. By partitioning this tree, Sheep finds an upper-bounded communication volume partitioning of the original graph. We describe the Sheep algorithm and analyze its space-time requirements, partition quality, and intuitive characteristics and limitations. We compare Sheep to contemporary partitioners and demonstrate that Sheep creates competitive partitions, scales to larger graphs, and has better runtime.},
  citationcount = {76},
  venue = {Proceedings of the VLDB Endowment},
  keywords = {communication}
}

@article{marialuisabonetSerialTransitiveClosure1995,
  title = {The {{Serial Transitive Closure Problem}} for {{Trees}}},
  author = {Maria Luisa Bonet and S. Buss},
  year = {1995},
  journal = {SIAM journal on computing (Print)},
  doi = {10.1137/S0097539792225303},
  abstract = {The serial transitive closure problem is the problem of, given a directed graph \$G\$ and a list of edges, called closure edges, which are in the transitive closure of the graph, to generate all the closure edges from edges in \$G\$. A nearly linear upper bound is given on the number of steps in optimal solutions to the serial transitive closure problem for the case of graphs which are trees. "Nearly linear" means \$O(n{\textbackslash}cdot {\textbackslash}alpha(n))\$ where \${\textbackslash}alpha\$ is the inverse Ackermann function. This upper bound is optimal to within a constant factor.},
  annotation = {Citation Count: 4}
}

@article{markarianRevisitingOnlineAlgorithms2024,
  title = {Revisiting Online Algorithms: A Survey of Set Cover Solutions beyond Competitive Analysis},
  author = {Markarian, Christine and Fachkha, Claude and Yassine, Noura},
  year = {2024},
  doi = {10.1109/ACCESS.2024.3504541},
  abstract = {Online algorithms are crucial for real-time decision-making and adaptability across diverse fields, such as operations research, computer science, and combinatorics. These algorithms handle data incrementally and make decisions without prior knowledge of future inputs, thereby effectively addressing complex challenges. This study provides a comprehensive survey of online algorithms, focusing on the Set Cover problem (SC). SC is one of the most extensively studied NP-complete problems, contributing significantly to advancements in approximation techniques, complexity theory, and online algorithms research. It involves selecting a minimal-weight subset from a collection of subsets to cover all elements. This survey examines online algorithms for Set Cover in multiple contexts, including competitive analysis, stochastic models, advice complexity, predictive algorithms, and recourse strategies. While competitive analysis, which compares online algorithms to the best possible offline solutions, remains a key evaluation method, this paper aims to extend the understanding beyond traditional worst-case scenarios. By providing in-depth insights into algorithm design, analytical methods, and practical applications, it seeks to advance the field and stimulate further research in evolving contexts. Additionally, it underscores the intersection of online algorithms with broader domains such as machine learning and predictive analytics, establishing new benchmarks for exploring their broader implications.},
  citationcount = {Unknown},
  venue = {IEEE Access}
}

@article{marsdenIAndJ2012,
  title = {I and {{J}}},
  author = {Marsden, W.},
  year = {2012},
  doi = {10.1017/CBO9781139207249.009},
  abstract = {No abstract available},
  citationcount = {168433},
  venue = {No venue available}
}

@article{marvinkunnemannTighterBoundsSimilarity2016,
  title = {Tight(Er) Bounds for Similarity Measures, Smoothed Approximation and Broadcasting},
  author = {Marvin K{\"u}nnemann},
  year = {2016},
  doi = {10.22028/D291-25431},
  abstract = {Faculty of Natural Sciences and Technology I Computer Science},
  annotation = {Citation Count: 0}
}

@article{marxClosestSubstringProblem2005,
  title = {The Closest Substring Problem with Small Distances},
  author = {Marx, D.},
  year = {2005},
  doi = {10.1109/SFCS.2005.70},
  abstract = {In the closest substring problem k strings s/sub 1/, ..., s/sub k/ are given, and the task is to find a string s of length L such that each string s/sub i/, has a consecutive substring of length L whose distance is at most d from s. The problem is motivated by applications in computational biology. We present two algorithms that can be efficient for small fixed values of d and k: for some functions f and g, the algorithms have running time f(d) /spl middot/ n(O(log d)) and g(d,k) /spl middot/ n(O(log log k)), respectively. The second algorithm is based on connections with the extremal combinatorics of hypergraphs. The closest substring problem is also investigated from the parameterized complexity point of view. Answering an open question from (Evans et al., 2003; Fellows et al.; Gramm et al., 2003), we show that the problem is W[1] hard even if both d and k are parameters. It follows as a consequence of this hardness result that our algorithms are optimal in the sense that the exponent of n in the running time cannot be improved to o(log d) or to o(log log k) (modulo some complexity-theoretic assumptions). Another consequence is that the running time n/sup O(1//spl epsiv/4)/ of the approximation scheme for closest substring presented in (Li et al., 2002) cannot be improved to f(/spl epsiv/) /spl middot/ n/sup c/, i.e. the /spl epsiv/ has to appear in the exponent of n.},
  citationcount = {25},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{marxClosestSubstringProblems2008,
  title = {Closest Substring Problems with Small Distances},
  author = {Marx, D.},
  year = {2008},
  doi = {10.1137/060673898},
  abstract = {We study two pattern matching problems that are motivated by applications in computational biology. In the Closest Substring problem k strings s{$_1$},{\dots},s\textsubscript{k} are given, and the task is to find a string s of length L such that each string s\textsubscript{i} has a consecutive substring of length L whose distance is at most d from s. We present two algorithms that aim to be efficient for small fixed values of d and k: for some functions f and g, the algorithms have running time f(d){$\cdot$}n\textsuperscript{\{\vphantom\}}O(d)\vphantom\{\} and g(d,k){$\cdot$}n\textsuperscript{\{\vphantom\}}O(k)\vphantom\{\}, respectively. The second algorithm is based on connections with the extremal combinatorics of hypergraphs. The Closest Substring problem is also investigated from the parameterized complexity point of view. Answering an open question from [P. A. Evans, A. D. Smith, and H. T. Wareham, Theoret. Comput. Sci., 306 (2003), pp. 407-430, M. R. Fellows, J. Gramm, and R. Niedermeier, Combinatorica, 26 (2006), pp. 141-167, J. Gramm, J. Guo, and R. Niedermeier, Lecture Notes in Comput. Sci. 2751, Springer, Berlin, 2003, pp. 195-209, J. Gramm, R. Niedermeier, and P. Rossmanith, Algorithmica, 37 (2003), pp. 25-42], we show that the problem is W[1]-hard even if both d and k are parameters. It follows as a consequence of this hardness result that our algorithms are optimal in the sense that the exponent of n in the running time cannot be improved to o(d) or to o(k) (modulo some complexity-theoretic assumptions). Consensus Patterns is the variant of the problem where, instead of the requirement that each s\textsubscript{i} has a substring that is of distance at most d from s, we have to select the substrings in such a way that the average of these k distances is at most {$\delta$}. By giving an f({$\delta$}){$\cdot$}n{$^9$} time algorithm, we show that the problem is fixed-parameter tractable. This answers an open question from [M. R. Fellows, J. Gramm, and R. Niedermeier, Combinatorica, 26 (2006), pp. 141-167].},
  citationcount = {51},
  venue = {SIAM journal on computing (Print)}
}

@article{marxOnTheOptimality2007,
  title = {On the Optimality of Planar and Geometric Approximation Schemes},
  author = {Marx, D.},
  year = {2007},
  doi = {10.1109/FOCS.2007.50},
  abstract = {We show for several planar and geometric problems that the best known approximation schemes are essentially optimal with respect to the dependence on epsi. For example, we show that the 2{\textexclamdown}sup{\textquestiondown}O(1/epsi){\textexclamdown}/sup{\textquestiondown}ldrn time approximation schemes for planar maximum independent set and for TSP on a metric defined bv a planar graph are essentially optimal: if there is a delta{\textquestiondown}0 such that any of these problems admits a 2{\textexclamdown}sup{\textquestiondown}O((1/epsi){\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}1-delta{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}){\textexclamdown}/sup{\textquestiondown}n{\textexclamdown}sup{\textquestiondown}O(1){\textexclamdown}/sup{\textquestiondown} time PTAS, then the exponential tune hypothesis (ETH) fails. It is known that maximum independent set on unit disk graphs and the planar logic problems MPSAT. TMIN, TMAX admit n{\textexclamdown}sup{\textquestiondown}O(1/epsi){\textexclamdown}/sup{\textquestiondown} time approximation schemes. We show that they are optimal in the sense that if there is a delta{\textquestiondown}0 such that any of these problems admits a 2{\textexclamdown}sup{\textquestiondown}(1/epsi){\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}O(1){\textexclamdown}/sup{\textquestiondown} n{\textexclamdown}sup{\textquestiondown}O((1/epsi){\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}1-delta{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}sup{\textquestiondown}){\textexclamdown}/sup{\textquestiondown} time PTAS, then ETH fails.},
  citationcount = {147},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{marxTractableHypergraphProperties2009,
  title = {Tractable Hypergraph Properties for Constraint Satisfaction and Conjunctive Queries},
  author = {Marx, D.},
  year = {2009},
  doi = {10.1145/2535926},
  abstract = {An important question in the study of constraint satisfaction problems (CSP) is understanding how the graph or hypergraph describing the incidence structure of the constraints influences the complexity of the problem. For binary CSP instances (that is, where each constraint involves only two variables), the situation is well understood: the complexity of the problem essentially depends on the treewidth of the graph of the constraints [Grohe 2007; Marx 2010b]. However, this is not the correct answer if constraints with unbounded number of variables are allowed, and in particular, for CSP instances arising from query evaluation problems in database theory. Formally, if H is a class of hypergraphs, then let CSP(H) be CSP restricted to instances whose hypergraph is in H. Our goal is to characterize those classes of hypergraphs for which CSP(H) is polynomial-time solvable or fixed-parameter tractable, parameterized by the number of variables. Note that in the applications related to database query evaluation, we usually assume that the number of variables is much smaller than the size of the instance, thus parameterization by the number of variables is a meaningful question. The most general known property of H that makes CSP(H) polynomial-time solvable is bounded fractional hypertree width. Here we introduce a new hypergraph measure called submodular width, and show that bounded submodular width of H (which is a strictly more general property than bounded fractional hypertree width) implies that CSP(H) is fixed-parameter tractable. In a matching hardness result, we show that if H has unbounded submodular width, then CSP(H) is not fixed-parameter tractable (and hence not polynomial-time solvable), unless the Exponential Time Hypothesis (ETH) fails. The algorithmic result uses tree decompositions in a novel way: instead of using a single decomposition depending on the hypergraph, the instance is split into a set of instances (all on the same set of variables as the original instance), and then the new instances are solved by choosing a different tree decomposition for each of them. The reason why this strategy works is that the splitting can be done in such a way that the new instances are ``uniform'' with respect to the number extensions of partial solutions, and therefore the number of partial solutions can be described by a submodular function. For the hardness result, we prove via a series of combinatorial results that if a hypergraph H has large submodular width, then a 3SAT instance can be efficiently simulated by a CSP instance whose hypergraph is H. To prove these combinatorial results, we need to develop a theory of (multicommodity) flows on hypergraphs and vertex separators in the case when the function b(S) defining the cost of separator S is submodular, which can be of independent interest.},
  citationcount = {177},
  venue = {JACM},
  keywords = {query}
}

@article{marxTractableHypergraphProperties2010,
  title = {Tractable Hypergraph Properties for Constraint Satisfaction and Conjunctive Queries},
  author = {Marx, D.},
  year = {2010},
  doi = {10.1145/1806689.1806790},
  abstract = {An important question in the study of constraint satisfaction problems (CSP) is understanding how the graph or hypergraph describing the incidence structure of the constraints influences the complexity of the problem. For binary CSP instances (i.e., where each constraint involves only two variables), the situation is well understood: the complexity of the problem essentially depends on the treewidth of the graph of the constraints. However, this is not the correct answer if constraints with unbounded number of variables are allowed, and in particular, for CSP instances arising from query evaluation problems in database theory. Formally, if H is a class of hypergraphs, then let CSP(H) be CSP restricted to instances whose hypergraph is in H. Our goal is to characterize those classes of hypergraphs for which CSP(H) is polynomial-time solvable or fixed-parameter tractable, parameterized by the number of variables. In the applications related to database query evaluation, we usually assume that the number of variables is much smaller than the size of the instance, thus parameterization by the number of variables is a meaningful question. The most general known property of H that makes CSP(H) polynomial-time solvable is bounded fractional hypertree width. Here we introduce a new hypergraph measure called submodular width, and show that bounded submodular width of H (which is a strictly more general property than bounded fractional hypertree width) implies that CSP(H) is fixed-parameter tractable. In a matching hardness result, we show that if H has unbounded submodular width, then CSP(H) is not fixed-parameter tractable (and hence not polynomial-time solvable), unless the Exponential Time Hypothesis (ETH) fails. The algorithmic result uses tree decompositions in a novel way: instead of using a single decomposition depending on the hypergraph, the instance is split into a set of instances (all on the same set of variables as the original instance), and then the new instances are solved by choosing a different tree decomposition for each of them. The reason why this strategy works is that the splitting can be done in such a way that the new instances are "uniform" with respect to the number extensions of partial solutions, and therefore the number of partial solutions can be described by a submodular function. For the hardness result, we prove via a series of combinatorial results that if a hypergraph H has large submodular width, then a 3SAT instance can be efficiently simulated by a CSP instance whose hypergraph is H. To prove these combinatorial results, we need to develop a theory of (multicommodity) flows on hypergraphs and vertex separators in the case when the function b(S) defining the cost of separator S is submodular.},
  citationcount = {23},
  venue = {Symposium on the Theory of Computing},
  keywords = {query}
}

@article{matejekCompressoEfficientCompression2017,
  title = {Compresso: {{Efficient}} Compression of Segmentation Data for Connectomics},
  author = {Matejek, Brian and Haehn, D. and Lekschas, Fritz and Mitzenmacher, M. and Pfister, H.},
  year = {2017},
  doi = {10.1007/978-3-319-66182-7_89},
  abstract = {No abstract available},
  citationcount = {14},
  venue = {International Conference on Medical Image Computing and Computer-Assisted Intervention}
}

@article{matiasDynamicGenerationOf1993,
  title = {Dynamic Generation of Discrete Random Variates},
  author = {Matias, Yossi and Vitter, J. and Ni, Wen-Chun},
  year = {1993},
  doi = {10.1007/s00224-003-1078-6},
  abstract = {No abstract available},
  citationcount = {53},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{matouekDiscrepancyAndApproximations1991,
  title = {Discrepancy and Approximations for Bounded {{VC-dimension}}},
  author = {Matou{\v s}ek, J. and Welzl, E. and Wernisch, L.},
  year = {1991},
  doi = {10.1109/SFCS.1991.185401},
  abstract = {AbstractLet (X, {$R$}) be a set system on ann-point setX. For a two-coloring onX, itsdiscrepancy is defined as the maximum number by which the occurrences of the two colors differ in any set in {$R$}. We show that if for anym-point subset  the number of distinct subsets induced by {$R$} onY is bounded byO(md) for a fixed integerd, then there is a coloring with discrepancy bounded byO(n1/2-1/2d(logn)1+1/2d). Also if any subcollection ofm sets of {$R$} partitions the points into at mostO(md) classes, then there is a coloring with discrepancy at mostO(n1/2-1/2dlogn). These bounds imply improved upper bounds on the size of {$\varepsilon$}-approximations for (X, {$R$}). All the bounds are tight up to polylogarithmic factors in the worst case. Our results allow to generalize several results of Beck bounding the discrepancy in certain geometric settings to the case when the discrepancy is taken relative to an arbitrary measure.},
  citationcount = {132},
  venue = {[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science}
}

@article{matouekEfficientPartitionTrees1991,
  title = {Efficient Partition Trees},
  author = {Matou{\v s}ek, J.},
  year = {1991},
  doi = {10.1145/109648.109649},
  abstract = {We prove a theorem on partitioning point sets inEd (d fixed) and give an efficient construction of partition trees based on it. This yields a simplex range searching structure with linear space,O(n logn) deterministic preprocessing time, andO(n1-1/d(logn)O(1)) query time. WithO(nlogn) preprocessing time, where {$\delta$} is an arbitrary positive constant, a more complicated data structure yields query timeO(n1-1/d(log logn)O(1)). This attains the lower bounds due to Chazelle [C1] up to polylogarithmic factors, improving and simplifying previous results of Chazelleet al. [CSW].The partition result implies that, forrd{$\leq$}n1-{$\delta$}, a (1/r)-approximation of sizeO(rd) with respect to simplices for ann-point set inEd can be computed inO(n logr) deterministic time. A (1/r)-cutting of sizeO(rd) for a collection ofn hyperplanes inEd can be computed inO(n logr) deterministic time, provided thatr{$\leq$}n1/(2d-1).},
  citationcount = {399},
  venue = {SCG '91}
}

@article{matouekGeometricRangeSearching1994,
  title = {Geometric Range Searching},
  author = {Matou{\v s}ek, J.},
  year = {1994},
  doi = {10.1145/197405.197408},
  abstract = {In geometric range searching, algorithmic problems of the following type are considered. Given an n-point set P in the plane, build a data structure so that, given a query triangle R, the number of points of P lying in R can be determined quickly. Similar questions can be asked for point sets in higher dimensions, with triangles replaced by simplices or by more complicated shapes. Algorithms of this type are of crucial importance in computational geometry, as they can be used as subroutines in solutions to many seemingly unrelated problems, which are often motivated by practical applications, for instance in computer graphics (ray tracing, hidden-surface removal etc.). We present a survey of theoretical results and the main techniques in geometric range searching.},
  citationcount = {97},
  venue = {CSUR}
}

@article{matouekOnEmbeddingExpanders1997,
  title = {On Embedding Expanders into {$\ell$}p Spaces},
  author = {Matou{\v s}ek, J.},
  year = {1997},
  doi = {10.1007/BF02773799},
  abstract = {No abstract available},
  citationcount = {132},
  venue = {No venue available}
}

@article{matouekOnTheDistortion1996,
  title = {On the Distortion Required for Embedding Finite Metric Spaces into Normed Spaces},
  author = {Matou{\v s}ek, J.},
  year = {1996},
  doi = {10.1007/BF02761110},
  abstract = {No abstract available},
  citationcount = {137},
  venue = {No venue available}
}

@article{matouekRangeSearchingWith1992,
  title = {Range Searching with Efficient Hierarchical Cuttings},
  author = {Matou{\v s}ek, J.},
  year = {1992},
  doi = {10.1145/142675.142732},
  abstract = {AbstractWe present an improved space/query-time tradeoff for the general simplex range searching problem, matching known lower bounds up to small polylogarithmic factors. In particular, we construct a linear-space simplex range searching data structure withO(n1-1/d) query time, which is optimal ford=2 and probably also ford{\textquestiondown}2. Further, we show that multilevel range searching data structures can be built with only a polylogarithmic overhead in space and query time per level (the previous solutions require at least a small fixed power ofn). We show that Hopcroft's problem (detecting an incidence amongn lines andn points) can be solved in time  . In all these algorithms we apply Chazelle's results on computing optimal cuttings.},
  citationcount = {259},
  venue = {SCG '92}
}

@article{matouekReportingPointsIn1992,
  title = {Reporting Points in Halfspaces},
  author = {Matou{\v s}ek, J.},
  year = {1992},
  doi = {10.1016/0925-7721(92)90006-E},
  abstract = {No abstract available},
  citationcount = {278},
  venue = {Computational geometry}
}

@article{matoussekReportingPointsIn1991,
  title = {Reporting Points in Halfspaces},
  author = {Matoussek, J.},
  year = {1991},
  doi = {10.1109/SFCS.1991.185370},
  abstract = {The author considers the halfspace range reporting problem: Given a finite set P of points in E/sup d/, preprocess it so that given a query halfspace gamma , the points of p intersection gamma can be reported efficiently. It is shown that, with almost linear storage, this problem can be solved substantially more efficiently than the more general simplex range searching problem. A data structure for halfspace range reporting in dimensions d{\textquestiondown}or=4 is given. It uses O(n log log n) space and O (n log n) deterministic preprocessing time. The query time is also given. Results for the halfspace emptiness problem, where one only wants to know whether P intersection gamma is empty, are also presented.<<ETX>>},
  citationcount = {27},
  venue = {[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science}
}

@article{matouVsekEfficientPartitionTrees1992,
  title = {Efficient Partition Trees},
  author = {Matou{\textasciicaron}sek, J.},
  year = {1992},
  journal = {Discrete and Computational Geom etry},
  volume = {8},
  pages = {315--334},
  doi = {10.1007/BF02293051},
  annotation = {P. K. Agarwal. Partitioning arrangements of lines, II: Applications.Discrete \& Computational Geometry, 5:533--573, 1990.\\
A. Aggarwal, M. Hansen, and T. Leighton. Solving query-retrieval problems by compacting Voronoi diagrams. InProc. 21st ACM Symposium on Theory of Computing, pages 331--340, 1990.\\
P. K. Agarwal and J. Matou{\v s}ek. Dynamic half-space range reporting and its applications. Tech. Report CS-91-43, Duke University, 1991. Extended abstract:Proc. 33rd IEEE Symposium on Foundations of Computer Science, to appear, 1992.\\
P. K. Agarwal and J. Matou{\v s}ek. Ray shooting and parametric search. Tech. Report CS-1991-22, Duke University, 1991. Extended abstract:Proc. 24th ACM Symposium on Theory of Computing, 1992.\\
P. K. Agarwal and M. Sharir. Applications of a new space partitioning scheme. InProc. 2nd Workshop on Algorithms and Data Structures, 1991.\\
J. L. Bentley. Decomposable searching problems.Information Processing Letters, 8:244--251, 1979.\\
B. Chazelle and J. Friedman. A deterministic view of random sampling and its use in geometry.Combinatorica, 10(3):229--249, 1990.\\
B. Chazelle, L. Guibas, and D. T. Lee. The power of geometric duality.BIT, 25(1), 1985.\\
B. Chazelle. Lower bounds on the complexity of polytope range searching.Journal of the American Mathematical Society, 2(4):637--666, 1989.\\
B. Chazelle. Cutting hyperplanes for divide-and-conquer. Tech. Report CS-TR-335-91, Princeton University, 1991. Preliminary version:Proc. 32nd IEEE Conference on Foundations of Computer Science, October 1991. To appear inDiscrete \& Computational Geometry.\\
B. Chazelle and F. P. Preparata. Halfspace range searching: An algorithmic application ofk-sets.Discrete \& Computational Geometry, 1:83--93, 1986.\\
B. Chazelle, M. Sharir, and E. Welzl. Quasi-optimal upper bounds for simplex range searching and new zone theorems. InProc. 6th ACM Symposium on Computational Geometry, pages 23--33, 1990.\\
B. Chazelle and E. Welzl. Quasi-optimal range searching in spaces of finite VC-dimension.Discrete \& Computational Geometry, 4:467--490, 1989.\\
H. Edelsbrunner.Algorithms in Combinatorial Geometry. Springer-Verlag, New York, 1987\\
H. Edelsbrunner and E. Welzl. Halpflanar range search in linear space andO(n 0.695) query time.Information Processing Letters, 23(6):289--293, 1986.\\
D. Haussler and E. Welzl.{$\varepsilon$}-nets and simplex range queries.Discrete \& Computational Geometry, 2:127--151, 1987.\\
J. Matou{\v s}ek. Approximations and optimal geometric divide-and-conquer. InProc. 23rd ACM Symposium on Theory of Computing, pages 506--511, 1991.\\
J. Matou{\v s}ek. Cutting hyperplane arrangements.Discrete \& Computational Geometry, 6(5):385--406, 1991.\\
J. Matou{\v s}ek. Range searching with efficient hierarchical cuttings. InProc. 8th ACM Symposium on Computational Geometry, pages 276--285, 1992.\\
J. Matou{\v s}ek. Reporting points in halfspace. inProc. 32nd IEEE Symposium on Foundations of Computer Science, pages 207--215, 1991. Also to appear inComputational Geometry: Theory and Applications.\\
J. Matou{\v s}ek, E. Welzl, and L. Wernisch. Discrepancy and{$\varepsilon$}-approximations for bounded VC-dimension.Combinatorica, to appear. Also inProc. 32nd IEEE Symposium on Foundations of Computer Science, pages 424--430, 1991.\\
M. H. Overmars.The design of Dynamic Data Structures. Springer-Verlag, Berlin, 1983.\\
H. Schipper and M. H. Overmars. Dynamic partition trees. InProc. 2nd Scandavian Workshop on Algorithms Theory, pages 404--417, 1990. Lecture Notes in Computer Science, Vol. 447. Springer-Verlag, Berlin, 1990. Also to appear inBIT.\\
V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities.Theory of Probability and its Applications, 16:264--280, 1971.\\
E. Welzl. Partition trees for triangle counting and other range searching problems. InProc. 4th ACM Symposium on Computational Geometry, pages 23--33, 1988.\\
D. E. Willard. Polygon retrieval.SIAM Journal on Computing, 11:149--165, 1982.\\
F. F. Yao and A. C. Yao. A general approach to geometric queries. InProc. 17th ACM Symposium on Theory of Computing, pages 163--168, 1985.},
  file = {/Users/tulasi/Zotero/storage/4WQYW9Y3/Matousek - 1992 - Efficient partition trees.pdf}
}

@article{matouVsekTightUpperBounds1995,
  title = {Tight Upper Bounds for the Discrepancy of Half-Spaces},
  author = {Matou{\textasciicaron}sek, J.},
  year = {1995},
  journal = {Discrete and Computational Geometry},
  volume = {13},
  pages = {593--601},
  doi = {10.1007/BF02574066},
  annotation = {[ABCH] N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. Scale-sensitive dimension, uniform convergence and learnability.Proc. 34th IEEE Symposium on Foundations of Computer Science, pages 292--300, 1993.\\
[Ag] P.K. Agarwal. Geometric partitioning and its applications. In J.E. Goodman, R. Pollack, and W. Steiger, editors,Computational Geometry: Papers from the DIMACS Special Year. American Mathematical Society, Providence, RI, 1991, pages 1--38.\\
[AHW] N. Alon, D. Haussler, and E. Welzl. Partitioning and geometric embedding of range spaces of finite Vapnik-Chervonenkis dimension.Proc. 3rd ACM Symposium on Computational Geometry, pages 331--340, 1987.\\
[Al] R. Alexander. Geometric methods in the theory of uniform distribution.Combinatorica, 10(2):115--136, 1990.\\
[AS] N. Alon and J. Spencer.The Probabilistic Method. Wiley, New York, 1993.\\
[BC] J. Beck and W. Chen.Irregularities of Distribution. Cambridge University Press, Cambridge, 1987.\\
[BCM] H. Br{\"o}nnimann, B. Chazelle, and J. Matou{\v s}ek. Product range spaces, sensitive sampling and derandomization.Proc. 34th IEEE Symposium on Foundations of Computer Science, 1993, pages 400--409.\\
[Be] J. Beck. Roth's estimate on the discrepancy of integer sequences is nearly sharp.Combinatorica 1(4):319--325, 1981.\\
[BS] J. Beck and V. S{\'o}s. Discrepancy theory.Handbook of Combinatorics, North-Holland, Amsterdam, to appear.\\
[Ch] B. Chazelle. Geometric discrepancy revisited.Proc. 34th IEEE Symposium on Foundations of Computer Science, 1993, pages 392--399.\\
[CW] B. Chazelle and E. Welzl. Quasi-optimal range searching in spaces of finite VC-dimension.Discrete Comput. Geom., 4:467--489, 1989.\\
[Ed] H. Edelsbrunner.Algorithms in Combinatorial Geometry. EATCS Monographs on Theoretical Computer Science, Vol. 10. Springer-Verlag, Heidelberg, 1987.\\
[Ha] D. Haussler. Sphere packing numbers for subsets of the Booleann-cube with bounded Vapnik-Chervonenkis dimension. Tech. Report UCSC-CRL-91-41, University of California at Santa Cruz, 1991. Also to appear inJ. Combin. Theory Ser. A.\\
[HW] D. Haussler and E. Welzl. Epsilon-nets and simplex range queries.Discrete Comput. Geom., 2:127--151, 1987.\\
[Ma] J. Matou{\v s}ek. Geometric range searching.ACM Comput. Surrveys, to appear.\\
[Mu] K. Mulmuley.Computational Geometry: An Introduction Through Randomized Algorithms. Prentice-Hall, New York, 1993.\\
[MWW] J. Matou{\v s}ek, E. Welzl, and L. Wernisch. Discrepancy and {$\varepsilon$}-approximations for bounded VC-dimension.Combinatorica, 13:455--466, 1993.\\
[Sp] J. Spencer. Six standard deviations suffice.Trans. Amer. Math. Soc., 289:679--706, 1985.\\
[VC] V.N. Vapnik and A.Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities.Theory Probab. Appl., 16:264--280, 1971.\\
[We] L. Wernisch. Note on stabbing numbers and sphere packing. Manuscript, Computer Science Institute, Free University Berlin, 1992.},
  file = {/Users/tulasi/Zotero/storage/6Z6LQMDT/Matousek - 1995 - Tight upper bounds for the discrepancy of half-spaces.pdf}
}

@article{mazumdarOnChebyshevRadius2013,
  title = {On {{Chebyshev}} Radius of a Set in {{Hamming}} Space and the Closest String Problem},
  author = {Mazumdar, A. and Polyanskiy, Yury and Saha, B.},
  year = {2013},
  doi = {10.1109/ISIT.2013.6620457},
  abstract = {The Chebyshev radius of a set in a metric space is defined to be the radius of the smallest ball containing the set. This quantity is closely related to the covering radius of the set and, in particular for Hamming set, is extensively studied in computational biology. This paper investigates some basic properties of radii of sets in n-dimensional Hamming space, provides a linear programing relaxation and gives tight bounds on the integrality gap. This results in a simple polynomial-time approximation algorithm that attains the performance of the best known such algorithms with shorter running time.},
  citationcount = {11},
  venue = {2013 IEEE International Symposium on Information Theory}
}

@article{mcmullenItProbablyWorks2015,
  title = {It Probably Works},
  author = {McMullen, Tyler},
  year = {2015},
  doi = {10.1145/2838344.2855183},
  abstract = {Probabilistic algorithms exist to solve problems that are either impossible or unrealistic (too expensive, too time-consuming, etc.) to solve precisely. In an ideal world, you would never actually need to use probabilistic algorithms. To programmers who are not familiar with them, the idea can be positively nervewracking: "How do I know that it will actually work? What if it's inexplicably wrong? How can I debug it? Maybe we should just punt on this problem, or buy a whole lot more servers..."},
  citationcount = {Unknown},
  venue = {Queue}
}

@article{md.mostofaalipatwaryExperimentsUnionFindAlgorithms2010,
  title = {Experiments on {{Union-Find Algorithms}} for the {{Disjoint-Set Data Structure}}},
  author = {Md. Mostofa Ali Patwary and J. Blair and F. Manne},
  year = {2010},
  journal = {The Sea},
  doi = {10.1007/978-3-642-13193-6_35},
  keywords = {data structure},
  annotation = {Citation Count: 56}
}

@article{md.mostofaalipatwaryNewScalableParallel2012,
  title = {A New Scalable Parallel {{DBSCAN}} Algorithm Using the Disjoint-Set Data Structure},
  author = {Md. Mostofa Ali Patwary and Diana Palsetia and Ankit Agrawal and W. Liao and F. Manne and A. Choudhary},
  year = {2012},
  journal = {International Conference for High Performance Computing, Networking, Storage and Analysis},
  doi = {10.1109/SC.2012.9},
  abstract = {DBSCAN is a well-known density based clustering algorithm capable of discovering arbitrary shaped clusters and eliminating noise data. However, parallelization of DBSCAN is challenging as it exhibits an inherent sequential data access order. Moreover, existing parallel implementations adopt a master-slave strategy which can easily cause an unbalanced workload and hence result in low parallel efficiency. We present a new parallel DBSCAN algorithm (PDSDBSCAN) using graph algorithmic concepts. More specifically, we employ the disjoint-set data structure to break the access sequentiality of DBSCAN. In addition, we use a tree-based bottom-up approach to construct the clusters. This yields a better-balanced workload distribution. We implement the algorithm both for shared and for distributed memory. Using data sets containing up to several hundred million high-dimensional points, we show that PDSDBSCAN significantly outperforms the master-slave approach, achieving speedups up to 25.97 using 40 cores on shared memory architecture, and speedups up to 5,765 using 8,192 cores on distributed memory architecture.},
  keywords = {data structure},
  annotation = {Citation Count: 178}
}

@article{md.mostofaalipatwaryPardicleParallelApproximate2014,
  title = {Pardicle: {{Parallel Approximate Density-Based Clustering}}},
  author = {Md. Mostofa Ali Patwary and N. Satish and N. Sundaram and F. Manne and S. Habib and P. Dubey},
  year = {2014},
  journal = {International Conference for High Performance Computing, Networking, Storage and Analysis},
  doi = {10.1109/SC.2014.51},
  abstract = {DBSCAN is a widely used is density-based clustering algorithm for particle data well-known for its ability to isolate arbitrarily-shaped clusters and to filter noise data. The algorithm is super-linear (O(nlogn)) and computationally expensive for large datasets. Given the need for speed, we propose a fast heuristic algorithm for DBSCAN using density based sampling, which performs equally well in quality compared to exact algorithms, but is more than an order of magnitude faster. Our experiments on astrophysics and synthetic massive datasets (8.5 billion numbers) shows that our approximate algorithm is up to 56{\texttimes} faster than exact algorithms with almost identical quality (Omega-Index {$\geq$} 0.99). We develop a new parallel DBSCAN algorithm, which uses dynamic partitioning to improve load balancing and locality. We demonstrate near-linear speedup on shared memory (15{\texttimes} using 16 cores, single node Intel{\textregistered} Xeon{\textregistered} processor) and distributed memory (3917{\texttimes} using 4096 cores, multinode) computers, with 2{\texttimes} additional performance improvement using Intel{\textregistered} Xeon Phi coprocessors. Additionally, existing exact algorithms can achieve up to 3.4 times speedup using dynamic partitioning.},
  keywords = {dynamic},
  annotation = {Citation Count: 37}
}

@article{md.mostofaalipatwaryScalableParallelOPTICS2013,
  title = {Scalable Parallel {{OPTICS}} Data Clustering Using Graph Algorithmic Techniques},
  author = {Md. Mostofa Ali Patwary and Diana Palsetia and Ankit Agrawal and W. Liao and F. Manne and A. Choudhary},
  year = {2013},
  journal = {2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
  doi = {10.1145/2503210.2503255},
  abstract = {OPTICS is a hierarchical density-based data clustering algorithm that discovers arbitrary-shaped clusters and eliminates noise using adjustable reachability distance thresholds. Parallelizing OPTICS is considered challenging as the algorithm exhibits a strongly sequential data access order. We present a scalable parallel OPTICS algorithm (POPTICS) designed using graph algorithmic concepts. To break the data access sequentiality, POPTICS exploits the similarities between the OPTICS algorithm and PRIM's Minimum Spanning Tree algorithm. Additionally, we use the disjoint-set data structure to achieve a high parallelism for distributed cluster extraction. Using high dimensional datasets containing up to a billion floating point numbers, we show scalable speedups of up to 27.5 for our OpenMP implementation on a 40-core shared-memory machine, and up to 3,008 for our MPI implementation on a 4,096-core distributed-memory machine. We also show that the quality of the results given by POPTICS is comparable to those given by the classical OPTICS algorithm.},
  keywords = {data structure},
  annotation = {Citation Count: 51}
}

@article{mehlhornDataStructuresAnd2012,
  title = {Data Structures and Algorithms 3},
  author = {Mehlhorn, K.},
  year = {2012},
  doi = {10.1007/978-3-642-69900-9},
  abstract = {No abstract available},
  citationcount = {319},
  venue = {EATCS Monographs on Theoretical Computer Science}
}

@article{mehlhornLasVegasIs1982,
  title = {Las {{Vegas}} Is Better than Determinism in {{VLSI}} and Distributed Computing ({{Extended Abstract}})},
  author = {Mehlhorn, K. and Schmidt, E. M.},
  year = {1982},
  doi = {10.1145/800070.802208},
  abstract = {In this paper we describe a new method for proving lower bounds on the complexity of VLSI - computations and more generally distributed computations. Lipton and Sedgewick observed that the crossing sequence arguments used to prove lower bounds in VLSI (or TM or distributed computing) apply to (accepting) nondeterministic computations as well as to deterministic computations. Hence whenever a boolean function f is such that f and -\&-fmarc; (the complement of f, -\&-fmarc; -\&-equil; 1 -\&-minus; f) have efficient nondeterministic chips then the known techniques are of no help for proving lower bounds on the complexity of deterministic chips. In this paper we describe a lower bound technique (Thm 1) which only applies to deterministic computations},
  citationcount = {250},
  venue = {Symposium on the Theory of Computing}
}

@article{meirLiftingWithSimple2020,
  title = {Lifting with Simple Gadgets and Applications to Circuit and Proof Complexity},
  author = {Meir, Or and Nordstr{\"o}m, Jakob and Pitassi, T. and Robere, Robert and {de Rezende}, Susanna F.},
  year = {2020},
  doi = {10.1109/FOCS46700.2020.00011},
  abstract = {We significantly strengthen and generalize the theorem lifting Nullstellensatz degree to monotone span program size by Pitassi and Robere (2018) so that it works for any gadget with high enough rank, in particular, for useful gadgets such as equality and greater-than. We apply our generalized theorem to solve three open problems: {$\bullet$}We present the first result that demonstrates a separation in proof power for cutting planes with unbounded versus polynomially bounded coefficients. Specifically, we exhibit CNF formulas that can be refuted in quadratic length and constant line space in cutting planes with unbounded coefficients, but for which there are no refutations in subexponential length and subpolynomial line space if coefficients are restricted to be of polynomial magnitude. {$\bullet$}We give the first explicit separation between monotone Boolean formulas and monotone real formulas. Specifically, we give an explicit family of functions that can be computed with monotone real formulas of nearly linear size but require monotone Boolean formulas of exponential size. Previously only a non-explicit separation was known. {$\bullet$}We give the strongest separation to-date between monotone Boolean formulas and monotone Boolean circuits. Namely, we show that the classical GEN problem, which has polynomial-size monotone Boolean circuits, requires monotone Boolean formulas of size 2\textsuperscript{\{\vphantom\}}{\textohm}(n/\{polylog\}(n))\vphantom\{\}. An important technical ingredient, which may be of independent interest, is that we show that the Nullstellensatz degree of refuting the pebbling formula over a DAG G over any field coincides exactly with the reversible pebbling price of G. In particular, this implies that the standard decision tree complexity and the parity decision tree complexity of the corresponding falsified clause search problem are equal. This is an extended abstract. The full version of the paper is available at https://arxiv.org/abs/2001.02144.},
  citationcount = {25},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{meiserPointLocationIn1993,
  title = {Point Location in Arrangements of Hyperplanes},
  author = {Meiser, S.},
  year = {1993},
  doi = {10.1006/INCO.1993.1057},
  abstract = {We present a solution to the point location problem in arrangements of hyperplanes in Ed with running time O(d5 log n) and space O(nd+?) for arbitrary ? {\textquestiondown} 0, where n is the number of hyperplanes. The main result is the d5 factor in the expression for the running time. All previously known algorithms are exponential in d or log n. This leads to nonuniform polynomial algorithms for NP-complete problems.},
  citationcount = {177},
  venue = {Information and Computation}
}

@article{mengDataStructuresFor2016,
  title = {Data Structures for Path Queries},
  author = {Meng, {\relax HE} and University, Dalhousie and Munro, J. I. and Zhou, Gelin},
  year = {2016},
  doi = {10.1145/2905368},
  abstract = {Consider a tree T on n nodes, each having a weight drawn from [1..{$\sigma$}]. In this article, we study the problem of supporting various path queries over the tree T. The path counting query asks for the number of the nodes on a query path whose weights are in a query range, while the path reporting query requires to report these nodes. The path median query asks for the median weight on a path between two given nodes, and the path selection query returns the k-th smallest weight. We design succinct data structures to encode T using n nH(WT) + 2n + o(nlg {$\sigma$}) bits of space, such that we can support path counting queries in O(lg {$\sigma$}/lg lg n + 1)) time, path reporting queries in O((occ+1)(lg {$\sigma$} / lg lg n + 1)) time, and path median and path selection queries in O(lg {$\sigma$} / lg lg {$\sigma$}) time, where H(WT) is the entropy of the multiset of the weights of the nodes in T and occ is the size of the output. Our results not only greatly improve the best known data structures [Chazelle 1987; Krizanc et al. 2005], but also match the lower bounds for path counting, median, and selection queries [P{\u a}tra{\c s}cu 2007, 2011; J{\o}rgensen and Larsen 2011] when {$\sigma$} = {\textohm}(n/polylog(n)).},
  citationcount = {9},
  venue = {ACM Trans. Algorithms},
  keywords = {data structure,lower bound,query}
}

@article{menghePathQueriesWeighted2011,
  title = {Path {{Queries}} in {{Weighted Trees}}},
  author = {Meng He and J. Munro and Gelin Zhou},
  year = {2011},
  journal = {International Symposium on Algorithms and Computation},
  doi = {10.1007/978-3-642-25591-5_16},
  keywords = {query},
  annotation = {Citation Count: 15}
}

@article{meyerTheEquivalenceProblem1972,
  title = {The Equivalence Problem for Regular Expressions with Squaring Requires Exponential Space},
  author = {Meyer, A. and Stockmeyer, L.},
  year = {1972},
  doi = {10.1109/SWAT.1972.29},
  abstract = {No abstract available},
  citationcount = {708},
  venue = {Scandinavian Workshop on Algorithm Theory}
}

@article{mienoShortestUniqueSubstring2016,
  title = {Shortest Unique Substring Queries on Run-Length Encoded Strings},
  author = {Mieno, Takuya and Inenaga, Shunsuke and Bannai, H. and Takeda, Masayuki},
  year = {2016},
  doi = {10.4230/LIPIcs.MFCS.2016.69},
  abstract = {We consider the problem of answering shortest unique substring (SUS) queries on run-length encoded strings. For a string S, a unique substring u = S[i..j] is said to be a shortest unique substring (SUS) of S containing an interval [s, t] (i j'-i', S[i'..j'] occurs at least twice in S. Given a run-length encoding of size m of a string of length N, we show that we can construct a data structure of size O(m+pi\_s(N, m)) in O(m log m + pi\_c(N, m)) time such that queries can be answered in O(pi\_q(N, m) + k) time, where k is the size of the output (the number of SUSs), and pi\_s(N,m), pi\_c(N,m), pi\_q(N,m) are, respectively, the size, construction time, and query time for a predecessor/successor query data structure of m elements for the universe of [1,N]. Using the data structure by Beam and Fich (JCSS 2002), this results in a data structure of O(m) space that is constructed in O(m log m) time, and answers queries in O(sqrt(log m/loglog m)+k) time.},
  citationcount = {18},
  venue = {International Symposium on Mathematical Foundations of Computer Science},
  keywords = {data structure,query,query time}
}

@article{miltersenBitProbeComplexity1992,
  title = {The Bit Probe Complexity Measure Revisited},
  author = {Miltersen, Peter Bro},
  year = {1992},
  doi = {10.1007/3-540-56503-5_65},
  abstract = {No abstract available},
  citationcount = {48},
  venue = {Symposium on Theoretical Aspects of Computer Science}
}

@inproceedings{miltersenBitProbeComplexity1993,
  title = {The Bit Probe Complexity Measure Revisited},
  booktitle = {Proc. 10th {{Symposium}} on {{Theoretical Aspects}} of {{Computer Science}} ({{STACS}})},
  author = {Miltersen, Peter Bro},
  year = {1993},
  pages = {662--671},
  doi = {10.1007/3-540-56503-5_65},
  keywords = {Communication Complexity,Feasible Problem,Good Lower Bound,Probe Complexity,Random Access Machine},
  annotation = {L. Adleman, Two theorems on random polynomial time, 19th Symp. Found. of Comp. Sci. (1978), 75--83.\\
M. Ajtai, A lower bound for finding predecessors in Yao's cell probe model, Combinatorica 8 (1988) 235--247.\\
L. Babai, P. Frankl, J. Simon, Complexity classes in communication complexity theory, Proc. 27th IEEE FOCS (1986) 337--347.\\
H. Chernoff, A measure of asymptotic efficiency for tests based on the sum of observations, Ann. Math. Statist. 23 (1952), 493--509.\\
P. Elias, R.A. Flower, The complexity of some simple retrieval problems, J. Ass. Comp. Mach. 22 (1975), 367--379.\\
T. Hagerup, C. R{\"u}b, A guided tour of Chernoff bounds, Inform. Proces. Lett. 33 (1990), 305--308.\\
L. Lov{\'a}sz, Communication complexity: A survey, in ``Paths, Flows, and VLSI Layout'', edited by B.H. Korte, Springer Verlag, Berlin New York (1990).\\
K. Mehlhorn, E.M. Schmidt, Las Vegas is better than determinism in VLSI and distributed computing, Proc. 14th ACM STOC (1982) 330--337.\\
A.R. Meyer, L. Stockmeyer, The equivalence problem for regular expressions with squaring requires exponential space, IEEE 13th Annual Symposium on Switching and Automata Theory (1972), 125--129.\\
P.B. Miltersen, On-line reevaluation of functions, Aarhus University Tech. Report DAIMI PB-380.\\
S. Sairam, J.S. Vitter, R. Tamassia, A complexity theoretic approach to incremental computation, these proceedings.\\
A. C.-C. Yao, Some complexity questions related to distributive computing, Proc. 11th ACM STOC (1979) 209--213.\\
A. C.-C. Yao, Should tables be sorted?, JACM 28 (1981), 615--628.},
  file = {/Users/tulasi/Zotero/storage/8WCF2T5Z/Miltersen - 1993 - The bit probe complexity measure revisited.pdf}
}

@article{miltersenCellProbeComplexity1999,
  title = {Cell Probe Complexity - a Survey},
  author = {Miltersen, Peter Bro},
  year = {1999},
  journal = {Lecture Notes in Computer Science},
  doi = {10.1007/978-3-540-70575-8_7},
  abstract = {The cell probe model is a general, combinatorial model of data structures. We give a survey of known results about the cell probe complexity of static and dynamic data structure problems, with an emphasis on techniques for proving lower bounds.},
  annotation = {Barkol, O., Rabani, Y.: Tighter lower bounds for nearest neighbor search and related problems in the cell probe model. Journal of Computer and System Sciences~64(4), 873--896 (2002)\\
Beame, P., Fich, F.: Optimal bounds for the predecessor problem and related problems. Journal of Computer and System Sciences~65(1), 38--72 (2002)\\
Borodin, A., Ostrovsky, R., Rabani, Y.: Lower bounds for high dimensional nearest neighbor search and related problems. In: Proceedings of the thirty-first annual ACM Symposium on Theory of Computing, pp. 312--321 (1999)\\
Buhrman, H., de Wolf, R.: Complexity measures and decision tree complexity: a survey. Theoretical Computer Science~288(1), 21--43 (2002)\\
Demaine, E., P{\u a}tra{\c s}cu, M.: Logarithmic lower bounds in the cell-probe model. SIAM Journal of Computing~35(4), 932--963 (2006)\\
Fredman, M., Saks, M.: The cell probe complexity of dynamic data structures. In: Proceedings of the twenty-first annual ACM Symposium on Theory of Computing, pp. 345--354 (1989)\\
Husfeldt, T., Rauhe, T.: Hardness results for dynamic problems by extensions of Fredman and Saks' chronogram method. In: Proceedings of the 25th International Colloquium on Automata, Languages and Programming, pp. 67--78 (1998)\\
Indyk, P., Goodman, J., O'Rourke, J.: Nearest neighbors in high-dimensional spaces. In: Handbook of Discrete and Computational Geometry, ch. 39 (2004)\\
Jayram, T., Khot, S., Kumar, R., Rabani, Y.: Cell-probe lower bounds for the partial match problem. Journal of Computer and System Sciences~69(3), 435--447 (2004)\\
Miltersen, P., Nisan, N., Safra, S., Wigderson, A.: On data structures and asymmetric communication complexity. Journal of Computer and System Sciences~57(1), 37--49 (1998)\\
Yao, A.: Should tables be sorted? Journal of the ACM~28(3), 615--628 (1981)\\
Barkol, O., Rabani, Y.: Tighter lower bounds for nearest neighbor search and related problems in the cell probe model. Journal of Computer and System Sciences~64(4), 873--896 (2002)\\
Beame, P., Fich, F.: Optimal bounds for the predecessor problem and related problems. Journal of Computer and System Sciences~65(1), 38--72 (2002)\\
Borodin, A., Ostrovsky, R., Rabani, Y.: Lower bounds for high dimensional nearest neighbor search and related problems. In: Proceedings of the thirty-first annual ACM Symposium on Theory of Computing, pp. 312--321 (1999)\\
Buhrman, H., de Wolf, R.: Complexity measures and decision tree complexity: a survey. Theoretical Computer Science~288(1), 21--43 (2002)\\
Demaine, E., P{\u a}tra{\c s}cu, M.: Logarithmic lower bounds in the cell-probe model. SIAM Journal of Computing~35(4), 932--963 (2006)\\
Fredman, M., Saks, M.: The cell probe complexity of dynamic data structures. In: Proceedings of the twenty-first annual ACM Symposium on Theory of Computing, pp. 345--354 (1989)\\
Husfeldt, T., Rauhe, T.: Hardness results for dynamic problems by extensions of Fredman and Saks' chronogram method. In: Proceedings of the 25th International Colloquium on Automata, Languages and Programming, pp. 67--78 (1998)\\
Indyk, P., Goodman, J., O'Rourke, J.: Nearest neighbors in high-dimensional spaces. In: Handbook of Discrete and Computational Geometry, ch. 39 (2004)\\
Jayram, T., Khot, S., Kumar, R., Rabani, Y.: Cell-probe lower bounds for the partial match problem. Journal of Computer and System Sciences~69(3), 435--447 (2004)\\
Miltersen, P., Nisan, N., Safra, S., Wigderson, A.: On data structures and asymmetric communication complexity. Journal of Computer and System Sciences~57(1), 37--49 (1998)\\
Yao, A.: Should tables be sorted? Journal of the ACM~28(3), 615--628 (1981)},
  file = {/Users/tulasi/Zotero/storage/L5Q985RF/Miltersen - 1999 - Cell probe complexity - a survey.pdf}
}

@article{miltersenComplexityModelsIncremental1994,
  title = {Complexity Models for Incremental Computation},
  author = {Miltersen, Peter Bro and Subramanian, Sairam and Vitter, J. and Tamassia, R.},
  year = {1994},
  doi = {10.1016/0304-3975(94)90159-7},
  abstract = {No abstract available},
  citationcount = {114},
  venue = {Theoretical Computer Science}
}

@inproceedings{miltersenDataStructuresAsymmetric1995,
  title = {On Data Structures and Asymmetric Communication Complexity},
  booktitle = {Proceedings of the Twenty-Seventh Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Miltersen, Peter Bro and Nisan, Noam and Safra, Shmuel and Wigderson, Avi},
  year = {1995},
  month = may,
  series = {{{STOC}} '95},
  pages = {103--111},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/225058.225093},
  url = {https://dl.acm.org/doi/10.1145/225058.225093},
  urldate = {2025-04-10},
  isbn = {978-0-89791-718-6},
  keywords = {cell probe,communication,communication complexity,data structure,lower bound,reduction,static},
  file = {/Users/tulasi/Zotero/storage/9JN6LXPU/Miltersen et al. - 1995 - On data structures and asymmetric communication complexity.pdf}
}

@article{miltersenDataStructuresAsymmetric1998,
  title = {On {{Data Structures}} and {{Asymmetric Communication Complexity}}},
  author = {Miltersen, Peter Bro and Nisan, Noam and Safra, Shmuel and Wigderson, Avi},
  year = {1998},
  month = aug,
  journal = {Journal of Computer and System Sciences},
  volume = {57},
  number = {1},
  pages = {37--49},
  issn = {0022-0000},
  doi = {10.1006/jcss.1998.1577},
  url = {https://www.sciencedirect.com/science/article/pii/S002200009891577X},
  urldate = {2024-11-18},
  abstract = {In this paper we consider two-party communication complexity, the ``asymmetric case'', when the input sizes of the two players differ significantly. Most of previous work on communication complexity only considers the total number of bits sent, but we study trade-offs between the number of bits the first player sends and the number of bits the second sends. These types of questions are closely related to the complexity of static data structure problems in the cell probe model. We derive two generally applicable methods of proving lower bounds and obtain several applications. These applications include new lower bounds for data structures in the cell probe model. Of particular interest is our ``round elimination'' lemma, which is interesting also for the usual symmetric communication case. This lemma generalizes and abstracts in a very clean form the ``round reduction'' techniques used in many previous lower bound proofs.},
  keywords = {cell probe,communication,communication complexity,data structure,lower bound,notion,reduction,static},
  annotation = {A. Andersson, Sublogarithmic searching without multiplications, Proc. FOCS 95\\
\\
\\
P. Beame, F. Fich},
  file = {/Users/tulasi/Zotero/storage/MXGHEHJ7/Miltersen et al. - 1998 - On data struc tures and asymmetric communication complexity.pdf}
}

@article{miltersenErrorCorrectingCodes1997,
  title = {Error Correcting Codes, Perfect Hashing Circuits, and Deterministic Dynamic Dictionaries},
  author = {Miltersen, Peter Bro},
  year = {1997},
  doi = {10.7146/BRICS.V4I17.18813},
  abstract = {We consider dictionaries of size n over the finite universe U = \{0, 1\} and introduce a new technique for their implementation: error correcting codes. The use of such codes makes it possible to replace the use of strong forms of hashing, such as universal hashing, with much weaker forms, such as clustering. We use our approach to construct, for any {\textquestiondown} 0, a deterministic solution to the dynamic dictionary problem using linear space, with worst case time O(n ) for insertions and deletions, and worst case time O(1) for lookups. This is the first deterministic solution to the dynamic dictionary problem with linear space, constant query time, and non-trivial update time. In particular, we get a solution to the static dictionary problem with O(n) space, worst case query time O(1), and deterministic initialization time O(n1+ ). The best previous deterministic initialization time for such dictionaries, due to Andersson, is O(n2+ ). The model of computation for these bounds is a unit cost {$\ast$}Supported by the ESPRIT Long Term Research Programme of the EU under project number 20244 (ALCOM-IT). {\dag}Basic Research in Computer Science, Centre of the Danish National Research Foundation},
  citationcount = {38},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,query,query time,static,update,update time}
}

@inproceedings{miltersenLowerBoundsUnionSplitFind1994,
  title = {Lower Bounds for {{Union-Split-Find}} Related Problems on Random Access Machines},
  booktitle = {Proc. 26th {{ACM Symposium}} on {{Theory}} of {{Computing}} ({{STOC}})},
  author = {Miltersen, Peter Bro},
  year = {1994},
  pages = {625--634},
  doi = {10.1145/195058.195415},
  abstract = {We prove {\textohm}({\textbackslash}sqrt\{{\textbackslash}log {\textbackslash}log n\}) lower bounds on the random access machine complexity of several dynamic, partially dynamic and static data structure problems, including the Union-Split-Find problem, dynamic prefix problems and one-dimensional range query problems. The proof techniques include a general technique using perfect hashing for reducing static data structure problems (with a restriction of the size of the structure) into partially dynamic data structure problems (with no such restriction), thus providing a way to transfer lower bounds. We use a generalization of a method due to Ajtai for proving the lower bounds on the static problems, but describe the proof in terms of communication complexity, revealing a striking similarity to the proof used by Karchmer and Wigderson for proving lower bounds on the monotone circuit depth of connectivity.},
  keywords = {communication,communication complexity,data structure,dynamic,lower bound,query,static},
  file = {/Users/tulasi/Zotero/storage/XULDDGSK/Miltersen - 1994 - Lower bounds for union-split-find related problems on random access machines.pdf}
}

@article{miltersenOnConvertingCnf2003,
  title = {On Converting {{CNF}} to {{DNF}}},
  author = {Miltersen, Peter Bro and Radhakrishnan, J. and Wegener, I.},
  year = {2003},
  doi = {10.1016/j.tcs.2005.07.029},
  abstract = {No abstract available},
  citationcount = {45},
  venue = {Theoretical Computer Science}
}

@article{miltersenOnlineReevaluationFunctions1992,
  title = {On-Line Reevaluation of Functions},
  author = {Miltersen, Peter Bro},
  year = {1992},
  doi = {10.7146/DPB.V21I380.6612},
  abstract = {Given a finite set S and a function f : S{\textasciicircum}n -{\textquestiondown} S{\textasciicircum}m, we consider the problem of making a data structure which maintains a value of x in S{\textasciicircum}n and allows us to efficiently change an arbitrary coordinate of x and efficiently evaluate f\_i(x) for arbitrary i. We both examine the problem for specific choices of f and relate the possibility of an efficient solution to general properties of f: expressibility as a formula, space complexity and time complexity.},
  citationcount = {8},
  venue = {No venue available},
  keywords = {data structure}
}

@article{missing-valueInformation1992,
  title = {Information},
  author = {{MISSING-VALUE}, MISSING-VALUE},
  year = {1992},
  doi = {10.1007/s00062-019-00765-5},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {British Journal of Music Education}
}

@article{mitaniFindingTopK2022,
  title = {Finding Top-k Longest Palindromes in Substrings},
  author = {Mitani, Kazuki and Mieno, Takuya and Seto, Kazuhisa and Horiyama, T.},
  year = {2022},
  doi = {10.1016/j.tcs.2023.114183},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Theoretical Computer Science}
}

@article{mitaniInternalLongestPalindrome2023,
  title = {Internal Longest Palindrome Queries in Optimal Time},
  author = {Mitani, Kazuki and Mieno, Takuya and Seto, Kazuhisa and Horiyama, T.},
  year = {2023},
  doi = {10.48550/arXiv.2210.02000},
  abstract = {. Palindromes are strings that read the same forward and backward. Problems of computing palindromic structures in strings have been studied for many years with a motivation of their application to biology. The longest palindrome problem is one of the most important and classical problems regarding palindromic structures, that is, to compute the longest palindrome appearing in a string T of length n . The problem can be solved in O ( n ) time by the famous algorithm of Man-acher [Journal of the ACM, 1975]. In this paper, we consider the problem in the internal model. The internal longest palindrome query is, given a substring T [ i..j ] of T as a query, to compute the longest palindrome appearing in T [ i..j ] . The best known data structure for this problem is the one proposed by Amir et al. [Algorithmica, 2020], which can answer any query in O (log n ) time. In this paper, we propose a linear-size data structure that can answer any internal longest palindrome query in constant time. Also, given the input string T , our data structure can be constructed in O ( n ) time.},
  citationcount = {3},
  venue = {Workshop on Algorithms and Computation},
  keywords = {data structure,query}
}

@article{mitchellThoughtRecognitionPredicting2011,
  title = {Thought Recognition: {{Predicting}} and Decoding Brain Activity Using the Zero-Shot Learning Model},
  author = {Mitchell, Tom Michael and Palatucci, Mark},
  year = {2011},
  doi = {10.1184/r1/6723869.v1},
  abstract = {Machine learning algorithms have been successfully applied to learning classifiers in many domains such as computer vision, fraud detection, and brain image analysis. Typically, classifiers are trained to predict a class value given a set of labeled training data that includes all possible class values, and sometimes additional unlabeled training data. Little research has been performed where the possible values for the class variable include values that have been omitted from the training examples. This is an important problem setting, especially in domains where the class value can take on many values, and the cost of obtaining labeled examples for all values is high. We show that the key to addressing this problem is not predicting the held-out classes directly, but rather by recognizing the semantic properties of the classes such as their physical or functional attributes. We formalize this method as zero-shot learning and show that by utilizing semantic knowledge mined from large text corpora and crowd-sourced humans, we can discriminate classes without explicitly collecting examples of those classes for a training set. As a case study, we consider this problem in the context of thought recognition, where the goal is to classify the pattern of brain activity observed from a non-invasive neural recording device. Specifically, we train classifiers to predict a specific concrete noun that a person is thinking about based on an observed image of that person's neural activity. We show that by predicting the semantic properties of the nouns such as "is it heavy?" and "is it edible?", we can discriminate concrete nouns that people are thinking about, even without explicitly collecting examples of those nouns for a training set. Further, this allows discrimination of certain nouns that are within the same category with significantly higher accuracies than previous work. In addition to being an important step forward for neural imaging and brain-computer-interfaces, we show that the zero-shot learning model has important implications for the broader machine learning community by providing a means for learning algorithms to extrapolate beyond their explicit training set.},
  citationcount = {8},
  venue = {No venue available}
}

@article{mitzenmacherDynamicAlgorithmsFor2020,
  title = {Dynamic Algorithms for {{LIS}} and Distance to Monotonicity},
  author = {Mitzenmacher, M. and Seddighin, Saeed},
  year = {2020},
  doi = {10.1145/3357713.3384240},
  abstract = {In this paper, we provide new approximation algorithms for dynamic variations of the longest increasing subsequence (LIS) problem, and the complementary distance to monotonicity (DTM) problem. In this setting, operations of the following form arrive sequentially: (i) add an element, (ii) remove an element, or (iii) substitute an element for another. At every point in time, the algorithm has an approximation to the longest increasing subsequence (or distance to monotonicity). We present a (1+{\cyrchar\cyrie})-approximation algorithm for DTM with polylogarithmic worst-case update time and a constant factor approximation algorithm for LIS with worst-case update time {\~O}(n {\cyrchar\cyrie}) for any constant {\cyrchar\cyrie} {\textquestiondown} 0.},
  citationcount = {13},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,update,update time}
}

@article{mollAnIntroductionTo1988,
  title = {An Introduction to Formal Language Theory},
  author = {Moll, R. and Arbib, M. and Kfoury, A.},
  year = {1988},
  doi = {10.1007/978-1-4613-9595-9},
  abstract = {No abstract available},
  citationcount = {814},
  venue = {Texts and Monographs in Computer Science}
}

@article{molloyTheProbabilisticMethod1998,
  title = {The Probabilistic Method},
  author = {Molloy, Michael},
  year = {1998},
  doi = {10.1007/978-3-662-12788-9_1},
  abstract = {No abstract available},
  citationcount = {2660},
  venue = {No venue available}
}

@article{moranGeneralizedLowerBounds1987,
  title = {Generalized Lower Bounds Derived from Hastad's Main Lemma},
  author = {Moran, S.},
  year = {1987},
  doi = {10.1016/0020-0190(87)90216-X},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {Information Processing Letters},
  keywords = {lower bound}
}

@article{morganAlgorithmsForThe2021,
  title = {Algorithms for the Minimum Dominating Set Problem in Bounded Arboricity Graphs: {{Simpler}}, Faster, and Combinatorial},
  author = {Morgan, Adir and Solomon, Shay and Wein, Nicole},
  year = {2021},
  doi = {10.4230/LIPIcs.DISC.2021.33},
  abstract = {We revisit the minimum dominating set problem on graphs with arboricity bounded by {$\alpha$}. Bansal and Umboh [BU17] gave an O({$\alpha$})-approximation LP rounding algorithm, which also translates into a near-linear time algorithm using general-purpose approximation results for explicit mixed packing and covering or pure covering LPs [KY14, You14, AZO19, Qua10]. Moreover, [BU17] showed that it is NP-hard to achieve an asymptotic improvement for the approximation factor. On the other hand, the previous two non-LP-based algorithms, by Lenzen and Wattenhofer [LW10], and Jones et al. [JLR+13], achieve an approximation factor of O({$\alpha^2$}) in linear time. There is a similar situation in the distributed setting: While there is an O({$^2$}n)-round LP-based O({$\alpha$})-approximation algorithm implied in [KMW06], the best non-LP-based algorithm by Lenzen and Wattenhofer [LW10] is an implementation of their centralized algorithm, providing an O({$\alpha^2$})-approximation within O(n) rounds. We address the questions of whether one can achieve an O({$\alpha$})-approximation algorithm that is not LP-based, either in the centralized setting or in the distributed setting. We resolve both questions in the affirmative, and en route achieve algorithms that are faster than the state-of-the-art LP-based algorithms. More specifically, our contribution is two-fold: 1. In the centralized setting, we provide a surprisingly simple combinatorial algorithm that is asymptotically optimal in terms of both approximation factor and running time: an O({$\alpha$})-approximation in linear time. 2. Based on our centralized algorithm, we design a distributed combinatorial O({$\alpha$})-approximation algorithm in the CONGEST model that runs in O({$\alpha$}n) rounds with high probability.},
  citationcount = {5},
  venue = {International Symposium on Distributed Computing}
}

@article{morgensternNoteOnA1973,
  title = {Note on a Lower Bound on the Linear Complexity of the Fast Fourier Transform},
  author = {Morgenstern, Jacques},
  year = {1973},
  doi = {10.1145/321752.321761},
  abstract = {A lower bound for the number of additions necessary to compute a family of linear functions by a linear algorithm is given when an upper bound {\textexclamdown}italic{\textquestiondown}c{\textexclamdown}/italic{\textquestiondown} can be assigned to the modulus of the complex numbers involved in the computation. In the case of the fast Fourier transform, the lower bound is ({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}/2) log{\textexclamdown}subscrpt{\textquestiondown}2{\textexclamdown}/subscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} when {\textexclamdown}italic{\textquestiondown}c{\textexclamdown}/italic{\textquestiondown} = 1.},
  citationcount = {120},
  venue = {JACM}
}

@article{morgensternTheLinearComplexity1975,
  title = {The Linear Complexity of Computation},
  author = {Morgenstern, Jacques},
  year = {1975},
  doi = {10.1145/321879.321881},
  abstract = {The notion of the linear algorithm to compute a family 7 of linear forms in r variables over a field is defined. Ways to save addltmns are investigated by analyzing the combinatorial aspects of linear dependences between subrows of a given matrix F. Further, an additive degree of freedom is defined, which turns out to be an exact measure of the complexity of computation of 7.},
  citationcount = {28},
  venue = {JACM}
}

@article{morozComputingTheDistance2011,
  title = {Computing the Distance between Piecewise-Linear Bivariate Functions},
  author = {Moroz, G. and Aronov, B.},
  year = {2011},
  doi = {10.1145/2847257},
  abstract = {We consider the problem of computing the distance between two piecewise-linear bivariate functions {\textexclamdown}i{\textquestiondown}f{\textexclamdown}/i{\textquestiondown} and {\textexclamdown}i{\textquestiondown}g{\textexclamdown}/i{\textquestiondown} defined over a common domain {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown}, induced by the {\textexclamdown}i{\textquestiondown}L{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}2{\textexclamdown}/sub{\textquestiondown} norm---that is, {\textbardbl} {\textexclamdown}i{\textquestiondown}f{\textexclamdown}/i{\textquestiondown} - {\textexclamdown}i{\textquestiondown}g{\textexclamdown}/i{\textquestiondown}{\textbardbl}2 = \&sqrt;\&int{\textexclamdown}i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}M{\textexclamdown}/sub{\textquestiondown}{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}f{\textexclamdown}/i{\textquestiondown} -- {\textexclamdown}i{\textquestiondown}g{\textexclamdown}/i{\textquestiondown}){\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}. If {\textexclamdown}i{\textquestiondown}f{\textexclamdown}/i{\textquestiondown} is defined by linear interpolation over a triangulation of {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown} with {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} triangles and {\textexclamdown}i{\textquestiondown}g{\textexclamdown}/i{\textquestiondown} is defined over another such triangulation, the obvious naive algorithm requires {$\Theta$}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}) arithmetic operations to compute this distance. We show that it is possible to compute it in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}log{\textexclamdown}sup{\textquestiondown}4{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}log log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) arithmetic operations by reducing the problem to multipoint evaluation of a certain type of polynomials. We also present several generalizations and an application to terrain matching.},
  citationcount = {5},
  venue = {TALG}
}

@article{mortensenDynamicRangeReporting2005,
  title = {On Dynamic Range Reporting in One Dimension},
  author = {Mortensen, C. and Pagh, R. and Patrascu, M.},
  year = {2005},
  doi = {10.1145/1060590.1060606},
  abstract = {We consider the problem of maintaining a dynamic set of integers and answering queries of the form: report a point (equivalently, all points) in a given interval. Range searching is a natural and fundamental variant of integer search, and can be solved using predecessor search. However, for a RAM with w-bit words, we show how to perform updates in O(lg w) time and answer queries in O(lg lg w) time. The update time is identical to the van Emde Boas structure, but the query time is exponentially faster. Existing lower bounds show that achieving our query time for predecessor search requires doubly-exponentially slower updates. We present some arguments supporting the conjecture that our solution is optimal.Our solution is based on a new and interesting recursion idea which is "more extreme" that the van Emde Boas recursion. Whereas van Emde Boas uses a simple recursion (repeated halving) on each path in a trie, we use a nontrivial, van Emde Boas-like recursion on every such path. Despite this, our algorithm is quite clean when seen from the right angle. To achieve linear space for our data structure, we solve a problem which is of independent interest. We develop the first scheme for dynamic perfect hashing requiring sublinear space. This gives a dynamic Bloomier filter (a storage scheme for sparse vectors) which uses low space. We strengthen previous lower bounds to show that these results are optimal.},
  citationcount = {69},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,dynamic,lower bound,query,query time,update,update time}
}

@article{mortensenFullyDynamicOrthogonal2006,
  title = {Fully Dynamic Orthogonal Range Reporting on {{RAM}}},
  author = {Mortensen, C.},
  year = {2006},
  doi = {10.1137/S0097539703436722},
  abstract = {We show that there exists a constant {$\omega<$}1 such that the fully dynamic d-dimensional orthogonal range reporting problem for any constant d{$\geq$}2 can be solved in time O(\textsuperscript{\{\vphantom\}}{$\omega$}+d-2\vphantom\{\}n) for updates and time O((n/n)\textsuperscript{\{\vphantom\}}d-1\vphantom\{\}+r) for queries. Here n is the number of points stored and r is the number of points reported. The space usage is O(n\textsuperscript{\{\vphantom\}}{$\omega$}+d-2\vphantom\{\}n). For d=2 our results are optimal in terms of time per operation, and this is the main contribution of this paper. Also for d=2, we give a new improved fully dynamic structure supporting 3-sided queries. The model of computation is a unit cost [email protected] We order the coordinates of points using list order as defined in the paper.},
  citationcount = {48},
  venue = {SIAM journal on computing (Print)}
}

@article{moserProblemsOnExtremal1985,
  title = {Problems on Extremal Properties of a Finite Set of Points},
  author = {Moser, W.},
  year = {1985},
  doi = {10.1111/j.1749-6632.1985.tb14538.x},
  abstract = {In 1977 I started a collection of problems in discrete geometry, distributing it and the subsequent revised versions to anyone who requested a copy. The most recent edition, ``Research Problems in Discrete Geometry, 1981'' (RPDG 1981) contains 68 problems; here I focus on problems from this collection that investigate extremal properties of a finite set of points. A revised version of each problem is given here to include whatever new information has become available. The designation number (\# n) locates the problem in RPDG 1981.},
  citationcount = {14},
  venue = {No venue available}
}

@article{mosesganardiCompressionContractingStraightLine2021,
  title = {Compression by {{Contracting Straight-Line Programs}}},
  author = {Moses Ganardi},
  year = {2021},
  journal = {Embedded Systems and Applications},
  doi = {10.4230/LIPIcs.ESA.2021.45},
  abstract = {In grammar-based compression a string is represented by a context-free grammar, also called a straight-line program (SLP), that generates only that string. We refine a recent balancing result stating that one can transform an SLP of size \$g\$ in linear time into an equivalent SLP of size \$O(g)\$ so that the height of the unique derivation tree is \$O({\textbackslash}log N)\$ where \$N\$ is the length of the represented string (FOCS 2019). We introduce a new class of balanced SLPs, called contracting SLPs, where for every rule \$A {\textbackslash}to {\textbackslash}beta\_1 {\textbackslash}dots {\textbackslash}beta\_k\$ the string length of every variable \${\textbackslash}beta\_i\$ on the right-hand side is smaller by a constant factor than the string length of \$A\$. In particular, the derivation tree of a contracting SLP has the property that every subtree has logarithmic height in its leaf size. We show that a given SLP of size \$g\$ can be transformed in linear time into an equivalent contracting SLP of size \$O(g)\$ with rules of constant length. We present an application to the navigation problem in compressed unranked trees, represented by forest straight-line programs (FSLPs). We extend a linear space data structure by Reh and Sieber (2020) by the operation of moving to the \$i\$-th child in time \$O({\textbackslash}log d)\$ where \$d\$ is the degree of the current node. Contracting SLPs are also applied to the finger search problem over SLP-compressed strings where one wants to access positions near to a pre-specified finger position, ideally in \$O({\textbackslash}log d)\$ time where \$d\$ is the distance between the accessed position and the finger. We give a linear space solution where one can access symbols or move the finger in time \$O({\textbackslash}log d + {\textbackslash}log{\textasciicircum}\{(t)\} N)\$ for any constant \$t\$ where \${\textbackslash}log{\textasciicircum}\{(t)\} N\$ is the \$t\$-fold logarithm of \$N\$. This improves a previous solution by Bille, Christiansen, Cording, and G\{{\textbackslash}o\}rtz (2018) with access/move time \$O({\textbackslash}log d + {\textbackslash}log {\textbackslash}log N)\$.},
  keywords = {data structure},
  annotation = {Citation Count: 7}
}

@article{moshikhershcovitchEfficientDynamicData2008,
  title = {I/{{O Efficient Dynamic Data Structures}} for {{Longest Prefix Queries}}},
  author = {Moshik Hershcovitch and Haim Kaplan},
  year = {2008},
  journal = {Algorithmica},
  doi = {10.1007/s00453-011-9594-2},
  keywords = {data structure,dynamic,query},
  annotation = {Citation Count: 1}
}

@article{mosselOnTheNoise2003,
  title = {On the Noise Sensitivity of Monotone Functions},
  author = {Mossel, Elchanan and O'Donnell, R.},
  year = {2003},
  doi = {10.1002/rsa.10097},
  abstract = {It is known that for all monotone functions f : \{0, 1\}n {$\rightarrow$} \{0, 1\}, if x {$\in$} \{0, 1\}n is chosen uniformly at random and y is obtained from x by flipping each of the bits of x independently with probability {$\epsilon$} = n-{$\alpha$}, then P[f(x) {$\neq$} f(y)] {\textexclamdown} cn-{$\alpha$}+1/2, for some c {\textquestiondown} 0.},
  citationcount = {59},
  venue = {Random Struct. Algorithms}
}

@article{motwaniLowerBoundsOn2005,
  title = {Lower Bounds on Locality Sensitive Hashing},
  author = {Motwani, R. and Naor, A. and Panigrahy, R.},
  year = {2005},
  doi = {10.1145/1137856.1137881},
  abstract = {Given a metric space {\textexclamdown}i{\textquestiondown}(X,d{\textexclamdown}sub{\textquestiondown}X{\textexclamdown}/sub{\textquestiondown}){\textexclamdown}/i{\textquestiondown}, {\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown}{$\geq$}1, {\textexclamdown}i{\textquestiondown}r{\textexclamdown}/i>>0, and {\textexclamdown}i{\textquestiondown}p,q{\textexclamdown}/i{\textquestiondown} {$\equiv$} [0,1], a distribution over mappings {\textexclamdown}i{\textquestiondown}H{\textexclamdown}/i{\textquestiondown} : {\textexclamdown}i{\textquestiondown}X{\textexclamdown}/i{\textquestiondown} {$\rightarrow$} {\textexclamdown}i{\textquestiondown}N{\textexclamdown}/i{\textquestiondown} is called a {\textexclamdown}i{\textquestiondown}(r,cr,p,q){\textexclamdown}/i{\textquestiondown}-sensitive hash family if any two points in {\textexclamdown}i{\textquestiondown}X{\textexclamdown}/i{\textquestiondown} at distance at most {\textexclamdown}i{\textquestiondown}r{\textexclamdown}/i{\textquestiondown} are mapped by {\textexclamdown}i{\textquestiondown}H{\textexclamdown}/i{\textquestiondown} to the same value with probability at least {\textexclamdown}i{\textquestiondown}p{\textexclamdown}/i{\textquestiondown}, and any two points at distance greater than {\textexclamdown}i{\textquestiondown}cr{\textexclamdown}/i{\textquestiondown} are mapped by {\textexclamdown}i{\textquestiondown}H{\textexclamdown}/i{\textquestiondown} to the same value with probability at most {\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown}. This notion was introduced by Indyk and Motwani in 1998 as the basis for an efficient approximate nearest neighbor search algorithm, and has since been used extensively for this purpose. The performance of these algorithms is governed by the parameter {$\supseteq$}=log(1/{\textexclamdown}i{\textquestiondown}p{\textexclamdown}/i{\textquestiondown})/log(1/{\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown}), and constructing hash families with small {$\supseteq$} automatically yields improved nearest neighbor algorithms. Here we show that for {\textexclamdown}i{\textquestiondown}X{\textexclamdown}/i{\textquestiondown}={\textexclamdown}i{\textquestiondown}l{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}1{\textexclamdown}/sub{\textquestiondown} it is impossible to achieve {$\supseteq$} {$\leq$} 1/2{\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown}. This almost matches the construction of Indyk and Motwani which achieves {$\supseteq$} {$\leq$} 1/{\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown}.},
  citationcount = {127},
  venue = {SCG '06}
}

@article{motwaniRandomizedAlgorithmsTail1995,
  title = {Randomized Algorithms: {{Tail}} Inequalities},
  author = {Motwani, R. and Raghavan, P.},
  year = {1995},
  doi = {10.1017/CBO9780511814075.005},
  abstract = {No abstract available},
  citationcount = {1442},
  venue = {No venue available}
}

@article{mullerBoundsToComplexities1975,
  title = {Bounds to Complexities of Networks for Sorting and for Switching},
  author = {Muller, D. E. and Preparata, F.},
  year = {1975},
  doi = {10.1145/321879.321882},
  abstract = {Abstract : A network which sorts n numbers when used to sort numbers of only two sizes, 0 and 1, can be regarded as forming the n frontal (unate) symmetric boolean functions of n arguments. When sorting networks are constructed from comparator modules they appear to require: (1) delay time or number of levels of order (log of n to the base 2) squared, (2) size or number of elements of order (log of n to the base 2) squared, and (3) formula length or number of literals of order n (log of n to the base 2). If one permits the use of negations in constructing the corresponding boolean functions, these three measures of complexity can be reduced to the orders of log of n to the base 2, n, and n to the 5th power respectively. The latter network however is incapable of sorting numbers and may be thought of as merely counting the number of inputs which are 1. One may incorporate this network, however, in a larger network which does sort and in time proportional to only log of n to the base 2. (Author)},
  citationcount = {167},
  venue = {JACM}
}

@article{muozEnumerationAndUpdates2023,
  title = {Enumeration and Updates for Conjunctive Linear Algebra Queries through Expressibility},
  author = {Mu{\~n}oz, Thomas and Riveros, Cristian and Vansummeren, Stijn},
  year = {2023},
  doi = {10.48550/arXiv.2310.04118},
  abstract = {Due to the importance of linear algebra and matrix operations in data analytics, there is significant interest in using relational query optimization and processing techniques for evaluating (sparse) linear algebra programs. In particular, in recent years close connections have been established between linear algebra programs and relational algebra that allow transferring optimization techniques of the latter to the former. In this paper, we ask ourselves which linear algebra programs in MATLANG correspond to the free-connex and q-hierarchical fragments of conjunctive first-order logic. Both fragments have desirable query processing properties: free-connex conjunctive queries support constant-delay enumeration after a linear-time preprocessing phase, and q-hierarchical conjunctive queries further allow constant-time updates. By characterizing the corresponding fragments of MATLANG, we hence identify the fragments of linear algebra programs that one can evaluate with constant-delay enumeration after linear-time preprocessing and with constant-time updates. To derive our results, we improve and generalize previous correspondences between MATLANG and relational algebra evaluated over semiring-annotated relations. In addition, we identify properties on semirings that allow to generalize the complexity bounds for free-connex and q-hierarchical conjunctive queries from Boolean annotations to general semirings.},
  citationcount = {Unknown},
  venue = {International Conference on Database Theory},
  keywords = {query,update}
}

@article{munroCompressedDataStructures2015,
  title = {Compressed Data Structures for Dynamic Sequences},
  author = {Munro, J. and Nekrich, Yakov},
  year = {2015},
  doi = {10.1007/978-3-662-48350-3_74},
  abstract = {No abstract available},
  citationcount = {29},
  venue = {Embedded Systems and Applications},
  keywords = {data structure,dynamic}
}

@article{munroDynamicDataStructures2015,
  title = {Dynamic Data Structures for Document Collections and Graphs},
  author = {Munro, I. and Nekrich, Yakov and Vitter, J.},
  year = {2015},
  doi = {10.1145/2745754.2745778},
  abstract = {In the dynamic indexing problem, we must maintain a changing collection of text documents so that we can efficiently support insertions, deletions, and pattern matching queries. We are especially interested in developing efficient data structures that store and query the documents in compressed form. All previous compressed solutions to this problem rely on answering rank and select queries on a dynamic sequence of symbols. Because of the lower bound in [Fredman and Saks, 1989], answering rank queries presents a bottleneck in compressed dynamic indexing. In this paper we show how this lower bound can be circumvented using our new framework. We demonstrate that the gap between static and dynamic variants of the indexing problem can be almost closed. Our method is based on a novel framework for adding dynamism to static compressed data structures. Our framework also applies more generally to dynamizing other problems. We show, for example, how our framework can be applied to develop compressed representations of dynamic graphs and binary relations.},
  citationcount = {17},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {data structure,dynamic,lower bound,query,static}
}

@article{munroSpaceEfficientConstruction2016,
  title = {Space-Efficient Construction of Compressed Indexes in Deterministic Linear Time},
  author = {Munro, J. and Navarro, G. and Nekrich, Yakov},
  year = {2016},
  doi = {10.1137/1.9781611974782.26},
  abstract = {We show that the compressed suffix array and the compressed suffix tree of a string T can be built in O(n) deterministic time using O(n{$\sigma$}) bits of space, where n is the string length and {$\sigma$} is the alphabet size. Previously described deterministic algorithms either run in time that depends on the alphabet size or need {$\omega$}(n{$\sigma$}) bits of working space. Our result has immediate applications to other problems, such as yielding the first linear-time LZ77 and LZ78 parsing algorithms that use O(n{$\sigma$}) bits.},
  citationcount = {61},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{munroSpaceEfficientData2019,
  title = {Space-Efficient Data Structures for Lattices},
  author = {Munro, J. and Sandlund, Bryce and Sinnamon, Corwin},
  year = {2019},
  doi = {10.4230/LIPIcs.SWAT.2020.31},
  abstract = {A lattice is a partially-ordered set in which every pair of elements has a unique meet (greatest lower bound) and join (least upper bound). We present new data structures for lattices that are simple, efficient, and nearly optimal in terms of space complexity. Our first data structure can answer partial order queries in constant time and find the meet or join of two elements in O(n\textsuperscript{\{\vphantom\}}3/4\vphantom\{\}) time, where n is the number of elements in the lattice. It occupies O(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}n) bits of space, which is only a {$\Theta$}(n) factor from the {$\Theta$}(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\})-bit lower bound for storing lattices. The preprocessing time is O(n{$^2$}). This structure admits a simple space-time tradeoff so that, for any c{$\in$}[\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{2\},1], the data structure supports meet and join queries in O(n\textsuperscript{\{\vphantom\}}1-c/2\vphantom\{\}) time, occupies O(n\textsuperscript{\{\vphantom\}}1+c\vphantom\{\}n) bits of space, and can be constructed in O(n{$^2$}+n\textsuperscript{\{\vphantom\}}1+3c/2\vphantom\{\}) time. Our second data structure uses O(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}n) bits of space and supports meet and join in O(d\textsuperscript{\{\vphantom\}}{\textfractionsolidus}n\vphantom\{\}\{d\}) time, where d is the maximum degree of any element in the transitive reduction graph of the lattice. This structure is much faster for lattices with low-degree elements. This paper also identifies an error in a long-standing solution to the problem of representing lattices. We discuss the issue with this previous work.},
  citationcount = {4},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {data structure,lower bound,query,reduction}
}

@article{munroSuccinctRepresentationOf2002,
  title = {Succinct Representation of Balanced Parentheses and Static Trees},
  author = {Munro, J. and Raman, Venkatesh},
  year = {2002},
  doi = {10.1137/S0097539799364092},
  abstract = {We consider the implementation of abstract data types for the static objects: binary tree, rooted ordered tree, and a balanced sequence of parentheses. Our representations use an amount of space within a lower order term of the information theoretic minimum and support, in constant time, a richer set of navigational operations than has previously been considered in similar work. In the case of binary trees, for instance, we can move from a node to its left or right child or to the parent in constant time while retaining knowledge of the size of the subtree at which we are positioned. The approach is applied to produce a succinct representation of planar graphs in which one can test adjacency in constant time.},
  citationcount = {308},
  venue = {SIAM journal on computing (Print)}
}

@article{munroSuccinctRepresentationsOf2011,
  title = {Succinct Representations of Permutations and Functions},
  author = {Munro, J. and Raman, R. and Raman, Venkatesh and S., Srinivasa Rao},
  year = {2011},
  doi = {10.1016/j.tcs.2012.03.005},
  abstract = {No abstract available},
  citationcount = {76},
  venue = {Theoretical Computer Science}
}

@article{n.jonesConstantTimeFactors1993,
  title = {Constant Time Factors Do Matter},
  author = {N. Jones},
  year = {1993},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/167088.167244},
  abstract = {The constant speedup theorem, so well known from Turing machine based complexity theory, is shown false for a natural imperative programming language I that manipulates tree-structured data. This relieves a tension between general programming practice, where linear factors are essential, and complexity theory, where linear time changes are traditionally regarded as trivial. Specifically, there is a constant b such that for any a {$>$} 0 there is a set X recognizable in time a {$\cdot$} b {$\cdot$} n but not in time a {$\cdot$} n. Thus LIN, the collection of all sets recognizable in linear time by deterministic I-programs, contains an infinite hierarchy ordered by constant coefficients. Constant hierarchies also exist for larger time bounds T (n), provided they are time-constructable. Second, a problem is exhibited which is complete for the nondeterministic linear time sets NLIN with respect to a natural notion of deterministic linear-time reduction. Third, Kleene's Second Recursion Theorem in essence shows that for any program p defined with self-reference, there is an equivalent nonreflexive program q. This is proven for an extension I{$\uparrow$} of I. Further, q can be simulated by an I program at most constantly slower than p. Language I{$\uparrow$} allows calls to the language's own interpretation function, and even to its running time function (without the usual high costs for nested levels of interpretation). The results all hold as well for a stronger language I allowing selective updating of tree-structured data. The results are robust in that classes LIN and NLIN are identical for I , Isu{$\uparrow$}, Schonhage's Storage Modification Machines, Knuth/Tarjan's pointer machines, and successor RAMs [13,14,11]. where n is the size of the input. DIKU, Department of Computer Science, University of Copenhagen, Universitetsparken 1, DK-2100 Copenhagen East, Denmark, E-mail: neil@diku.dk. If the ``more realistic and precise measure'' of SMM computation time is used [13], and similarly for the other models.},
  keywords = {reduction},
  annotation = {Citation Count: 44}
}

@article{n.pippengerPureImpureLisp1996,
  title = {Pure versus Impure {{Lisp}}},
  author = {N. Pippenger},
  year = {1996},
  journal = {ACM-SIGACT Symposium on Principles of Programming Languages},
  doi = {10.1145/237721.237741},
  abstract = {The aspect of purity versus impurity that we address involves the absence versus presence of mutation: the use of primitives (RPLACA and RPLACD in Lisp, set-car! and set-cdr! in Scheme) that change the state of pairs without creating new pairs. It is well known that cyclic list structures can be created by impure programs, but not by pure ones. In this sense, impure Lisp is "more powerful" than pure Lisp. If the inputs and outputs of programs are restricted to be sequences of atomic symbols, however, this difference in computability disappears. We shall show that if the temporal sequence of input and output operations must be maintained (that is, if computations must be "online"), then a difference in complexity remains: for a pure program to do what an impure program does in n steps, O(n log n) steps are sufficient, and in some cases {\textquestiondown}(n log n) steps are necessary.},
  annotation = {Citation Count: 9}
}

@article{nachtergaeleSystemAndArchitecture2000,
  title = {System and Architecture-Level Power Reduction of Microprocessor-Based Communication and Multi-Media Applications},
  author = {Nachtergaele, L. and Tiwari, Vivek and Dutt, N.},
  year = {2000},
  doi = {10.1109/ICCAD.2000.896533},
  abstract = {Current microprocessor architectures become more and more dominated by the data access bottlenecks in the cache, system bus and main memory subsystems. These also have a major influence on the system (board-level) power consumption. In practice this means lower energy consumption for a given throughput requirement. In the booming domain of (largely embedded) cost-sensitive communication and multi-media applications, more and more implementations make use of microprocessor based platforms for flexibility reasons. However, in order to provide sufficiently high data throughput at reasonable power consumption for these demanding applications, novel solutions for the memory access and data transfer will have to be introduced. These will have to be situated both at the processor architecture and the algorithm/compiler level. The question we want to address in this paper is what would these solutions look like. We show that they will be based on processor architecture optimizations, on novel approaches in the application of compiler technology, and on exploiting the interface between the system hardware and software.},
  citationcount = {8502},
  venue = {IEEE/ACM International Conference on Computer Aided Design. ICCAD - 2000. IEEE/ACM Digest of Technical Papers (Cat. No.00CH37140)},
  keywords = {communication,reduction}
}

@article{nalamMaximalKEdge2023,
  title = {Maximal K-{{Edge-Connected}} Subgraphs in Weighted Graphs via Local Random Contraction},
  author = {Nalam, Chaitanya and Saranurak, Thatchaphol},
  year = {2023},
  doi = {10.48550/arXiv.2302.02290},
  abstract = {The \{maximal k-edge-connected subgraphs\} problem is a classical graph clustering problem studied since the 70's. Surprisingly, no non-trivial technique for this problem in weighted graphs is known: a very straightforward recursive-mincut algorithm with {\textohm}(mn) time has remained the fastest algorithm until now. All previous progress gives a speed-up only when the graph is unweighted, and k is small enough (e.g. Henzinger et al. (ICALP'15), Chechik et al. (SODA'17), and Forster et al. (SODA'20)). We give the first algorithm that breaks through the long-standing O\vphantom\{\}(mn)-time barrier in \{weighted undirected\} graphs. More specifically, we show a maximal k-edge-connected subgraphs algorithm that takes only O\vphantom\{\}(m{$\cdot$} m{\textasciicircum}\{3/4\},n{\textasciicircum}\{4/5\} ) time. As an immediate application, we can (1+{$\epsilon$})-approximate the \{strength\} of all edges in undirected graphs in the same running time. Our key technique is the first local cut algorithm with \{exact\} cut-value guarantees whose running time depends only on the output size. All previous local cut algorithms either have running time depending on the cut value of the output, which can be arbitrarily slow in weighted graphs or have approximate cut guarantees.},
  citationcount = {3},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{nanongkaiDynamicMinimumSpanning2017,
  title = {Dynamic Minimum Spanning Forest with Subpolynomial Worst-Case Update Time},
  author = {Nanongkai, Danupon and Saranurak, Thatchaphol and {Wulff-Nilsen}, Christian},
  year = {2017},
  doi = {10.1109/FOCS.2017.92},
  abstract = {We present a Las Vegas algorithm for dynamically maintaining a minimum spanning forest of an n-node graph undergoing edge insertions and deletions. Our algorithm guarantees an O(n{\textasciicircum}\{o(1)\})\vphantom\{\} worst-case\vphantom\{\} update time with high probability. This significantly improves the two recent Las Vegas algorithms by Wulff-Nilsen \{Wulff-Nilsen16a\} with update time O(n{\textasciicircum}\{0.5-\&\#x2265;ilon\}) for some constant \&\#x2265;ilon 0 and, independently, by Nanongkai and Saranurak \{NanongkaiS16\} with update time O(n{\textasciicircum}\{0.494\}) (the latter works only for maintaining a spanning forest).Our result is obtained by identifying the common framework that both two previous algorithms rely on, and then improve and combine the ideas from both works. There are two main algorithmic components of the framework that are newly improved and critical for obtaining our result. First, we improve the update time from O(n{\textasciicircum}\{0.5-\&\#x2265;ilon\}) in \{Wulff-Nilsen16a\} to O(n{\textasciicircum}\{o(1)\}) for decrementally removing all low-conductance cuts in an expander undergoing edge deletions. Second, by revisiting the contraction technique by Henzinger and King \{HenzingerK97b\} and Holm et al. \{HolmLT01, we show a new approach for maintaining a minimum spanning forest in connected graphs with very few (at most (1+o(1))n) edges. This significantly improves the previous approach in \{Wulff-Nilsen16a, NanongkaiS16\} which is based on Fredericksons 2-dimensional topology tree \{Frederickson85\} and illustrates a new application to this old technique.\vphantom\}},
  citationcount = {115},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,update,update time}
}

@article{nanongkaiDynamicSpanningForest2016,
  title = {Dynamic Spanning Forest with Worst-Case Update Time: Adaptive, {{Las Vegas}}, and {{O}}(N1/2 - {$\varepsilon$})-Time},
  author = {Nanongkai, Danupon and Saranurak, Thatchaphol},
  year = {2016},
  doi = {10.1145/3055399.3055447},
  abstract = {We present two algorithms for dynamically maintaining a spanning forest of a graph undergoing edge insertions and deletions. Our algorithms guarantee worst-case update time and work against an adaptive adversary, meaning that an edge update can depend on previous outputs of the algorithms. We provide the first polynomial improvement over the long-standing O({\textsurd}n) bound of [Frederickson STOC'84, Eppstein, Galil, Italiano and Nissenzweig FOCS'92] for such type of algorithms. The previously best improvement was O({\textsurd}n (loglogn)2/logn) [Kejlberg-Rasmussen, Kopelowitz, Pettie and Thorup ESA'16]. We note however that these bounds were obtained by deterministic algorithms while our algorithms are randomized. Our first algorithm is Monte Carlo and guarantees an O(n0.4+o(1)) worst-case update time, where the o(1) term hides the O({\textsurd}loglogn/logn) factor. Our second algorithm is Las Vegas and guarantee an O(n0.49306) worst-case update time with high probability. Algorithms with better update time either needed to assume that the adversary is oblivious (e.g. [Kapron, King and Mountjoy SODA'13]) or can only guarantee an amortized update time. Our second result answers an open problem by Kapron et al. To the best of our knowledge, our algorithms are among a few non-trivial randomized dynamic algorithms that work against adaptive adversaries. The key to our results is a decomposition of graphs into subgraphs that either have high expansion or sparse. This decomposition serves as an interface between recent developments on (static) flow computation and many old ideas in dynamic graph algorithms: On the one hand, we can combine previous dynamic graph techniques to get faster dynamic spanning forest algorithms if such decomposition is given. On the other hand, we can adapt flow-related techniques (e.g. those from [Khandekar, Rao and Vazirani STOC'06], [Peng SODA'16], and [Orecchia and Zhu SODA'14]) to maintain such decomposition. To the best of our knowledge, this is the first time these flow techniques are used in fully dynamic graph algorithms.},
  citationcount = {101},
  venue = {Symposium on the Theory of Computing},
  keywords = {adaptive,dynamic,static,update,update time}
}

@article{naorSmallBiasProbability1990,
  title = {Small-Bias Probability Spaces: Efficient Constructions and Applications},
  author = {Naor, J. and Naor, M.},
  year = {1990},
  doi = {10.1145/100216.100244},
  abstract = {It is shown how to efficiently construct a small probability space on n binary random variables such that for every subset, its parity is either zero or one with ``almost'' equal probability. They are called {$\epsilon$}-biased random variables. The number of random bits needed to generate the random variables is O(n+\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{{$\epsilon$}\}). Thus, if {$\epsilon$} is polynomially small, then the size of the sample space is also polynomial. Random variables that are {$\epsilon$}-biased can be used to construct ``almost'' k-wise independent random variables where {$\epsilon$} is a function of k.These probability spaces have various applications: l. Derandomization of algorithms: Many randomized algorithms that require only k-wise independence of their random bits (where k is bounded by O(n)), can be derandomized by using {$\epsilon$}-biased random variables. 2. Reducing the number of random bits required by certain randomized algorithms, e.g., verification of matrix multiplication. 3. Exhaustive tes...},
  citationcount = {831},
  venue = {Symposium on the Theory of Computing}
}

@article{naorSplittersAndNear1995,
  title = {Splitters and Near-Optimal Derandomization},
  author = {Naor, M. and Schulman, L. and Srinivasan, A.},
  year = {1995},
  doi = {10.1109/SFCS.1995.492475},
  abstract = {We present a fairly general method for finding deterministic constructions obeying what we call k-restrictions; this yields structures of size not much larger than the probabilistic bound. The structures constructed by our method include (n,k)-universal sets (a collection of binary vectors of length n such that for any subset of size k of the indices, all 2/sup k/ configurations appear) and families of perfect hash functions. The near-optimal constructions of these objects imply the very efficient derandomization of algorithms in learning, of fixed-subgraph finding algorithms, and of near optimal /spl Sigma/II/spl Sigma/ threshold formulae. In addition, they derandomize the reduction showing the hardness of approximation of set cover. They also yield deterministic constructions for a local-coloring protocol, and for exhaustive testing of circuits.},
  citationcount = {342},
  venue = {Proceedings of IEEE 36th Annual Foundations of Computer Science}
}

@article{narayananFastComputationOf2016,
  title = {Fast Computation of Isomorphisms between Finite Fields Using Elliptic Curves},
  author = {Narayanan, Anand Kumar},
  year = {2016},
  doi = {10.1007/978-3-030-05153-2_4},
  abstract = {No abstract available},
  citationcount = {6},
  venue = {International Workshop on Arithmetic of Finite Fields}
}

@article{narayananPolynomialFactorizationOver2015,
  title = {Polynomial Factorization over Finite Fields by Computing Euler-Poincare Characteristics of Drinfeld Modules},
  author = {Narayanan, Anand Kumar},
  year = {2015},
  doi = {10.1016/j.ffa.2018.08.003},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {Finite Fields Their Appl.}
}

@article{navarroAdaptiveDynamicBitvectors2024,
  title = {Adaptive Dynamic Bitvectors},
  author = {Navarro, Gonzalo},
  year = {2024},
  doi = {10.48550/arXiv.2405.15088},
  abstract = {While operations \{\vphantom\}\emph{rank}\vphantom\{\}\emph{ and }\{\vphantom\}\emph{select}\vphantom\{\}\emph{ on static bitvectors can be supported in constant time, lower bounds show that supporting updates raises the cost per operation to {$\Theta$}(n/n) on bitvectors holding n bits. This is a shame in scenarios where updates are possible but uncommon. We develop a representation of bitvectors that we call adaptive dynamic bitvector, which uses the asymptotically optimal n+o(n) bits of space and, if there are q queries per update, supports all the operations in O((n/q)/n) amortized time. Further, we prove that this time is optimal in the cell probe model.}},
  citationcount = {1},
  venue = {SPIRE},
  keywords = {adaptive,cell probe,dynamic,lower bound,query,static,update}
}

@article{navarroComputingMemsAnd2022,
  title = {Computing Mems and Relatives on Repetitive Text Collections},
  author = {Navarro, G.},
  year = {2022},
  doi = {10.1145/3701561},
  abstract = {We consider the problem of computing the Maximal Exact Matches (MEMs) of a given pattern P[1\{..\}m] on a large repetitive text collection T[1\{..\}n] over an alphabet of size {$\sigma$} , which is represented as a (hopefully much smaller) run-length context-free grammar of size g\textsubscript{\{\vphantom\}}rl\vphantom\{\} . We show that the problem can be solved in time O(m\textsuperscript{\{\vphantom\}}2\vphantom\{\}\textsuperscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}n) , for any constant {$\epsilon$}0 , on a data structure of size O(g\textsubscript{\{\vphantom\}}rl\vphantom\{\}) . Further, on a locally consistent grammar of size O({$\delta$}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}{$\sigma$}\vphantom\{\}\{{$\delta$}n\}) , the time decreases to O(mm(m+\textsuperscript{\{\vphantom\}}{$\epsilon$}\vphantom\{\}n)) . The value {$\delta$} is a function of the substring complexity of T and {\textohm}({$\delta$}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}{$\sigma$}\vphantom\{\}\{{$\delta$}n\}) is a tight lower bound on the compressibility of repetitive texts T , so our structure has optimal size in terms of n , {$\sigma$} , and {$\delta$} . We extend our results to several related problems, such as finding k -MEMs, MUMs, rare MEMs, and applications. Categories and Subject Descriptors: E.1 [Data structures] ; E.2 [Data storage representations] ; E.4 [Coding and information theory]: Data compaction and compression; F.2.2 [Analysis of algorithms and problem complexity] : Nonnumerical algorithms and problems--- Pattern matching, Computations on discrete structures, Sorting and searching},
  citationcount = {5},
  venue = {ACM Trans. Algorithms},
  keywords = {data structure,lower bound}
}

@article{navarroIndexingHighlyRepetitive2020,
  title = {Indexing Highly Repetitive String Collections, Part {{II}}},
  author = {Navarro, G.},
  year = {2020},
  doi = {10.1145/3432999},
  abstract = {Two decades ago, a breakthrough in indexing string collections made it possible to represent them within their compressed space while at the same time offering indexed search functionalities. As this new technology permeated through applications like bioinformatics, the string collections experienced a growth that outperforms Moore's Law and challenges our ability of handling them even in compressed form. It turns out, fortunately, that many of these rapidly growing string collections are highly repetitive, so that their information content is orders of magnitude lower than their plain size. The statistical compression methods used for classical collections, however, are blind to this repetitiveness, and therefore a new set of techniques has been developed to properly exploit it. The resulting indexes form a new generation of data structures able to handle the huge repetitive string collections that we are facing. In this survey, formed by two parts, we cover the algorithmic developments that have led to these data structures. In this second part, we describe the fundamental algorithmic ideas and data structures that form the base of all the existing indexes, and the various concrete structures that have been proposed, comparing them both in theoretical and practical aspects, and uncovering some new combinations. We conclude with the current challenges in this fascinating field.},
  citationcount = {55},
  venue = {ACM Computing Surveys},
  keywords = {data structure}
}

@article{navarroOptimalDynamicSequence2012,
  title = {Optimal Dynamic Sequence Representations},
  author = {Navarro, G. and Nekrich, Yakov},
  year = {2012},
  doi = {10.1137/1.9781611973105.62},
  abstract = {We describe a data structure that supports access, rank and select queries, as well as symbol insertions and deletions, on a string S[1, n] over alphabet [1..{$\sigma$}] in time O(lg n/lg lg n), which is optimal. The time is worst-case for the queries and amortized for the updates. This complexity is better than the best previous ones by a {$\Theta$}(1 + lg {$\sigma$}/lg lg n) factor. Our structure uses nH0(S) + O(n + {$\sigma$}(lg {$\sigma$} + lg1+en)) bits, where H0(S) is the zero-order entropy of S and 0},
  citationcount = {78},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,query,update}
}

@article{navarroOptimalEncodingsFor2014,
  title = {Optimal Encodings for Range Majority Queries},
  author = {Navarro, G. and Thankachan, Sharma V.},
  year = {2014},
  doi = {10.1007/s00453-015-9987-8},
  abstract = {No abstract available},
  citationcount = {6},
  venue = {Algorithmica},
  keywords = {query}
}

@article{navarroPredecessorSearch2020,
  title = {Predecessor Search},
  author = {Navarro, G. and {Rojas-Ledesma}, J.},
  year = {2020},
  doi = {10.1145/3409371},
  abstract = {The predecessor problem is a key component of the fundamental sorting-and-searching core of algorithmic problems. While binary search is the optimal solution in the comparison model, more realistic machine models on integer sets open the door to a rich universe of data structures, algorithms, and lower bounds. In this article, we review the evolution of the solutions to the predecessor problem, focusing on the important algorithmic ideas, from the famous data structure of van Emde Boas to the optimal results of Patrascu and Thorup. We also consider lower bounds, variants, and special cases, as well as the remaining open questions.},
  citationcount = {4},
  venue = {ACM Computing Surveys},
  keywords = {data structure,lower bound}
}

@article{navarroSpaceEfficientData2011,
  title = {Space-Efficient Data-Analysis Queries on Grids},
  author = {Navarro, G. and Russo, L.},
  year = {2011},
  doi = {10.1007/978-3-642-25591-5_34},
  abstract = {No abstract available},
  citationcount = {41},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {query}
}

@article{navarroSpacesTreesAnd2013,
  title = {Spaces, Trees, and Colors},
  author = {Navarro, G.},
  year = {2013},
  doi = {10.1145/2535933},
  abstract = {Document retrieval is one of the best-established information retrieval activities since the '60s, pervading all search engines. Its aim is to obtain, from a collection of text documents, those most relevant to a pattern query. Current technology is mostly oriented to ``natural language'' text collections, where inverted indexes are the preferred solution. As successful as this paradigm has been, it fails to properly handle various East Asian languages and other scenarios where the ``natural language'' assumptions do not hold. Inthis survey, we cover the recent research in extending the document retrieval techniques to a broader class of sequence collections, which has applications in bioinformatics, data and web mining, chemoinformatics, software engineering, multimedia information retrieval, and many other fields. We focus on the algorithmic aspects of the techniques, uncovering a rich world of relations between document retrieval challenges and fundamental problems on trees, strings, range queries, discrete geometry, and other areas.},
  citationcount = {48},
  venue = {ACM Computing Surveys},
  keywords = {query}
}

@article{navarroTopKDocument2025,
  title = {Top-k Document Retrieval in Compressed Space},
  author = {Navarro, Gonzalo and Nekrich, Yakov},
  year = {2025},
  doi = {10.1137/1.9781611978322.137},
  abstract = {Let D be a collection of D strings of total length n over an alphabet of size {$\sigma$} . We consider the so-called top-k document retrieval problem: given a short string P and an integer k , list the identifiers of k strings in D most relevant to P , in decreasing order of relevance. Relevance may be a fixed value associated with the strings where P occurs, or the number of times P occurs in the strings. While RAM-optimal solutions using O ( n log n ) bits and O ( {\textbar} P {\textbar} / log {$\sigma$} n + k ) time exist, solving the problem optimally within space close to O ( n log {$\sigma$} ) bits is open. We describe a data structure for the top-k document retrieval problem that uses O (log log n ) bits per symbol on top of any compressed suffix array (CSA) of D , and supports queries in essentially optimal time, in the following sense. Given a CSA using {\textbar} CSA {\textbar} bits of space, that finds the suffix array range of a query string P in time t cnt , and accesses a suffix array entry in time t SA , listing any k pattern occurrences would take time O ( t cnt + k t SA ). Our top-k data structure uses {\textbar} CSA {\textbar} + O ( n log log n ) bits and reports k most relevant documents that contain P in time O ( t cnt + k ( t SA + log log n )). On every known CSA using O ( n log {$\sigma$} ) bits, t SA is {\textohm}(log log n ) in virtually all cases, thus our time is O ( t cnt + k t SA ) in most situations. When the query string P is sufficiently long, some CSAs reach time O ( t cnt + k ) to list any k occurrences of P . Our structure achieves similar performance in this case, obtaining time O ( t cnt + t sort ( k, n )) on every known CSA, where t sort ( k, n ) is the time to sort k integers in [1 , n ]. This time is already O ( t cnt + k ) in the typical regimes, k = O (polylog n ) and k = {\textohm}( n {$\varepsilon$} ) for any constant {$\varepsilon$} {\textquestiondown} 0. If we can deliver the results in unsorted order of relevance, then the time for long patterns is always O ( t cnt + k ), which is optimal with respect to the CSA, and reaches the RAM-optimal time O ( {\textbar} P {\textbar} / log {$\sigma$} n + k ) on a particular CSA. No top-k solution using o ( n log D ) bits of space has achieved this before.},
  citationcount = {Unknown},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,query}
}

@article{navarroUniversalCompressedText2018,
  title = {Universal Compressed Text Indexing},
  author = {Navarro, G. and Prezza, N.},
  year = {2018},
  doi = {10.1016/j.tcs.2018.09.007},
  abstract = {No abstract available},
  citationcount = {49},
  venue = {Theoretical Computer Science}
}

@article{navarroWaveletTreesFor2012,
  title = {Wavelet Trees for All},
  author = {Navarro, G.},
  year = {2012},
  doi = {10.1016/j.jda.2013.07.004},
  abstract = {No abstract available},
  citationcount = {216},
  venue = {J. Discrete Algorithms}
}

@article{neilv.murrayObservationsEquivalenceHandling1981,
  title = {Some {{Observations}} on {{Equivalence Handling Methods}}},
  author = {Neil V. Murray},
  year = {1981},
  journal = {IEEE transactions on computers},
  doi = {10.1109/TC.1981.1675795},
  abstract = {The online disjoint set union problem is solved essentially by implementing two operations which manipulate disjoint sets. FIND(x) computes the name of the unique set of which x is a member. UNION(A,B) merges the two sets A and B into one new set named either A or B. Given a set of n elements, we may perform f {$\geq$} n FIND's and n-1 intermixed UNION's in time 0(f{$\alpha$}(f,n)) and space 0(n), where {$\alpha$}(f,n) is related to a functional inverse of Ackermann's function and grows slowly. We show that certain modifications to the UNION operation that are convenient in some applications do not affect the running time complexity. We also show that under appropriate input restrictions, UNION and FIND can be made to run in linear time. However, the space required is 0(n+e), where e is the number of equivalence pairs to be processed.},
  annotation = {Citation Count: 1}
}

@inproceedings{nekrichDataStructureMultidimensional2007,
  title = {A Data Structure for Multi-Dimensional Range Reporting},
  booktitle = {Proceedings of the Twenty-Third Annual Symposium on {{Computational}} Geometry},
  author = {Nekrich, Yakov},
  year = {2007},
  month = jun,
  series = {{{SCG}} '07},
  pages = {344--353},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1247069.1247130},
  url = {https://dl.acm.org/doi/10.1145/1247069.1247130},
  urldate = {2024-09-20},
  abstract = {We present a static data structure for orthogonal range reporting on a U x U x U integer grid. Our data structure supports orthogonal range reporting queries in O((log log n)2 + log log U + k) time and uses O(n log4 n) space, where k is the size of the answer. We describe a static data structure for range reporting in R4 with O(log nlog log n +k) query time and a static data structure for range reporting in Rd, d {$\geq$} 5, with query time O((logd-3 n)/(log log n)d-5 +k). We also describe a semi-dynamic data structure that supports range reporting queries in R3 in O(log nlog log n +k) time and insertions in polylogarithmic time.},
  isbn = {978-1-59593-705-6},
  keywords = {data structure,dynamic,query,query time,static},
  file = {/Users/tulasi/Zotero/storage/Q9GZJZY4/Nekrich - 2007 - A data structure for multi-dimensional range reporting.pdf}
}

@article{nekrichDataStructuresFor2009,
  title = {Data Structures for Approximate Orthogonal Range Counting},
  author = {Nekrich, Yakov},
  year = {2009},
  doi = {10.1007/978-3-642-10631-6_20},
  abstract = {No abstract available},
  citationcount = {9},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {data structure}
}

@article{nekrichDataStructuresWith2008,
  title = {Data Structures with Local Update Operations},
  author = {Nekrich, Yakov},
  year = {2008},
  doi = {10.1007/978-3-540-69903-3_14},
  abstract = {No abstract available},
  citationcount = {5},
  venue = {Scandinavian Workshop on Algorithm Theory},
  keywords = {data structure,update}
}

@article{nekrichDynamicRangeReporting2010,
  title = {Dynamic Range Reporting in External Memory},
  author = {Nekrich, Yakov},
  year = {2010},
  doi = {10.1007/978-3-642-17514-5_3},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {dynamic}
}

@article{nekrichEfficientRangeSearching2014,
  title = {Efficient Range Searching for Categorical and Plain Data},
  author = {Nekrich, Yakov},
  year = {2014},
  doi = {10.1145/2543924},
  abstract = {In the orthogonal range-searching problem, we store a set of input points S in a data structure; the answer to a query Q is a piece of information about points in Q{$\cap$}S, for example, the list of all points in Q{$\cap$}S or the number of points in Q. In the colored (or categorical) range-searching problem, the set of input points is partitioned into categories; the answer to a query is a piece of information about categories of points in a query range. In this article, we describe several new results for one- and two-dimensional range-searching problems. We obtain an optimal adaptive data structure for counting the number of objects in a three-sided range and for counting categories of objects in a one-dimensional range. We also obtain new results on color range reporting in two dimensions, approximate color counting in one dimension, and some other related problems.},
  citationcount = {24},
  venue = {ACM Transactions on Database Systems},
  keywords = {adaptive,data structure,query}
}

@article{nekrichExternalMemoryRange2007,
  title = {External Memory Range Reporting on a Grid},
  author = {Nekrich, Yakov},
  year = {2007},
  doi = {10.1007/978-3-540-77120-3_46},
  abstract = {No abstract available},
  citationcount = {12},
  venue = {International Symposium on Algorithms and Computation}
}

@article{nekrichFourDimensionalDominance2020,
  title = {Four-Dimensional Dominance Range Reporting in Linear Space},
  author = {Nekrich, Yakov},
  year = {2020},
  doi = {10.4230/LIPIcs.SoCG.2020.59},
  abstract = {In this paper we study the four-dimensional dominance range reporting problem and present data structures with linear or almost-linear space usage. Our results can be also used to answer four-dimensional queries that are bounded on five sides. The first data structure presented in this paper uses linear space and answers queries in O(\textsuperscript{\{\vphantom\}}1+{$\varepsilon$}\vphantom\{\}n+k\textsuperscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}n) time, where k is the number of reported points, n is the number of points in the data structure, and {$\varepsilon$} is an arbitrarily small positive constant. Our second data structure uses O(n\textsuperscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}n) space and answers queries in O(n+k) time. These are the first data structures for this problem that use linear (resp. O(n\textsuperscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}n)) space and answer queries in poly-logarithmic time. For comparison the fastest previously known linear-space or O(n\textsuperscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}n)-space data structure supports queries in O(n\textsuperscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}+k) time (Bentley and Mauer, 1980). Our results can be generalized to d{$\geq$}4 dimensions. For example, we can answer d-dimensional dominance range reporting queries in O(n(n/n)\textsuperscript{\{\vphantom\}}d-3\vphantom\{\}+k) time using O(n\textsuperscript{\{\vphantom\}}d-4+{$\varepsilon$}\vphantom\{\}n) space. Compared to the fastest previously known result (Chan, 2013), our data structure reduces the space usage by O(n) without increasing the query time.},
  citationcount = {3},
  venue = {International Symposium on Computational Geometry},
  keywords = {data structure,query,query time}
}

@article{nekrichIOEfficient2008,
  title = {I/o-Efficient Point Location in a Set of Rectangles},
  author = {Nekrich, Yakov},
  year = {2008},
  doi = {10.1007/978-3-540-78773-0_59},
  abstract = {No abstract available},
  citationcount = {10},
  venue = {Latin American Symposium on Theoretical Informatics}
}

@article{nekrichNewDataStructures2020,
  title = {New Data Structures for Orthogonal Range Reporting and Range Minima Queries},
  author = {Nekrich, Yakov},
  year = {2020},
  doi = {10.1137/1.9781611976465.73},
  abstract = {In this paper we present new data structures for two extensively studied variants of the orthogonal range searching problem. First, we describe a data structure that supports two-dimensional orthogonal range minima queries in O(n) space and O(\textsuperscript{\{\vphantom\}}{$\varepsilon$}\vphantom\{\}n) time, where n is the number of points in the data structure and {$\varepsilon$} is an arbitrarily small positive constant. Previously known linear-space solutions for this problem require O(\textsuperscript{\{\vphantom\}}1+{$\varepsilon$}\vphantom\{\}n) (Chazelle, 1988) or O(nn) time (Farzan et al., 2012). A modification of our data structure uses space O(nn) and supports range minima queries in time O(n). Both results can be extended to support three-dimensional five-sided reporting queries. Next, we turn to the four-dimensional orthogonal range reporting problem and present a data structure that answers queries in optimal O(n/n+k) time, where k is the number of points in the answer. This is the first data structure that achieves the optimal query time for this problem. Our results are obtained by exploiting the properties of three-dimensional shallow cuttings.},
  citationcount = {9},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,query,query time}
}

@article{nekrichOptimalColorRange2013,
  title = {Optimal Color Range Reporting in One Dimension},
  author = {Nekrich, Yakov and Vitter, J.},
  year = {2013},
  doi = {10.1007/978-3-642-40450-4_63},
  abstract = {No abstract available},
  citationcount = {13},
  venue = {Embedded Systems and Applications}
}

@article{nekrichOrthogonalRangeSearching2016,
  title = {Orthogonal Range Searching on Discrete Grids},
  author = {Nekrich, Yakov},
  year = {2016},
  doi = {10.1007/978-1-4939-2864-4_631},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Encyclopedia of Algorithms}
}

@article{nekrichSortedRangeReporting2012,
  title = {Sorted Range Reporting},
  author = {Nekrich, Yakov and Navarro, G.},
  year = {2012},
  doi = {10.1007/978-3-642-31155-0_24},
  abstract = {No abstract available},
  citationcount = {58},
  venue = {Scandinavian Workshop on Algorithm Theory}
}

@article{nekrichSpaceefficientRangeReporting2012,
  title = {Space-Efficient Range Reporting for Categorical Data},
  author = {Nekrich, Yakov},
  year = {2012},
  doi = {10.1145/2213556.2213575},
  abstract = {In the colored (or categorical) range reporting problem the set of input points is partitioned into categories and stored in a data structure; a query asks for categories of points that belong to the query range. In this paper we study two-dimensional colored range reporting in the external memory model and present I/O-efficient data structures for this problem. In particular, we describe data structures that answer three-sided colored reporting queries in O(K/B) I/Os and two-dimensional colored reporting queries in(log2logB N + K/B) I/Os when points lie on an N x N grid, K is the number of reported colors, and B is the block size. The space usage of both data structures is close to optimal.},
  citationcount = {11},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {data structure,query}
}

@article{nelsonBillionScaleMatrix2019,
  title = {Billion-Scale Matrix Compression and Multiplication with Implications in Data Mining},
  author = {Nelson, M. and Radhakrishnan, S. and Sekharan, C.},
  year = {2019},
  doi = {10.1109/IRI.2019.00067},
  abstract = {Billion-scale Boolean matrices in the era of big data occupy storage that is measured in 100's of petabytes to zetabytes. The fundamental operation on these matrices for data mining involves multiplication which suffers a significant slow-down as the required data cannot fit in most main memories. In this paper, we propose new algorithms to perform Matrix-Vector and Matrix-Matrix operations directly on compressed Boolean matrices using innovative techniques extended from our previous work on compression. Our extension involves the development of a row-by-row differential compression technique which reduces the overall space requirement and the number of matrix operations. We have provided extensive empirical results on billion-scale Boolean matrices that are Boolean adjacency matrices of web graphs. Our work has significant implications on key problems such as page-ranking and itemset mining that use matrix multiplication.},
  citationcount = {2},
  venue = {IEEE International Conference on Information Reuse and Integration}
}

@article{newmanPrivateVsCommon1991,
  title = {Private vs. Common Random Bits in Communication Complexity},
  author = {Newman, I.},
  year = {1991},
  doi = {10.1016/0020-0190(91)90157-D},
  abstract = {No abstract available},
  citationcount = {333},
  venue = {Information Processing Letters}
}

@article{ngoBeyondWorstCase2013,
  title = {Beyond Worst-Case Analysis for Joins with Minesweeper},
  author = {Ngo, H. and Nguyen, D. and R{\'e}, C. and Rudra, A.},
  year = {2013},
  doi = {10.1145/2594538.2594547},
  abstract = {We describe a new algorithm, Minesweeper, that is able to satisfy stronger runtime guarantees than previous join algorithms (colloquially ``beyond worst-case'' guarantees) for data in indexed search trees. Our first contribution is developing a framework to measure this stronger notion of complexity, which we call "certificate complexity," that extends notions of Barbay et al. and Demaine et al.; a certificate is a set of propositional formulae that certifies that the output is correct. This notion captures a natural class of join algorithms. In addition, the certificate allows us to define a strictly stronger notion of runtime complexity than traditional worst-case guarantees. Our second contribution is to develop a dichotomy theorem for the certificate-based notion of complexity. Roughly, we show that Minesweeper evaluates {$\beta$}-acyclic queries in time linear in the certificate plus the output size, while for any {$\beta$}-cyclic query, there is some instance that takes superlinear time in the certificate (and for which the output is no larger than the certificate size). We also extend our certificate-complexity analysis to queries with bounded treewidth and the triangle query. We present empirical results that certificates can be much smaller than the input size, which suggests that ideas in minesweeper might lead to faster algorithms in practice.},
  citationcount = {57},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {query}
}

@article{nicholsonASurveyOf2013,
  title = {A Survey of Data Structures in the Bitprobe Model},
  author = {Nicholson, Patrick K. and Raman, Venkatesh and Rao, S.},
  year = {2013},
  doi = {10.1007/978-3-642-40273-9_19},
  abstract = {No abstract available},
  citationcount = {27},
  venue = {Space-Efficient Data Structures, Streams, and Algorithms},
  keywords = {data structure}
}

@article{nicolasdelfosseAlmostlinearTimeDecoding2017,
  title = {Almost-Linear Time Decoding Algorithm for Topological Codes},
  author = {Nicolas Delfosse and Naomi H. Nickerson},
  year = {2017},
  journal = {Quantum},
  doi = {10.22331/q-2021-12-02-595},
  abstract = {In order to build a large scale quantum computer, one must be able to correct errors extremely fast. We design a fast decoding algorithm for topological codes to correct for Pauli errors and erasure and combination of both errors and erasure. Our algorithm has a worst case complexity of O(n{$\alpha$}(n)), where n is the number of physical qubits and {$\alpha$} is the inverse of Ackermann's function, which is very slowly growing. For all practical purposes, {$\alpha$}(n){$\leq$}3. We prove that our algorithm performs optimally for errors of weight up to (d-1)/2 and for loss of up to d-1 qubits, where d is the minimum distance of the code. Numerically, we obtain a threshold of 9.9\% for the 2d-toric code with perfect syndrome measurements and 2.6\% with faulty measurements.},
  annotation = {Citation Count: 206}
}

@article{nievergeltBinarySearchTrees1972,
  title = {Binary Search Trees of Bounded Balance},
  author = {Nievergelt, J. and Reingold, E.},
  year = {1972},
  doi = {10.1145/800152.804906},
  abstract = {A new class of binary search trees, called trees of bounded balance, is introduced. These trees are easy to maintain in their form despite insertions and deletions of nodes, and the search time is only moderately longer than in completely balanced trees. Trees of bounded balance differ from other classes of binary search trees in that they contain a parameter which can be varied so the compromise between short search time and infrequent restructuring can be chosen arbitrarily.},
  citationcount = {313},
  venue = {SIAM journal on computing (Print)}
}

@article{nievergeltTheGridFile1984,
  title = {The Grid File: {{An}} Adaptable, Symmetric Multikey File Structure},
  author = {Nievergelt, J. and Hinterberger, Hans and Sevcik, K.},
  year = {1984},
  doi = {10.1145/348.318586},
  abstract = {Traditional file structures that provide multikey access to records, for example, inverted files, are extensions of file structures originally designed for single-key access. They manifest various deficiencies in particular for multikey access to highly dynamic files. We study the dynamic aspects of file structures that treat all keys symmetrically, that is, file structures which avoid the distinction between primary and secondary keys. We start from a bitmap approach and treat the problem of file design as one of data compression of a large sparse matrix. This leads to the notions of a grid partition of the search space and of a grid directory, which are the keys to a dynamic file structure called the grid file. This file system adapts gracefully to its contents under insertions and deletions, and thus achieves an upper bound of two disk accesses for single record retrieval; it also handles range queries and partially specified queries efficiently. We discuss in detail the design decisions that led to the grid file, present simulation results of its behavior, and compare it to other multikey access file structures.},
  citationcount = {1186},
  venue = {TODS}
}

@article{ningjiweiIntegerProgrammingFormulations2021,
  title = {Integer {{Programming Formulations}} for {{Minimum Spanning Tree Interdiction}}},
  author = {Ningji Wei and J. Walteros and Foad Mahdavi Pajouh},
  year = {2021},
  journal = {INFORMS journal on computing},
  doi = {10.1287/IJOC.2020.1018},
  abstract = {We consider a two-player interdiction problem staged over a graph where the attacker's objective is to minimize the cost of removing edges from the graph so that the defender's objective, that is, the weight of a minimum spanning tree in the residual graph, is increased up to a predefined level r. Standard approaches for graph interdiction frame this type of problems as bilevel formulations, which are commonly solved by replacing the inner problem by its dual to produce a single-level reformulation. In this paper, we study an alternative integer program derived directly from the attacker's solution space and show that this formulation yields a stronger linear relaxation than the bilevel counterpart. Furthermore, we analyze the convex hull of the feasible solutions of the problem and identify several families of facet-defining inequalities that can be used to strengthen this integer program. We then proceed by introducing a different formulation defined by a set of so-called supervalid inequalities that may exclude feasible solutions, albeit solutions whose objective value is not better than that of an edge cut of minimum cost. We discuss several computational aspects required for an efficient implementation of the proposed approaches. Finally, we perform an extensive set of computational experiments to test the quality of these formulations, analyzing and comparing the benefits of each model, as well as identifying further enhancements. Summary of Contribution: Network interdiction has received significant attention over the last couple of decades, with a notable peak of interest in recent years. This paper provides an interesting balance between the theoretical and computational aspects of solving a challenging network interdiction problem via integer programming. We present several technical developments, including a detailed study of the problem's solution space, multiple formulations, and a polyhedral analysis of the convex hull of feasible solutions. We then analyze the results of an extensive set of computational experiments that were used to validate the effectiveness of the different methods we developed in this paper.},
  annotation = {Citation Count: 14}
}

@article{nisanAlgorithmicGameTheory2007,
  title = {Algorithmic Game Theory},
  author = {Nisan, N. and Roughgarden, Tim and Tardos, {\'E}va and Vazirani, Vijay V.},
  year = {2007},
  doi = {10.1017/CBO9780511800481},
  abstract = {Online mechanisms extend the methods of mechanism design to dynamic environments with multiple agents and private information. Decisions must be made as information about types is revealed online and without knowledge of the future in the sense of online algorithms. We first consider single-valued preference domains and characterize the space of decision policies that can be truthfully implemented in a dominant strategy equilibrium. Working in a model-free environment we present truthful auctions for domains with ex-piring items and limited-supply items. Turning to a more general preference domain, and assuming the existence of a probabilistic model for agent types, we define a dynamic Vickrey-Clarke-Groves mechanism that is efficient and Bayes-Nash incentive compatible. We close with some thoughts about future research directions in this area.},
  citationcount = {3312},
  venue = {No venue available}
}

@article{nisanHardnessVsRandomness1988,
  title = {Hardness vs. Randomness},
  author = {Nisan, N. and Wigderson, A.},
  year = {1988},
  doi = {10.1109/SFCS.1988.21916},
  abstract = {A simple construction for a pseudorandom bit generator is presented. It stretches a short string of truly random bits into a long string that looks random to any algorithm from a complexity class C (e.g. P, NC, PSPACE, etc.), using an arbitrary function that is hard for C. This generator reveals an equivalence between the problems of proving lower bounds and the problem of generating good pseudorandom sequences. Combining this construction with other arguments, a number of consequences are obtained.<<ETX>>},
  citationcount = {981},
  venue = {[Proceedings 1988] 29th Annual Symposium on Foundations of Computer Science}
}

@article{nisanLowerBoundsFor1991,
  title = {Lower Bounds for Non-Commutative Computation},
  author = {Nisan, N.},
  year = {1991},
  doi = {10.1145/103418.103462},
  abstract = {JVe consider algebraic computations which are not allowed to rely on the commut,at,ivity of multiplication. We obtain various lower bounds for algebraic formula size in this model: (1) Computing the determinant is as hard as computing the permanent and tight exponential upper and lower bounds are given. (2) Computation cannot be parallelized, as opposed to in the commutative case -- this solves in the negative an open problem of hliller et al [8]. (3) The question of the power of negation in this model is shown to be closely related to a well known open problem relating communication complexity and rank. We then take modest steps towards extending our results to general, commutative algebraic computation, and prove exponential lower bounds for monotone algebraic circuit size, as well as for the size of certain types of constant depth algebraic circuits.},
  citationcount = {208},
  venue = {Symposium on the Theory of Computing}
}

@article{nisanProductsAndHelp1994,
  title = {Products and Help Bits in Decision Trees},
  author = {Nisan, N. and Rudich, S. and Saks, M.},
  year = {1994},
  doi = {10.1109/SFCS.1994.365683},
  abstract = {We investigate two problems concerning the complexity of evaluating a function f at k-tuple of unrelated inputs by k parallel decision tree algorithms. In the product problem, for some fixed depth bound d, we seek to maximize the fraction of input k-tuples for which all k decision trees are correct. Assume that for a single input to f, the best decision tree algorithm of depth d is correct on a fraction p of inputs. We prove that the maximum fraction of k-tuples on which k depth d algorithms are all correct is at most p/sup k/, which is the trivial lower bound. We show that if we replace the depth d restriction by "expected depth d", then this result fails. In the help-bit problem, we are permitted to ask k-1 arbitrary binary questions about the k-tuple of inputs. For each possible k-1-tuple of answers to these queries we will have a k-tuple of decision trees which are supposed to correctly compute all functions on k-tuples that are consistent with the particular answers. The complexity here is the maximum depth of any of the trees in the algorithm. We show that for all k sufficiently large, this complexity is equal to deg/sup s/(f) which is the minimum degree of a multivariate polynomial whose sign is equal to f. Finally, we give a brief discussion of these problems in the context of other complexity models.<<ETX>>},
  citationcount = {46},
  venue = {Proceedings 35th Annual Symposium on Foundations of Computer Science}
}

@article{nisanRoundsInCommunication1991,
  title = {Rounds in Communication Complexity Revisited},
  author = {Nisan, N. and Wigderson, A.},
  year = {1991},
  doi = {10.1145/103418.103463},
  abstract = {The k-round two-party communication complexity was studied in the deterministic model by [P. H. Papadimitriou and M. Sipser, Proc. of the 14th STOC, 1982, pp. 330--337] and [P. Duris, Z. Galil, and G. Schnitger, Proc. of the 16th STOC, 1984, pp. 81--91] and in the probabilistic model by [A. C. Yao, Proc. of the 24th FOCS, 1983, pp. 420--428] and [B. Halstenberg and R. Reischuk, Proc. of the 20th STOC, 1988, pp. 162--172]. This paper presents new lower bounds that give (1) randomization is more powerful than determinism in k-round protocols, and (2) an explicit function which exhibits an exponential gap between its k and (k-1)-round randomized complexity.This paper also studies the three-party communication model, and exhibits an exponential gap in 3-round protocols that differ in the starting player.Finally, this paper shows new connections of these questions to circuit complexity, that motivate further work in this direction.},
  citationcount = {228},
  venue = {Symposium on the Theory of Computing}
}

@article{nishimotoACompressedDynamic2017,
  title = {A Compressed Dynamic Self-Index for Highly Repetitive Text Collections},
  author = {Nishimoto, Takaaki and Takabatake, Yoshimasa and Tabei, Yasuo},
  year = {2017},
  doi = {10.1016/j.ic.2020.104518},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Information and Computation},
  keywords = {dynamic}
}

@article{nishimotoADynamicCompressed2018,
  title = {A Dynamic Compressed Self-Index for Highly Repetitive Text Collections},
  author = {Nishimoto, Takaaki and Takabatake, Yoshimasa and Tabei, Yasuo},
  year = {2018},
  doi = {10.1109/DCC.2018.00037},
  abstract = {We present a novel compressed dynamic self-index for highly repetitive text collections. Signature encoding, an existing self-index of this type, has a large disadvantage of slow pattern search for short patterns. We obtain faster pattern search by leveraging the idea behind a truncated suffix tree (TST) to develop the first compressed dynamic self-index, called the TST-index, that supports not only fast pattern search but also dynamic update operations for highly repetitive texts. Experiments with a benchmark dataset show that the pattern search performance of the TST-index is significantly improved.},
  citationcount = {1},
  venue = {Data Compression Conference},
  keywords = {dynamic,update}
}

@article{nishimotoConversionFromRlbwt2019,
  title = {Conversion from {{RLBWT}} to {{LZ77}}},
  author = {Nishimoto, Takaaki and Tabei, Yasuo},
  year = {2019},
  doi = {10.4230/LIPIcs.CPM.2019.9},
  abstract = {Converting a compressed format of a string into another compressed format without an explicit decompression is one of the central research topics in string processing. We discuss the problem of converting the run-length Burrows-Wheeler Transform (RLBWT) of a string to Lempel-Ziv 77 (LZ77) phrases of the reversed string. The first results with Policriti and Prezza's conversion algorithm [Algorithmica 2018] were O(nr) time and O(r) working space for length of the string n, number of runs r in the RLBWT, and number of LZ77 phrases z. Recent results with Kempa's conversion algorithm [SODA 2019] are O(n/n+r\textsuperscript{\{\vphantom\}}9\vphantom\{\}n+z\textsuperscript{\{\vphantom\}}9\vphantom\{\}n) time and O(n/\textsubscript{\{\vphantom\}}{$\sigma$}\vphantom\{\}n+r\textsuperscript{\{\vphantom\}}8\vphantom\{\}n) working space for the alphabet size {$\sigma$} of the RLBWT. In this paper, we present a new conversion algorithm by improving Policriti and Prezza's conversion algorithm where dynamic data structures for general purpose are used. We argue that these dynamic data structures can be replaced and present new data structures for faster conversion. The time and working space of our conversion algorithm with new data structures are O(n n,{\textsurd}\{\vphantom\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}r\vphantom\{\}\{r\}\vphantom\{\} ) and O(r), respectively.},
  citationcount = {4},
  venue = {Annual Symposium on Combinatorial Pattern Matching},
  keywords = {data structure,dynamic}
}

@article{nishimotoDynamicIndexAnd2016,
  title = {Dynamic Index and {{LZ}} Factorization in Compressed Space},
  author = {Nishimoto, Takaaki and I., T. and Inenaga, Shunsuke and Bannai, H. and Takeda, Masayuki},
  year = {2016},
  doi = {10.1016/j.dam.2019.01.014},
  abstract = {No abstract available},
  citationcount = {40},
  venue = {Prague Stringology Conference},
  keywords = {dynamic}
}

@article{nishimotoFullyDynamicData2016,
  title = {Fully Dynamic Data Structure for {{LCE}} Queries in Compressed Space},
  author = {Nishimoto, Takaaki and I., T. and Inenaga, Shunsuke and Bannai, H. and Takeda, Masayuki},
  year = {2016},
  doi = {10.4230/LIPIcs.MFCS.2016.72},
  abstract = {A Longest Common Extension (LCE) query on a text T of length N asks for the length of the longest common prefix of suffixes starting at given two positions. We show that the signature encoding \{G\} of size w=O((zN\textsuperscript{*}M,N)) [Mehlhorn et al., Algorithmica 17(2):183-198, 1997] of T, which can be seen as a compressed representation of T, has a capability to support LCE queries in O(N+{$\ell$}\textsuperscript{*}M) time, where {$\ell$} is the answer to the query, z is the size of the Lempel-Ziv77 (LZ77) factorization of T, and M{$\geq$}4N is an integer that can be handled in constant time under word RAM model. In compressed space, this is the fastest deterministic LCE data structure in many cases. Moreover, \{G\} can be enhanced to support efficient update operations: After processing \{G\} in O(wf\textsubscript{\{\vphantom\}}\{A\}\vphantom\{\}) time, we can insert/delete any (sub)string of length y into/from an arbitrary position of T in O((y+N\textsuperscript{*}M)f\textsubscript{\{\vphantom\}}\{A\}\vphantom\{\}) time, where f\textsubscript{\{\vphantom\}}\{A\}\vphantom\{\}=O( \textsuperscript{\{\vphantom\}}{\textfractionsolidus}Mw\vphantom\{\}\{M\},{\textsurd}\{\vphantom\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}w\vphantom\{\}\{w\}\vphantom\{\} ). This yields the first fully dynamic LCE data structure. We also present efficient construction algorithms from various types of inputs: We can construct \{G\} in O(Nf\textsubscript{\{\vphantom\}}\{A\}\vphantom\{\}) time from uncompressed string T; in O(nnN\textsuperscript{*}M) time from grammar-compressed string T represented by a straight-line program of size n; and in O(zf\textsubscript{\{\vphantom\}}\{A\}\vphantom\{\}N\textsuperscript{*}M) time from LZ77-compressed string T with z factors. On top of the above contributions, we show several applications of our data structures which improve previous best known results on grammar-compressed string processing.},
  citationcount = {64},
  venue = {International Symposium on Mathematical Foundations of Computer Science},
  keywords = {data structure,dynamic,query,update}
}

@article{nishimuraEnumeratingNeighbourAnd2012,
  title = {Enumerating Neighbour and Closest Strings},
  author = {Nishimura, N. and Simjour, Narges},
  year = {2012},
  doi = {10.1007/978-3-642-33293-7_24},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {International Symposium on Parameterized and Exact Computation}
}

@article{norbertblumLowerBoundSingleOperation1994,
  title = {A {{Lower Bound}} on the {{Single-Operation Worst-Case Time Complexity}} of the {{Union-Find Problem}} on {{Intervals}}},
  author = {Norbert Blum and Henning Rochow},
  year = {1994},
  journal = {Information Processing Letters},
  doi = {10.1016/0020-0190(94)00082-4},
  keywords = {lower bound},
  annotation = {Citation Count: 4}
}

@article{norbertblumSingleOperationWorstCaseTime1985,
  title = {On the {{Single-Operation Worst-Case Time Complexity}} on the {{Disjoint Set Union Problem}}},
  author = {Norbert Blum},
  year = {1985},
  journal = {Symposium on Theoretical Aspects of Computer Science},
  doi = {10.1007/BFb0023992},
  annotation = {Citation Count: 40}
}

@article{nskenFastMultipointEvaluation2004,
  title = {Fast Multipoint Evaluation of Bivariate Polynomials},
  author = {N{\"u}sken, Michael and Ziegler, M.},
  year = {2004},
  doi = {10.1007/978-3-540-30140-0_49},
  abstract = {No abstract available},
  citationcount = {43},
  venue = {Embedded Systems and Applications}
}

@article{o'donnellOptimalLowerBounds2009,
  title = {Optimal Lower Bounds for Locality-Sensitive Hashing (except When q Is Tiny)},
  author = {O'Donnell, R. and Wu, Yi and Zhou, Yuan},
  year = {2009},
  doi = {10.1145/2578221},
  abstract = {We study lower bounds for Locality-Sensitive Hashing (LSH) in the strongest setting: point sets in \{0,1\}{\textexclamdown}sup{\textquestiondown}d{\textexclamdown}/sup{\textquestiondown} under the Hamming distance. Recall that H is said to be an ({\textexclamdown}i{\textquestiondown}r{\textexclamdown}/i{\textquestiondown}, {\textexclamdown}i{\textquestiondown}cr{\textexclamdown}/i{\textquestiondown}, {\textexclamdown}i{\textquestiondown}p{\textexclamdown}/i{\textquestiondown}, {\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown})-sensitive hash family if all pairs {\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}, {\textexclamdown}i{\textquestiondown}y{\textexclamdown}/i{\textquestiondown} {$\in$} \{0,1\}{\textexclamdown}sup{\textquestiondown}d{\textexclamdown}/sup{\textquestiondown} with dist({\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}, {\textexclamdown}i{\textquestiondown}y{\textexclamdown}/i{\textquestiondown}) {$\leq$} {\textexclamdown}i{\textquestiondown}r{\textexclamdown}/i{\textquestiondown} have probability at least {\textexclamdown}i{\textquestiondown}p{\textexclamdown}/i{\textquestiondown} of collision under a randomly chosen {\textexclamdown}i{\textquestiondown}h{\textexclamdown}/i{\textquestiondown} {$\in$} H, whereas all pairs {\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}, {\textexclamdown}i{\textquestiondown}y{\textexclamdown}/i{\textquestiondown} {$\in$} \{0, 1\}{\textexclamdown}sup{\textquestiondown}d{\textexclamdown}/sup{\textquestiondown} with dist({\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}, {\textexclamdown}i{\textquestiondown}y{\textexclamdown}/i{\textquestiondown}) {$\geq$} {\textexclamdown}i{\textquestiondown}cr{\textexclamdown}/i{\textquestiondown} have probability at most {\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown} of collision. Typically, one considers {\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown} {$\rightarrow$} {$\infty$}, with {\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown} {\textquestiondown} 1 fixed and {\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown} bounded away from 0. For its applications to approximate nearest-neighbor search in high dimensions, the quality of an LSH family H is governed by how small its {\textexclamdown}i{\textquestiondown}{$\rho$}{\textexclamdown}/i{\textquestiondown} {\textexclamdown}i{\textquestiondown}parameter{\textexclamdown}/i{\textquestiondown} {\textexclamdown}i{\textquestiondown}{$\rho$}{\textexclamdown}/i{\textquestiondown} = ln(1/{\textexclamdown}i{\textquestiondown}p{\textexclamdown}/i{\textquestiondown})/ln(1/{\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown}) is as a function of the parameter {\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown}. The seminal paper of Indyk and Motwani [1998] showed that for each {\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown} {$\geq$} 1, the extremely simple family H = \{{\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown} {$\mapsto$} {\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}i{\textexclamdown}/sub{\textquestiondown} : {\textexclamdown}i{\textquestiondown}i{\textexclamdown}/i{\textquestiondown} {$\in$} [{\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}]\} achieves {\textexclamdown}i{\textquestiondown}{$\rho$}{\textexclamdown}/i{\textquestiondown} {$\leq$} 1/{\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown}. The only known lower bound, due to Motwani et al. [2007], is that {\textexclamdown}i{\textquestiondown}{$\rho$}{\textexclamdown}/i{\textquestiondown} must be at least ( {\textexclamdown}i{\textquestiondown}e{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}1/c{\textexclamdown}/sup{\textquestiondown} - 1)/({\textexclamdown}i{\textquestiondown}e{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}1/c{\textexclamdown}/sup{\textquestiondown} + 1) {$\geq$} .46/{\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown} (minus {\textexclamdown}i{\textquestiondown}o{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}d{\textexclamdown}/sub{\textquestiondown}(1)). The contribution of this article is twofold. (1) We show the ``optimal'' lower bound for {\textexclamdown}i{\textquestiondown}{$\rho$}{\textexclamdown}/i{\textquestiondown}: it must be at least 1/{\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown} (minus {\textexclamdown}i{\textquestiondown}o{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}d{\textexclamdown}/sub{\textquestiondown}(1)). Our proof is very simple, following almost immediately from the observation that the noise stability of a boolean function at time {\textexclamdown}i{\textquestiondown}t{\textexclamdown}/i{\textquestiondown} is a log-convex function of {\textexclamdown}i{\textquestiondown}t{\textexclamdown}/i{\textquestiondown}. (2) We raise and discuss the following issue: neither the application of LSH to nearest-neighbor search nor the known LSH lower bounds hold as stated if the {\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown} parameter is tiny. Here, ``tiny'' means {\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown} = 2{\textexclamdown}sup{\textquestiondown}-{$\Theta$}(d){\textexclamdown}/sup{\textquestiondown}, a parameter range we believe is natural.},
  citationcount = {115},
  venue = {TOCT},
  keywords = {lower bound}
}

@article{ohrhallingerAnEfficientAlgorithm2013,
  title = {An Efficient Algorithm for Determining an Aesthetic Shape Connecting Unorganized {{2D}} Points},
  author = {Ohrhallinger, S. and Mudur, S.},
  year = {2013},
  doi = {10.1111/cgf.12162},
  abstract = {We present anefficient algorithm for determining an aesthetically pleasing shape boundary connecting all the points in a given unorganized set of 2D points, with no other information than point coordinates. By posing shape construction as a minimisation problem which follows the Gestalt laws, our desired shape Bmin is non-intersecting, interpolates all points and minimizes a criterion related to these laws. The basis for our algorithm is an initial graph, an extension of the Euclidean minimum spanning tree but with no leaf nodes, called as the minimum boundary complex BCmin . BCmin and Bmin can be expressed similarly by parametrizing a topological constraint. A close approximation of BCmin , termed BC0 can be computed fast using a greedy algorithm. BC0 is then transformed into a closed interpolating boundary Bout in two steps to satisfy Bmin 's topological and minimization requirements. Computing Bmin exactly is an NP (Non-Polynomial)-hard problem, whereas Bout is computed in linearithmic time. We present many examples showing considerable improvement over previous techniques, especially for shapes with sharp corners. Source code is available online.},
  citationcount = {21},
  venue = {Computer graphics forum (Print)}
}

@article{okanoharaPracticalEntropyCompressed2006,
  title = {Practical Entropy-Compressed Rank/Select Dictionary},
  author = {Okanohara, Daisuke and Sadakane, K.},
  year = {2006},
  doi = {10.1137/1.9781611972870.6},
  abstract = {Rank/Select dictionaries are data structures for an ordered set S {$\subset$} \{0,1, . . ., n - 1\} to compute rank(x, S) (the number of elements in S that are no greater than x), and select(i, S) (the i-th smallest element in S), which are the fundamental components of succinct data structures of strings, trees, graphs, etc.. In these data structures, however, only asymptotic behavior has been considered and their performance for real data is not satisfactory. In this paper, we propose four novel Rank/Select dictionaries: esp, recrank, vcode and sdarray, each of which is small if the number of elements in S is small, and indeed close to nH0(S) (H0(S) {$\leq$} 1 is the zero-th order empirical entropy of S) in practice. Furthermore, their query times are superior to those of existing structures. Experimental results reveal the characteristics of our data structures and also show that these data structures are superior to existing implementations, both in terms of size and query time.},
  citationcount = {317},
  venue = {Workshop on Algorithm Engineering and Experimentation},
  keywords = {data structure,query,query time}
}

@article{olkowskiDynamicDataStructures2022,
  title = {Dynamic Data Structures for Parameterized String Problems},
  author = {Olkowski, Jedrzej and Pilipczuk, Michal and Rychlicki, Mateusz and Wegrzycki, Karol and {Zych-Pawlewicz}, Anna},
  year = {2022},
  doi = {10.48550/arXiv.2205.00441},
  abstract = {We revisit classic string problems considered in the area of parameterized complexity, and study them through the lens of dynamic data structures. That is, instead of asking for a static algorithm that solves the given instance efficiently, our goal is to design a data structure that efficiently maintains a solution, or reports a lack thereof, upon updates in the instance. We first consider the Closest String problem, for which we design randomized dynamic data structures with amortized update times d\textsuperscript{\{\vphantom\}}\{O\}(d)\vphantom\{\} and {\textbar}{$\Sigma\vert$}\textsuperscript{\{\vphantom\}}\{O\}(d)\vphantom\{\}, respectively, where {$\Sigma$} is the alphabet and d is the assumed bound on the maximum distance. These are obtained by combining known static approaches to Closest String with color-coding. Next, we note that from a result of Frandsen et al. [J. ACM'97] one can easily infer a meta-theorem that provides dynamic data structures for parameterized string problems with worst-case update time of the form \{O\}(n), where k is the parameter in question and n is the length of the string. We showcase the utility of this meta-theorem by giving such data structures for problems Disjoint Factors and Edit Distance. We also give explicit data structures for these problems, with worst-case update times \{O\}(k2\textsuperscript{\{\vphantom\}}k\vphantom\{\}n) and \{O\}(k{$^2$}n), respectively. Finally, we discuss how a lower bound methodology introduced by Amarilli et al. [ICALP'21] can be used to show that obtaining update time \{O\}(f(k)) for Disjoint Factors and Edit Distance is unlikely already for a constant value of the parameter k.},
  citationcount = {4},
  venue = {Symposium on Theoretical Aspects of Computer Science},
  keywords = {data structure,dynamic,lower bound,static,update,update time}
}

@article{olsonAnAdditionTheorem1968,
  title = {An Addition Theorem modulo p},
  author = {Olson, J. E.},
  year = {1968},
  doi = {10.1016/S0021-9800(68)80027-4},
  abstract = {No abstract available},
  citationcount = {98},
  venue = {No venue available}
}

@article{olsonBalancingFamiliesOf1978,
  title = {Balancing Families of Sets},
  author = {Olson, J. E. and Spencer, J.},
  year = {1978},
  doi = {10.1016/0097-3165(78)90028-6},
  abstract = {No abstract available},
  citationcount = {19},
  venue = {Journal of Combinatorial Theory}
}

@article{olteanuRecentIncrementsIn2024,
  title = {Recent Increments in Incremental View Maintenance},
  author = {Olteanu, Dan},
  year = {2024},
  doi = {10.1145/3635138.3654763},
  abstract = {We overview recent progress on the longstanding problem of incremental view maintenance (IVM), with a focus on the fine-grained complexity and optimality of IVM for classes of conjunctive queries. This theoretical progress is accompanied by efforts that show its benefits in practical settings. When taken in isolation, each of the reported advancements is but a small increment. Yet when taken together, they may well pave the way to a deeper understanding of the IVM problem. This paper accompanies the invited Gems of PODS 2024 talk with the same title. Some of the works highlighted in this paper are based on prior and on-going collaborations with: Ahmet Kara, Milos Nikolic, and Haozhe Zhang from the F-IVM project at University of Edinburgh and University of Zurich; and Mahmoud Abo Khamis, Niko Goebels, Hung Ngo, and Dan Suciu from RelationalAI and University of Washington.},
  citationcount = {Unknown},
  venue = {PODS Companion},
  keywords = {query}
}

@article{ostrovskyPolynomialTimeApproximation2000,
  title = {Polynomial Time Approximation Schemes for Geometric K-Clustering},
  author = {Ostrovsky, R. and Rabani, Y.},
  year = {2000},
  doi = {10.1109/SFCS.2000.892123},
  abstract = {We deal with the problem of clustering data points. Given n points in a larger set (for example, R/sup d/) endowed with a distance function (for example, L/sup 2/ distance), we would like to partition the data set into k disjoint clusters, each with a "cluster center", so as to minimize the sum over all data points of the distance between the point and the center of the cluster containing the point. The problem is provably NP-hard in some high dimensional geometric settings, even for k=2. We give polynomial time approximation schemes for this problem in several settings, including the binary cube (0, 1)/sup d/ with Hamming distance, and R/sup d/ either with L/sup 1/ distance, or with L/sup 2/ distance, or with the square of L/sup 2/ distance. In all these settings, the best previous results were constant factor approximation guarantees. We note that our problem is similar in flavor to the k-median problem (and the related facility location problem), which has been considered in graph-theoretic and fixed dimensional geometric settings, where it becomes hard when k is part of the input. In contrast, we study the problem when k is fixed, but the dimension is part of the input. Our algorithms are based on a dimension reduction construction for the Hamming cube, which may be of independent interest.},
  citationcount = {57},
  venue = {Proceedings 41st Annual Symposium on Foundations of Computer Science}
}

@article{overmarsEfficientDataStructures1988,
  title = {Efficient Data Structures for Range Searching on a Grid},
  author = {Overmars, M.},
  year = {1988},
  doi = {10.1016/0196-6774(88)90041-7},
  abstract = {No abstract available},
  citationcount = {101},
  venue = {J. Algorithms}
}

@article{overmarsMaintainingRangeTrees1990,
  title = {Maintaining Range Trees in Secondary Memory},
  author = {Overmars, M. and Smid, M. and Berg, M. D. and Kreveld, M. V.},
  year = {1990},
  doi = {10.1007/BF00289018},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Acta Informatica}
}

@article{overmarsTheDesignOf1987,
  title = {The Design of Dynamic Data Structures},
  author = {Overmars, M.},
  year = {1987},
  doi = {10.1007/BFb0014927},
  abstract = {No abstract available},
  citationcount = {452},
  venue = {Lecture Notes in Computer Science},
  keywords = {data structure,dynamic}
}

@article{oxleyMatroidTheory1992,
  title = {Matroid Theory},
  author = {Oxley, J.},
  year = {1992},
  doi = {10.1090/conm/197},
  abstract = {The comments below apply to all printings of the book dated 2005 or earlier. The table following contains more than just a list of typing errors. Some statements and proofs have been corrected, simplified, or clarified. Moreover, the current status has been given for all the unsolved problems or conjectures that appear in Chapter 14. For those changes that simply involve the insertion of extra words, the corrected text is given with the inserted words underlined. It is planned to update this table at regular intervals and, eventually, these changes should be incorporated into the next printing of the book. The reader is encouraged to send the author {\textexclamdown}oxley@math.lsu.edu{\textquestiondown} corrections that do not appear in the table below.},
  citationcount = {2922},
  venue = {No venue available}
}

@article{p.agarwalOefficientBatchedUnionfind2006,
  title = {I/{{O-efficient}} Batched Union-Find and Its Applications to Terrain Analysis},
  author = {P. Agarwal and L. Arge and K. Yi},
  year = {2006},
  journal = {SCG '06},
  doi = {10.1145/1137856.1137884},
  abstract = {Despite extensive study over the last four decades and numerous applications, no I/O-efficient algorithm is known for the union-find problem. In this paper we present an I/O-efficient algorithm for the batched (off-line) version of the union-find problem. Given any sequence of N union and find operations, where each union operation joins two distinct sets, our algorithm uses O(sort(N)) = O(N/BlogM/BN/B) I/Os, where M is the memory size and B is the disk block size. This bound is asymptotically optimal in the worst case. If there are union operations that join a set with itself, our algorithm uses O(sort(N) + mst(N)) I/Os, where mst(N) is the number of I/Os needed to compute the minimum spanning tree of a graph with N edges. We also describe a simple and practical O(sort(N)log(N/M))-I/O algorithm for this problem, which we have implemented.We are interested in the union-find problem because of its applications in terrain analysis. A terrain can be abstracted as a height function defined over R2, and many problems that deal with such functions require a union-find data structure. With the emergence of modern mapping technologies, huge amount of elevation data is being generated that is too large to fit in memory, thus I/O-efficient algorithms are needed to process this data efficiently. In this paper, we study two terrain analysis problems that benefit from a union-find data structure: (i) computing topological persistence and (ii) constructing the contour tree. These structures have important applications such as terrain modeling, flow analysis, topological feature extraction, etc. We give the first O(sort(N))-I/O algorithms for these two problems, assuming that the input terrain is represented as a triangular mesh with N vertices.Finally, we report some preliminary experimental results, showing that our algorithms give order-of-magnitude improvement over previous methods on large data sets that do not fit in memory.},
  keywords = {data structure},
  annotation = {Citation Count: 51}
}

@article{p.agarwalOefficientBatchedUnionfind2010,
  title = {I/{{O-efficient}} Batched Union-Find and Its Applications to Terrain Analysis},
  author = {P. Agarwal and L. Arge and K. Yi},
  year = {2010},
  journal = {TALG},
  doi = {10.1145/1868237.1868249},
  abstract = {In this article we present an I/O-efficient algorithm for the batched (off-line) version of the union-find problem. Given any sequence of \emph{N} union and find operations, where each union operation joins two distinct sets, our algorithm uses \emph{O}(SORT(\emph{N})) = \emph{O}(\&frac;\emph{N}\emph{B} log\textsubscript{\emph{M/B}}\&frac;\emph{N}\emph{B}) I/Os, where \emph{M} is the memory size and \emph{B} is the disk block size. This bound is asymptotically optimal in the worst case. If there are union operations that join a set with itself, our algorithm uses \emph{O}(SORT(\emph{N}) + MST(\emph{N})) I/Os, where MST(\emph{N}) is the number of I/Os needed to compute the minimum spanning tree of a graph with \emph{N} edges. We also describe a simple and practical \emph{O}(SORT(\emph{N}) log(\&frac;\emph{N}\emph{M}))-I/O algorithm for this problem, which we have implemented.  We are interested in the union-find problem because of its applications in terrain analysis. A terrain can be abstracted as a height function defined over R\textsuperscript{2}, and many problems that deal with such functions require a union-find data structure. With the emergence of modern mapping technologies, huge amount of elevation data is being generated that is too large to fit in memory, thus I/O-efficient algorithms are needed to process this data efficiently. In this article, we study two terrain-analysis problems that benefit from a union-find data structure: (i) computing topological persistence and (ii) constructing the contour tree. We give the first \emph{O}(SORT(\emph{N}))-I/O algorithms for these two problems, assuming that the input terrain is represented as a triangular mesh with \emph{N} vertices.},
  keywords = {data structure},
  annotation = {Citation Count: 18}
}

@article{p.agarwalOptimalDynamicData2012,
  title = {An {{Optimal Dynamic Data Structure}} for {{Stabbing-Semigroup Queries}}},
  author = {P. Agarwal and L. Arge and Haim Kaplan and Eyal Molad and R. Tarjan and K. Yi},
  year = {2012},
  journal = {SIAM journal on computing (Print)},
  doi = {10.1137/10078791X},
  abstract = {Let \$S\$ be a set of \$n\$ intervals in \${\textbackslash}mathbb\{R\}\$, and let \$({\textbackslash}mathbf\{S\}, +)\$ be any commutative semigroup. We assign a weight \${\textbackslash}omega(s) {\textbackslash}in {\textbackslash}mathbf\{S\}\$ to each interval in \$S\$. For a point \$x {\textbackslash}in {\textbackslash}mathbb\{R\}\$, let \$S(x) {\textbackslash}subseteq S\$ be the set of intervals that contain \$x\$. Given a point \$q {\textbackslash}in {\textbackslash}mathbb\{R\}\$, the stabbing-semigroup query asks for computing \${\textbackslash}sum\_\{s {\textbackslash}in S(q)\} {\textbackslash}omega(s)\$. We propose a linear-size dynamic data structure, under the pointer-machine model, that answers queries in worst-case \$O({\textbackslash}log n)\$ time and supports both insertions and deletions of intervals in amortized \$O({\textbackslash}log n)\$ time. It is the first data structure that attains the optimal \$O({\textbackslash}log n)\$ bound for all three operations. Furthermore, our structure can easily be adapted to external memory, where we obtain a linear-size structure that answers queries and supports updates in \$O({\textbackslash}log\_B n)\$ I/Os, where \$B\$ is the disk block size. For the restricted case of a nested family of intervals (either every pair of intervals is disjoint or one contains the other), we present a simpler solution based on dynamic trees.},
  keywords = {data structure,dynamic,query,update},
  annotation = {Citation Count: 15}
}

@article{p.ferraginaStringBtreeNew1999,
  title = {The String {{B-tree}}: A New Data Structure for String Search in External Memory and Its Applications},
  author = {P. Ferragina and R. Grossi},
  year = {1999},
  journal = {JACM},
  doi = {10.1145/301970.301973},
  abstract = {We introduce a new text-indexing data structure, the String B-Tree, that can be seen as a link between some traditional external-memory and string-matching data structures. In a short phrase, it is a combination of B-trees and Patricia tries for internal-node indices that is made more effective by adding extra pointers to speed up search and update operations. Consequently, the String B-Tree overcomes the theoretical limitations of inverted files, B-trees, prefix B-trees, suffix arrays, compacted tries and suffix trees. String B-trees have the same worst-case performance as B-trees but they manage unbounded-length strings and perform much more powerful search operations such as the ones supported by suffix trees. String B-trees are also effective in main memory (RAM model) because they improve the online suffix tree search on a dynamic set of strings. They also can be successfully applied to database indexing and software duplication.},
  keywords = {data structure,dynamic,update},
  annotation = {Citation Count: 338}
}

@article{pachWileyInterscienceSeries2011,
  title = {Wiley-interscience Series in Discrete Mathematics and Optimization},
  author = {Pach, J. and Agarwal, P.},
  year = {2011},
  doi = {10.1002/9781118033203.SCARD},
  abstract = {No abstract available},
  citationcount = {152},
  venue = {No venue available}
}

@article{pagelDeflatingTheDimensionality2000,
  title = {Deflating the Dimensionality Curse Using Multiple Fractal Dimensions},
  author = {Pagel, Bernd-Uwe and Korn, Flip and Faloutsos, C.},
  year = {2000},
  doi = {10.1109/ICDE.2000.839457},
  abstract = {Nearest neighbor queries are important in many settings, including spatial databases (find the k closet cities) and multimedia databases (find the k most similar images). Previous analyses have concluded that nearest neighbor search is hopeless in high dimensions, due to the notorious "curse of dimensionality". However, their precise analysis over real data sets is still an open problem. The typical and often implicit assumption in previous studies is that the data is uniformly distributed, with independence between attributes. However, real data sets overwhelmingly disobey these assumptions; rather, they typically are skewed and exhibit intrinsic ("fractal") dimensionalities that are much lower than their embedding dimension, e.g., due to subtle dependencies between attributes. We show how the Hausdorff and correlation fractal dimensions of a data set can yield extremely accurate formulas that can predict I/O performance to within one standard deviation. The practical contributions of this work are our accurate formulas which can be used for query optimization in spatial and multimedia databases. The theoretical contribution is the 'deflation' of the dimensionality curse. Our theoretical and empirical results show that previous worst-case analysis of nearest neighbor search in high dimensions are over-pessimistic, to the point of being unrealistic. The performance depends critically on the intrinsic ("fractal") dimensionality as opposed to the embedding dimension that the uniformity assumption incorrectly implies.},
  citationcount = {114},
  venue = {Proceedings / International Conference on Data Engineering},
  keywords = {query}
}

@article{pagelTowardsAnAnalysis1993,
  title = {Towards an Analysis of Range Query Performance in Spatial Data Structures},
  author = {Pagel, Bernd-Uwe and Six, H. and Toben, H. and Widmayer, P.},
  year = {1993},
  doi = {10.1145/153850.153878},
  abstract = {In this paper, we motivate four different user defined window query classes and derive a probabilistic model for each of them. For each model, we characterize the efficiency of spatial data structures in terms of the expected number of data bucket accesses needed to perform a window query. Our analytical approach exhibits the performance phenomena independent of data structure and implementation details and whether the objects are points or non-point objects.},
  citationcount = {261},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems}
}

@article{paghCuckooHashing2001,
  title = {Cuckoo Hashing},
  author = {Pagh, R. and Rodler, Flemming Friche},
  year = {2001},
  doi = {10.1007/978-0-387-30162-4_97},
  abstract = {No abstract available},
  citationcount = {1388},
  venue = {Encyclopedia of Algorithms}
}

@article{paghFasterDeterministicDictionaries1999,
  title = {Faster Deterministic Dictionaries},
  author = {Pagh, R.},
  year = {1999},
  doi = {10.7146/BRICS.V6I48.20118},
  abstract = {We consider static dictionaries over the universe U = \{0, 1\}{\textasciicircum}w on a unit-cost RAM with word size w. Construction of a static dictionary with linear space consumption and constant lookup time can be done in linear expected time by a randomized algorithm. In contrast, the best previous deterministic algorithm for constructing such a dictionary with n elements runs in time O(n{\textasciicircum}(1+epsilon)) for epsilon {\textquestiondown} 0. This paper narrows the gap between deterministic and randomized algorithms exponentially, from the factor of n{\textasciicircum}epsilon to an O(log n) factor. The algorithm is weakly non-uniform, i.e. requires certain precomputed constants dependent on w. A by-product of the result is a lookup time vs insertion time trade-off for dynamic dictionaries, which is optimal for a certain class of deterministic hashing schemes.},
  citationcount = {17},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,static}
}

@article{paghLowRedundancyStatic1999,
  title = {Low Redundancy in Static Dictionaries with {{O}}(1) Worst Case Lookup Time},
  author = {Pagh, R.},
  year = {1999},
  doi = {10.1007/3-540-48523-6_56},
  abstract = {No abstract available},
  citationcount = {50},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {static}
}

@article{paghLowRedundancyStatic2002,
  title = {Low Redundancy in Static Dictionaries with Constant Query Time},
  author = {Pagh, R.},
  year = {2002},
  doi = {10.1137/S0097539700369909},
  abstract = {A static dictionary is a data structure storing subsets of a finite universe U, answering membership queries. We show that on a unit cost RAM with word size {$\Theta$}({\textbar}U{\textbar}), a static dictionary for n-element sets with constant worst case query time can be obtained using B+O({\textbar}U{\textbar})+o(n) bits of storage, where B=\{{$_2$}\{{\textbar}U{\textbar}\}\{n\}\} is the minimum number of bits needed to represent all n-element subsets of U.},
  citationcount = {45},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,query,query time,static}
}

@article{panarioOpenProblemsFor2014,
  title = {Open Problems for Polynomials over Finite Fields and Applications},
  author = {Panario, D.},
  year = {2014},
  doi = {10.1007/978-3-319-10683-0_6},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Open Problems in Mathematics and Computational Science}
}

@article{panigrahyAnImprovedAlgorithm2008,
  title = {An Improved Algorithm Finding Nearest Neighbor Using Kd-Trees},
  author = {Panigrahy, R.},
  year = {2008},
  doi = {10.1007/978-3-540-78773-0_34},
  abstract = {No abstract available},
  citationcount = {51},
  venue = {Latin American Symposium on Theoretical Informatics}
}

@article{panigrahyEntropyBasedNearest2005,
  title = {Entropy Based Nearest Neighbor Search in High Dimensions},
  author = {Panigrahy, R.},
  year = {2005},
  doi = {10.1145/1109557.1109688},
  abstract = {In this paper we study the problem of finding the approximate nearest neighbor of a query point in the high dimensional space, focusing on the Euclidean space. The earlier approaches use locality-preserving hash functions (that tend to map nearby points to the same value) to construct several hash tables to ensure that the query point hashes to the same bucket as its nearest neighbor in at least one table. Our approach is different - we use one (or a few) hash table and hash several randomly chosen points in the neighborhood of the query point showing that at least one of them will hash to the bucket containing its nearest neighbor. We show that the number of randomly chosen points in the neighborhood of the query point {\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown} required depends on the entropy of the hash value {\textexclamdown}i{\textquestiondown}h(p){\textexclamdown}/i{\textquestiondown} of a random point {\textexclamdown}i{\textquestiondown}p{\textexclamdown}/i{\textquestiondown} at the same distance from {\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown} at its nearest neighbor, given {\textexclamdown}i{\textquestiondown}q{\textexclamdown}/i{\textquestiondown} and the locality preserving hash function {\textexclamdown}i{\textquestiondown}h{\textexclamdown}/i{\textquestiondown} chosen randomly from the hash family. Precisely, we show that if the entropy {\textexclamdown}i{\textquestiondown}I(h(p){\textbar}q, h){\textexclamdown}/i{\textquestiondown} = {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown} and {\textexclamdown}i{\textquestiondown}g{\textexclamdown}/i{\textquestiondown} is a bound on the probability that two far-off points will hash to the same bucket, then we can find the approximate nearest neighbor in {\textexclamdown}i{\textquestiondown}O(n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}{\textexclamdown}i{\textquestiondown}p{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sup{\textquestiondown}) time and near linear {\textexclamdown}i{\textquestiondown}{\~O}{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) space where {\textexclamdown}i{\textquestiondown}p{\textexclamdown}/i{\textquestiondown} = {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown}/log(1/g). Alternatively we can build a data structure of size {\textexclamdown}i{\textquestiondown}{\~O}{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}1/(1-p){\textexclamdown}/sup{\textquestiondown}) to answer queries in {\textexclamdown}i{\textquestiondown}{\~O}{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}) time. By applying this analysis to the locality preserving hash functions in [17, 21, 6] and adjusting the parameters we show that the {\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown} nearest neighbor can be computed in time {\textexclamdown}i{\textquestiondown}{\~O}{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}p{\textexclamdown}/sup{\textquestiondown}) and near linear space where {\textexclamdown}i{\textquestiondown}p{\textexclamdown}/i{\textquestiondown} {$\approx$} 2.06/{\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown} as {\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown} becomes large.},
  citationcount = {257},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,query}
}

@inproceedings{panigrahyGeometricApproachLower2008,
  title = {A {{Geometric Approach}} to {{Lower Bounds}} for {{Approximate Near-Neighbor Search}} and {{Partial Match}}},
  booktitle = {2008 49th {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Panigrahy, Rina and Talwar, Kunal and Wieder, Udi},
  year = {2008},
  month = oct,
  pages = {414--423},
  issn = {0272-5428},
  doi = {10.1109/FOCS.2008.68},
  url = {https://ieeexplore.ieee.org/abstract/document/4690975},
  urldate = {2024-09-20},
  abstract = {This work investigates a geometric approach to proving cell probe lower bounds for data structure problems.We consider the {\textbackslash}em approximate nearest neighbor search problem on the Boolean hypercube ({\textbackslash}bool{\textasciicircum}d,{\o}nenorm{\textbackslash}cdot) with d={\textbackslash}Theta({\l}og n). We show that any (randomized) data structure for the problem that answers c-approximate nearest neighbor search queries using t probes must use space at least n\textsuperscript{1+{\O}mega(1/ct)}. In particular, our bound implies that any data structure that uses space {\textbackslash}tildeO(n) with polylogarithmic word size, and with constant probability gives a constant approximation to nearest neighbor search queries must be probed {\O}mega({\l}og n/ {\l}og{\l}og n) times. This improves on the lower bound of {\O}mega({\l}og{\l}og d/{\l}og{\l}og{\l}og d) probes shown by Chakrabarti and Regev {\textbackslash}citeChakrabartiR04 for any polynomial space data structure, and the {\O}mega({\l}og{\l}og d) lower bound in {\textbackslash}Patrascu and Thorup {\textbackslash}citePatrascuT07 for linear space data structures.Our lower bound holds for the {\textbackslash}em near neighbor problem, where the algorithm knows in advance a good approximation to the distance to the nearest neighbor.Additionally, it is an {\textbackslash}em average case lower bound for the natural distribution for the problem. Our approach also gives the same bound for (2-{\textbackslash}frac1c)-approximation to the farthest neighbor problem.For the case of non-adaptive algorithms we can improve the bound slightly and show a {\O}mega({\l}og n) lower bound on the time complexity of data structures with O(n) space and logarithmic word size.We also show similar lower bounds for the partial match problem: any randomized t-probe data structure that solves the partial match problem on \{0,1,{\textbackslash}star\}{\textasciicircum}d for d={\textbackslash}Theta({\l}og n) must use space n\textsuperscript{1+{\O}mega(1/t)}. This implies an {\O}mega({\l}og n/{\l}og{\l}og n) lower bound for time complexity of near linear space data structures, slightly improving the {\O}mega({\l}og n /({\l}og {\l}og n){\textasciicircum}2) lower bound from {\textbackslash}citePatrascuT06a,{\textbackslash}citeJayramKKR03 for this range of d. Recently and independently {\textbackslash}Patrascu achieved similar bounds {\textbackslash}citepatrascu08. Our results also generalize to approximate partial match, improving on the bounds of {\textbackslash}citeBarkolR02,PatrascuT06a.},
  keywords = {Approximation algorithms,cell probe,Computational biology,Computer science,data structure,Data structures,Geometry,Hypercubes,Information retrieval,lower bound,Machine learning algorithms,Near Neighbor Search,Nearest neighbor searches,non-adaptive,Partial Match,Polynomials,Probes,query},
  file = {/Users/tulasi/Zotero/storage/MJY8U2JY/Panigrahy et al. - 2008 - A Geometric Approach to Lower Bounds for Approximate Near-Neighbor Search and Partial Match.pdf}
}

@article{panigrahyGeometricApproachLower2008a,
  title = {A Geometric Approach to Lower Bounds for Approximate Near-Neighbor Search and Partial Match},
  author = {Panigrahy, R. and Talwar, Kunal and Wieder, Udi},
  year = {2008},
  doi = {10.1109/FOCS.2008.68},
  abstract = {This work investigates a geometric approach to proving cell probe lower bounds for data structure problems.We consider the \{\vphantom\}\emph{approximate nearest neighbor search problem}\vphantom\{\}\emph{ on the Boolean hypercube (\textsuperscript{d},\{{$\cdot$}\}) with d={$\Theta$}(n). We show that any (randomized) data structure for the problem that answers c-approximate nearest neighbor search queries using t probes must use space at least n\textsuperscript{\{\vphantom\}}1+{\textohm}(1/ct)\vphantom\{\}. In particular, our bound implies that any data structure that uses space O\vphantom\{\}(n) with polylogarithmic word size, and with constant probability gives a constant approximation to nearest neighbor search queries must be probed {\textohm}(n/n) times. This improves on the lower bound of {\textohm}(d/d) probes shown by Chakrabarti and Regev \{ChakrabartiR04\vphantom\}}\vphantom\{\}\emph{ for any polynomial space data structure, and the {\textohm}(d) lower bound in and Thorup \{PatrascuT07\vphantom\}}\vphantom\{\}\emph{ for linear space data structures.Our lower bound holds for the }\{\vphantom\}\emph{near neighbor problem}\vphantom\{\}\emph{, where the algorithm knows in advance a good approximation to the distance to the nearest neighbor.Additionally, it is an }\{\vphantom\}\emph{average case}\vphantom\{\}\emph{ lower bound for the natural distribution for the problem. Our approach also gives the same bound for (2-\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{c\})-approximation to the farthest neighbor problem.For the case of non-adaptive algorithms we can improve the bound slightly and show a {\textohm}(n) lower bound on the time complexity of data structures with O(n) space and logarithmic word size.We also show similar lower bounds for the partial match problem: any randomized t-probe data structure that solves the partial match problem on  0,1,{$\star$} \textsuperscript{d} for d={$\Theta$}(n) must use space n\textsuperscript{\{\vphantom\}}1+{\textohm}(1/t)\vphantom\{\}. This implies an {\textohm}(n/n) lower bound for time complexity of near linear space data structures, slightly improving the {\textohm}(n/(n){$^2$}) lower bound from \{PatrascuT06a\vphantom\}}\vphantom\{\}\emph{,\{JayramKKR03\vphantom\}}\vphantom\{\}\emph{ for this range of d. Recently and independently achieved similar bounds \{patrascu08\vphantom\}}\vphantom\{\}\emph{. Our results also generalize to approximate partial match, improving on the bounds of \{BarkolR02,PatrascuT06a\vphantom\}}\vphantom\{\}\emph{.}},
  citationcount = {43},
  venue = {2008 49th Annual IEEE Symposium on Foundations of Computer Science},
  keywords = {cell probe,data structure,lower bound,non-adaptive,query}
}

@inproceedings{panigrahyLowerBoundsNeighbor2010,
  title = {Lower {{Bounds}} on {{Near Neighbor Search}} via {{Metric Expansion}}},
  booktitle = {2010 {{IEEE}} 51st {{Annual Symposium}} on {{Foundations}} of {{Computer Science}}},
  author = {Panigrahy, Rina and Talwar, Kunal and Wieder, Udi},
  year = {2010},
  month = oct,
  pages = {805--814},
  publisher = {IEEE},
  address = {Las Vegas, NV, USA},
  doi = {10.1109/FOCS.2010.82},
  url = {http://ieeexplore.ieee.org/document/5671355/},
  urldate = {2024-09-20},
  isbn = {978-1-4244-8525-3},
  keywords = {Approximation methods,Artificial neural networks,cell probe,communication,communication complexity,Complexity theory,data structure,Data structures,Data Structures,dynamic,Expansion,lower bound,Measurement,Metric Spaces,Probes,Robustness,time-space,update},
  file = {/Users/tulasi/Zotero/storage/YMDQXD3G/Panigrahy et al. - 2010 - Lower Bounds on Near Neighbor Search via Metric Expansion.pdf;/Users/tulasi/Zotero/storage/ZM9RSHFQ/Panigrahy et al. - 2010 - Lower Bounds on Near Neighbor Search via Metric Expansion.pdf}
}

@article{panMethodsOfComputing1966,
  title = {Methods of Computing Values of Polynomials},
  author = {Pan, V.},
  year = {1966},
  doi = {10.1070/RM1966V021N01ABEH004147},
  abstract = {CONTENTSIntroduction {\S} 1. Lower bounds for the number of operations in schemes without initial conditioning of the coefficients {\S} 2. Lower bounds for the number of operations in schemes with initial conditioning of the coefficients {\S} 3. Construction of schemes with initial conditioning of the coefficients for the computation of one polynomial {\S} 4. Schemes with initial conditioning of the coefficients for simultaneous computation of the values of several polynomialsReferences},
  citationcount = {112},
  venue = {No venue available}
}

@article{panStructuredMatricesAnd2001,
  title = {Structured Matrices and Polynomials},
  author = {Pan, V.},
  year = {2001},
  doi = {10.1007/978-1-4612-0129-8},
  abstract = {No abstract available},
  citationcount = {191},
  venue = {No venue available}
}

@article{papadimitriouCommunicationComplexity1984,
  title = {Communication Complexity},
  author = {Papadimitriou, Christos H. and Sipser, Michael},
  year = {1984},
  doi = {10.1016/0022-0000(84)90069-2},
  abstract = {No abstract available},
  citationcount = {1320},
  venue = {Journal of computer and system sciences (Print)},
  keywords = {communication,communication complexity}
}

@article{pappalardoStringSelectionProblems2013,
  title = {String Selection Problems},
  author = {Pappalardo, E. and Pardalos, P. and Stracquadanio, Giovanni},
  year = {2013},
  doi = {10.1007/978-1-4614-9053-1_4},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {No venue available}
}

@article{pardoAPromenadeThrough2020,
  title = {A Promenade through Correct Test Sequences {{I}}: {{Degree}} of Constructible Sets, {{B{\'e}zout}}'s {{Inequality}} and Density},
  author = {Pardo, L. M. and Sebastian, Danielache},
  year = {2020},
  doi = {10.1016/J.JCO.2021.101588},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Journal of Complexity}
}

@article{parsaADeterministicO2012,
  title = {A Deterministic {{O}}(Mlogm)[12pt]\{minimal\} \{amsmath\} \{wasysym\} \{amsfonts\} \{amssymb\} \{amsbsy\} \{mathrsfs\} \{upgreek\} \{\}\{-69pt\} \{document\}\{document\} Time Algorithm f},
  author = {Parsa, S.},
  year = {2012},
  doi = {10.1145/2261250.2261289},
  abstract = {We present a deterministic algorithm to compute the Reeb graph of a PL real-valued function on a simplicial complex in O(mlogm)[12pt]\{minimal\} \{amsmath\} \{wasysym\} \{amsfonts\} \{amssymb\} \{amsbsy\} \{mathrsfs\} \{upgreek\} \{\}\{-69pt\} \{document\}\{document\} time, where m[12pt]\{minimal\} \{amsmath\} \{wasysym\} \{amsfonts\} \{amssymb\} \{amsbsy\} \{mathrsfs\} \{upgreek\} \{\}\{-69pt\} \{document\}\{document\} is the size of the 2-skeleton. The problem can be solved using dynamic graph connectivity. We obtain the running time by using offline graph connectivity which assumes that the deletion time of every arc inserted is known at the time of insertion. The algorithm is implemented and experimental results are given. In addition, we reduce the offline graph connectivity problem to computing the Reeb graph.},
  citationcount = {56},
  venue = {Discrete \& Computational Geometry},
  keywords = {dynamic}
}

@article{parterConnectivityLabelingAnd2023,
  title = {Connectivity Labeling and Routing with Multiple Vertex Failures},
  author = {Parter, M. and Petruschka, Asaf and Pettie, Seth},
  year = {2023},
  doi = {10.1145/3618260.3649729},
  abstract = {We present succinct labeling schemes for answering connectivity queries in graphs subject to a specified number of vertex failures. An f-vertex/edge fault tolerant (f-V/EFT) connectivity labeling is a scheme that produces succinct labels for the vertices (and possibly to the edges) of an n-vertex graph G, such that given only the labels of two vertices s,t and of at most f faulty vertices/edges F, one can infer if s and t are connected in G-F. The primary complexity measure is the maximum label length (in bits). The f-EFT setting is relatively well understood: [Dory and Parter, PODC 2021] gave a randomized scheme with succinct labels of O(log3 n) bits, which was subsequently derandomized by [Izumi et al., PODC 2023] with {\~O}(f2)-bit labels. As both noted, handling vertex faults is more challenging. The known bounds for the f-VFT setting are far away: [Parter and Petruschka, DISC 2022] gave {\~O}(n1-1/2{$\Theta$}(f))-bit labels, which is linear in n already for f ={\textohm}(loglogn). In this work we present an efficient f-VFT connectivity labeling scheme using poly(f, logn) bits. Specifically, we present a randomized scheme with O(f3 log5 n)-bit labels, and a derandomized version with O(f7 log13 n)-bit labels, compared to an {\textohm}(f)-bit lower bound on the required label length. Our schemes are based on a new low-degree graph decomposition that improves on [Duan and Pettie, SODA 2017], and facilitates its distributed representation into labels. This is accompanied with specialized linear graph sketches that extend the techniques of the Dory and Parter to the vertex fault setting, which are derandomized by adapting the approach of Izumi et al. and combining it with hit-miss hash families of [Karthik and Parter, SODA 2021]. Finally, we show that our labels naturally yield routing schemes avoiding a given set of at most f vertex failures with table and header sizes of only poly(f,logn) bits. This improves significantly over the linear size bounds implied by the EFT routing scheme of Dory and Parter.},
  citationcount = {4},
  venue = {Symposium on the Theory of Computing},
  keywords = {lower bound,query}
}

@article{pasinmanurangsiAlmostpolynomialRatioETHhardness2016,
  title = {Almost-Polynomial Ratio {{ETH-hardness}} of Approximating Densest k-Subgraph},
  author = {Pasin Manurangsi},
  year = {2016},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/3055399.3055412},
  abstract = {In the Densest k-Subgraph (DkS) problem, given an undirected graph G and an integer k, the goal is to find a subgraph of G on k vertices that contains maximum number of edges. Even though Bhaskara et al.'s state-of-the-art algorithm for the problem achieves only O(n1/4 + {$\epsilon$}) approximation ratio, previous attempts at proving hardness of approximation, including those under average case assumptions, fail to achieve a polynomial ratio; the best ratios ruled out under any worst case assumption and any average case assumption are only any constant (Raghavendra and Steurer) and 2O(log2/3 n) (Alon et al.) respectively. In this work, we show, assuming the exponential time hypothesis (ETH), that there is no polynomial-time algorithm that approximates Densest k-Subgraph to within n1/(loglogn)c factor of the optimum, where c {$>$} 0 is a universal constant independent of n. In addition, our result has perfect completeness, meaning that we prove that it is ETH-hard to even distinguish between the case in which G contains a k-clique and the case in which every induced k-subgraph of G has density at most 1/n-1/(loglogn)c in polynomial time. Moreover, if we make a stronger assumption that there is some constant {\^I}{\textmu} {$>$} 0 such that no subexponential-time algorithm can distinguish between a satisfiable 3SAT formula and one which is only (1 - {$\varepsilon$})-satisfiable (also known as Gap-ETH), then the ratio above can be improved to nf(n) for any function f whose limit is zero as n goes to infinity (i.e. f {$\epsilon$} o(1)).},
  annotation = {Citation Count: 150}
}

@article{pasinmanurangsiStrongishPlantedClique2020,
  title = {The {{Strongish Planted Clique Hypothesis}} and {{Its Consequences}}},
  author = {Pasin Manurangsi and A. Rubinstein and T. Schramm},
  year = {2020},
  journal = {Information Technology Convergence and Services},
  doi = {10.4230/LIPIcs.ITCS.2021.10},
  abstract = {We formulate a new hardness assumption, the Strongish Planted Clique Hypothesis (SPCH), which postulates that any algorithm for planted clique must run in time \$n{\textasciicircum}\{{\textbackslash}Omega({\textbackslash}log\{n\})\}\$ (so that the state-of-the-art running time of \$n{\textasciicircum}\{O({\textbackslash}log n)\}\$ is optimal up to a constant in the exponent).  We provide two sets of applications of the new hypothesis. First, we show that SPCH implies (nearly) tight inapproximability results for the following well-studied problems in terms of the parameter \$k\$: Densest \$k\$-Subgraph, Smallest \$k\$-Edge Subgraph, Densest \$k\$-Subhypergraph, Steiner \$k\$-Forest, and Directed Steiner Network with \$k\$ terminal pairs. For example, we show, under SPCH, that no polynomial time algorithm achieves \$o(k)\$-approximation for Densest \$k\$-Subgraph. This inapproximability ratio improves upon the previous best \$k{\textasciicircum}\{o(1)\}\$ factor from (Chalermsook et al., FOCS 2017). Furthermore, our lower bounds hold even against fixed-parameter tractable algorithms with parameter \$k\$.  Our second application focuses on the complexity of graph pattern detection. For both induced and non-induced graph pattern detection, we prove hardness results under SPCH, which improves the running time lower bounds obtained by (Dalirrooyfard et al., STOC 2019) under the Exponential Time Hypothesis.},
  keywords = {lower bound},
  annotation = {Citation Count: 23}
}

@article{patelLowerBoundsFor2020,
  title = {Lower Bounds for Encrypted Multi-Maps and Searchable Encryption in the Leakage Cell Probe Model},
  author = {Patel, Sarvar and Persiano, G. and Yeo, Kevin},
  year = {2020},
  doi = {10.1007/978-3-030-56784-2_15},
  abstract = {No abstract available},
  citationcount = {16},
  venue = {Annual International Cryptology Conference},
  keywords = {cell probe,lower bound}
}

@article{patersonPointRetrievalFor1986,
  title = {Point Retrieval for Polygons},
  author = {Paterson, M. and Yao, F.},
  year = {1986},
  doi = {10.1016/0196-6774(86)90033-7},
  abstract = {No abstract available},
  citationcount = {22},
  venue = {J. Algorithms}
}

@article{pathakImprovedDoubleSelection2017,
  title = {Improved Double Selection Sort Using Algorithm},
  author = {Pathak, N. and Tiwari, Shubham},
  year = {2017},
  doi = {10.18090/samriddhi.v9i02.10866},
  abstract = {In this paper, we present the work regarding the selection sorting technique for double ended selection sort. This sorting algorithm is both theoretical and programmatically analysis show that the introduce advance selection sort algorithm which enhances the performance of selection sort. It is much faster than the selection sort because of its selection of minimum and maximum elements simultaneously. Advance selection sort algorithm possibility of enhancing execution speed up to 30programming Language. So easy to understand the concept of this sorting algorithm by everyone because C is the popular language. Results and discussion show a higher level of performance for the sorting algorithm. It can theoretically prove that the algorithm can reduce steps with the selection short and will improve N2 sorts toward NlogN sort.},
  citationcount = {Unknown},
  venue = {No venue available}
}

@article{patilFasterRangeLcp2013,
  title = {Faster Range {{LCP}} Queries},
  author = {Patil, Manish and Shah, Rahul and Thankachan, Sharma V.},
  year = {2013},
  doi = {10.1007/978-3-319-02432-5_29},
  abstract = {No abstract available},
  citationcount = {11},
  venue = {SPIRE},
  keywords = {query}
}

@article{patnaikDynFoA1997,
  title = {Dyn-{{FO}}: A Parallel, Dynamic Complexity Class},
  author = {Patnaik, Sushant and Immerman, N.},
  year = {1997},
  doi = {10.1006/jcss.1997.1520},
  abstract = {Traditionally, computational complexity has considered only static problems. Classical complexity classes such as NC, P, and NP are defined in terms of the complexity of checking?upon presentation of an entire input?whether the input satisfies a certain property. For many applications of computers it is more appropriate to model the process as a dynamic one. There is a fairly large object being worked on over a period of time. The object is repeatedly modified by users and computations are performed. We develop a theory of dynamic complexity. We study the new complexity class, dynamic first-order logic (Dyn-FO). This is the set of properties that can be maintained and queried in first-order logic, i.e., relational calculus, on a relational database. We show that many interesting properties are in Dyn-FO, including multiplication, graph connectivity, bipartiteness, and the computation of minimum spanning trees. Note that none of these problems is in static FO, and this fact has been used to justify increasing the power of query languages beyond first-order. It is thus striking that these problems are indeed dynamic first-order and, thus, were computable in first-order database languages all along. We also define ``bounded-expansion reductions'' which honor dynamic complexity classes. We prove that certain standard complete problems for static complexity classes, such as REACHafor P, remain complete via these new reductions. On the other hand, we prove that other such problems, including REACH for NL and REACHdfor L, are no longer complete via bounded-expansion reductions. Furthermore, we show that a version of REACHa, called REACHa+, is not in Dyn-FO unless all of P is contained in parallel linear time.},
  citationcount = {133},
  venue = {Journal of computer and system sciences (Print)},
  keywords = {dynamic,query,reduction,static}
}

@article{patrascuANewInfinity2012,
  title = {A New Infinity of Distance Oracles for Sparse Graphs},
  author = {Patrascu, M. and Roditty, L. and Thorup, M.},
  year = {2012},
  doi = {10.1109/FOCS.2012.44},
  abstract = {Given a weighted undirected graph, our basic goal is to represent all pairwise distances using much less than quadratic space, such that we can estimate the distance between query vertices in constant time. We will study the inherent trade-off between space of the representation and the stretch (multiplicative approximation disallowing underestimates) of the estimates when the input graph is sparse with m = {\~O}(n) edges. In this paper, for any fixed positive integers k and {$\ell$}, we obtain stretches = 2k + 1 {\textpm} 2/{$\ell$} = 2k + 1 - 2/{$\ell$}, 2k + 1 + 2/{$\ell$}, using space S({$\alpha$}, m) = {\~O}(m{\textexclamdown}sup{\textquestiondown}1+2/({$\alpha$}+1){\textexclamdown}/sup{\textquestiondown}). The query time is O(k + {$\ell$}) = O(1). For integer stretches, this coincides with the previous bounds (odd stretches with {$\ell$} = 1 and even stretches with {$\ell$} = 2). The infinity of fractional stretches between consecutive integers are all new (even though {$\ell$} is fixed as a constant independent of the input, the number of integers {$\ell$} is still countably infinite). We will argue that the new fractional points are not just arbitrary, but that they, at least for fixed stretches below 3, provide a complete picture of the inherent trade-off between stretch and space in m. Consider any fixed stretch {$\alpha$} {\textexclamdown}; 3. Based on the hardness of set intersection, we argue that if {$\ell$} is the largest integer such that 3-2/{$\ell$} {$\leq$} {$\alpha$}, then {\~{\textohm}}(S(3 - 2/{$\ell$}, m)) space is needed for stretch . In particular, for fixed stretch below 22/3, we improve Patrascu and Roditty's lower bound from {\~{\textohm}}(m{\textexclamdown}sup{\textquestiondown}3/2{\textexclamdown}/sup{\textquestiondown}) to {\~{\textohm}}(m{\textexclamdown}sup{\textquestiondown}5/3{\textexclamdown}/sup{\textquestiondown}), thus matching their upper bound for stretch 2. For space in terms of m, this is the first hardness matching the space of a non-trivial/sub-quadratic distance oracle.},
  citationcount = {39},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{patrascuCellprobeLowerBounds2010,
  title = {Cell-Probe Lower Bounds for Succinct Partial Sums},
  author = {Patrascu, M. and Viola, Emanuele},
  year = {2010},
  doi = {10.1137/1.9781611973075.11},
  abstract = {The partial sums problem in succinct data structures asks to preprocess an array {\textexclamdown}i{\textquestiondown}A{\textexclamdown}/i{\textquestiondown}[1..{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}] of bits into a data structure using as close to {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} bits as possible, and answer queries of the form Rank({\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}) = {$\Sigma$}{\textexclamdown}sup{\textquestiondown}{\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sup{\textquestiondown}{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}i{\textexclamdown}/i{\textquestiondown}=1{\textexclamdown}/sub{\textquestiondown} {\textexclamdown}i{\textquestiondown}A{\textexclamdown}/i{\textquestiondown}[{\textexclamdown}i{\textquestiondown}i{\textexclamdown}/i{\textquestiondown}]. The problem has been intensely studied, and features as a subroutine in a number of succinct data structures. We show that, if we answer Rank({\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}) queries by probing {\textexclamdown}i{\textquestiondown}t{\textexclamdown}/i{\textquestiondown} cells of {\textexclamdown}i{\textquestiondown}w{\textexclamdown}/i{\textquestiondown} bits, then the space of the data structure must be at least {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {\textexclamdown}i{\textquestiondown}n/w{\textexclamdown}sup{\textquestiondown}O(t){\textexclamdown}/sup{\textquestiondown}{\textexclamdown}/i{\textquestiondown} bits. This redundancy/probe trade-off is essentially optimal: Patrascu [FOCS'08] showed how to achieve {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}/({\textexclamdown}i{\textquestiondown}w/t{\textexclamdown}/i{\textquestiondown}){\textexclamdown}sup{\textquestiondown}{\textohm}({\textexclamdown}i{\textquestiondown}t{\textexclamdown}/i{\textquestiondown}){\textexclamdown}/sup{\textquestiondown} bits. We also extend our lower bound to the closely related Select queries, and to the case of sparse arrays.},
  citationcount = {51},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,data structure,lower bound,query}
}

@article{patrascuDataSTRUCTURES2008,
  title = {({{Data}}) {{STRUCTURES}}},
  author = {Patrascu, M.},
  year = {2008},
  doi = {10.1109/FOCS.2008.69},
  abstract = {We show that a large fraction of the data-structure lower bounds known today in fact follow by reduction from the communication complexity of lopsided (asymmetric) set disjointness! This includes lower bounds for: (a) high-dimensional problems, where the goal is to show large space lower bounds; (b) constant-dimensional geometric problems, where the goal is to bound the query time for space O(n polylg n); (c) dynamic problems, where we are looking for a trade-off between query and update time. (In this case, our bounds are slightly weaker than the originals, losing a lglg n factor.) Our reductions also imply the following new results: (a) an Omega(lg n / lg lg n) bound for 4-dimensional range reporting, given space O(n ldr poly lg n). This is very timely, since a recent result [Nekrich, SoCG'07] solved 3D reporting in near-constant time, raising the prospect that higher dimensions could also be easy; (b) a tight space lower bound for the partial match problem, for constant query time.(c) the first lower bound for reachability oracles.},
  citationcount = {14},
  venue = {2008 49th Annual IEEE Symposium on Foundations of Computer Science},
  keywords = {communication,communication complexity,data structure,dynamic,lower bound,query,query time,reduction,update,update time}
}

@inproceedings{patrascuDontRushUnion2011,
  title = {Don't Rush into a Union: Take Time to Find Your Roots},
  shorttitle = {Don't Rush into a Union},
  booktitle = {Proceedings of the Forty-Third Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {P{\u a}tra{\c s}cu, Mihai and Thorup, Mikkel},
  year = {2011},
  month = jun,
  series = {{{STOC}} '11},
  pages = {559--568},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1993636.1993711},
  url = {https://dl.acm.org/doi/10.1145/1993636.1993711},
  urldate = {2024-09-02},
  abstract = {We present a new threshold phenomenon in data structure lower bounds where slightly reduced update times lead to exploding query times. Consider incremental connectivity, letting tu be the time to insert an edge and tq be the query time. For tu = Omega(tq), the problem is equivalent to the well-understood union-find problem: proc\{InsertEdge\}(s,t) can be implemented by Union(Find(s), Find(t)). This gives worst-case time tu = tq = O(lg n / lg lg n) and amortized tu = tq = O({$\alpha$}(n)). By contrast, we show that if tu = o(lg n / lg lg n), the query time explodes to tq {$\geq$} n1-o(1). In other words, if the data structure doesn't have time to find the roots of each disjoint set (tree) during edge insertion, there is no effective way to organize the information!For amortized complexity, we demonstrate a new inverse-Ackermann type trade-off in the regime tu = o(tq).A similar lower bound is given for fully dynamic connectivity, where an update time of o(lg n) forces the query time to be n1-o(1). This lower bound allows for amortization and Las Vegas randomization, and comes close to the known O(lg n {$\bullet$} (lg lg n)O(1)) upper bound.},
  isbn = {978-1-4503-0691-1},
  keywords = {data structure,dynamic,lower bound,notion,query,query time,update,update time},
  file = {/Users/tulasi/Zotero/storage/GETDCS68/Ptracu and Thorup - 2011 - Don't rush into a union take time to find your roots.pdf}
}

@article{patrascuDynamicIntegerSets2014,
  title = {Dynamic Integer Sets with Optimal Rank, Select, and Predecessor Search},
  author = {Patrascu, M. and Thorup, M.},
  year = {2014},
  doi = {10.1109/FOCS.2014.26},
  abstract = {We present a data structure representing a dynamic set S of w-bit integers on a w-bit word RAM. With S = n and w {$\geq$} logn and space O(n), we support the following standard operations in O(log n/ log w) time: . insert(x) sets S = S {$\cup$} \{x\}. .delete(x) sets S = S \{x\}. . predecessor(x) returns max\{y {$\in$} S y {\textexclamdown}; x\}. . successor(x) returns min\{y {$\in$} S y {$\geq$} x\}. . rank(x) returns \# \{y {$\in$} S y {\textexclamdown}; x\}. . select(i) returns y {$\in$} S with rank(y) = i, if any. Our O (log n/ log w) bound is optimal for dynamic rank and select, matching a lower bound of Fredman and Saks [STOC'99]. When the word length is large, our time bound is also optimal for dynamic predecessor, matching a static lower bound of Beame and Fich [STOC' 99] whenever log n/ log w = O (log w/ log log w). Technically, the most interesting aspect of our data structure is that it supports all the above operations in constant time for sets of size n = wO(1). This resolves a main open problem of Ajtai, Komlos, and Fredman [FOCS'83]. Ajtai et al. presented such a data structure in Yao's abstract cell-probe model with w-bit cells/words, but pointed out that the functions used could not be implemented. As a partial solution to the problem, Fredman and Willard [STOC'90] introduced a fusion node that could handle queries in constant time, but used polynomial time on the updates. We call our small set data structure a dynamic fusion node as it does both queries and updates in constant time.},
  citationcount = {71},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {cell probe,data structure,dynamic,lower bound,query,static,update}
}

@inproceedings{patrascuHigherLowerBounds2006a,
  title = {Higher {{Lower Bounds}} for {{Near-Neighbor}} and {{Further Rich Problems}}},
  booktitle = {2006 47th {{Annual IEEE Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}}'06)},
  author = {Patrascu, Mihai and Thorup, Mikkel},
  year = {2006},
  month = oct,
  pages = {646--654},
  issn = {0272-5428},
  doi = {10.1109/FOCS.2006.35},
  url = {https://ieeexplore.ieee.org/abstract/document/4031399},
  urldate = {2024-11-20},
  abstract = {We convert cell-probe lower bounds for polynomial space into stronger lower bound for near-linear space. Our technique applies to any lower bound proved through the richness method. For example, it applies to partial match, and to near-neighbor problems, either for randomized exact search, or for deterministic approximate search (which are thought to exhibit the curse of dimensionality). These problems are motivated by search in large data bases, so near-linear space is the most relevant regime. Typically, richness has been used to imply {\O}mega(d/ lg n) lower bounds for polynomial-space data structures, where d is the number of bits of a query. This is the highest lower bound provable through the classic reduction to communication complexity. However, for space n lg{\textasciicircum}O(1) n, we now obtain bounds of {\O}mega(d/ lg d). This is a significant improvement for natural values of d, such as lg{\textasciicircum}O(1) n. In the most important case of d = {\textbackslash}Theta(lg n), we have the first superconstant lower bound. From a complexity-theoretic perspective, our lower bounds are the highest known for any static data-structure problem, significantly improving on previous records.},
  keywords = {cell probe,communication,communication complexity,Complexity theory,Computational geometry,Computer science,data structure,Data structures,Databases,lower bound,Nearest neighbor searches,Polynomials,Probes,Protocols,query,reduction,static},
  file = {/Users/tulasi/Zotero/storage/M3DS3NGI/Patrascu and Thorup - 2006 - Higher Lower Bounds for Near-Neighbor and Further Rich Problems.pdf}
}

@article{patrascuLogarithmicLowerBounds2006,
  title = {Logarithmic {{Lower Bounds}} in the {{Cell-Probe Model}}},
  author = {Patrascu, Mihai and Demaine, Erik D.},
  year = {2006},
  month = jan,
  journal = {SIAM J. Comput.},
  volume = {35},
  number = {4},
  pages = {932--963},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/S0097539705447256},
  url = {https://epubs.siam.org/doi/abs/10.1137/S0097539705447256},
  urldate = {2024-11-20},
  abstract = {We show that a large fraction of the data-structure lower bounds known today in fact follow by reduction from the communication complexity of lopsided (asymmetric) set disjointness. This includes lower bounds for (i) high-dimensional problems, where the goal is to show large space lower bounds; (ii) constant-dimensional geometric problems, where the goal is to bound the query time for space \$O(n{\textbackslash}cdot{\textbackslash}mathrm\{polylog\}n)\$; and (iii) dynamic problems, where we are looking for a trade-off between query and update time. (In the last case, our bounds are slightly weaker than the originals, losing a \${\textbackslash}lg{\textbackslash}lg n\$ factor.) Our reductions also imply the following new results: (i) an \${\textbackslash}Omega({\textbackslash}lg n/{\textbackslash}lg{\textbackslash}lg n)\$ bound for four-dimensional range reporting, given space \$O(n{\textbackslash}cdot{\textbackslash}mathrm\{polylog\}n)\$ (this is quite timely, since a recent result [Y. Nekrich, in Proceedings of the 23rd ACM Symposium on Computational Geometry (SoCG), 2007, pp. 344--353] solved three-dimensional reporting in \$O({\textbackslash}lg{\textasciicircum}2{\textbackslash}lg n)\$ time, raising the prospect that higher dimensions could also be easy); (ii) a tight space lower bound for the partial match problem, for constant query time; and (iii) the first lower bound for reachability oracles. In the process, we prove optimal randomized lower bounds for lopsided set disjointness.},
  keywords = {cell probe,communication,communication complexity,data structure,dynamic,information transfer,lower bound,query,query time,reduction,update,update time},
  file = {/Users/tulasi/Zotero/storage/EXAEBN4V/Patrascu and Demaine - 2006 - Logarithmic Lower Bounds in the Cell-Probe Model.pdf;/Users/tulasi/Zotero/storage/L2AG2MRI/Patrascu and Demaine - 2006 - Logarithmic lower bounds in the cell-probe model.pdf}
}

@inproceedings{patrascuLowerBounds2dimensional2007,
  title = {Lower Bounds for 2-Dimensional Range Counting},
  booktitle = {Proceedings of the Thirty-Ninth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Patrascu, Mihai},
  year = {2007},
  month = jun,
  series = {{{STOC}} '07},
  pages = {40--46},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1250790.1250797},
  url = {https://dl.acm.org/doi/10.1145/1250790.1250797},
  urldate = {2024-11-19},
  abstract = {Proving lower bounds for range queries has been an active topic of research since the late 70s, but so far nearly all results have been limited to the (rather restrictive) semigroup model. We consider one of the most basic range problem, orthogonal range counting in two dimensions, and show almost optimal bounds in the group model and the (holy grail) cell-probe model.Specifically, we show the following bounds, which were known in the semigroup model, but are major improvements in the more general models:* In the group and cell-probe models, a static data structure of size n lgO(1) n requires Omega(lg n lglg n) time per query. This is an exponential improvement over previous bounds, and matches known upper bounds.* In the group model, a dynamic data structure takes time Omega((lg n lglg n)2) per operation. This is close to the O(lg2 n) upper bound, where as the previous lower bound was Omega(lg n).Proving such (static and dynamic) bounds in the group model has been regarded as an important challenge at least since [Fredman, JACM 1982] and [Chazelle, FOCS 1986].},
  isbn = {978-1-59593-631-8},
  keywords = {cell probe,data structure,dynamic,lower bound,query,static},
  file = {/Users/tulasi/Zotero/storage/JG7UED2L/Patrascu - 2007 - Lower bounds for 2-dimensional range counting.pdf}
}

@inproceedings{patrascuLowerBoundsDynamic2004,
  title = {Lower Bounds for Dynamic Connectivity},
  booktitle = {Proceedings of the Thirty-Sixth Annual {{ACM}} Symposium on {{Theory}} of Computing},
  author = {P{\v a}tra{\c s}cu, Mihai and Demaine, Erik D.},
  year = {2004},
  month = jun,
  series = {{{STOC}} '04},
  pages = {546--553},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1007352.1007435},
  url = {https://dl.acm.org/doi/10.1145/1007352.1007435},
  urldate = {2024-09-03},
  abstract = {We prove an {\textohm}(lg Erik n) cell-probe lower bound on maintaining connectivity in dynamic graphs, as well as a more general trade-off between updates and queries. Our bound holds even if the graph is formed by disjoint paths, and thus also applies to trees and plane graphs. The bound is known to be tight for these restricted cases, proving optimality of these data structures (e. g., Sleator and Tarjan's dynamic trees). Our trade-off is known to be tight for trees, and the best two data structures for dynamic connectivity in general graphs are points on our trade-off curve. In this sense these two data structures are optimal, and this tightness serves as strong evidence that our lower bounds are the best possible. From a more theoretical perspective, our result is the first logarithmic cell-probe lower bound for any problem in the natural class of dynamic language membership problems, breaking the long standing record of {\textohm}(lg n / lg lg n). In this sense, our result is the first data-structure lower bound that is "truly" logarithmic, i. e., logarithmic in the problem size counted in bits. Obtaining such a bound is listed as one of three major challenges for future research by Miltersen [13] (the other two challenges remain unsolved). Our techniques form a general framework for proving cell-probe lower bounds on dynamic data structures. We show how our framework also applies to the partial-sums problem to obtain a nearly complete understanding of the problem in cell-probe and algebraic models, solving several previously posed open problems.},
  isbn = {978-1-58113-852-8},
  keywords = {cell probe,data structure,dynamic,information transfer,lower bound,query,update},
  annotation = {P. B. Miltersen . Cell probe complexity - a survey . In Advances in Data Structures Workshop, Conference on the Foundations of Software Technology and Theoretical Computer Science (FSTTCS) , 1999 .]] P. B. Miltersen. Cell probe complexity - a survey. In Advances in Data Structures Workshop, Conference on the Foundations of Software Technology and Theoretical Computer Science (FSTTCS), 1999.]]\\
\\
M. P{\v a}tra7\#351;cu and E. D. Demaine . Tight bounds for the partial-sums problem . In Proc. 15th ACM/SIAM Symposium on Discrete Algorithms (SODA) , pages 20 -- 29 , 2004 .]] M. P{\v a}tra7\#351;cu and E. D. Demaine. Tight bounds for the partial-sums problem. In Proc. 15th ACM/SIAM Symposium on Discrete Algorithms (SODA), pages 20--29, 2004.]]},
  file = {/Users/tulasi/Zotero/storage/5JDG4X49/Ptracu and Demaine - 2004 - Lower bounds for dynamic connectivity.pdf}
}

@article{patrascuLowerBoundsFor2004,
  title = {Lower Bounds for Dynamic Connectivity},
  author = {Patrascu, M.},
  year = {2004},
  doi = {10.1145/1007352.1007435},
  abstract = {We prove an {\textohm}(lg Erik n) cell-probe lower bound on maintaining connectivity in dynamic graphs, as well as a more general trade-off between updates and queries. Our bound holds even if the graph is formed by disjoint paths, and thus also applies to trees and plane graphs. The bound is known to be tight for these restricted cases, proving optimality of these data structures (e. g., Sleator and Tarjan's dynamic trees). Our trade-off is known to be tight for trees, and the best two data structures for dynamic connectivity in general graphs are points on our trade-off curve. In this sense these two data structures are optimal, and this tightness serves as strong evidence that our lower bounds are the best possible. From a more theoretical perspective, our result is the first logarithmic cell-probe lower bound for any problem in the natural class of dynamic language membership problems, breaking the long standing record of {\textohm}(lg n / lg lg n). In this sense, our result is the first data-structure lower bound that is "truly" logarithmic, i. e., logarithmic in the problem size counted in bits. Obtaining such a bound is listed as one of three major challenges for future research by Miltersen [13] (the other two challenges remain unsolved). Our techniques form a general framework for proving cell-probe lower bounds on dynamic data structures. We show how our framework also applies to the partial-sums problem to obtain a nearly complete understanding of the problem in cell-probe and algebraic models, solving several previously posed open problems.},
  citationcount = {44},
  venue = {Symposium on the Theory of Computing},
  keywords = {cell probe,data structure,dynamic,lower bound,query,update}
}

@article{patrascuOnThePossibility2010,
  title = {On the Possibility of Faster {{SAT}} Algorithms},
  author = {Patrascu, M. and Williams, Ryan},
  year = {2010},
  doi = {10.1137/1.9781611973075.86},
  abstract = {We describe reductions from the problem of determining the satisfiability of Boolean CNF formulas (CNF-SAT) to several natural algorithmic problems. We show that attaining any of the following bounds would improve the state of the art in algorithms for SAT: {$\bullet$} an {\textexclamdown}i{\textquestiondown}O(n{\textexclamdown}sup{\textquestiondown}k-{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown}){\textexclamdown}/i{\textquestiondown} algorithm for {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}-Dominating Set, for any {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown} {$\geq$} 3, {$\bullet$} a (computationally efficient) protocol for 3-party set disjointness with {\textexclamdown}i{\textquestiondown}o(m){\textexclamdown}/i{\textquestiondown} bits of communication, {$\bullet$} an {\textexclamdown}i{\textquestiondown}n{\textexclamdown}sup{\textquestiondown}{$^\circ$}(d){\textexclamdown}/sup{\textquestiondown}{\textexclamdown}/i{\textquestiondown} algorithm for {\textexclamdown}i{\textquestiondown}d{\textexclamdown}/i{\textquestiondown}-SUM, {$\bullet$} an {\textexclamdown}i{\textquestiondown}O(n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}5-{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown}) algorithm for 2-SAT formulas with {\textexclamdown}i{\textquestiondown}m = n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}1+0(1){\textexclamdown}/sup{\textquestiondown} clauses, where {\textexclamdown}i{\textquestiondown}two{\textexclamdown}/i{\textquestiondown} clauses may have unrestricted length, and {$\bullet$} an {\textexclamdown}i{\textquestiondown}O((n + m)k-{$\varepsilon$}){\textexclamdown}/i{\textquestiondown} algorithm for HornSat with {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown} unrestricted length clauses. One may interpret our reductions as new attacks on the complexity of SAT, or sharp lower bounds conditional on exponential hardness of SAT.},
  citationcount = {242},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{patrascuPlanarPointLocation2006,
  title = {Planar Point Location in Sublogarithmic Time},
  author = {Patrascu, M.},
  year = {2006},
  doi = {10.1109/FOCS.2006.61},
  abstract = {We consider the static planar point location problem in an arbitrary polygonal subdivision given by n segments. We assume points come from the [u]2 grid, and consider algorithms for the RAM with words of O(lg u) bits. We give the first solution to the problem which can surpass the traditional query time of O(lgn). Specifically, we can obtain a query time of O(radic(lg u)). Though computational geometry on a grid has been investigated for a long time (including for this problem), it is generally not known how to make good use of a bounded universe in problems of such nonorthogonal flavor. Our result shows this limitation can be surpassed, at least for planar point location. A result by Timothy Chan, appearing independently in FOCS'06, also achieves sublogarithmic query times. Combining the two results, we obtain the following bound. For any S ges 2, the exists a data structure using space O(n middot S) which supports queries in time: O(min \{((lg n)/(lg lg n)), (radic((lg u)/(lg lg u))), ((lg u)/(lg S))\})},
  citationcount = {24},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {data structure,query,query time,static}
}

@inproceedings{patrascuPolynomialLowerBounds2010,
  title = {Towards Polynomial Lower Bounds for Dynamic Problems},
  booktitle = {Proceedings of the Forty-Second {{ACM}} Symposium on {{Theory}} of Computing},
  author = {Patrascu, Mihai},
  year = {2010},
  month = jun,
  series = {{{STOC}} '10},
  pages = {603--610},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1806689.1806772},
  url = {https://dl.acm.org/doi/10.1145/1806689.1806772},
  urldate = {2024-09-03},
  abstract = {We consider a number of dynamic problems with no known poly-logarithmic upper bounds, and show that they require n{\textohm}(1) time per operation, unless 3SUM has strongly subquadratic algorithms. Our result is modular: (1) We describe a carefully-chosen dynamic version of set disjointness (the "multiphase problem"), and conjecture that it requires n{\textasciicircum}Omega(1) time per operation. All our lower bounds follow by easy reduction. (2) We reduce 3SUM to the multiphase problem. Ours is the first nonalgebraic reduction from 3SUM, and allows 3SUM-hardness results for combinatorial problems. For instance, it implies hardness of reporting all triangles in a graph. (3) It is plausible that an unconditional lower bound for the multiphase problem can be established via a number-on-forehead communication game.},
  isbn = {978-1-4503-0050-6},
  keywords = {communication,dynamic,lower bound,reduction},
  file = {/Users/tulasi/Zotero/storage/KRAFQVMZ/Patrascu - 2010 - Towards polynomial lower bounds for dynamic problems.pdf}
}

@inproceedings{patrascuRandomizationDoesNot2007,
  title = {Randomization Does Not Help Searching Predecessors},
  booktitle = {Proceedings of the Eighteenth Annual {{ACM-SIAM}} Symposium on {{Discrete}} Algorithms},
  author = {P{\v a}tra{\c s}cu, Mihai and Thorup, Mikkel},
  year = {2007},
  month = jan,
  series = {{{SODA}} '07},
  pages = {555--564},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {USA},
  urldate = {2024-11-17},
  abstract = {At STOC'06, we presented a new technique for proving cell-probe lower bounds for static data structures with deterministic queries. This was the first technique which could prove a bound higher than communication complexity, and it gave the first separation between data structures with linear and polynomial space. The new technique was, however, heavily tuned for the deterministic worst-case, demonstrating long query times only for an exponentially small fraction of the input. In this paper, we extend the technique to give lower bounds for randomized query algorithms with constant error probability.Our main application is the problem of searching predecessors in a static set of n integers, each contained in a l-bit word. Our trade-off lower bounds are tight for any combination of parameters. For small space, i.e. n1+o(1), proving such lower bounds was inherently impossible through known techniques. An interesting new consequence is that for near linear space, the classic van Emde Boas search time of O(lg l) cannot be improved, even if we allow randomization. This is a separation from polynomial space, since Beame and Fich [STOC'02] give a predecessor search time of O(lg l/lg lg l) using quadratic space.We also show a tight {\textohm}(lg lg n) lower bound for 2-dimensional range queries, via a new reduction. This holds even in rank space, where no superconstant lower bound was known, neither randomized nor worst-case. We also slightly improve the best lower bound for the approximate nearest neighbor problem, when small space is available.},
  isbn = {978-0-89871-624-5},
  keywords = {cell probe,communication,communication complexity,data structure,lower bound,query,query time,reduction,static},
  file = {/Users/tulasi/Zotero/storage/D3VA2LN2/Ptracu and Thorup - 2007 - Randomization does not help searching predecessors.pdf}
}

@article{patrascuSuccincter2008,
  title = {Succincter},
  author = {Patrascu, M.},
  year = {2008},
  doi = {10.1109/FOCS.2008.83},
  abstract = {We can represent an array of n values from \{0,1,2\} using ceil(n log2 3) bits (arithmetic coding), but then we cannot retrieve a single element efficiently. Instead, we can encode every block of t elements using ceil(t log2 3) bits, and bound the retrieval time by t. This gives a linear trade-off between the redundancy of the representation and the query time.In fact, this type of linear trade-off is ubiquitous in known succinct data structures, and in data compression. The folk wisdom is that if we want to waste one bit per block, the encoding is so constrained that it cannot help the query in any way. Thus, the only thing a query can do is to read the entire block and unpack it.We break this limitation and show how to use recursion to improve redundancy. It turns out that if a block is encoded with two (!) bits of redundancy, we can decode a single element, and answer many other interesting queries, in time logarithmic in the block size.Our technique allows us to revisit classic problems in succinct data structures, and give surprising new upper bounds. We also construct a locally-decodable version of arithmetic coding.},
  citationcount = {157},
  venue = {2008 49th Annual IEEE Symposium on Foundations of Computer Science},
  keywords = {data structure,query,query time}
}

@article{patrascuThePowerOf2010,
  title = {The Power of Simple Tabulation Hashing},
  author = {Patrascu, M. and Thorup, M.},
  year = {2010},
  doi = {10.1145/1993636.1993638},
  abstract = {Randomized algorithms are often enjoyed for their simplicity, but the hash functions used to yield the desired theoretical guarantees are often neither simple nor practical. Here we show that the simplest possible tabulation hashing provides unexpectedly strong guarantees. The scheme itself dates back to Carter and Wegman (STOC'77). Keys are viewed as consisting of c characters. We initialize c tables T\_1, ..., T\_c mapping characters to random hash codes. A key x=(x\_1, ..., x\_c) is hashed to T\_1[x\_1] xor ... xor T\_c[x\_c]. While this scheme is not even 4-independent, we show that it provides many of the guarantees that are normally obtained via higher independence, e.g., Chernoff-type concentration, min-wise hashing for estimating set intersection, and cuckoo hashing.},
  citationcount = {150},
  venue = {Symposium on the Theory of Computing}
}

@article{patrascuThePowerOf2012,
  title = {The Power of Simple Tabulation Hashing},
  author = {Patrascu, M. and Thorup, Mikkel},
  year = {2012},
  doi = {10.1145/2220357.2220361},
  abstract = {Randomized algorithms are often enjoyed for their simplicity, but the hash functions used to yield the desired theoretical guarantees are often neither simple nor practical. Here we show that the simplest possible tabulation hashing provides unexpectedly strong guarantees. The scheme itself dates back to Zobrist in 1970 who used it for game playing programs. Keys are viewed as consisting of {\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown} characters. We initialize {\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown} tables {\textexclamdown}i{\textquestiondown}H{\textexclamdown}/i{\textquestiondown}1, ..., {\textexclamdown}i{\textquestiondown}H{\textexclamdown}/i{\textquestiondown}{\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown} mapping characters to random hash codes. A key {\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}\,=\,({\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}1{\textexclamdown}/sub{\textquestiondown}, ..., {\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}{\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown}) is hashed to {\textexclamdown}i{\textquestiondown}H{\textexclamdown}/i{\textquestiondown}1[{\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}1]\,{$\oplus$}\,{$\cdots$}\,{$\oplus$}\,{\textexclamdown}i{\textquestiondown}H{\textexclamdown}/i{\textquestiondown}{\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown}[{\textexclamdown}i{\textquestiondown}x{\textexclamdown}/i{\textquestiondown}{\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown}], where {$\oplus$} denotes bit-wise exclusive-or. While this scheme is not even 4-independent, we show that it provides many of the guarantees that are normally obtained via higher independence, for example, Chernoff-type concentration, min-wise hashing for estimating set intersection, and cuckoo hashing.},
  citationcount = {18},
  venue = {JACM}
}

@article{patrascuTightBoundsPartialsums2004,
  title = {Tight Bounds for the Partial-Sums Problem},
  author = {P{\v a}tra{\c}scu, M. and Demaine, E. D.},
  year = {2004},
  journal = {Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA)},
  pages = {20--29},
  doi = {10.4064/aa138-1-3},
  file = {/Users/tulasi/Zotero/storage/FKUTGKEV/Patrascu and Demaine - Tight Bounds for the Partial-Sums Problem.pdf}
}

@article{patrascuTightBoundsPartialsums2004a,
  title = {Tight Bounds for the Partial-Sums Problem},
  author = {Patrascu, M. and Demaine, E.},
  year = {2004},
  doi = {10.5555/982792.982796},
  abstract = {We close the gaps between known lower and upper bounds for the online partial-sums problem in the RAM and group models of computation. If elements are chosen from an abstract group, we prove an {\textohm}(lg {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) lower bound on the number of algebraic operations that must be performed, matching a well-known upper bound. In the RAM model with {\textexclamdown}i{\textquestiondown}b{\textexclamdown}/i{\textquestiondown}-bit memory registers, we consider the well-studied case when the elements of the array can be changed additively by {$\Delta$}-bit integers. We give a RAM algorithm that achieves a running time of {$\Theta$}(1 + lg {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} / lg({\textexclamdown}i{\textquestiondown}b{\textexclamdown}/i{\textquestiondown} / {$\Delta$})) and prove a matching lower bound in the cell-probe model. Our lower bound is for the amortized complexity, and makes minimal assumptions about the relations between {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}, {\textexclamdown}i{\textquestiondown}b{\textexclamdown}/i{\textquestiondown}, and {$\Delta$}. The best previous lower bound was {\textohm}(lg {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} = (lg lg {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}+lg {\textexclamdown}i{\textquestiondown}b{\textexclamdown}/i{\textquestiondown})), and the best previous upper bound matched only in the special case {\textexclamdown}i{\textquestiondown}b{\textexclamdown}/i{\textquestiondown} = {$\Theta$}(lg {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) and {$\Delta$} = {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(lg lg {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}).},
  citationcount = {60},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,lower bound}
}

@inproceedings{patrascuTimespaceTradeoffsPredecessor2006,
  title = {Time-Space Trade-Offs for Predecessor Search},
  booktitle = {Proceedings of the Thirty-Eighth Annual {{ACM}} Symposium on {{Theory}} of {{Computing}}},
  author = {P{\u a}tra{\c s}cu, Mihai and Thorup, Mikkel},
  year = {2006},
  month = may,
  series = {{{STOC}} '06},
  pages = {232--240},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1132516.1132551},
  url = {https://dl.acm.org/doi/10.1145/1132516.1132551},
  urldate = {2024-11-17},
  abstract = {We develop a new technique for proving cell-probe lower bounds for static data structures. Previous lower bounds used a reduction to communication games, which was known not to be tight by counting arguments. We give the first lower bound for an explicit problem which breaks this communication complexity barrier. In addition, our bounds give the first separation between polynomial and near linear space. Such a separation is inherently impossible by communication complexity.Using our lower bound technique and new upper bound constructions, we obtain tight bounds for searching predecessors among a static set of integers. Given a set Y of n integers of l bits each, the goal is to efficiently find PREDECESSOR (x) = max (y {$\in$} Y {\textbar} y {$\leq$} x). For this purpose, we represent Y on a RAM with word length b using S {$\geq$} nl bits of space. Defining a = lg S/n, we show that the optimal search time is, up to constant factors: min(logbn, lgl-lg n / n, lg(l/a) / lg(a/lg n * lg l/a), lg (l/a) / lg (lg (l/a) / lg (lg n / a)).In external memory (b \&gt; l), it follows that the optimal strategy is to use either standard B-trees, or a RAM algorithm ignoring the larger block size. In the important case of b = l = {$\gamma$} lg n, for {$\gamma$} \&gt; 1 (i.e. polynomial universes), and near linear space (such as S = n {$\bullet$} lgO(1) n), the optimal search time is {$\Theta$}(lg l). Thus, our lower bound implies the surprising conclusion that van Emde Boas' classic data structure from [FOCS'75] is optimal in this case. Note that for space n1+{$\varepsilon$}, a running time of O(lg l / lg lg l) was given by Beame and Fich [STOC'99].},
  isbn = {978-1-59593-134-4},
  keywords = {cell probe,communication,communication complexity,data structure,lower bound,reduction,static,time-space},
  annotation = {N. Alon and J. Spencer. The Probabilistic Method. John Wiley 2nd edition 2000.  N. Alon and J. Spencer. The Probabilistic Method. John Wiley 2nd edition 2000.},
  file = {/Users/tulasi/Zotero/storage/MPUZECYJ/Ptracu and Thorup - 2006 - Time-space trade-offs for predecessor search.pdf;/Users/tulasi/Zotero/storage/RMP5U7M6/Ptracu and Thorup - 2006 - Time-space trade-offs for predecessor search.pdf}
}

@misc{patrascuUnifyingLandscapeCellProbe2010,
  title = {Unifying the {{Landscape}} of {{Cell-Probe Lower Bounds}}},
  author = {Patrascu, Mihai},
  year = {2010},
  month = oct,
  number = {arXiv:1010.3783},
  eprint = {1010.3783},
  primaryclass = {cs},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1010.3783},
  urldate = {2024-09-03},
  abstract = {We show that a large fraction of the data-structure lower bounds known today in fact follow by reduction from the communication complexity of lopsided (asymmetric) set disjointness. This includes lower bounds for: * high-dimensional problems, where the goal is to show large space lower bounds. * constant-dimensional geometric problems, where the goal is to bound the query time for space O(n polylog n). * dynamic problems, where we are looking for a trade-off between query and update time. (In this case, our bounds are slightly weaker than the originals, losing a lglg n factor.) Our reductions also imply the following new results: * an Omega(lg n / lglg n) bound for 4-dimensional range reporting, given space O(n polylog n). This is quite timely, since a recent result solved 3D reporting in O(lglg n) time, raising the prospect that higher dimensions could also be easy. * a tight space lower bound for the partial match problem, for constant query time. * the first lower bound for reachability oracles. In the process, we prove optimal randomized lower bounds for lopsided set disjointness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {cell probe,communication,communication complexity,Computer Science - Computational Complexity,Computer Science - Computational Geometry,Computer Science - Data Structures and Algorithms,data structure,dynamic,lower bound,query,query time,reduction,update,update time},
  file = {/Users/tulasi/Zotero/storage/QESHNLV7/1010.pdf}
}

@article{patrickw.dymondPointersArithmeticPRAMs1993,
  title = {Pointers versus Arithmetic in {{PRAMs}}},
  author = {Patrick W. Dymond and Faith Ellen and N. Nishimura and P. Ragde and W. L. Ruzzo},
  year = {1993},
  journal = {[1993] Proceedings of the Eigth Annual Structure in Complexity Theory Conference},
  doi = {10.1109/SCT.1993.336522},
  abstract = {A parallel pointer machine, (PPM) is a parallel model having pointers as its principal data type. PPMs have been characterized as PRAMs obeying two restrictions: restricted arithmetic capabilities and the CROW (concurrent read, owner write) memory access restriction. Results concerning the relative power of PPMs (and other arithmetically restricted PRAMs) versus CROW PRAMs having ordinary arithmetic capabilities are presented. First, lower bounds separating PPMs from CROW PRAMs are proved. Second, it is shown that this lower bound is tight. As a corollary, sharply improved PPM algorithms are obtained for a variety of problems, including deterministic context-free language recognition.{$<<$}ETX{$>>$}},
  keywords = {lower bound},
  annotation = {Citation Count: 12}
}

@article{patt-shamirApproximateDistributedTop2008,
  title = {Approximate Distributed Top-k Queries},
  author = {{Patt-Shamir}, B. and Shafrir, A.},
  year = {2008},
  doi = {10.1007/s00446-008-0055-3},
  abstract = {No abstract available},
  citationcount = {6},
  venue = {Distributed computing},
  keywords = {query}
}

@article{patt-shamirApproximateTopK2006,
  title = {Approximate Top-k Queries in Sensor Networks},
  author = {{Patt-Shamir}, B. and Shafrir, A.},
  year = {2006},
  doi = {10.1007/11780823_25},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Colloquium on Structural Information \& Communication Complexity},
  keywords = {query}
}

@article{paVtrascuUnifyingLandscapeCellProbe2011,
  title = {Unifying the {{Landscape}} of {{Cell-Probe Lower Bounds}}},
  author = {Pa{\textasciicaron}tra{\c s}cu, Mihai},
  year = {2011},
  month = jan,
  journal = {SIAM J. Comput.},
  volume = {40},
  number = {3},
  pages = {827--847},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/09075336X},
  url = {https://epubs.siam.org/doi/abs/10.1137/09075336X},
  urldate = {2024-11-18},
  abstract = {We show that a large fraction of the data-structure lower bounds known today in fact follow by reduction from the communication complexity of lopsided (asymmetric) set disjointness. This includes lower bounds for: * high-dimensional problems, where the goal is to show large space lower bounds. * constant-dimensional geometric problems, where the goal is to bound the query time for space O(n polylog n). * dynamic problems, where we are looking for a trade-off between query and update time. (In this case, our bounds are slightly weaker than the originals, losing a lglg n factor.) Our reductions also imply the following new results: * an Omega(lg n / lglg n) bound for 4-dimensional range reporting, given space O(n polylog n). This is quite timely, since a recent result solved 3D reporting in O(lglg n) time, raising the prospect that higher dimensions could also be easy. * a tight space lower bound for the partial match problem, for constant query time. * the first lower bound for reachability oracles. In the process, we prove optimal randomized lower bounds for lopsided set disjointness.},
  keywords = {cell probe,communication,communication complexity,data structure,dynamic,lower bound,query,query time,reduction,update,update time},
  file = {/Users/tulasi/Zotero/storage/NJFER7SP/Patracu - 2011 - Unifying the Landscape of Cell-Probe Lower Bounds.pdf}
}

@article{pellegriniEfficientIpTable2004,
  title = {Efficient {{IP}} Table Lookup via Adaptive Stratified Trees with Selective Reconstructions},
  author = {Pellegrini, M. and Fusco, G.},
  year = {2004},
  doi = {10.1145/1227161.1278376},
  abstract = {IP address lookup is a critical operation for high-bandwidth routers in packet-switching networks, such as Internet. The lookup is a nontrivial operation, since it requires searching for the longest prefix, among those stored in a (large) given table, matching the IP address. Ever increasing routing table size, traffic volume, and links speed demand new and more efficient algorithms. Moreover, the imminent move to IPv6 128-bit addresses will soon require a rethinking of previous technical choices. This article describes a the new data structure for solving the IP table lookup problem christened the adaptive stratified tree (AST). The proposed solution is based on casting the problem in geometric terms and on repeated application of efficient local geometric optimization routines. Experiments with this approach have shown that in terms of storage, query time, and update time the AST is at a par with state of the art algorithms based on data compression or string manipulations (and often it is better on some of the measured quantities).},
  citationcount = {1},
  venue = {Embedded Systems and Applications},
  keywords = {adaptive,data structure,query,query time,update,update time}
}

@article{pengFullyDynamicTo2022,
  title = {Fully-Dynamic-to-Incremental Reductions with Known Deletion Order (e.g. Sliding Window)},
  author = {Peng, Binghui and Rubinstein, A.},
  year = {2022},
  doi = {10.48550/arXiv.2211.05178},
  abstract = {Dynamic algorithms come in three main flavors: \{incremental\} (insertions-only), \{decremental\} (deletions-only), or \{fully\} \{dynamic\} (both insertions and deletions). Fully dynamic is the holy grail of dynamic algorithm design; it is obviously more general than the other two, but is it strictly harder? Several works managed to reduce fully dynamic to the incremental or decremental models by taking advantage of either specific structure of the incremental/decremental algorithms (e.g. [HK99, HLT01, BKS12, ADKKP16, BS80, OL81, OvL81]), or specific order of insertions/deletions (e.g. [AW14,HKNS15,KPP16]). Our goal in this work is to get a black-box fully-to-incremental reduction that is as general as possible. We find that the following conditions are necessary: {$\bullet$} The incremental algorithm must have a worst-case (rather than amortized) running time guarantee. {$\bullet$} The reduction must work in what we call the \{deletions\}-\{look\}-\{ahead\} \{model\}, where the order of deletions among current elements is known in advance. A notable practical example is the"sliding window"(FIFO) order of updates. Under those conditions, we design: {$\bullet$} A simple, practical, amortized-fully-dynamic to worst-case-incremental reduction with a (T)-factor overhead on the running time, where T is the total number of updates. {$\bullet$} A theoretical worst-case-fully-dynamic to worst-case-incremental reduction with a \{polylog\}(T)-factor overhead on the running time.},
  citationcount = {4},
  venue = {SIAM Symposium on Simplicity in Algorithms},
  keywords = {dynamic,reduction,update}
}

@article{pengMinimumMotifCut2024,
  title = {Minimum Motif-Cut: A Workload-Aware {{RDF}} Graph Partitioning Strategy},
  author = {Peng, Peng and Ji, Shen and {\"O}zsu, M. Tamer and Zou, Lei},
  year = {2024},
  doi = {10.1007/s00778-024-00860-1},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {The VLDB journal}
}

@article{pengMpcMinimumProperty2022,
  title = {{{MPC}}: {{Minimum}} Property-Cut {{RDF}} Graph Partitioning},
  author = {Peng, Peng and Ozsu, M. Tamer and Zou, Lei and Yan, Cen and Liu, Chengjun},
  year = {2022},
  doi = {10.1109/icde53745.2022.00019},
  abstract = {Scaling-out RDF processing to deal with graph size usually requires partitioning the RDF graph. Typical partitioning approaches minimize edge-cuts or vertex-cuts. In this paper we argue that these approaches do not avoid or reduce joins between different partitions (i.e., inter-partition join), and propose an approach based on minimizing the number of distinct crossing properties, which we call Minimum Property-Cut (MPC). This approach enables more queries to be independently evaluated without inter-partition join. However, the minimum property-cut partitioning is a NP-hard problem and we propose a heuristic greedy algorithm to address that. Extensive experiments over a variety of synthetic and real RDF graphs show that the proposed technique can significantly avoid joins and results in good performance.},
  citationcount = {2},
  venue = {IEEE International Conference on Data Engineering},
  keywords = {query}
}

@article{pengOptimalOfflineDynamic2017,
  title = {Optimal Offline Dynamic 2, 3-Edge/Vertex Connectivity},
  author = {Peng, Richard and Sandlund, Bryce and Sleator, D.},
  year = {2017},
  doi = {10.1007/978-3-030-24766-9_40},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Workshop on Algorithms and Data Structures},
  keywords = {dynamic}
}

@article{pentlandPhotobookToolsFor1994,
  title = {Photobook: Tools for Content-Based Manipulation of Image Databases},
  author = {Pentland, A. and Picard, Rosalind W. and Sclaroff, S.},
  year = {1994},
  doi = {10.1117/12.200805},
  abstract = {We describe the Photobook system, which is a set of interactive tools for browsing and searching images and image sequences. These tools differ from those used in standard image databases in that they make direct use of the image content rather than relying on annotations. Direct search on image content is made possible by use of semantics-preserving image compression, which reduces images to a small set of perceptually significant coefficients. We describe three Photobook tools in particular: one that allows search based on gray-level appearance, one that uses 2-D shape, and a third that allows search based on textural properties.},
  citationcount = {372},
  venue = {Other Conferences}
}

@article{pentlandPhotobookToolsFor1994,
  title = {Photobook: Tools for Content-Based Manipulation of Image Databases},
  author = {Pentland, A. and Picard, Rosalind W. and Sclaroff, S.},
  year = {1994},
  doi = {10.1117/12.171786},
  abstract = {We describe the Photobook system, which is a set of interactive tools for browsing and searching images and image sequences. These tools differ from those used in standard image databases in that they make direct use of the image content rather than relying on annotations. Direct search on image content is made possible by use of semantics-preserving image compression, which reduces images to a small set of perceptually-significant coefficients. We describe three Photobook tools in particular: one that allows search based on grey-level appearance, one that uses 2D shape, and a third that allows search based on textural properties.},
  citationcount = {867},
  venue = {Electronic imaging}
}

@article{perfectApplicationsOfMenger1968,
  title = {Applications of {{Menger}}'s Graph Theorem},
  author = {Perfect, H.},
  year = {1968},
  doi = {10.1016/0022-247X(68)90163-7},
  abstract = {No abstract available},
  citationcount = {95},
  venue = {No venue available}
}

@article{perlUnderstandingTheComplexity1977,
  title = {Understanding the Complexity of Interpolation Search},
  author = {Perl, Y. and Reingold, E.},
  year = {1977},
  doi = {10.1016/0020-0190(77)90072-2},
  abstract = {No abstract available},
  citationcount = {47},
  venue = {Information Processing Letters}
}

@article{perreaultMedianFilteringIn2007,
  title = {Median Filtering in Constant Time},
  author = {Perreault, S. and H{\'e}bert, P.},
  year = {2007},
  doi = {10.1109/TIP.2007.902329},
  abstract = {The median filter is one of the basic building blocks in many image processing situations. However, its use has long been hampered by its algorithmic complexity O(tau) of in the kernel radius. With the trend toward larger images and proportionally larger filter kernels, the need for a more efficient median filtering algorithm becomes pressing. In this correspondence, a new, simple, yet much faster, algorithm exhibiting O(1) runtime complexity is described and analyzed. It is compared and benchmarked against previous algorithms. Extensions to higher dimensional or higher precision data and an approximation to a circular kernel are presented, as well.},
  citationcount = {329},
  venue = {IEEE Transactions on Image Processing}
}

@article{persianoLimitsOfBreach2023,
  title = {Limits of Breach-Resistant and Snapshot-Oblivious Rams},
  author = {Persiano, G. and Yeo, Kevin},
  year = {2023},
  doi = {10.1007/978-3-031-38551-3_6},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {IACR Cryptology ePrint Archive}
}

@article{persianoLimitsOfPreprocessing2022,
  title = {Limits of Preprocessing for Single-Server {{PIR}}},
  author = {Persiano, G. and Yeo, Kevin},
  year = {2022},
  doi = {10.1137/1.9781611977073.99},
  abstract = {We present a lower bound for the static cryptographic data structure problem of single-server private information retrieval (PIR) . PIR considers the setting where a server holds a database of n entries and a client wishes to privately retrieve the i -th entry without revealing the index i to the server. In our work, we focus on PIR with preprocessing where an r -bit hint may be computed in a preprocessing stage and stored by the server to be used to perform private queries in expected time t . We consider the public preprocessing setting of Beimel et al. [JoC, 2004] where the hint is publicly available to everyone including the adversary. We prove that for any single-server computationally secure PIR with preprocessing it must be that tr = {\textohm}( n log n ) when r = {\textohm}(log n ). If r = O (log n ), we show that t = {\textohm}( n ). Our lower bound holds even when the scheme errs with probability 1 /n 2 and the adversary's distinguishing advantage is 1 /n . Our work improves upon the tr = {\textohm}( n ) lower bound of Beimel et al. [JoC, 2004]. We prove our lower bound in a variant of the cell probe model where only accesses to the memory are charged cost and computation and accesses to the hint are free. Our main technical contribution is a novel use of the cell sampling technique (also known as the incompressibility technique) used to obtain lower bounds on data structures. In previous works, this technique only leveraged the correctness guarantees to prove lower bounds even when used for cryptographic primitives. Our work combines the cell sampling technique with the privacy guarantees of PIR to construct a powerful, polynomial-time adversary that is critical to proving our higher lower bounds.},
  citationcount = {14},
  venue = {IACR Cryptology ePrint Archive},
  keywords = {cell probe,cell sampling,data structure,lower bound,query,static}
}

@article{persianoLowerBoundFramework2022,
  title = {Lower Bound Framework for Differentially Private and Oblivious Data Structures},
  author = {Persiano, G. and Yeo, Kevin},
  year = {2022},
  doi = {10.1007/978-3-031-30545-0_17},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {IACR Cryptology ePrint Archive},
  keywords = {data structure,lower bound}
}

@article{persianoLowerBoundsFor2019,
  title = {Lower Bounds for Differentially Private Rams},
  author = {Persiano, G. and Yeo, Kevin},
  year = {2019},
  doi = {10.1007/978-3-030-17653-2_14},
  abstract = {No abstract available},
  citationcount = {28},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {lower bound}
}

@article{pestovIndexabilityConcentrationAnd2010,
  title = {Indexability, Concentration, and {{VC}} Theory},
  author = {Pestov, V.},
  year = {2010},
  doi = {10.1016/j.jda.2011.10.002},
  abstract = {No abstract available},
  citationcount = {30},
  venue = {J. Discrete Algorithms}
}

@article{pestovLowerBoundsOn2008,
  title = {Lower Bounds on Performance of Metric Tree Indexing Schemes for Exact Similarity Search in High Dimensions},
  author = {Pestov, V.},
  year = {2008},
  doi = {10.1007/s00453-012-9638-2},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {Algorithmica},
  keywords = {lower bound}
}

@article{petersenImprovedBoundsFor2008,
  title = {Improved Bounds for Range Mode and Range Median Queries},
  author = {Petersen, H.},
  year = {2008},
  doi = {10.1007/978-3-540-77566-9_36},
  abstract = {No abstract available},
  citationcount = {34},
  venue = {Conference on Current Trends in Theory and Practice of Informatics},
  keywords = {query}
}

@article{petersenRangeModeAnd2009,
  title = {Range Mode and Range Median Queries in Constant Time and Sub-Quadratic Space},
  author = {Petersen, H. and Grabowski, S.},
  year = {2009},
  doi = {10.1016/j.ipl.2008.10.007},
  abstract = {No abstract available},
  citationcount = {40},
  venue = {Information Processing Letters},
  keywords = {query}
}

@article{petrieGraphTheoreticMultisample2016,
  title = {Graph-Theoretic Multisample Tests of Equality in Distribution for High Dimensional Data},
  author = {Petrie, Adam},
  year = {2016},
  doi = {10.1016/j.csda.2015.11.003},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Computational Statistics \& Data Analysis}
}

@article{petrovSumsOfIndependent1975,
  title = {Sums of Independent Random Variables},
  author = {Petrov, V. V.},
  year = {1975},
  doi = {10.1007/978-3-642-65809-9},
  abstract = {No abstract available},
  citationcount = {99},
  venue = {No venue available}
}

@article{pettieOptimalVertexConnectivity2022,
  title = {Optimal Vertex Connectivity Oracles},
  author = {Pettie, Seth and Saranurak, Thatchaphol and Yin, Longhui},
  year = {2022},
  doi = {10.1145/3519935.3519945},
  abstract = {A k-vertex connectivity oracle for undirected G is a data structure that, given u,v{$\in$} V(G), reports min\{k,{$\kappa$}(u,v)\}, where {$\kappa$}(u,v) is the pairwise vertex connectivity between u,v. There are three main measures of efficiency: construction time, query time, and space. Prior work of Izsak and Nutov [Inf. Process. Lett. 2012] shows that a data structure of total size O(knlogn), which can even be encoded as a O(klog3 n)-bit labeling scheme, can answer vertex-connectivity queries in O(klogn) time. The construction time is polynomial, but unspecified. In this paper we address the top three complexity measures. The first is the space consumption. We prove that any k-vertex connectivity oracle requires {\textohm}(kn) bits of space. This answers a long-standing question on the structural complexity of vertex connectivity, and gives a strong separation between the complexity of vertex- and edge-connectivity. Both Izsak and Nutov [Inf. Process. Lett. 2012] and the data structure we will present in this work match this lower bound up to polylogarithmic factors. The second is the query time. We answer queries in O(logn) time, independent of k, improving on {\textohm}(klogn) time of Izsak and Nutov [Inf. Process. Lett. 2012]. The main idea is to build instances of SetIntersection data structures, with additional structure based on affine planes. This structure allows for optimum query time that is linear in the output size (This evades the general k1/2-o(1) and k1-o(1) lower bounds on SetIntersection from the 3SUM or OMv hypotheses, resp. Kopelowitz et al. [SODA 2016] and Henzinger et al. [STOC 2015].) The third is the construction time. We build the data structure in time of roughly a max-flow computation on a unit-capacity graph, which is m4/3+o(1) using state-of-the-art algorithm by Tarun et al. [FOCS 2020]. Max-flow is a natural barrier for many problems that have an all-pairs-min-cut flavor. The main technical contribution here is a fast algorithm for computing a k-bounded version of a Gomory-Hu tree for element connectivity, a notion that generalizes edge and vertex connectivity.},
  citationcount = {7},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,lower bound,query,query time}
}

@article{peymanafshaniDataStructureLower2016,
  title = {Data {{Structure Lower Bounds}} for {{Document Indexing Problems}}},
  author = {Peyman Afshani and J. Nielsen},
  year = {2016},
  journal = {International Colloquium on Automata, Languages and Programming},
  doi = {10.4230/LIPIcs.ICALP.2016.93},
  abstract = {We study data structure problems related to document indexing and pattern matching queries and our main contribution is to show that the pointer machine model of computation can be extremely useful in proving high and unconditional lower bounds that cannot be obtained in any other known model of computation with the current techniques. Often our lower bounds match the known space-query time trade-off curve and in fact for all the problems considered, there is a very good and reasonable match between the our lower bounds and the known upper bounds, at least for some choice of input parameters. The problems that we consider are set intersection queries (both the reporting variant and the semi-group counting variant), indexing a set of documents for two-pattern queries, or forbiddenpattern queries, or queries with wild-cards, and indexing an input set of gapped-patterns (or two-patterns) to find those matching a document given at the query time. 1998 ACM Subject Classification F.2.2 Nonnumerical Algorithms and Problems},
  keywords = {data structure,lower bound,query,query time},
  annotation = {Citation Count: 17}
}

@article{peymanafshaniHigherdimensionalOrthogonalRange2012,
  title = {Higher-Dimensional Orthogonal Range Reporting and Rectangle Stabbing in the Pointer Machine Model},
  author = {Peyman Afshani and L. Arge and Kasper Green Larsen},
  year = {2012},
  journal = {International Symposium on Computational Geometry},
  doi = {10.1145/2261250.2261299},
  abstract = {In this paper, we consider two fundamental problems in the pointer machine model of computation, namely orthogonal range reporting and rectangle stabbing. Orthogonal range reporting is the problem of storing a set of n points in d-dimensional space in a data structure, such that the t points in an axis-aligned query rectangle can be reported efficiently. Rectangle stabbing is the "dual" problem where a set of n axis-aligned rectangles should be stored in a data structure, such that the t rectangles that contain a query point can be reported efficiently. Very recently an optimal O(log n+t) query time pointer machine data structure was developed for the three-dimensional version of the orthogonal range reporting problem. However, in four dimensions the best known query bound of O(log2n / log log n + t) has not been improved for decades.  We describe an orthogonal range reporting data structure that is the first structure to achieve significantly less than O(log2n+t) query time in four dimensions. More precisely, we develop a structure that uses O(n (log n /log log n)d) space and can answer d-dimensional orthogonal range reporting queries (for d {$\geq$} 4) in O( log n (log n /log log n)d-4+1/(d-2) + t) time. Ignoring log log n factors, this speeds up the best previous query time by a log1-1/(d-2)n factor. For the rectangle stabbing problem, we show that any data structure that uses nh space must use {\textohm}(log n (log n / log h)d-2 + t) time to answer a query. This improves the previous results by a log h factor, and is the first lower bound that is optimal for a large range of h, namely for h {$\geq$} logd-2+{$\varepsilon$} n where {$\varepsilon>$}0 is an arbitrarily small constant. By a simple geometric transformation, our result also implies an improved query lower bound for orthogonal range reporting.},
  annotation = {Citation Count: 40}
}

@article{peymanafshaniLowerBoundDynamic2020,
  title = {A {{Lower Bound}} for {{Dynamic Fractional Cascading}}},
  author = {Peyman Afshani},
  year = {2020},
  journal = {ACM-SIAM Symposium on Discrete Algorithms},
  doi = {10.1137/1.9781611976465.133},
  abstract = {We investigate the limits of one of the fundamental ideas in data structures: fractional cascading. This is an important data structure technique to speed up repeated searches for the same key in multiple lists and it has numerous applications. Specifically, the input is a "catalog" graph, \$G\$, of constant degree together with a list of values assigned to every vertex of \$G\$. The goal is to preprocess the input such that given a connected subgraph \$H\$ of \$G\$ and a single query value \$q\$, one can find the predecessor of \$q\$ in every list that belongs to \${\textbackslash}scat\$. The classical result by Chazelle and Guibas shows that in a pointer machine, this can be done in the optimal time of \$O({\textbackslash}log n + {\textbar}{\textbackslash}scat{\textbar})\$ where \$n\$ is the total number of values. However, if insertion and deletion of values are allowed, then the query time slows down to \$O({\textbackslash}log n + {\textbar}{\textbackslash}scat{\textbar} {\textbackslash}log{\textbackslash}log n)\$. If only insertions (or deletions) are allowed, then once again, an optimal query time can be obtained but by using amortization at update time.  We prove a lower bound of \${\textbackslash}Omega( {\textbackslash}log n {\textbackslash}sqrt\{{\textbackslash}log{\textbackslash}log n\})\$ on the worst-case query time of dynamic fractional cascading, when queries are paths of length \$O({\textbackslash}log n)\$. The lower bound applies both to fully dynamic data structures with amortized polylogarithmic update time and incremental data structures with polylogarithmic worst-case update time. As a side, this also roves that amortization is crucial for obtaining an optimal incremental data structure.  This is the first non-trivial pointer machine lower bound for a dynamic data structure that breaks the \${\textbackslash}Omega({\textbackslash}log n)\$ barrier. In order to obtain this result, we develop a number of new ideas and techniques that hopefully can be useful to obtain additional dynamic lower bounds in the pointer machine model.},
  keywords = {data structure,dynamic,lower bound,query,query time,update,update time},
  annotation = {Citation Count: 0}
}

@article{peymanafshaniNewLowerBound2019,
  title = {A {{New Lower Bound}} for {{Semigroup Orthogonal Range Searching}}},
  author = {Peyman Afshani},
  year = {2019},
  journal = {International Symposium on Computational Geometry},
  doi = {10.4230/LIPICS.SOCG.2019.3},
  abstract = {We report the first improvement in the space-time trade-off of lower bounds for the orthogonal range searching problem in the semigroup model, since Chazelle's result from 1990. This is one of the very fundamental problems in range searching with a long history. Previously, Andrew Yao's influential result had shown that the problem is already non-trivial in one dimension [Yao, 1982]: using m units of space, the query time Q(n) must be Omega(alpha(m,n) + n/(m-n+1)) where alpha(*,*) is the inverse Ackermann's function, a very slowly growing function. In d dimensions, Bernard Chazelle [Chazelle, 1990] proved that the query time must be Q(n) = Omega((log\_beta n){\textasciicircum}\{d-1\}) where beta = 2m/n. Chazelle's lower bound is known to be tight for when space consumption is "high" i.e., m = Omega(n log{\textasciicircum}\{d+epsilon\}n).  We have two main results. The first is a lower bound that shows Chazelle's lower bound was not tight for "low space": we prove that we must have m Q(n) = Omega(n (log n log log n){\textasciicircum}\{d-1\}). Our lower bound does not close the gap to the existing data structures, however, our second result is that our analysis is tight. Thus, we believe the gap is in fact natural since lower bounds are proven for idempotent semigroups while the data structures are built for general semigroups and thus they cannot assume (and use) the properties of an idempotent semigroup. As a result, we believe to close the gap one must study lower bounds for non-idempotent semigroups or building data structures for idempotent semigroups. We develope significantly new ideas for both of our results that could be useful in pursuing either of these directions.},
  keywords = {data structure,lower bound,query,query time},
  annotation = {Citation Count: 3}
}

@article{peymanafshaniOrthogonalRangeReporting2009,
  title = {Orthogonal {{Range Reporting}} in {{Three}} and {{Higher Dimensions}}},
  author = {Peyman Afshani and L. Arge and Kasper Green Larsen},
  year = {2009},
  journal = {2009 50th Annual IEEE Symposium on Foundations of Computer Science},
  doi = {10.1109/FOCS.2009.58},
  abstract = {In orthogonal range reporting we are to preprocess N points in d-dimensional space so that the points inside a d-dimensional axis-aligned query box can be reported efficiently. This is a fundamental problem in various fields, including spatial databases and computational geometry. In this paper we provide a number of improvements for three and higher dimensional orthogonal range reporting: In the pointer machine model, we improve all the best previous results, some of which have not seen any improvements in almost two decades. In the I/O-model, we improve the previously known three-dimensional structures and provide the first (non-trivial) structures for four and higher dimensions.},
  keywords = {query},
  annotation = {Citation Count: 43}
}

@article{peymanafshaniOrthogonalRangeReporting2010,
  title = {Orthogonal Range Reporting: Query Lower Bounds, Optimal Structures in 3-d, and Higher-Dimensional Improvements},
  author = {Peyman Afshani and L. Arge and Kasper Green Larsen},
  year = {2010},
  journal = {SCG},
  doi = {10.1145/1810959.1811001},
  abstract = {Orthogonal range reporting is the problem of storing a set of n points in d-dimensional space, such that the k points in an axis-orthogonal query box can be reported efficiently. While the 2-d version of the problem was completely characterized in the pointer machine model more than two decades ago, this is not the case in higher dimensions. In this paper we provide a space optimal pointer machine data structure for 3-d orthogonal range reporting that answers queries in O(log n + k) time. Thus we settle the complexity of the problem in 3-d. We use this result to obtain improved structures in higher dimensions, namely structures with a log n/ log log n factor increase in space and query time per dimension. Thus for d e 3 we obtain a structure that both uses optimal O(n(log n/ log log n)d--1) space and answers queries in the best known query bound O(log n(log n/ log log n)d--3 + k). Furthermore, we show that any data structure for the d-dimensional orthogonal range reporting problem in the pointer machine model of computation that uses S(n) space must spend {\textohm}((log n/ log(S(n)/n)){$\lfloor$}d/2{$\rfloor$}--1) time to answer queries. Thus, if S(n)/n is poly-logarithmic, then the query time is at least {\textohm}((log n/ log log n){$\lfloor$}d/2{$\rfloor$}--1). This is the first known non-trivial higher dimensional orthogonal range reporting query lower bound and it has two important implications. First, it shows that the query bound increases with dimension. Second, in combination with our upper bounds it shows that the optimal query bound increases from {$\Theta$}(log n + k) to {\textohm}((log n/ log log n)2 + k) somewhere between three and six dimensions. Finally, we show that our techniques also lead to improved structures for point location in rectilinear subdivisions, that is, the problem of storing a set of n disjoint d-dimensional axis-orthogonal rectangles, such that the rectangle containing a query point q can be found efficiently.},
  annotation = {Citation Count: 41}
}

@article{peymanafshaniRectangleStabbingOrthogonal2022,
  title = {Rectangle {{Stabbing}} and {{Orthogonal Range Reporting Lower Bounds}} in {{Moderate Dimensions}}},
  author = {Peyman Afshani and Rasmus Killmann},
  year = {2022},
  journal = {Canadian Conference on Computational Geometry},
  doi = {10.1016/j.comgeo.2022.101959},
  keywords = {lower bound},
  annotation = {Citation Count: 1}
}

@article{phamSimpleYetEfficient2021,
  title = {Simple yet Efficient Algorithms for Maximum Inner Product Search via Extreme Order Statistics},
  author = {Pham, Ninh D.},
  year = {2021},
  doi = {10.1145/3447548.3467345},
  abstract = {We present a novel dimensionality reduction method for the approximate maximum inner product search (MIPS), named CEOs, based on the theory of concomitants of extreme order statistics. Utilizing the asymptotic behavior of these concomitants, we show that a few projections associated with the extreme values of the query signature are enough to estimate inner products. This yields a sublinear approximate MIPS algorithm with search recall guarantee under a mild condition. The indexing space is exponential but optimal for the approximate MIPS on a unit sphere. To deal with the exponential space complexity, we present practical variants, including CEOs-TA and coCEOs, that use near-linear indexing space and time. CEOs-TA exploits the threshold algorithm (TA) and provides superior search recalls to LSH-based MIPS solvers. coCEOs is a new data and dimension co-reduction technique that outperforms CEOs-TA and other competitive methods. Empirically, they are simple to implement and achieve at least 100x speedup compared to the bruteforce search while returning top-10 MIPS with recall of at least 90},
  citationcount = {13},
  venue = {Knowledge Discovery and Data Mining},
  keywords = {query,reduction}
}

@article{philipbilleTopTreeCompression2019,
  title = {Top {{Tree Compression}} of {{Tries}}},
  author = {Philip Bille and Pawe{\l} Gawrychowski and Inge Li G{\o}rtz and G. M. Landau and Oren Weimann},
  year = {2019},
  journal = {Algorithmica},
  doi = {10.1007/s00453-021-00869-w},
  annotation = {Citation Count: 7}
}

@article{phillipsLowerBoundsFor2011,
  title = {Lower Bounds for Number-in-Hand Multiparty Communication Complexity, Made Easy},
  author = {Phillips, J. M. and Verbin, Elad and Zhang, Qin},
  year = {2011},
  doi = {10.1137/15M1007525},
  abstract = {In this paper we prove lower bounds on randomized multiparty communication complexity, both in the blackboard model (where each message is written on a blackboard for all players to see) and (mainly) in the message-passing model, where messages are sent player-to-player. We introduce a new technique for proving such bounds, called symmetrization, which is natural, intuitive, and often easy to use. For example, for the problem where each of k players gets a bit-vector of length n, and the goal is to compute the coordinate-wise XOR of these vectors, we prove a tight lower bounds of {\textohm}(nk) in the blackboard model. For the same problem with AND instead of XOR, we prove a lower bounds of roughly {\textohm}(nk) in the message-passing model (assuming k {$\leq$} n/3200) and {\textohm}(n log k) in the blackboard model. We also prove lower bounds for bit-wise majority, for a graphconnectivity problem, and for other problems; the technique seems applicable to a wide range of other problems as well. The obtained communication lower bounds imply new lower bounds in the functional monitoring model [11] (also called the distributed streaming model). All of our lower bounds allow randomized communication protocols with two-sided error. We also use the symmetrization technique to prove several direct-sum-like results for multiparty communication.},
  citationcount = {95},
  venue = {SIAM journal on computing (Print)},
  keywords = {communication,communication complexity,lower bound}
}

@article{pibiriDynamicEliasFano2017,
  title = {Dynamic Elias-Fano Representation},
  author = {Pibiri, Giulio Ermanno and Venturini, Rossano},
  year = {2017},
  doi = {10.4230/LIPIcs.CPM.2017.30},
  abstract = {We show that it is possible to store a dynamic ordered set S of n integers drawn from a bounded universe of size u in space close to the information-theoretic lower bound and preserve, at the same time, the asymptotic time optimality of the operations. Our results leverage on the Elias-Fano representation of monotone integer sequences, which can be shown to be less than half a bit per element away from the information-theoretic minimum. In particular, considering a RAM model with memory word size Theta(log u) bits, when integers are drawn from a polynomial universe of size u = n{\textasciicircum}gamma for any gamma = Theta(1), we add o(n) bits to the static Elias-Fano representation in order to: 1. support static predecessor/successor queries in O(min\{1+log(u/n), loglog n\}); 2. make S grow in an append-only fashion by spending O(1) per inserted element; 3. describe a dynamic data structure supporting random access in O(log n / loglog n) worst-case, insertions/deletions in O(log n / loglog n) amortized and predecessor/successor queries in O(min\{1+log(u/n), loglog n\}) worst-case time. These time bounds are optimal.},
  citationcount = {14},
  venue = {Annual Symposium on Combinatorial Pattern Matching},
  keywords = {data structure,dynamic,information theoretic,lower bound,query,static}
}

@article{pibiriPracticalTradeOffs2020,
  title = {Practical Trade-offs for the Prefix-sum Problem},
  author = {Pibiri, Giulio Ermanno and Venturini, Rossano},
  year = {2020},
  doi = {10.1002/spe.2918},
  abstract = {Given an integer array A, the prefix-sum problem is to answer sum(i) queries that return the sum of the elements in A[0..i], knowing that the integers in A can be changed. It is a classic problem in data structure design with a wide range of applications in computing from coding to databases. In this work, we propose and compare practical solutions to this problem, showing that new trade-offs between the performance of queries and updates can be achieved on modern hardware.},
  citationcount = {6},
  venue = {Software, Practice \& Experience},
  keywords = {data structure,query,update}
}

@article{pibiriRankSelectQueries2020,
  title = {Rank/Select Queries over Mutable Bitmaps},
  author = {Pibiri, Giulio Ermanno and Kanda, Shunsuke},
  year = {2020},
  doi = {10.1016/j.is.2021.101756},
  abstract = {No abstract available},
  citationcount = {10},
  venue = {Information Systems},
  keywords = {query}
}

@article{pilipczukCompactRepresentationFor2021,
  title = {Compact Representation for Matrices of Bounded Twin-Width},
  author = {Pilipczuk, Michal and Soko{\l}owski, Marek and {Zych-Pawlewicz}, Anna},
  year = {2021},
  doi = {10.4230/LIPIcs.STACS.2022.52},
  abstract = {For every fixed d{$\in$}\{N\}, we design a data structure that represents a binary n{\texttimes}n matrix that is d-twin-ordered. The data structure occupies O\textsubscript{d}(n) bits, which is the least one could hope for, and can be queried for entries of the matrix in time O\textsubscript{d}(n) per query.},
  citationcount = {18},
  venue = {Symposium on Theoretical Aspects of Computer Science},
  keywords = {data structure,query}
}

@article{pippengerSuperconcentratorsOfDepth1982,
  title = {Superconcentrators of Depth 2},
  author = {Pippenger, N.},
  year = {1982},
  doi = {10.1016/0022-0000(82)90056-3},
  abstract = {No abstract available},
  citationcount = {31},
  venue = {Journal of computer and system sciences (Print)}
}

@article{plakuFastAndReliable2007,
  title = {Fast and Reliable Analysis of Molecular Motion Using Proximity Relations and Dimensionality Reduction},
  author = {Plaku, E. and Stamati, Hernan F. and Clementi, C. and Kavraki, L.},
  year = {2007},
  doi = {10.1002/prot.21337},
  abstract = {The analysis of molecular motion starting from extensive sampling of molecular configurations remains an important and challenging task in computational biology. Existing methods require a significant amount of time to extract the most relevant motion information from such data sets. In this work, we provide a practical tool for molecular motion analysis. The proposed method builds upon the recent ScIMAP (Scalable Isomap) method, which, by using proximity relations and dimensionality reduction, has been shown to reliably extract from simulation data a few parameters that capture the main, linear and/or nonlinear, modes of motion of a molecular system. The results we present in the context of protein folding reveal that the proposed method characterizes the folding process essentially as well as ScIMAP. At the same time, by projecting the simulation data and computing proximity relations in a low-dimensional Euclidean space, it renders such analysis computationally practical. In many instances, the proposed method reduces the computational cost from several CPU months to just a few CPU hours, making it possible to analyze extensive simulation data in a matter of a few hours using only a single processor. These results establish the proposed method as a reliable and practical tool for analyzing motions of considerably large molecular systems and proteins with complex folding mechanisms. Proteins 2007. {\copyright} 2007 Wiley-Liss, Inc.},
  citationcount = {46},
  venue = {Proteins: Structure, Function, and Bioinformatics},
  keywords = {reduction}
}

@article{plakuNonlinearDimensionalityReduction2007,
  title = {Nonlinear Dimensionality Reduction Using Approximate Nearest Neighbors},
  author = {Plaku, E. and Kavraki, L.},
  year = {2007},
  doi = {10.1137/1.9781611972771.17},
  abstract = {Nonlinear dimensionality reduction methods often rely on the nearest-neighbors graph to extract low-dimensional embeddings that reliably capture the underlying structure of high-dimensional data. Research however has shown that computing nearest neighbors of a point from a highdimensional data set generally requires time proportional to the size of the data set itself, rendering the computation of the nearest-neighbors graph prohibitively expensive. This work significantly reduces the major computational bottleneck of many nonlinear dimensionality reduction methods by ecien tly and accurately approximating the nearest-neighbors graph. The approximation relies on a distance-based projection of high-dimensional data onto low-dimensional Euclidean spaces. As indicated by experimental results, the advantage of the proposed approximation is that while it reliably maintains the accuracy of nonlinear dimensionality reduction methods, it significantly reduces the computational time.},
  citationcount = {3},
  venue = {SDM},
  keywords = {reduction}
}

@article{plakuQuantitativeAnalysisOf2006,
  title = {Quantitative Analysis of Nearest-Neighbors Search in High-Dimensional Sampling-Based Motion Planning},
  author = {Plaku, E. and Kavraki, L.},
  year = {2006},
  doi = {10.1007/978-3-540-68405-3_1},
  abstract = {No abstract available},
  citationcount = {53},
  venue = {Workshop on the Algorithmic Foundations of Robotics}
}

@article{policritiAverageLinearTime2015,
  title = {Average Linear Time and Compressed Space Construction of the Burrows-Wheeler Transform},
  author = {Policriti, A. and Gigante, N. and Prezza, N.},
  year = {2015},
  doi = {10.1007/978-3-319-15579-1_46},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Language and Automata Theory and Applications}
}

@article{poteauxComplexityBoundsFor2011,
  title = {Complexity Bounds for the Rational {{Newton-Puiseux}} Algorithm over Finite Fields},
  author = {Poteaux, A. and Rybowicz, M.},
  year = {2011},
  doi = {10.1007/s00200-011-0144-6},
  abstract = {No abstract available},
  citationcount = {27},
  venue = {Applicable Algebra in Engineering, Communication and Computing}
}

@article{poutrMaintenanceOf21993,
  title = {Maintenance of 2- and 3-Edge- Connected Components of Graphs {{I}}},
  author = {Poutr{\'e}, H. L. and {van Leeuwen}, Jan and Overmars, M.},
  year = {1993},
  doi = {10.1016/0012-365X(93)90376-5},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Discrete Mathematics}
}

@article{poutrMaintenanceOf22000,
  title = {Maintenance of 2- and 3-Edge-Connected Components of Graphs {{II}}},
  author = {Poutr{\'e}, H. L.},
  year = {2000},
  doi = {10.1137/S0097539793257770},
  abstract = {Data structures and algorithms are presented to efficiently maintain the 2- and 3-edge-connected components of a general graph, under insertions of edges and nodes in the graph. At any moment, the data structure can answer whether two nodes are 2- or 3-edge-connected. The algorithms run in O(n+ .{$\alpha$}(m,n)) time, where m is the total number of queries and edge insertions. Furthermore, a linear-time algorithm is presented for maintaining the 2-edge-connected components in case the initial graph is connected. Finally, a new solution is presented for the 2-vertex-connected components of a graph.},
  citationcount = {27},
  venue = {SIAM journal on computing (Print)},
  keywords = {data structure,query}
}

@article{poutrMaintenanceOfTriconnected1992,
  title = {Maintenance of Triconnected Components of Graphs (Extended Abstract)},
  author = {Poutr{\'e}, H. L.},
  year = {1992},
  doi = {10.1007/3-540-55719-9_87},
  abstract = {No abstract available},
  citationcount = {31},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{prabhakarStrongerTradeoffsFor2018,
  title = {Stronger Tradeoffs for Orthogonal Range Querying in the Semigroup Model},
  author = {Prabhakar, Swaroop N. and Sharma, Vikram},
  year = {2018},
  doi = {10.4230/LIPIcs.FSTTCS.2018.45},
  abstract = {In this paper, we focus on lower bounds for data structures supporting orthogonal range querying on m points in n-dimensions in the semigroup model. Such a data structure usually maintains a family of "canonical subsets" of the given set of points and on a range query, it outputs a disjoint union of the appropriate subsets. Fredman showed that in order to prove lower bounds in the semigroup model, it suffices to prove a lower bound on a certain combinatorial tradeoff between two parameters: (a) the total sizes of the canonical subsets, and (b) the total number of canonical subsets required to cover all query ranges. In particular, he showed that the arithmetic mean of these two parameters is Omega(m log{\textasciicircum}n m). We strengthen this tradeoff by showing that the geometric mean of the same two parameters is Omega(m log{\textasciicircum}n m). Our second result is an alternate proof of Fredman's tradeoff in the one dimensional setting. The problem of answering range queries using canonical subsets can be formulated as factoring a specific boolean matrix as a product of two boolean matrices, one representing the canonical sets and the other capturing the appropriate disjoint unions of the former to output all possible range queries. In this formulation, we can ask what is an optimal data structure, i.e., a data structure that minimizes the sum of the two parameters mentioned above, and how does the balanced binary search tree compare with this optimal data structure in the two parameters? The problem of finding an optimal data structure is a non-linear optimization problem. In one dimension, Fredman's result implies that the minimum value of the objective function is Omega(m log m), which means that at least one of the parameters has to be Omega(m log m). We show that both the parameters in an optimal solution have to be Omega(m log m). This implies that balanced binary search trees are near optimal data structures for range querying in one dimension. We derive intermediate results on factoring matrices, not necessarily boolean, while trying to minimize the norms of the factors, that may be of independent interest.},
  citationcount = {1},
  venue = {Foundations of Software Technology and Theoretical Computer Science},
  keywords = {data structure,lower bound,query}
}

@article{praveshkothariSumofsquaresMeetsNash2018,
  title = {Sum-of-Squares Meets {{Nash}}: Lower Bounds for Finding Any Equilibrium},
  author = {Pravesh Kothari and R. Mehta},
  year = {2018},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/3188745.3188892},
  abstract = {Computing Nash equilibrium (NE) in two-player game is a central question in algorithmic game theory. The main motivation of this work is to understand the power of sum-of-squares method in computing equilibria, both exact and approximate. Previous works in this context have focused on hardness of approximating ``best'' equilibria with respect to some natural quality measure on equilibria such as social welfare. Such results, however, do not directly relate to the complexity of the problem of finding any equilibrium. In this work, we propose a framework of roundings for the sum-of-squares algorithm (and convex relaxations in general) applicable to finding approximate/exact equilbria in two player bimatrix games. Specifically, we define the notion of oblivious roundings with verification oracle (OV). These are algorithms that can access a solution to the degree d SoS relaxation to construct a list of candidate (partial) solutions and invoke a verification oracle to check if a candidate in the list gives an (exact or approximate) equilibrium. This framework captures most known approximation algorithms in combinatorial optimization including the celebrated semi-definite programming based algorithms for Max-Cut, Constraint-Satisfaction Problems, and the recent works on SoS relaxations for Unique Games/Small-Set Expansion, Best Separable State, and many problems in unsupervised machine learning. Our main results are strong unconditional lower bounds in this framework. Specifically, we show that for {\cyrchar\cyrie} = {$\Theta$}(1/poly(n)), there's no algorithm that uses a o(n)-degree SoS relaxation to construct a 2o(n)-size list of candidates and obtain an {\cyrchar\cyrie}-approximate NE. For some constant {\cyrchar\cyrie}, we show a similar result for degree o(log(n)) SoS relaxation and list size no(log(n)). Our results can be seen as an unconditional confirmation, in our restricted algorithmic framework, of the recent Exponential Time Hypothesis for PPAD. Our proof strategy involves constructing a family of games that all share a common sum-of-squares solution but every (approximate) equilibrium of any game is far from every equilibrium of any other game in the family (in either player's strategy). Along the way, we strengthen the classical unconditional lower bound against enumerative algorithms for finding approximate equilibria due to Daskalakis-Papadimitriou and the classical hardness of computing equilibria due to Gilbow-Zemel.},
  keywords = {lower bound},
  annotation = {Citation Count: 15}
}

@article{preparataFullyDynamicPoint1989,
  title = {Fully Dynamic Point Location in a Monotone Subdivision},
  author = {Preparata, F. and Tamassia, R.},
  year = {1989},
  doi = {10.1137/0218056},
  abstract = {In this paper a dynamic technique for locating a point in a monotone planar subdivision, whose current number of vertices is n, is presented. The (complete set of) update operations are insertion o...},
  citationcount = {68},
  venue = {SIAM journal on computing (Print)},
  keywords = {dynamic,update}
}

@article{prezzaOptimalRankAnd2018,
  title = {Optimal Rank and Select Queries on Dictionary-Compressed Text},
  author = {Prezza, N.},
  year = {2018},
  doi = {10.4230/LIPIcs.CPM.2019.4},
  abstract = {We study the problem of supporting queries on a string S of length n within a space bounded by the size {$\gamma$} of a string attractor for S. Recent works showed that random access on S can be supported in optimal O((n/{$\gamma$})/n) time within O({$\gamma$}\{polylog\}n) space. In this paper, we extend this result to \{rank\} and \{select\} queries and provide lower bounds matching our upper bounds on alphabets of polylogarithmic size. Our solutions are given in the form of a space-time trade-off that is more general than the one previously known for grammars and that improves existing bounds on LZ77-compressed text by a n time-factor in \{select\} queries. We also provide matching lower and upper bounds for \{partial sum\} and \{predecessor\} queries within attractor-bounded space, and extend our lower bounds to encompass navigation of dictionary-compressed tree representations.},
  citationcount = {19},
  venue = {Annual Symposium on Combinatorial Pattern Matching},
  keywords = {lower bound,query}
}

@article{prezzaOptimalSubstringEquality2018,
  title = {Optimal Substring Equality Queries with Applications to Sparse Text Indexing},
  author = {Prezza, N.},
  year = {2018},
  doi = {10.1145/3426870},
  abstract = {We consider the problem of encoding a string of length n from an integer alphabet of size {$\sigma$} so access, substring equality, and Longest Common Extension (LCE) queries can be answered efficiently. We describe a new space-optimal data structure supporting logarithmic-time queries. Access and substring equality query times can furthermore be improved to the optimal O(1) if O(log n) additional precomputed words are allowed in the total space. Additionally, we provide in-place algorithms for converting between the string and our data structure. Using this new string representation, we obtain the first in-place subquadratic algorithms for several string-processing problems in the restore model: The input string is rewritable and must be restored before the computation terminates. In particular, we describe the first in-place subquadratic Monte Carlo solutions to the sparse suffix sorting, sparse LCP array construction, and suffix selection problems. With the sole exception of suffix selection, our algorithms are also the first running in sublinear time for small enough sets of input suffixes. Combining these solutions, we obtain the first sublinear-time Monte Carlo algorithm for building the sparse suffix tree in compact space. We also show how to build a correct version of our data structure using small working space. This leads to the first Las Vegas in-place algorithm computing the full LCP array in O(nlog n) time w.h.p. and to the first Las Vegas in-place algorithms solving the sparse suffix sorting and sparse LCP array construction problems in O(n1.5 {\textsurd} log {$\sigma$}) time w.h.p.},
  citationcount = {5},
  venue = {ACM Trans. Algorithms},
  keywords = {data structure,query,query time}
}

@article{probstOnTheComplexity2018,
  title = {On the Complexity of the (Approximate) Nearest Colored Node Problem},
  author = {Probst, Maximilian},
  year = {2018},
  doi = {10.4230/LIPIcs.ESA.2018.68},
  abstract = {Given a graph G=(V,E) where each vertex is assigned a color from the set C=\{c\_1, c\_2, .., c\_sigma\}. In the (approximate) nearest colored node problem, we want to query, given v in V and c in C, for the (approximate) distance dist{\textasciicircum}(v, c) from v to the nearest node of color c. For any integer 1 {\textexclamdown}= k {\textexclamdown}= log n, we present a Color Distance Oracle (also often referred to as Vertex-label Distance Oracle) of stretch 4k-5 using space O(kn sigma{\textasciicircum}\{1/k\}) and query time O(log\{k\}). This improves the query time from O(k) to O(log\{k\}) over the best known Color Distance Oracle by Chechik [Chechik, 2012]. We then prove a lower bound in the cell probe model showing that even for unweighted undirected paths any static data structure that uses space S requires at least Omega (log (log\{sigma\} / log(S/n)+log log\{n\})) query time to give a distance estimate of stretch O(polylog(n)). This implies for the important case when sigma = Theta(n{\textasciicircum}\{epsilon\}) for some constant 0 {\textexclamdown} epsilon {\textexclamdown} 1, that our Color Distance Oracle has asymptotically optimal query time in regard to k, and that recent Color Distance Oracles for trees [Tsur, 2018] and planar graphs [Mozes and Skop, 2018] achieve asymptotically optimal query time in regard to n. We also investigate the setting where the data structure additionally has to support color-reassignments. We present the first Color Distance Oracle that achieves query times matching our lower bound from the static setting for large stretch yielding an exponential improvement over the best known query time [Chechik, 2014]. Finally, we give new conditional lower bounds proving the hardness of answering queries if edge insertions and deletion are allowed that strictly improve over recent bounds in time and generality.},
  citationcount = {4},
  venue = {Embedded Systems and Applications},
  keywords = {cell probe,data structure,lower bound,query,query time,static}
}

@article{puatracscuLowerBoundSuccinct2009,
  title = {A {{Lower Bound}} for {{Succinct Rank Queries}}},
  author = {Puatracscu, Mihai},
  year = {2009},
  journal = {CoRR},
  volume = {abs/0907.1103},
  eprint = {0907.1103},
  doi = {10.1145/3357713.3384260},
  url = {http://arxiv.org/abs/0907.1103},
  urldate = {2024-08-29},
  archiveprefix = {arXiv},
  keywords = {cell probe,data structure,lower bound,query,query time},
  annotation = {Andr{\'e}s Abeliuk Rodrigo C{\'a}novas and Gonzalo Navarro. 2013. Practical Compressed Sufix Trees. Algorithms 6 2 ( 2013 ) 319-351.  Andr{\'e}s Abeliuk Rodrigo C{\'a}novas and Gonzalo Navarro. 2013. Practical Compressed Sufix Trees. Algorithms 6 2 ( 2013 ) 319-351.\\
\\
\\
\\
\\
M Eug{\'e}ne Catalan. 1887. Sur les nombres de Segner. Rendiconti del Circolo Matematico di Palermo (1884-1940) 1 1 ( 1887 ) 190-201.  M Eug{\'e}ne Catalan. 1887. Sur les nombres de Segner. Rendiconti del Circolo Matematico di Palermo (1884-1940) 1 1 ( 1887 ) 190-201.\\
\\
Gang Chen Simon J. Puglisi and W. F. Smyth. 2008. Lempel-Ziv Factorization Using Less Time \& Space. Mathematics in Computer Science 1 4 ( 01 Jun 2008 ) 605-623.  Gang Chen Simon J. Puglisi and W. F. Smyth. 2008. Lempel-Ziv Factorization Using Less Time \& Space. Mathematics in Computer Science 1 4 ( 01 Jun 2008 ) 605-623.\\
Kuan-Yu Chen and Kun-Mao Chao . 2007. On the range maximum-sum segment query problem. Discrete Applied Mathematics 155, 16 ( 2007 ) , 2043 -2052. Kuan-Yu Chen and Kun-Mao Chao. 2007. On the range maximum-sum segment query problem. Discrete Applied Mathematics 155, 16 ( 2007 ), 2043-2052.\\
Rapha{\"e}l Cliford , Allan Gr{\o}nlund , and Kasper Green Larsen . 2015 . New Unconditional Hardness Results for Dynamic and Online Problems. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015 , Berkeley, CA, USA , 17-20 October, 2015. 1089 - 1107 . Rapha{\"e}l Cliford, Allan Gr{\o}nlund, and Kasper Green Larsen. 2015. New Unconditional Hardness Results for Dynamic and Online Problems. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015. 1089-1107.\\
Maxime Crochemore Costas S. Iliopoulos Marcin Kubica M. Sohel Rahman German Tischler and Tomasz Walen. 2012. Improved algorithms for the range next value problem and applications. Theor. Comput. Sci. 434 ( 2012 ) 23-34.  Maxime Crochemore Costas S. Iliopoulos Marcin Kubica M. Sohel Rahman German Tischler and Tomasz Walen. 2012. Improved algorithms for the range next value problem and applications. Theor. Comput. Sci. 434 ( 2012 ) 23-34.\\
Pooya Davoodi Rajeev Raman and Srinivasa Rao Satti. 2017. On Succinct Representations of Binary Trees. Mathematics in Computer Science 11 2 ( 2017 ) 177-189.  Pooya Davoodi Rajeev Raman and Srinivasa Rao Satti. 2017. On Succinct Representations of Binary Trees. Mathematics in Computer Science 11 2 ( 2017 ) 177-189.\\
Johannes Fischer and Volker Heun . 2007 . A New Succinct Representation of RMQInformation and Improvements in the Enhanced Sufix Array. In Combinatorics, Algorithms, Probabilistic and Experimental Methodologies , First International Symposium, ESCAPE 2007 , Hangzhou, China , April 7-9, 2007, Revised Selected Papers. 459 - 470 . Johannes Fischer and Volker Heun. 2007. A New Succinct Representation of RMQInformation and Improvements in the Enhanced Sufix Array. In Combinatorics, Algorithms, Probabilistic and Experimental Methodologies, First International Symposium, ESCAPE 2007, Hangzhou, China, April 7-9, 2007, Revised Selected Papers. 459-470.\\
\\
Johannes Fischer , Volker Heun , and Stefan Kramer . 2006. Optimal String Mining Under Frequency Constraints . In Knowledge Discovery in Databases: PKDD 2006 , 10th European Conference on Principles and Practice of Knowledge Discovery in Databases, Berlin, Germany, September 18-22, 2006, Proceedings . 139-150. Johannes Fischer, Volker Heun, and Stefan Kramer. 2006. Optimal String Mining Under Frequency Constraints. In Knowledge Discovery in Databases: PKDD 2006, 10th European Conference on Principles and Practice of Knowledge Discovery in Databases, Berlin, Germany, September 18-22, 2006, Proceedings. 139-150.\\
Johannes Fischer Veli M{\"a}kinen and Gonzalo Navarro. 2009. Faster entropybounded compressed sufix trees. Theor. Comput. Sci. 410 51 ( 2009 ) 5354-5364.  Johannes Fischer Veli M{\"a}kinen and Gonzalo Navarro. 2009. Faster entropybounded compressed sufix trees. Theor. Comput. Sci. 410 51 ( 2009 ) 5354-5364.\\
\\
Anna G{\'a}l and Peter Bro Miltersen. 2007. The cell probe complexity of succinct data structures. Theor. Comput. Sci. 379 3 ( 2007 ) 405-417.  Anna G{\'a}l and Peter Bro Miltersen. 2007. The cell probe complexity of succinct data structures. Theor. Comput. Sci. 379 3 ( 2007 ) 405-417.\\
Pawel Gawrychowski , Seungbum Jo , Shay Mozes , and Oren Weimann . 2019. Compressed Range Minimum Queries. CoRR abs/ 1902 .04427 ( 2019 ). Pawel Gawrychowski, Seungbum Jo, Shay Mozes, and Oren Weimann. 2019. Compressed Range Minimum Queries. CoRR abs/ 1902.04427 ( 2019 ).\\
Loukas Georgiadis and Robert Endre Tarjan . 2004 . Finding dominators revisited: extended abstract . In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2004 , New Orleans, Louisiana, USA , January 11-14, 2004. 869 - 878 . Loukas Georgiadis and Robert Endre Tarjan. 2004. Finding dominators revisited: extended abstract. In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2004, New Orleans, Louisiana, USA, January 11-14, 2004. 869-878.\\
\\
Allan Gr{\o}nlund and Kasper Green Larsen . 2016 . Towards Tight Lower Bounds for Range Reporting on the RAM. In 43rd International Colloquium on Automata, Languages, and Programming, ICALP 2016 , July 11-15, 2016, Rome, Italy. 92 : 1-92 : 12. Allan Gr{\o}nlund and Kasper Green Larsen. 2016. Towards Tight Lower Bounds for Range Reporting on the RAM. In 43rd International Colloquium on Automata, Languages, and Programming, ICALP 2016, July 11-15, 2016, Rome, Italy. 92 : 1-92 : 12.\\
\\
Wing-Kai Hon , Rahul Shah , and Jefrey Scott Vitter . 2009 . Space-Eficient Framework for Top-k String Retrieval Problems. In 50th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2009 , October 25-27, 2009, Atlanta, Georgia, USA. 713-722. Wing-Kai Hon, Rahul Shah, and Jefrey Scott Vitter. 2009. Space-Eficient Framework for Top-k String Retrieval Problems. In 50th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2009, October 25-27, 2009, Atlanta, Georgia, USA. 713-722.\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
Mihai Pa{\textasciicaron}tra{\c s}cu . 2008 . Succincter. In 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008 , October 25-28, 2008, Philadelphia, PA, USA. 305-313. Mihai Pa{\textasciicaron}tra{\c s}cu. 2008. Succincter. In 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008, October 25-28, 2008, Philadelphia, PA, USA. 305-313.\\
\\
\\
Mihai Pa{\textasciicaron}tra{\c s}cu and Emanuele Viola . 2010 . Cell-Probe Lower Bounds for Succinct Partial Sums . In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2010 , Austin, Texas, USA , January 17-19, 2010. 117 - 122 . Mihai Pa{\textasciicaron}tra{\c s}cu and Emanuele Viola. 2010. Cell-Probe Lower Bounds for Succinct Partial Sums. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2010, Austin, Texas, USA, January 17-19, 2010. 117-122.\\
Vijaya Ramachandran and Uzi Vishkin . 1988 . Eficient Parallel Triconnectivity in Logarithmic Time. In VLSI Algorithms and Architectures , 3rd Aegean Workshop on Computing, AWOC 88, Corfu, Greece, June 28-July 1, 1988, Proceedings. 33-42 . Vijaya Ramachandran and Uzi Vishkin. 1988. Eficient Parallel Triconnectivity in Logarithmic Time. In VLSI Algorithms and Architectures, 3rd Aegean Workshop on Computing, AWOC 88, Corfu, Greece, June 28-July 1, 1988, Proceedings. 33-42.\\
Alon Regev. 2012. A proof of Catalan's Convolution formula. Integers 12 5 ( 2012 ) 929-934.  Alon Regev. 2012. A proof of Catalan's Convolution formula. Integers 12 5 ( 2012 ) 929-934.\\
Kunihiko Sadakane. 2007. Compressed Sufix Trees with Full Functionality. Theory Comput. Syst. 41 4 ( 2007 ) 589-607.  Kunihiko Sadakane. 2007. Compressed Sufix Trees with Full Functionality. Theory Comput. Syst. 41 4 ( 2007 ) 589-607.\\
\\
\\
Tetsuo Shibuya and Igor Kurochkin . 2003 . Match Chaining Algorithms for cDNA Mapping. In Algorithms in Bioinformatics , Third International Workshop, WABI 2003, Budapest, Hungary, September 15-20, 2003, Proceedings. 462-475 . Tetsuo Shibuya and Igor Kurochkin. 2003. Match Chaining Algorithms for cDNA Mapping. In Algorithms in Bioinformatics, Third International Workshop, WABI 2003, Budapest, Hungary, September 15-20, 2003, Proceedings. 462-475.\\
Niko V{\"a}lim{\"a}ki and Veli M{\"a}kinen . 2007 . Space-Eficient Algorithms for Document Retrieval. In Combinatorial Pattern Matching , 18th Annual Symposium, CPM 2007, London, Canada, July 9-11, 2007, Proceedings. 205-215 . Niko V{\"a}lim{\"a}ki and Veli M{\"a}kinen. 2007. Space-Eficient Algorithms for Document Retrieval. In Combinatorial Pattern Matching, 18th Annual Symposium, CPM 2007, London, Canada, July 9-11, 2007, Proceedings. 205-215.\\
Andrew Chi-Chih Yao . 1981 . Should tables be sorted ? Journal of the ACM (JACM) 28 , 3 ( 1981 ), 615-628. Andrew Chi-Chih Yao. 1981. Should tables be sorted ? Journal of the ACM (JACM) 28, 3 ( 1981 ), 615-628.\\
Yitong Yin . 2016 . Simple Average-Case Lower Bounds for Approximate NearNeighbor from Isoperimetric Inequalities. In 43rd International Colloquium on Automata, Languages, and Programming, ICALP 2016 , July 11-15, 2016, Rome, Italy. 84 : 1-84 : 13. Yitong Yin. 2016. Simple Average-Case Lower Bounds for Approximate NearNeighbor from Isoperimetric Inequalities. In 43rd International Colloquium on Automata, Languages, and Programming, ICALP 2016, July 11-15, 2016, Rome, Italy. 84 : 1-84 : 13.\\
Andr{\'e}s Abeliuk Rodrigo C{\'a}novas and Gonzalo Navarro. 2013. Practical Compressed Sufix Trees. Algorithms 6 2 ( 2013 ) 319-351.  Andr{\'e}s Abeliuk Rodrigo C{\'a}novas and Gonzalo Navarro. 2013. Practical Compressed Sufix Trees. Algorithms 6 2 ( 2013 ) 319-351.\\
\\
\\
\\
\\
M Eug{\'e}ne Catalan. 1887. Sur les nombres de Segner. Rendiconti del Circolo Matematico di Palermo (1884-1940) 1 1 ( 1887 ) 190-201.  M Eug{\'e}ne Catalan. 1887. Sur les nombres de Segner. Rendiconti del Circolo Matematico di Palermo (1884-1940) 1 1 ( 1887 ) 190-201.\\
\\
Gang Chen Simon J. Puglisi and W. F. Smyth. 2008. Lempel-Ziv Factorization Using Less Time \& Space. Mathematics in Computer Science 1 4 ( 01 Jun 2008 ) 605-623.  Gang Chen Simon J. Puglisi and W. F. Smyth. 2008. Lempel-Ziv Factorization Using Less Time \& Space. Mathematics in Computer Science 1 4 ( 01 Jun 2008 ) 605-623.\\
Kuan-Yu Chen and Kun-Mao Chao . 2007. On the range maximum-sum segment query problem. Discrete Applied Mathematics 155, 16 ( 2007 ) , 2043 -2052. Kuan-Yu Chen and Kun-Mao Chao. 2007. On the range maximum-sum segment query problem. Discrete Applied Mathematics 155, 16 ( 2007 ), 2043-2052.\\
Rapha{\"e}l Cliford , Allan Gr{\o}nlund , and Kasper Green Larsen . 2015 . New Unconditional Hardness Results for Dynamic and Online Problems. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015 , Berkeley, CA, USA , 17-20 October, 2015. 1089 - 1107 . Rapha{\"e}l Cliford, Allan Gr{\o}nlund, and Kasper Green Larsen. 2015. New Unconditional Hardness Results for Dynamic and Online Problems. In IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015. 1089-1107.\\
Maxime Crochemore Costas S. Iliopoulos Marcin Kubica M. Sohel Rahman German Tischler and Tomasz Walen. 2012. Improved algorithms for the range next value problem and applications. Theor. Comput. Sci. 434 ( 2012 ) 23-34.  Maxime Crochemore Costas S. Iliopoulos Marcin Kubica M. Sohel Rahman German Tischler and Tomasz Walen. 2012. Improved algorithms for the range next value problem and applications. Theor. Comput. Sci. 434 ( 2012 ) 23-34.\\
Pooya Davoodi Rajeev Raman and Srinivasa Rao Satti. 2017. On Succinct Representations of Binary Trees. Mathematics in Computer Science 11 2 ( 2017 ) 177-189.  Pooya Davoodi Rajeev Raman and Srinivasa Rao Satti. 2017. On Succinct Representations of Binary Trees. Mathematics in Computer Science 11 2 ( 2017 ) 177-189.\\
Johannes Fischer and Volker Heun . 2007 . A New Succinct Representation of RMQInformation and Improvements in the Enhanced Sufix Array. In Combinatorics, Algorithms, Probabilistic and Experimental Methodologies , First International Symposium, ESCAPE 2007 , Hangzhou, China , April 7-9, 2007, Revised Selected Papers. 459 - 470 . Johannes Fischer and Volker Heun. 2007. A New Succinct Representation of RMQInformation and Improvements in the Enhanced Sufix Array. In Combinatorics, Algorithms, Probabilistic and Experimental Methodologies, First International Symposium, ESCAPE 2007, Hangzhou, China, April 7-9, 2007, Revised Selected Papers. 459-470.\\
\\
Johannes Fischer , Volker Heun , and Stefan Kramer . 2006. Optimal String Mining Under Frequency Constraints . In Knowledge Discovery in Databases: PKDD 2006 , 10th European Conference on Principles and Practice of Knowledge Discovery in Databases, Berlin, Germany, September 18-22, 2006, Proceedings . 139-150. Johannes Fischer, Volker Heun, and Stefan Kramer. 2006. Optimal String Mining Under Frequency Constraints. In Knowledge Discovery in Databases: PKDD 2006, 10th European Conference on Principles and Practice of Knowledge Discovery in Databases, Berlin, Germany, September 18-22, 2006, Proceedings. 139-150.\\
Johannes Fischer Veli M{\"a}kinen and Gonzalo Navarro. 2009. Faster entropybounded compressed sufix trees. Theor. Comput. Sci. 410 51 ( 2009 ) 5354-5364.  Johannes Fischer Veli M{\"a}kinen and Gonzalo Navarro. 2009. Faster entropybounded compressed sufix trees. Theor. Comput. Sci. 410 51 ( 2009 ) 5354-5364.\\
\\
Anna G{\'a}l and Peter Bro Miltersen. 2007. The cell probe complexity of succinct data structures. Theor. Comput. Sci. 379 3 ( 2007 ) 405-417.  Anna G{\'a}l and Peter Bro Miltersen. 2007. The cell probe complexity of succinct data structures. Theor. Comput. Sci. 379 3 ( 2007 ) 405-417.\\
Pawel Gawrychowski , Seungbum Jo , Shay Mozes , and Oren Weimann . 2019. Compressed Range Minimum Queries. CoRR abs/ 1902 .04427 ( 2019 ). Pawel Gawrychowski, Seungbum Jo, Shay Mozes, and Oren Weimann. 2019. Compressed Range Minimum Queries. CoRR abs/ 1902.04427 ( 2019 ).\\
Loukas Georgiadis and Robert Endre Tarjan . 2004 . Finding dominators revisited: extended abstract . In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2004 , New Orleans, Louisiana, USA , January 11-14, 2004. 869 - 878 . Loukas Georgiadis and Robert Endre Tarjan. 2004. Finding dominators revisited: extended abstract. In Proceedings of the Fifteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2004, New Orleans, Louisiana, USA, January 11-14, 2004. 869-878.\\
\\
Allan Gr{\o}nlund and Kasper Green Larsen . 2016 . Towards Tight Lower Bounds for Range Reporting on the RAM. In 43rd International Colloquium on Automata, Languages, and Programming, ICALP 2016 , July 11-15, 2016, Rome, Italy. 92 : 1-92 : 12. Allan Gr{\o}nlund and Kasper Green Larsen. 2016. Towards Tight Lower Bounds for Range Reporting on the RAM. In 43rd International Colloquium on Automata, Languages, and Programming, ICALP 2016, July 11-15, 2016, Rome, Italy. 92 : 1-92 : 12.\\
\\
Wing-Kai Hon , Rahul Shah , and Jefrey Scott Vitter . 2009 . Space-Eficient Framework for Top-k String Retrieval Problems. In 50th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2009 , October 25-27, 2009, Atlanta, Georgia, USA. 713-722. Wing-Kai Hon, Rahul Shah, and Jefrey Scott Vitter. 2009. Space-Eficient Framework for Top-k String Retrieval Problems. In 50th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2009, October 25-27, 2009, Atlanta, Georgia, USA. 713-722.\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
\\
Mihai Pa{\textasciicaron}tra{\c s}cu . 2008 . Succincter. In 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008 , October 25-28, 2008, Philadelphia, PA, USA. 305-313. Mihai Pa{\textasciicaron}tra{\c s}cu. 2008. Succincter. In 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2008, October 25-28, 2008, Philadelphia, PA, USA. 305-313.\\
\\
\\
Mihai Pa{\textasciicaron}tra{\c s}cu and Emanuele Viola . 2010 . Cell-Probe Lower Bounds for Succinct Partial Sums . In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2010 , Austin, Texas, USA , January 17-19, 2010. 117 - 122 . Mihai Pa{\textasciicaron}tra{\c s}cu and Emanuele Viola. 2010. Cell-Probe Lower Bounds for Succinct Partial Sums. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2010, Austin, Texas, USA, January 17-19, 2010. 117-122.\\
Vijaya Ramachandran and Uzi Vishkin . 1988 . Eficient Parallel Triconnectivity in Logarithmic Time. In VLSI Algorithms and Architectures , 3rd Aegean Workshop on Computing, AWOC 88, Corfu, Greece, June 28-July 1, 1988, Proceedings. 33-42 . Vijaya Ramachandran and Uzi Vishkin. 1988. Eficient Parallel Triconnectivity in Logarithmic Time. In VLSI Algorithms and Architectures, 3rd Aegean Workshop on Computing, AWOC 88, Corfu, Greece, June 28-July 1, 1988, Proceedings. 33-42.\\
Alon Regev. 2012. A proof of Catalan's Convolution formula. Integers 12 5 ( 2012 ) 929-934.  Alon Regev. 2012. A proof of Catalan's Convolution formula. Integers 12 5 ( 2012 ) 929-934.\\
Kunihiko Sadakane. 2007. Compressed Sufix Trees with Full Functionality. Theory Comput. Syst. 41 4 ( 2007 ) 589-607.  Kunihiko Sadakane. 2007. Compressed Sufix Trees with Full Functionality. Theory Comput. Syst. 41 4 ( 2007 ) 589-607.\\
\\
\\
Tetsuo Shibuya and Igor Kurochkin . 2003 . Match Chaining Algorithms for cDNA Mapping. In Algorithms in Bioinformatics , Third International Workshop, WABI 2003, Budapest, Hungary, September 15-20, 2003, Proceedings. 462-475 . Tetsuo Shibuya and Igor Kurochkin. 2003. Match Chaining Algorithms for cDNA Mapping. In Algorithms in Bioinformatics, Third International Workshop, WABI 2003, Budapest, Hungary, September 15-20, 2003, Proceedings. 462-475.\\
Niko V{\"a}lim{\"a}ki and Veli M{\"a}kinen . 2007 . Space-Eficient Algorithms for Document Retrieval. In Combinatorial Pattern Matching , 18th Annual Symposium, CPM 2007, London, Canada, July 9-11, 2007, Proceedings. 205-215 . Niko V{\"a}lim{\"a}ki and Veli M{\"a}kinen. 2007. Space-Eficient Algorithms for Document Retrieval. In Combinatorial Pattern Matching, 18th Annual Symposium, CPM 2007, London, Canada, July 9-11, 2007, Proceedings. 205-215.\\
Andrew Chi-Chih Yao . 1981 . Should tables be sorted ? Journal of the ACM (JACM) 28 , 3 ( 1981 ), 615-628. Andrew Chi-Chih Yao. 1981. Should tables be sorted ? Journal of the ACM (JACM) 28, 3 ( 1981 ), 615-628.\\
Yitong Yin . 2016 . Simple Average-Case Lower Bounds for Approximate NearNeighbor from Isoperimetric Inequalities. In 43rd International Colloquium on Automata, Languages, and Programming, ICALP 2016 , July 11-15, 2016, Rome, Italy. 84 : 1-84 : 13. Yitong Yin. 2016. Simple Average-Case Lower Bounds for Approximate NearNeighbor from Isoperimetric Inequalities. In 43rd International Colloquium on Automata, Languages, and Programming, ICALP 2016, July 11-15, 2016, Rome, Italy. 84 : 1-84 : 13.},
  file = {/Users/tulasi/Zotero/storage/ER56PBBY/Puatracscu - 2009 - A Lower Bound for Succinct Rank Queries.pdf}
}

@article{pudlkANoteOn2000,
  title = {A Note on the Use of Determinant for Proving Lower Bounds on the Size of Linear Circuits},
  author = {Pudl{\'a}k, P.},
  year = {2000},
  doi = {10.1016/S0020-0190(00)00058-2},
  abstract = {No abstract available},
  citationcount = {24},
  venue = {Information Processing Letters}
}

@article{pudlkBooleanCircuitsTensor1997,
  title = {Boolean Circuits, Tensor Ranks, and Communication Complexity},
  author = {Pudl{\'a}k, P. and R{\"o}dl, V. and Sgall, J.},
  year = {1997},
  doi = {10.1137/S0097539794264809},
  abstract = {We investigate two methods for proving lower bounds on the size of small-depth circuits, namely the approaches based on multiparty communication games and algebraic characterizations extending the concepts of the tensor rank and rigidity of matrices. Our methods are combinatorial, but we think that our main contribution concerns the algebraic concepts used in this area (tensor ranks and rigidity). Our main results are following. (i) An o(n)-bit protocol for a communication game for computing shifts, which also gives an upper bound of o(n{$^2$}) on the contact rank of the tensor of multiplication of polynomials; this disproves some earlier conjectures. A related probabilistic construction gives an o(n) upper bound for computing all permutations and an O(nn) upper bound on the communication complexity of pointer jumping with permutations. (ii) A lower bound on certain restricted circuits of depth 2 which are related to the problem of proving a superlinear lower bound on the size of logarithmic-depth circuits; this bound has interpretations both as a lower bound on the rigidity of the tensor of multiplication of polynomials and as a lower bound on the communication needed to compute the shift function in a restricted model. (iii) An upper bound on Boolean circuits of depth 2 for computing shifts and, more generally, all permutations; this shows that such circuits are more efficient than the model based on sending bits along vertex-disjoint paths.},
  citationcount = {72},
  venue = {SIAM journal on computing (Print)}
}

@article{pudlkCommunicationInBounded1994,
  title = {Communication in Bounded Depth Circuits},
  author = {Pudl{\'a}k, P.},
  year = {1994},
  doi = {10.1007/BF01215351},
  abstract = {No abstract available},
  citationcount = {77},
  venue = {Comb.}
}

@article{pudlkOnShiftingNetworks1993,
  title = {On Shifting Networks},
  author = {Pudl{\'a}k, P. and Savick{\'y}, P.},
  year = {1993},
  doi = {10.1016/0304-3975(93)90332-N},
  abstract = {No abstract available},
  citationcount = {16},
  venue = {Theoretical Computer Science}
}

@article{pudlkSomeCombinatorialAlgebraic1994,
  title = {Some Combinatorial-Algebraic Problems from Complexity Theory},
  author = {Pudl{\'a}k, P. and R{\"o}dl, V.},
  year = {1994},
  doi = {10.1016/0012-365X(94)00115-Y},
  abstract = {No abstract available},
  citationcount = {41},
  venue = {Discrete Mathematics}
}

@article{pVatrascuDynamicBitprobeComplexity2007,
  title = {On Dynamic Bit-Probe Complexity},
  author = {P{\textasciicaron}atra{\c}scu, Mihai and Tarnit{\c}{\textasciicaron}a, Corina},
  year = {2007},
  journal = {Theoretical Computer Science},
  volume = {380},
  pages = {127--142},
  doi = {10.1016/j.tcs.2007.02.058},
  keywords = {Bit-probe complexity,cell probe,dynamic,Lower bounds},
  annotation = {Stephen Alstrup, Thore Husfeldt, Theis Rauhe, Marked ancestor problems, in: Proc. 39th IEEE Symposium on Foundations of Computer Science, FOCS, 1998, pp. 534--543\\
\\
\\
\\
Michael L. Fredman, Michael E. Saks, The cell probe complexity of dynamic data structures, in: Proc. 21st ACM Symposium on Theory of Computing, STOC, 1989, pp. 345--354\\
\\
Peter Bro Miltersen, Cell probe complexity --- a survey, in: 19th Conference on the Foundations of Software Technology and Theoretical Computer Science, FSTTCS, 1999. Advances in Data Structures Workshop\\
Christian Worm Mortensen, Rasmus Pagh, Mihai Pa{\textasciicaron}tra{\c s}cu, On dynamic range reporting in one dimension, in: Proc. 37th ACM Symposium on Theory of Computing, STOC, 2005, pp. 104--111\\
\\
\\
Mikkel Thorup, Near-optimal fully-dynamic graph connectivity, in: Proc. 32nd ACM Symposium on Theory of Computing STOC, 2000, pp. 343--350},
  file = {/Users/tulasi/Zotero/storage/WF3CHCE5/document.pdf}
}

@article{qihaowangMotifPathConnectivity2024,
  title = {From {{Motif}} to {{Path}}: {{Connectivity}} and {{Homophily}}},
  author = {Qihao Wang and Hongtai Cao and Xiaodong Li and {Kevin Chen--Chuan Chang} and Reynold Cheng},
  year = {2024},
  journal = {IEEE International Conference on Data Engineering},
  doi = {10.1109/ICDE60146.2024.00227},
  abstract = {While motif has been widely employed in graph analytics, a fundamental question remains open: How should overlapping motif edges connect into a path? Existing works address this question with simple but inconsistent generalizations from standard graphs. This paper studies this issue by proposing the concept of connectivity degree (CD), i.e. the number of overlapping nodes needed for motif edges to be adjacent, as the requirement for path connection. We further study three research questions. First, is CD significant? We study how CD impacts motif analytics, more specifically, three motif-based methods. Second, how to estimate the right CD? We develop a minimax estimator based on minimizing the worst-case risk. Finally, how to detect the connected components with connectivity degree, an important task by itself and necessary for our estimator. As the traditional BFS or DFS approaches are not valid anymore, we develop a disjoint set algorithm instead. Our experiments validate that our CD can improve the performance of motif analytics. Also, our estimator is effective and our connected component detection algorithm is efficient.},
  annotation = {Citation Count: 0}
}

@article{qinDensestPeriodicSubgraph2023,
  title = {Densest Periodic Subgraph Mining on Large Temporal Graphs},
  author = {Qin, Hongchao and Li, Ronghua and Yuan, Ye and Dai, Yongheng and Wang, Guoren},
  year = {2023},
  doi = {10.1109/TKDE.2022.3233788},
  abstract = {Densest subgraphs are often interpreted as {\textexclamdown}italic{\textquestiondown}communities{\textexclamdown}/italic{\textquestiondown}, based on a basic assumption that the connections inside a community are much denser than those between communities. In a graph with temporal information, a densest periodic subgraph is the most densely connected periodic behavior which needs to be captured. Unfortunately, the existing work do not model the densest periodic subgraph in temporal graphs, and the current algorithms for mining the densest subgraph cannot be applied to detect the densest periodic subgraph in the temporal networks. To tackle this problem, we propose a novel model, called the densest {\textexclamdown}inline-formula{\textquestiondown}{\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}{$\sigma$}{\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}alternatives{\textquestiondown}{\textexclamdown}mml:math{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}{$\sigma$}{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}/mml:math{\textquestiondown}{\textexclamdown}inline-graphic xlink:href="wang-ieq1-3233788.gif"/{\textquestiondown}{\textexclamdown}/alternatives{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}-periodic subgraph, which presents the densest periodic subgraph whose period size is {\textexclamdown}inline-formula{\textquestiondown}{\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}{$\sigma$}{\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}alternatives{\textquestiondown}{\textexclamdown}mml:math{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}{$\sigma$}{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}/mml:math{\textquestiondown}{\textexclamdown}inline-graphic xlink:href="wang-ieq2-3233788.gif"/{\textquestiondown}{\textexclamdown}/alternatives{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}. We prove that finding the densest {\textexclamdown}inline-formula{\textquestiondown}{\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}{$\sigma$}{\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}alternatives{\textquestiondown}{\textexclamdown}mml:math{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}{$\sigma$}{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}/mml:math{\textquestiondown}{\textexclamdown}inline-graphic xlink:href="wang-ieq3-3233788.gif"/{\textquestiondown}{\textexclamdown}/alternatives{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}-periodic subgraph can be solved in polynomial time, but it is still challenging because the naive algorithm needs to repeatedly invoke a maximum flow algorithm for many periodic subgraphs. To compute the densest {\textexclamdown}inline-formula{\textquestiondown}{\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}{$\sigma$}{\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}alternatives{\textquestiondown}{\textexclamdown}mml:math{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}{$\sigma$}{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}/mml:math{\textquestiondown}{\textexclamdown}inline-graphic xlink:href="wang-ieq4-3233788.gif"/{\textquestiondown}{\textexclamdown}/alternatives{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}-periodic subgraph efficiently, we first develop an effective pruning technique based on the degeneracy of the graph to significantly prune the number of the periodic subgraphs. Then, we present a more efficient algorithm that can reduce the computations for the degeneracy and maximum flow. Next, we develop a greedy algorithm that can compute the approximate densest {\textexclamdown}inline-formula{\textquestiondown}{\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}{$\sigma$}{\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}alternatives{\textquestiondown}{\textexclamdown}mml:math{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}{$\sigma$}{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}/mml:math{\textquestiondown}{\textexclamdown}inline-graphic xlink:href="wang-ieq5-3233788.gif"/{\textquestiondown}{\textexclamdown}/alternatives{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}-periodic subgraph and achieve an approximation ratio of 1/2. Finally, the results of extensive experiments on several real-life datasets demonstrate the efficiency, scalability, and effectiveness of our algorithms.},
  citationcount = {5},
  venue = {IEEE Transactions on Knowledge and Data Engineering}
}

@article{r.bixbyRecentAlgorithmsTwo1984,
  title = {Recent {{Algorithms}} for {{Two Versions}} of {{Graph Realization}} and {{Remarks}} on {{Applications}} to {{Linear Programming}}},
  author = {R. Bixby},
  year = {1984},
  doi = {10.1016/B978-0-12-566780-7.50009-X},
  annotation = {Citation Count: 8}
}

@article{r.fleischerSelectedTopicsComputational1992,
  title = {Selected {{Topics}} from {{Computational Geometry}}, {{Data Structures}} and {{Motion Planning}}},
  author = {R. Fleischer and O. Fries and K. Mehlhorn and S. Meiser and S. N{\"a}her and H. Rohnert and S. Schirra and K. Simon and A. Tsakalidis and Christian Uhrig},
  year = {1992},
  journal = {Data Structures and Efficient Algorithms},
  doi = {10.1007/3-540-55488-2_20},
  keywords = {data structure},
  annotation = {Citation Count: 0}
}

@article{r.nieuwenhuisProofProducingCongruenceClosure2005,
  title = {Proof-{{Producing Congruence Closure}}},
  author = {R. Nieuwenhuis and Albert Oliveras},
  year = {2005},
  journal = {International Conference on Rewriting Techniques and Applications},
  doi = {10.1007/978-3-540-32033-3_33},
  annotation = {Citation Count: 123}
}

@article{r.paigeViewingProgramTransformation1994,
  title = {Viewing {{A Program Transformation System At Work}}},
  author = {R. Paige},
  year = {1994},
  journal = {Symposium on Programming Language Implementation and Logic Programming},
  doi = {10.1007/3-540-58402-1_3},
  annotation = {Citation Count: 43}
}

@article{r.seidelTopDownAnalysisPath2005,
  title = {Top-{{Down Analysis}} of {{Path Compression}}},
  author = {R. Seidel and M. Sharir},
  year = {2005},
  journal = {SIAM journal on computing (Print)},
  doi = {10.1137/S0097539703439088},
  abstract = {We present a new analysis of the worst-case cost of path compression, which is an operation that is used in various well-known "union-find" algorithms. In contrast to previous analyses which are essentially based on bottom-up approaches, our method proceeds top-down, yielding recurrence relations from which the various bounds arise naturally. In particular the famous quasi-linear bound involving the inverse Ackermann function can be derived without having to introduce the Ackermann function itself.},
  annotation = {Citation Count: 42}
}

@article{r.tarjanProblemsDataStructures2005,
  title = {Problems in Data Structures and Algorithms},
  author = {R. Tarjan},
  year = {2005},
  doi = {10.1007/0-387-25036-0_2},
  keywords = {data structure},
  annotation = {Citation Count: 2}
}

@article{r.tarjanWorstcaseAnalysisSet1984,
  title = {Worst-Case {{Analysis}} of {{Set Union Algorithms}}},
  author = {R. Tarjan and J. Leeuwen},
  year = {1984},
  journal = {JACM},
  doi = {10.1145/62.2160},
  abstract = {This paper analyzes the asymptotic worst-case running time of a number of variants of the well-known method of path compression for maintaining a collection of disjoint sets under union. We show that two one-pass methods proposed by van Leeuwen and van der Weide are asymptotically optimal, whereas several other methods, including one proposed by Rein and advocated by Dijkstra, are slower than the best methods.},
  annotation = {Citation Count: 493}
}

@article{radhakrishnanBoundsForDispersers2000,
  title = {Bounds for Dispersers, Extractors, and Depth-Two Superconcentrators},
  author = {Radhakrishnan, J. and {Ta-Shma}, A.},
  year = {2000},
  doi = {10.1137/S0895480197329508},
  abstract = {We show that the size of the smallest depth-two N-superconcentrator is  Before this work, optimal bounds were known for all depths except two. For the upper bound, we build superconcentrators by putting together a small number of disperser graphs; these disperser graphs are obtained using a probabilistic argument. For obtaining lower bounds, we present two different methods. First, we show that superconcentrators contain several disjoint disperser graphs. When combined with the lower bound for disperser graphs of Kovari, Sos, and Turan, this gives an almost optimal lower bound of {\textohm}(N(N/N){$^2$}) on the size of N-superconcentrators. The second method, based on the work of Hansel, gives the optimal lower bound. The method of Kovari, Sos, and Turan can be extended to give tight lower bounds for extractors, in terms of both the number of truly random bits needed to extract one additional bit and the unavoidable entropy loss in the system. If the input is an n-bit source with min-entropy k and the output is required to be within a distance of {$\epsilon$} from uniform distribution, then to extract even one additional bit, one must invest at least (n-k)+2(1/{$\epsilon$})-O(1) truly random bits; to obtain m output bits one must invest at least m-k+2(1/{$\epsilon$})-O(1). Thus, there is a loss of 2(1/{$\epsilon$}) bits during the extraction. Interestingly, in the case of dispersers this loss in entropy is only about (1/{$\epsilon$}).},
  citationcount = {255},
  venue = {SIAM Journal on Discrete Mathematics}
}

@article{radhakrishnanExplicitDeterministicConstructions2001,
  title = {Explicit Deterministic Constructions for Membership in the Bitprobe Model},
  author = {Radhakrishnan, J. and Raman, Venkatesh and Rao, S.},
  year = {2001},
  doi = {10.1007/3-540-44676-1_24},
  abstract = {No abstract available},
  citationcount = {32},
  venue = {Embedded Systems and Applications}
}

@article{radislavvaismanReliabilityImportanceMeasure2021,
  title = {Reliability and Importance Measure Analysis of Networks with Shared Risk Link Groups},
  author = {Radislav Vaisman and Yuting Sun},
  year = {2021},
  journal = {Reliability Engineering \& System Safety},
  doi = {10.1016/j.ress.2021.107578},
  annotation = {Citation Count: 8}
}

@article{radoszewskiHardnessOfDetecting2021,
  title = {Hardness of Detecting Abelian and Additive Square Factors in Strings},
  author = {Radoszewski, J. and Rytter, W. and Straszy'nski, Juliusz and Wale'n, Tomasz and Zuba, Wiktor},
  year = {2021},
  doi = {10.4230/LIPIcs.ESA.2021.77},
  abstract = {We prove 3SUM-hardness (no strongly subquadratic-time algorithm, assuming the 3SUM conjecture) of several problems related to finding Abelian square and additive square factors in a string. In particular, we conclude conditional optimality of the state-of-the-art algorithms for finding such factors. Overall, we show 3SUM-hardness of (a) detecting an Abelian square factor of an odd half-length, (b) computing centers of all Abelian square factors, (c) detecting an additive square factor in a length-n string of integers of magnitude n\textsuperscript{\{\vphantom\}}\{O\}(1)\vphantom\{\}, and (d) a problem of computing a double 3-term arithmetic progression (i.e., finding indices i{$\neq$}j such that (x\textsubscript{i}+x\textsubscript{j})/2=x\textsubscript{\{\vphantom\}}(i+j)/2\vphantom\{\}) in a sequence of integers x{$_1$},{\dots},x\textsubscript{n} of magnitude n\textsuperscript{\{\vphantom\}}\{O\}(1)\vphantom\{\}. Problem (d) is essentially a convolution version of the AVERAGE problem that was proposed in a manuscript of Erickson. We obtain a conditional lower bound for it with the aid of techniques recently developed by Dudek et al. [STOC 2020]. Problem (d) immediately reduces to problem (c) and is a step in reductions to problems (a) and (b). In conditional lower bounds for problems (a) and (b) we apply an encoding of Amir et al. [ICALP 2014] and extend it using several string gadgets that include arbitrarily long Abelian-square-free strings. Our reductions also imply conditional lower bounds for detecting Abelian squares in strings over a constant-sized alphabet. We also show a subquadratic upper bound in this case, applying a result of Chan and Lewenstein [STOC 2015].},
  citationcount = {2},
  venue = {Embedded Systems and Applications},
  keywords = {lower bound,reduction}
}

@article{rahmanRankAndSelect2016,
  title = {Rank and Select Operations on Binary Strings},
  author = {Rahman, Naila and Raman, R.},
  year = {2016},
  doi = {10.1007/978-0-387-30162-4_332},
  abstract = {No abstract available},
  citationcount = {16},
  venue = {Encyclopedia of Algorithms}
}

@article{rahulApproximateRangeCounting2015,
  title = {Approximate Range Counting Revisited},
  author = {Rahul, S.},
  year = {2015},
  doi = {10.4230/LIPIcs.SoCG.2017.55},
  abstract = {We study range-searching for colored objects, where one has to count (approximately) the number of colors present in a query range. The problems studied mostly involve orthogonal range-searching in two and three dimensions, and the dual setting of rectangle stabbing by points. We present optimal and near-optimal solutions for these problems. Most of the results are obtained via reductions to the approximate uncolored version, and improved data-structures for them. An additional contribution of this work is the introduction of nested shallow cuttings.},
  citationcount = {11},
  venue = {International Symposium on Computational Geometry},
  keywords = {data structure,query,reduction}
}

@article{rahulOnTopK2015,
  title = {On Top-k Range Reporting in {{2D}} Space},
  author = {Rahul, S. and Tao, Yufei},
  year = {2015},
  doi = {10.1145/2745754.2745777},
  abstract = {Orthogonal range reporting (ORR) is a classic problem in computational geometry and databases, where the objective is to preprocess a set P of points in R2 such that, given an axis-parallel rectangle q, all the points in P {$\cap$} Q can be reported efficiently. This paper studies a natural variant of the problem called top-k ORR, where each point p {$\in$} P carries a weight w(p) {$\in$}R;. Besides q, a query also specifies an integer k {$\in$} [1, {\textbar}P{\textbar}], and needs to report the k points in q {$\cap$} P with the largest weights. We present optimal or near-optimal structures for solving the top-k ORR problem in the pointer machine and external memory models. As a side product, our structures give new space-query tradeoff for the orthogonal range max problem, which is a special case of top-k ORR with k = 1.},
  citationcount = {13},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {query}
}

@article{rajsbaumDistributedComputing2001,
  title = {Distributed Computing},
  author = {Rajsbaum, Sergio},
  year = {2001},
  doi = {10.1007/3-540-45414-4},
  abstract = {No abstract available},
  citationcount = {170},
  venue = {Iberian Conference on Information Systems and Technologies}
}

@article{ramachandranEfficientParallelTriconnectivity1988,
  title = {Efficient Parallel Triconnectivity in Logarithmic Time},
  author = {Ramachandran, V. and Vishkin, U.},
  year = {1988},
  doi = {10.1007/BFb0040371},
  abstract = {No abstract available},
  citationcount = {36},
  venue = {Aegean Workshop on Computing}
}

@article{ramamoorthyEquivalenceOfSystematic2019,
  title = {Equivalence of Systematic Linear Data Structures and Matrix Rigidity},
  author = {Ramamoorthy, Sivaramakrishnan Natarajan and Rashtchian, Cyrus},
  year = {2019},
  doi = {10.4230/LIPIcs.ITCS.2020.35},
  abstract = {Recently, Dvir, Golovnev, and Weinstein have shown that sufficiently strong lower bounds for linear data structures would imply new bounds for rigid matrices. However, their result utilizes an algorithm that requires an NP oracle, and hence, the rigid matrices are not explicit. In this work, we derive an equivalence between rigidity and the systematic linear model of data structures. For the n-dimensional inner product problem with m queries, we prove that lower bounds on the query time imply rigidity lower bounds for the query set itself. In particular, an explicit lower bound of {$\omega$}(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{n}\vphantom\{\}\{r\}m) for r redundant storage bits would yield better rigidity parameters than the best bounds due to Alon, Panigrahy, and Yekhanin. We also prove a converse result, showing that rigid matrices directly correspond to hard query sets for the systematic linear model. As an application, we prove that the set of vectors obtained from rank one binary matrices is rigid with parameters matching the known results for explicit sets. This implies that the vector-matrix-vector problem requires query time {\textohm}(n\textsuperscript{\{\vphantom\}}3/2\vphantom\{\}/r) for redundancy r{$\geq$}{\textsurd}\{n\} in the systematic linear model, improving a result of Chakraborty, Kamma, and Larsen. Finally, we prove a cell probe lower bound for the vector-matrix-vector problem in the high error regime, improving a result of Chattopadhyay, Kouck{\'y}, Loff, and Mukhopadhyay.},
  citationcount = {10},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {cell probe,data structure,lower bound,query,query time}
}

@article{ramamoorthyHowCompressAsymmetric2015,
  title = {How to Compress Asymmetric Communication},
  author = {Ramamoorthy, Sivaramakrishnan Natarajan and Rao, Anup},
  year = {2015},
  doi = {10.4230/LIPIcs.CCC.2015.102},
  abstract = {We study the relationship between communication and information in 2-party communication protocols when the information is asymmetric. If IA denotes the number of bits of information revealed by the first party, IB denotes the information revealed by the second party, and C is the number of bits of communication in the protocol, we show that {$\bullet$} one can simulate the protocol using order [EQUATION] bits of communication, {$\bullet$} one can simulate the protocol using order IA {$\cdot$} 2O(IB) bits of communication. The first result gives the best known bound on the complexity of a simulation when IA G IB,C3/4. The second gives the best known bound when IB L log C. In addition we show that if a function is computed by a protocol with asymmetric information complexity, then the inputs must have a large, nearly monochromatic rectangle of the right dimensions, a fact that is useful for proving lower bounds on lopsided communication problems.},
  citationcount = {14},
  venue = {Cybersecurity and Cyberforensics Conference},
  keywords = {communication,lower bound}
}

@article{ramamoorthyLowerBoundsOn2018,
  title = {Lower Bounds on Non-Adaptive Data Structures Maintaining Sets of Numbers, from Sunflowers},
  author = {Ramamoorthy, Sivaramakrishnan Natarajan and Rao, Anup},
  year = {2018},
  doi = {10.4230/LIPIcs.CCC.2018.27},
  abstract = {We prove new cell-probe lower bounds for dynamic data structures that maintain a subset of \{1, 2, ..., n\}, and compute various statistics of the set. The data structure is said to handle insertions non-adaptively if the locations of memory accessed depend only on the element being inserted, and not on the contents of the memory. For any such data structure that can compute the median of the set, we prove that: [EQUATION] where tins is the number of memory locations accessed during insertions, tmed is the number of memory locations accessed to compute the median, and w is the number of bits stored in each memory location. When the data structure is able to perform deletions non-adaptively and compute the minimum non-adaptively, we prove [EQUATION] where tmin is the number of locations accessed to compute the minimum, and tdel is the number of locations accessed to perform deletions. For the predecessor search problem, where the data structure is required to compute the predecessor of any element in the set, we prove that if computing the predecessors can be done non-adaptively, then [EQUATION] where tpred is the number of locations accessed to compute predecessors. These bounds are nearly matched by Binary Search Trees in some range of parameters. Our results follow from using the Sunflower Lemma of Erd{\H o}s and Rado [11] together with several kinds of encoding arguments.},
  citationcount = {11},
  venue = {Cybersecurity and Cyberforensics Conference},
  keywords = {cell probe,data structure,dynamic,lower bound,non-adaptive}
}

@article{ramanPriorityQueuesSmall1996,
  title = {Priority Queues: {{Small}}, Monotone and Trans-Dichotomous},
  author = {Raman, R.},
  year = {1996},
  doi = {10.1007/3-540-61680-2_51},
  abstract = {No abstract available},
  citationcount = {114},
  venue = {Embedded Systems and Applications}
}

@article{ramanRangeExtremumQueries2012,
  title = {Range Extremum Queries},
  author = {Raman, R.},
  year = {2012},
  doi = {10.1007/978-3-642-35926-2_30},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {International Workshop on Combinatorial Algorithms},
  keywords = {query}
}

@article{ramanStaticDictionariesSupporting1999,
  title = {Static Dictionaries Supporting Rank},
  author = {Raman, Venkatesh and Rao, S.},
  year = {1999},
  doi = {10.1007/3-540-46632-0_3},
  abstract = {No abstract available},
  citationcount = {20},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {static}
}

@article{ramanSuccinctDynamicData2001,
  title = {Succinct Dynamic Data Structures},
  author = {Raman, R. and Raman, Venkatesh and Rao, S.},
  year = {2001},
  doi = {10.1007/3-540-44634-6_39},
  abstract = {No abstract available},
  citationcount = {109},
  venue = {Workshop on Algorithms and Data Structures},
  keywords = {data structure,dynamic}
}

@article{ramanSuccinctDynamicDictionaries2003,
  title = {Succinct Dynamic Dictionaries and Trees},
  author = {Raman, R. and Rao, S.},
  year = {2003},
  doi = {10.1007/3-540-45061-0_30},
  abstract = {No abstract available},
  citationcount = {102},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {dynamic}
}

@article{ramanSuccinctIndexableDictionaries2007,
  title = {Succinct Indexable Dictionaries with Applications to Encoding K-Ary Trees, Prefix Sums and Multisets},
  author = {Raman, R. and Raman, Venkatesh and Satti, S. R.},
  year = {2007},
  doi = {10.1145/1290672.1290680},
  abstract = {We consider the indexable dictionary problem, which consists of storing a set S {$\subseteq$} \{0,{\dots},m - 1\} for some integer m while supporting the operations of rank(x), which returns the number of elements in S that are less than x if x {$\in$} S, and -1 otherwise; and select(i), which returns the ith smallest element in S. We give a data structure that supports both operations in O(1) time on the RAM model and requires B(n, m) + o(n) + O(lg lg m) bits to store a set of size n, where B(n, m) = {$\lfloor$}lg (m/n){$\rfloor$} is the minimum number of bits required to store any n-element subset from a universe of size m. Previous dictionaries taking this space only supported (yes/no) membership queries in O(1) time. In the cell probe model we can remove the O(lg lg m) additive term in the space bound, answering a question raised by Fich and Miltersen [1995] and Pagh [2001]. We present extensions and applications of our indexable dictionary data structure, including: ---an information-theoretically optimal representation of a k-ary cardinal tree that supports standard operations in constant time; ---a representation of a multiset of size n from \{0,{\dots},m - 1\} in B(n, m + n) + o(n) bits that supports (appropriate generalizations of) rank and select operations in constant time; and + O(lg lg m) ---a representation of a sequence of n nonnegative integers summing up to m in B(n, m + n) + o(n) bits that supports prefix sum queries in constant time.},
  citationcount = {386},
  venue = {ACM Trans. Algorithms},
  keywords = {cell probe,data structure,information theoretic,query}
}

@article{ramanSuccinctRepresentationsOf2013,
  title = {Succinct Representations of Ordinal Trees},
  author = {Raman, R. and Rao, S.},
  year = {2013},
  doi = {10.1007/978-3-642-40273-9_20},
  abstract = {No abstract available},
  citationcount = {18},
  venue = {Space-Efficient Data Structures, Streams, and Algorithms}
}

@article{ramaswamyOodbIndexingBy1995,
  title = {{{OODB}} Indexing by Class-Division},
  author = {Ramaswamy, S. and Kanellakis, P.},
  year = {1995},
  doi = {10.1145/223784.223809},
  abstract = {Indexing a class hierarchy, in order to efficiently search or update the objects of a class according to a (range of) value(s) of an attribute, impacts OODB performance heavily. For this indexing problem, most systems use the class hierarchy index (CH) technique of [15] implemented using B+-trees. Other techniques, such as those of [14, 18,31], can lead to improved average-case performance but involve the implementation of new data-structures. As a special form of external dynamic two-dimensional range searching, this OODB indexing problem is solvable within reasonable worst-case bounds [12]. Based on this insight, we have developed a technique, called indexing by class-division (CD), which we believe can be used as a practical alternative to CH. We present an optimized implementation and experimental validation of CD's average-case performance. The main advantages of the CD technique are: (1) CD is an extension of CH that provides a significant speed-up over CH for a wide spectrum of range queries--this speed-up is at least linear in the number of classes queried for uniform data and larger otherwise; and (2) CD queries, updates and concurrent use are implementable using existing B+-tree technology. The basic idea of class-division involves a time-space tradeoff and CD requires some space and update overhead in comparison to CH. In practice, this overhead is a small factor (2 to 3) and, in worst-case, is bounded by the depth of the hierarchy and the logarithm of its size.},
  citationcount = {55},
  venue = {ACM SIGMOD Conference}
}

@article{ramaswamyPathCachingExtended1994,
  title = {Path Caching (Extended Abstract): A Technique for Optimal External Searching},
  author = {Ramaswamy, S. and Subramanian, Sairam},
  year = {1994},
  doi = {10.1145/182591.182595},
  abstract = {External 2-dimensional searching is a fundamental problem with many applications in relational, object-oriented, spatial, and temporal databases. For example, interval intersection can be reduced to 2-sided, 2-dimensional searching and indexing class hierarchies of objects to 3-sided, 2-dimensional searching. Path caching is a new technique that can be used to transform a number of time/space efficient data structures for internal 2-dimensional searching (such as segment trees, interval trees, and priority search trees) into I/O efficient external ones. Let n be the size of the database, B the page size, and t the output size of a query. Using path caching, we provide the first data structure with optimal I/O query time O(logBn+t/B) for 2-sided, 2-dimensional searching. Furthermore, we show that path caching requires a small space overhead O(n{\textdiv}Blog2log2B) and is simple enough to admit dynamic updates in optimal O(logBn) amortized time. We also extend this data structure to handle 3-sided, 2-dimensional searching with optimal I/O query-time, at the expense of slightly higher storage and update overheads.},
  citationcount = {79},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems}
}

@article{ramkumarAccessRedundancyTradeoffs2023,
  title = {Access-Redundancy Tradeoffs in Quantized Linear Computations},
  author = {Ramkumar, Vinayak and Raviv, Netanel and Tamo, Itzhak},
  year = {2023},
  doi = {10.1109/ISIT54713.2023.10206571},
  abstract = {Linear real-valued computations over distributed datasets are common in many applications, most notably as part of machine learning inference. In particular, linear computations which are quantized, i.e., where the coefficients are restricted to a predetermined set of values (such as {\textpm}1), gained increasing interest lately due to their role in efficient, robust, or private machine learning models. Given a dataset to store in a distributed system, we wish to encode it so that all such computations could be conducted by accessing a small number of servers, called the access parameter of the system. Doing so relieves the remaining servers to execute other tasks, and reduces the overall communication in the system. Minimizing the access parameter gives rise to an access-redundancy tradeoff, where smaller access parameter requires more redundancy in the system, and vice versa. In this paper we study this tradeoff, and provide several explicit code constructions based on covering codes in a novel way. While the connection to covering codes has been observed in the past, our results strictly outperform the state-of-the-art, and extend the framework to new families of computations.},
  citationcount = {2},
  venue = {International Symposium on Information Theory},
  keywords = {communication}
}

@article{ramkumarAccessRedundancyTradeoffs2024,
  title = {Access-Redundancy Tradeoffs in Quantized Linear Computations},
  author = {Ramkumar, Vinayak and Raviv, Netanel and Tamo, Itzhak},
  year = {2024},
  doi = {10.1109/TIT.2024.3425808},
  abstract = {Linear real-valued computations over distributed datasets are common in many applications, most notably as part of machine learning inference. In particular, linear computations that are quantized, i.e., where the coefficients are restricted to a predetermined set of values (such as {\textpm}1), have gained increasing interest lately due to their role in efficient, robust, or private machine learning models. Given a dataset to store in a distributed system, we wish to encode it so that all such computations could be conducted by accessing a small number of servers, called the access parameter of the system. Doing so relieves the remaining servers to execute other tasks. Minimizing the access parameter gives rise to an access-redundancy tradeoff, where a smaller access parameter requires more redundancy in the system, and vice versa. In this paper, we study this tradeoff and provide several explicit low-access schemes for  {\textpm}1  quantized linear computations based on covering codes in a novel way. While the connection to covering codes has been observed in the past, our results strictly outperform the state-of-the-art for two-valued linear computations. We further show that the same storage scheme can be used to retrieve any linear combination with two distinct coefficients---regardless of what those coefficients are---with the same access parameter. This universality result is then extended to all possible quantizations with any number of values; while the storage remains identical, the access parameter increases according to a new additive-combinatorics property we call coefficient complexity. We then turn to study the coefficient complexity---we characterize the complexity of small sets of coefficients, provide bounds, and identify coefficient sets having the highest and lowest complexity. Interestingly, arithmetic progressions have the lowest possible complexity, and some geometric progressions have the highest possible complexity, the former being particularly attractive for its common use in uniform quantization.},
  citationcount = {1},
  venue = {IEEE Transactions on Information Theory}
}

@article{raoCodingForSunflowers2019,
  title = {Coding for Sunflowers},
  author = {Rao, Anup B.},
  year = {2019},
  doi = {10.19086/DA.11887},
  abstract = {A sunflower is a family of sets that have the same pairwise intersections. We simplify a recent result of Alweiss, Lovett, Wu and Zhang that gives an upper bound on the size of every family of sets of size k that does not contain a sunflower. We show how to use the converse of Shannon's noiseless coding theorem to give a cleaner proof of their result.},
  citationcount = {39},
  venue = {Discrete Analysis}
}

@article{raoCommunicationComplexity2020,
  title = {Communication Complexity},
  author = {Rao, Anup and Yehudayoff, A.},
  year = {2020},
  doi = {10.1017/9781108671644},
  abstract = {No abstract available},
  citationcount = {53},
  venue = {No venue available},
  keywords = {communication,communication complexity}
}

@article{raoExtensionComplexityOf2020,
  title = {Extension Complexity of Polytopes},
  author = {Rao, Anup and Yehudayoff, A.},
  year = {2020},
  doi = {10.1017/9781108671644.016},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Communication Complexity}
}

@article{raoNumbersOnForeheads2020,
  title = {Numbers on Foreheads},
  author = {Rao, Anup and Yehudayoff, A.},
  year = {2020},
  doi = {10.1017/9781108671644.007},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Communication Complexity}
}

@article{raoSmallDistortionAnd1999,
  title = {Small Distortion and Volume Preserving Embeddings for Planar and {{Euclidean}} Metrics},
  author = {Rao, Satish},
  year = {1999},
  doi = {10.1145/304893.304983}
}

@article{raoSunflowersFromSoil2022,
  title = {Sunflowers: From Soil to Oil},
  author = {Rao, Anup},
  year = {2022},
  doi = {10.1090/bull/1777},
  abstract = {A sunflower is a collection of sets whose pairwise intersections are identical. In this article, we shall go sunflower-picking. We find sunflowers in several seemingly unrelated fields, before turning to discuss recent progress on the famous sunflower conjecture of Erd{\H o}s and Rado, made by Alweiss, Lovett, Wu, and Zhang, as well as a related resolution of the threshold vs expectation threshold conjecture of Kahn and Kalai discovered by Park and Pham. We give short proofs for both of these results.},
  citationcount = {5},
  venue = {Electron. Colloquium Comput. Complex.}
}

@article{rashtchianVectorMatrixVector2020,
  title = {Vector-Matrix-Vector Queries for Solving Linear Algebra, Statistics, and Graph Problems},
  author = {Rashtchian, Cyrus and Woodruff, David P. and Zhu, Hanlin},
  year = {2020},
  doi = {10.4230/LIPIcs.APPROX/RANDOM.2020.26},
  abstract = {We consider the general problem of learning about a matrix through vector-matrix-vector queries. These queries provide the value of \{u\}\textsuperscript{\{\vphantom\}}\{T\}\vphantom\{\}\{M\}\{v\} over a fixed field \{F\} for a specified pair of vectors \{u\},\{v\}{$\in$}\{F\}{$^n$}. To motivate these queries, we observe that they generalize many previously studied models, such as independent set queries, cut queries, and standard graph queries. They also specialize the recently studied matrix-vector query model. Our work is exploratory and broad, and we provide new upper and lower bounds for a wide variety of problems, spanning linear algebra, statistics, and graphs. Many of our results are nearly tight, and we use diverse techniques from linear algebra, randomized algorithms, and communication complexity.},
  citationcount = {24},
  venue = {International Workshop and International Workshop on Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques},
  keywords = {communication,communication complexity,lower bound,query}
}

@article{rauchImprovedDataStructures1994,
  title = {Improved Data Structures for Fully Dynamic Biconnectivity},
  author = {Rauch, M.},
  year = {1994},
  doi = {10.1145/195058.195434},
  abstract = {We present fully dynamic algorithms for maintaining the biconnected components in general and plane graphs. A fully dynamic algorithm maintains a graph during a sequence of insertions and and deletions of edges or isolated vertices. Let m be the number of edges and n be the number of vertices in a graph. The time per operation of the best known algorithms are O({\textsurd}\{n\}) in general graphs and O(n) in plane graphs for fully dynamic connectivity and O( m{\textasciicircum}\{2/3\},n ) in general graphs and O({\textsurd}\{n\}) in plane graphs for fully dynamic biconnectivity. We improve the later running times to ( {\textsurd}\{m\}n,n ) in general graphs and O(\textsuperscript{\{\vphantom\}}2\vphantom\{\}n) in plane graphs. Our algorithm for general graphs can also find the biconnected components of all vertices in time O(n). The update times in general graphs are amortized. This shows that the biconnected components of a graph can be dynamically maintained almost as efficiently as the connected components.},
  citationcount = {22},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,dynamic,update,update time}
}

@article{ravikumarANovelParameter2021,
  title = {A Novel Parameter-Free Energy Efficient Fuzzy Nearest Neighbor Classifier for Time Series Data},
  author = {Ravikumar, Penugonda and Kiran, R. U. and Unnam, N. and Watanobe, Yutaka and Goda, K. and Devi, V. and Reddy, P. K.},
  year = {2021},
  doi = {10.1109/FUZZ45933.2021.9494521},
  abstract = {Time series classification is an important model in data mining. It involves assigning a class label to a test instance based on the training data with known class labels. Most previous studies developed time series classifiers by disregarding the fuzzy nature of events (i.e., events with similar values may belong to different classes) within the data. Consequently, these studies suffered from performance issues, including decreased accuracy and increased memory, runtime, and energy requirements. With this motivation, this paper proposes a novel fuzzy nearest neighbor classifier for time series data. The basic idea of our classifier is to transform the very large training data into a relatively small representative training data and use it to label a test instance by employing a new fuzzy distance measure known as Ravi. Experimental results on real world benchmark datasets demonstrate that the proposed classifier outperforms the current parameter-free time series classifiers and also the popular deep learning techniques.},
  citationcount = {1},
  venue = {IEEE International Conference on Fuzzy Systems}
}

@article{razAParallelRepetition1995,
  title = {A Parallel Repetition Theorem},
  author = {Raz, R.},
  year = {1995},
  doi = {10.1145/225058.225181},
  abstract = {We show that a parallel repetition of any two-prover one-round proof system (MIP(2,1)) decreases the probability of error at an exponential rate. No constructive bound was previously known. The constant in the exponent (in our analysis) depends only on the original probability of error and on the total number of possible answers of the two provers. The dependency on the total number of possible answers is logarithmic, which was recently proved to be almost the best possible [U. Feige and O. Verbitsky, Proc.11th Annual IEEE Conference on Computational Complexity, IEEE Computer Society Press, Los Alamitos, CA, 1996, pp. 70--76].},
  citationcount = {832},
  venue = {Symposium on the Theory of Computing}
}

@article{razborovOnTheDistributional1990,
  title = {On the Distributional Complexity of Disjointness},
  author = {Razborov, A.},
  year = {1990},
  doi = {10.1016/0304-3975(92)90260-M},
  abstract = {No abstract available},
  citationcount = {534},
  venue = {Theoretical Computer Science}
}

@article{razLowerBoundsAnd2008,
  title = {Lower Bounds and Separations for Constant Depth Multilinear Circuits},
  author = {Raz, R. and Yehudayoff, A.},
  year = {2008},
  doi = {10.1007/s00037-009-0270-8},
  abstract = {No abstract available},
  citationcount = {119},
  venue = {2008 23rd Annual IEEE Conference on Computational Complexity}
}

@article{razLowerBoundsFor2001,
  title = {Lower Bounds for Matrix Product, in Bounded Depth Circuits with Arbitrary Gates},
  author = {Raz, R. and Shpilka, Amir},
  year = {2001},
  doi = {10.1145/380752.380833},
  abstract = {We prove super-linear lower bounds for the number of edges in constant depth circuits with {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} inputs and up to {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} outputs. Our lower bounds are proved for all types of constant depth circuits, e.g., constant depth arithmetic circuits and constant depth Boolean circuits with arbitrary gates. The bounds apply for several explicit functions, and, most importantly, for matrix product. In particular, we obtain the following results: We show that the number of edges in any constant depth arithmetic circuit for {\textexclamdown}bold{\textquestiondown}matrix product{\textexclamdown}/bold{\textquestiondown} (over any field is super-linear in {\textexclamdown}italic{\textquestiondown}m{\textasciicircum}2{\textexclamdown}/italic{\textquestiondown} (where {\textexclamdown}italic{\textquestiondown}m {\texttimes}m{\textexclamdown}/italic{\textquestiondown} is the size of each matrix). That is, the lower bound is super-linear in the number of input variables. Moreover, if the circuit is bilinear the result applies also for the case where the circuit gets for free any product of two linear functions. We show that the number of edges in any constant depth arithmetic circuit for the trace of the product of 3 matrices (over fields with characteristic 0) is super-linear in {\textexclamdown}italic{\textquestiondown}m{\textasciicircum}2{\textexclamdown}/italic{\textquestiondown}. (Note that the trace is a {\textexclamdown}bold{\textquestiondown}single-output{\textexclamdown}/bold{\textquestiondown} function).{\textexclamdown}/par We give explicit examples for {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} Boolean functions {\textexclamdown}italic{\textquestiondown}f\_1,{\dots},f\_ {\textexclamdown}/italic{\textquestiondown}, such that any constant depth {\textexclamdown}bold.Boolean circuit with arbitrary gates{\textexclamdown}/bold{\textquestiondown} for {\textexclamdown}italic{\textquestiondown}f\_1,...,f\_n{\textexclamdown}/italic{\textquestiondown} has a super-linear number of edges. The lower bound is proved also for {\textexclamdown}bold{\textquestiondown}circuits with arbitrary gates over any finite field{\textexclamdown}/bold{\textquestiondown}. The bound applies for matrix product over finite fields as well as for several other explicit functions.},
  citationcount = {49},
  venue = {Symposium on the Theory of Computing}
}

@article{razMultiLinearFormulas2009,
  title = {Multi-Linear Formulas for Permanent and Determinant Are of Super-Polynomial Size},
  author = {Raz, R.},
  year = {2009},
  doi = {10.1145/1502793.1502797},
  abstract = {An arithmetic formula is multilinear if the polynomial computed by each of its subformulas is multilinear. We prove that any multilinear arithmetic formula for the permanent or the determinant of an n {\texttimes} n matrix is of size super-polynomial in n. Previously, super-polynomial lower bounds were not known (for any explicit function) even for the special case of multilinear formulas of constant depth.},
  citationcount = {61},
  venue = {JACM}
}

@article{razSeparationOfMultilinear2006,
  title = {Separation of Multilinear Circuit and Formula Size},
  author = {Raz, R.},
  year = {2006},
  doi = {10.4086/toc.2006.v002a006},
  abstract = {An arithmetic circuit or formula is multilinear if the polynomial computed at each of its wires is multilinear. We give an explicit polynomial f(x1,..., xn) with coeffi- cients in \{0, 1\} such that over any field: 1. f can be computed by a polynomial-size multilinear circuit of depth O(log 2 n).},
  citationcount = {96},
  venue = {Theory of Computing}
}

@article{razSeparationOfThe1997,
  title = {Separation of the Monotone {{NC}} Hierarchy},
  author = {Raz, R. and McKenzie, P.},
  year = {1997},
  doi = {10.1109/SFCS.1997.646112},
  abstract = {, for the monotone depth of functions in monotone-P. As a result we achieve the separation of the following classes. 1.{\enspace}monotone-NC {$\neq$} monotone-P. 2.{\enspace}For every i{$\geq$}1, monotone-{$\neq$} monotone-. 3.{\enspace}More generally: For any integer function D(n), up to (for some {$\varepsilon$}{\textquestiondown}0), we give an explicit example of a monotone Boolean function, that can be computed by polynomial size monotone Boolean circuits of depth D(n), but that cannot be computed by any (fan-in 2) monotone Boolean circuits of depth less than Const{$\cdot$}D(n) (for some constant Const).Only a separation of monotone- from monotone- was previously known. Our argument is more general: we define a new class of communication complexity search problems, referred to below as DART games, and we prove a tight lower bound for the communication complexity of every member of this class. As a result we get lower bounds for the monotone depth of many functions. In particular, we get the following bounds: 1.{\enspace} For st-connectivity, we get a tight lower bound of . That is, we get a new proof for Karchmer--Wigderson's theorem, as an immediate corollary of our general result. 2.{\enspace} For the k-clique function, with , we get a tight lower bound of {\textohm}(k log n). This lower bound was previously known for k{$\leq$} log n [1]. For larger k, however, only a bound of {\textohm}(k) was previously known.},
  citationcount = {200},
  venue = {Proceedings 38th Annual Symposium on Foundations of Computer Science}
}

@article{regevAProofOf2011,
  title = {A Proof of Catalan's Convolution Formula},
  author = {Regev, A.},
  year = {2011},
  doi = {10.1515/integers-2012-0014},
  abstract = {Abstract. A new proof is given for the k-fold convolution of the Catalan numbers. This is done by enumerating a certain class of polygonal dissections called k-in-n dissections.},
  citationcount = {15},
  venue = {Integers}
}

@article{renDemandSupplyBalancing2015,
  title = {Demand-Supply Balancing Using Multi-Agent System for Bus-Oriented Microgrids},
  author = {Ren, Qiangguo and Bai, Li and Biswas, S. and Ferrese, F. and Dong, Q.},
  year = {2015},
  doi = {10.1109/RWEEK.2015.7287415},
  abstract = {To balance demand and supply in a microgrid can be considered as a demand-supply balancing problem, and it can be solved in either a static way or a dynamic way. In static algorithms knowledge about the system is known ahead of time. The connection of load and generator is made at the designing phase. A demand-supply balancing algorithm using the knowledge of system state at runtime has potentials of performing better than static algorithms. Demand-supply balancing problem is a large optimization problem with many variables for consumption or generation power flow at connected buses. We developed a market-based multi-agent system solving the problem in a decentralized way. Our approach is iterative with a fixed number of inter-agent communication steps. Consumer agents exchange simple messages with producers in the power network and then solve their own optimization problem by maximizing their own benefits. It has been shown that this approach converges to a solution when the agents' objectives are achieved without a global coordinator. We have simulated different scaled bus-oriented microgrids and the simulation results show that the performance of the approach is promising.},
  citationcount = {3},
  venue = {IEEE Radio and Wireless Symposium},
  keywords = {communication,dynamic,static}
}

@article{rennerSmoothRenyiEntropy2004,
  title = {Smooth {{Renyi}} Entropy and Applications},
  author = {Renner, R. and Wolf, S.},
  year = {2004},
  doi = {10.1109/ISIT.2004.1365269},
  abstract = {We introduce a new entropy measure, called smooth Renyi entropy. The measure characterizes fundamental properties of a random variable Z, such as the amount of uniform randomness that can be extracted from Z or the minimum length of an encoding of Z.},
  citationcount = {231},
  venue = {International Symposium onInformation Theory, 2004. ISIT 2004. Proceedings.}
}

@article{reusFamousProblemsIn2016,
  title = {Famous Problems in {{P}}},
  author = {Reus, Bernhard},
  year = {2016},
  doi = {10.1007/978-3-319-27889-6_16},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {No venue available}
}

@article{rivestOnRecognizingGraph1976,
  title = {On Recognizing Graph Properties from Adjacency Matrices},
  author = {Rivest, R. and Vuillemin, J.},
  year = {1976},
  doi = {10.1016/0304-3975(76)90053-0},
  abstract = {No abstract available},
  citationcount = {201},
  venue = {Theoretical Computer Science}
}

@article{rivestOptimalArrangementOf1978,
  title = {Optimal Arrangement of Keys in a Hash Table},
  author = {Rivest, R.},
  year = {1978},
  doi = {10.1145/322063.322065},
  abstract = {When open addressing IS used to resolve collisions in a hash table, a given set of keys may be arranged in many ways, typically this depends on the order in which the keys are inserted It is shown that arrangements minimizing either the average or worst-case number of probes required to retrieve any key in the table can be found using an algorithm for the assignment problem. The worst-case retrieval time can be reduced to O(log2(M)) with probablhty 1 - e(M) when storing Mkeys In a table of size M, where  (M)  0 as M     We also examine insertion algorithms to see how to apply these ideas for a dynamically changing set of keys},
  citationcount = {45},
  venue = {JACM}
}

@article{rivestPartialMatchRetrieval1976,
  title = {Partial-Match Retrieval Algorithms},
  author = {Rivest, R.},
  year = {1976},
  doi = {10.1137/0205003},
  abstract = {We examine the efficiency of hash-coding and tree-search algorithms for retrieving from a file of k-letter words all words which match a partially-specified input queryword (for example, retrieving all six-letter English words of the form S**R*H where "*" is a "don't care" character). We precisely characterize those balanced hash-coding algorithms withminimum average number of lists examined. Use of the first few letters of each word as a list index is shown to be one such optimal algorithm.A new class of combinatorial designs (called associative block designs) provides better hash functions with a greatly reduced worst-case number of lists examined, yet with optimal average behavior maintained. Another efficient variant involves storing each word in several lists. Tree-search algorithms are shown to be approximately as efficient as hash-coding algorithms, on the average. In general, these algorithms require time about O(n {\textexclamdown}k-s)/k) to respond to a query word with s letters specified, given a file of n k-letter words. Previous algorithms either required time O(s n/k) or else used exorbitant amounts of storage.},
  citationcount = {265},
  venue = {SIAM journal on computing (Print)}
}

@article{robereExponentialLowerBounds2016,
  title = {Exponential Lower Bounds for Monotone Span Programs},
  author = {Robere, Robert and Pitassi, T. and Rossman, Benjamin and Cook, S.},
  year = {2016},
  doi = {10.1109/FOCS.2016.51},
  abstract = {Monotone span programs are a linear-algebraic model of computation which were introduced by Karchmer and Wigderson in 1993 [1]. They are known to be equivalent to linear secret sharing schemes, and have various applications in complexity theory and cryptography. Lower bounds for monotone span programs have been difficult to obtain because they use non-monotone operations to compute monotone functions, in fact, the best known lower bounds are quasipolynomial for a function in (nonmonotone) P [2]. A fundamental open problem is to prove exponential lower bounds on monotone span program size for any explicit function. We resolve this open problem by giving exponential lower bounds on monotone span program size for a function in monotone P. This also implies the first exponential lower bounds for linear secret sharing schemes. Our result is obtained by proving exponential lower bounds using Razborov's rank method [3], a measure that is strong enough to prove lower bounds for many monotone models. As corollaries we obtain new proofs of exponential lower bounds for monotone formula size, monotone switching network size, and the first lower bounds for monotone comparator circuit size for a function in monotone P. We also obtain new polynomial degree lower bounds for Nullstellensatz refutations using an interpolation theorem of Pudlak and Sgall [4]. Finally, we obtain quasipolynomial lower bounds on the rank measure for the st-connectivity function, implying tight bounds for st-connectivity in all of the computational models mentioned above.},
  citationcount = {75},
  venue = {IEEE Annual Symposium on Foundations of Computer Science}
}

@article{rodittyAFullyDynamic2004,
  title = {A Fully Dynamic Reachability Algorithm for Directed Graphs with an Almost Linear Update Time},
  author = {Roditty, L. and Zwick, Uri},
  year = {2004},
  doi = {10.1145/1007352.1007387},
  abstract = {We obtain a new fully dynamic algorithm for the reachability problem in directed graphs. Our algorithm has an amortized update time of {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown}+{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) and a worst-case query time of {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}), where {\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown} is the current number of edges in the graph, and {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} is the number of vertices in the graph. Each update operation either inserts a set of edges that touch the same vertex, or deletes an arbitrary set of edges. The algorithm is deterministic and uses fairly simple data structures. This is the first algorithm that breaks the {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}) update barrier for all graphs with {\textexclamdown}i{\textquestiondown}o{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}) edges.One of the ingredients used by this new algorithm may be interesting in its own right. It is a new dynamic algorithm for {\textexclamdown}i{\textquestiondown}strong{\textexclamdown}/i{\textquestiondown} connectivity in directed graphs with an interesting persistency property. Each insert operation creates a new version of the graph. A delete operation deletes edges from emphall versions. Strong connectivity queries can be made on each version of the graph. The algorithm handles each update in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}m{$\alpha$}{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown},{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown})) amortized time, and each query in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(1) time, where {\textexclamdown}i{\textquestiondown}{$\alpha$}{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown},{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) is a functional inverse of Ackermann's function appearing in the analysis of the union-find data structure. Note that the update time of {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}m{$\alpha$}{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}m{\textexclamdown}/i{\textquestiondown},{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown})), in case of a delete operation, is the time needed for updating {\textexclamdown}i{\textquestiondown}all{\textexclamdown}/i{\textquestiondown} versions of the graph.},
  citationcount = {133},
  venue = {Symposium on the Theory of Computing}
}

@article{rodittyApproximatingTheGirth2011,
  title = {Approximating the Girth},
  author = {Roditty, L. and Tov, Roei},
  year = {2011},
  doi = {10.1137/1.9781611973082.112},
  abstract = {This paper considers the problem of computing a minimum weight cycle in weighted undirected graphs. Given a weighted undirected graph {\textexclamdown}i{\textquestiondown}G{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}V,E,w{\textexclamdown}/i{\textquestiondown}), let {\textexclamdown}i{\textquestiondown}C{\textexclamdown}/i{\textquestiondown} be a minimum weight cycle of {\textexclamdown}i{\textquestiondown}G{\textexclamdown}/i{\textquestiondown}, let {\textexclamdown}i{\textquestiondown}w{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}C{\textexclamdown}/i{\textquestiondown}) be the weight of {\textexclamdown}i{\textquestiondown}C{\textexclamdown}/i{\textquestiondown} and let {\textexclamdown}i{\textquestiondown}w{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}max{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown} ({\textexclamdown}i{\textquestiondown}C{\textexclamdown}/i{\textquestiondown}) be the weight of the maximal edge of {\textexclamdown}i{\textquestiondown}C{\textexclamdown}/i{\textquestiondown}. We obtain three new approximation algorithms for the minimum weight cycle problem: 1. For integral weights from the range [1, {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown}] an algorithm that reports a cycle of weight at most 4/3{\textexclamdown}i{\textquestiondown}w{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}C{\textexclamdown}/i{\textquestiondown}) in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + log {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown})) time. 2. For integral weights from the range [1, {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown}] an algorithm that reports a cycle of weight at most {\textexclamdown}i{\textquestiondown}w{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}C{\textexclamdown}/i{\textquestiondown}) + {\textexclamdown}i{\textquestiondown}w{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sub{\textquestiondown}{\textexclamdown}i{\textquestiondown}max{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/sub{\textquestiondown} ({\textexclamdown}i{\textquestiondown}C{\textexclamdown}/i{\textquestiondown}) in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + log {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown})) time. 3. For non-negative real edge weights an algorithm that for any {$\varepsilon$} {\textquestiondown} 0 reports a cycle of weight at most (4/3 + {$\varepsilon$}){\textexclamdown}i{\textquestiondown}w{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}C{\textexclamdown}/i{\textquestiondown}) in {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(1/{$\varepsilon$} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}(log log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown})) time. In a recent breakthrough Vassilevska Williams and Williams [WW10] showed that a subcubic algorithm that computes the exact minimum weight cycle in undirected graphs with integral weights from the range [1, {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown}] implies a subcubic algorithm for computing all-pairs shortest paths in directed graphs with integral weights from the range [- {\textexclamdown}i{\textquestiondown}M, M{\textexclamdown}/i{\textquestiondown}]. This implies that in order to get a subcubic algorithm for computing a minimum weight cycle we have to relax the problem and to consider an approximated solution. Lingas and Lundell [LL09] were the first to consider approximation in the context of minimum weight cycle in weighted graphs. They presented a 2-approximation algorithm for integral weights with {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown} log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + log {\textexclamdown}i{\textquestiondown}M{\textexclamdown}/i{\textquestiondown})) running time. They also posed as an open problem the question whether it is possible to obtain a subcubic algorithm with a {\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown}-approximation, where {\textexclamdown}i{\textquestiondown}c{\textexclamdown}/i{\textquestiondown} {\textexclamdown} 2. The current paper answers this question in the affirmative, by presenting an algorithm with 4/3-approximation and the same running time. Surprisingly, the approximation factor of 4/3 is not accidental. We show using the new result of Vassilevska Williams and Williams [WW10] that a subcubic combinatorial algorithm with (4/3 - {$\varepsilon$})-approximation, where 0 {\textexclamdown} {$\varepsilon$} {$\leq$} 1/3, implies a subcubic combinatorial algorithm for multiplying two boolean matrices.},
  citationcount = {21},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{rodittyDynamicApproximateAll2004,
  title = {Dynamic Approximate All-Pairs Shortest Paths in Undirected Graphs},
  author = {Roditty, L. and Zwick, Uri},
  year = {2004},
  doi = {10.1137/090776573},
  abstract = {We obtain three dynamic algorithms for the approximate all-pairs shortest paths problem in unweighted undirected graphs: 1) For any fixed /spl epsiv/ {\textquestiondown} 0, a decremental algorithm with an expected total running time of O(mn), where m is the number of edges and n is the number of vertices in the initial graph. Each distance query is answered in O(1) worst-case time, and the stretch of the returned distances is at most 1 + /spl epsiv/. The algorithm uses O(n/sup 2/) space; 2) For any fixed integer k /spl ges/ 1, a decremental algorithm with an expected total running time of O(mn). Each query is answered in O(1) worst-case time, and the stretch of the returned distances is at most 2k - 1. This algorithm uses, however, only O(m + n/sup 1+1/k/) space. It is obtained by dynamizing techniques of Thorup and Zwick. In addition to being more space efficient, this algorithm is also one of the building blocks used to obtain the first algorithm; 3) For any fixed /spl epsiv/, /spl delta/ {\textquestiondown} 0 and every t /spl les/ m/sup 1/2-/spl delta//, a fully dynamic algorithm with an expected amortized update time of O(mn/t) and worst-case query time of O(t). The stretch of the returned distances is at most 1+/spl epsiv/. All algorithms can also be made to work on undirected graphs with small integer edge weights. If the largest edge weight is b, then all bounds on the running times are multiplied by b.},
  citationcount = {126},
  venue = {45th Annual IEEE Symposium on Foundations of Computer Science}
}

@article{rodittyOnDynamicShortest2004,
  title = {On Dynamic Shortest Paths Problems},
  author = {Roditty, L. and Zwick, Uri},
  year = {2004},
  doi = {10.1007/s00453-010-9401-5},
  abstract = {No abstract available},
  citationcount = {229},
  venue = {Algorithmica}
}

@article{roghaniBeatingTheFolklore2021,
  title = {Beating the Folklore Algorithm for Dynamic Matching},
  author = {Roghani, M. and Saberi, A. and Wajc, David},
  year = {2021},
  doi = {10.4230/LIPIcs.ITCS.2022.111},
  abstract = {The maximum matching problem in dynamic graphs subject to edge updates (insertions and deletions) has received much attention over the last few years; a multitude of approximation/time tradeoffs were obtained, improving upon the folklore algorithm, which maintains a maximal (and hence 2-approximate) matching in O(n) worst-case update time in n-node graphs. We present the first deterministic algorithm which outperforms the folklore algorithm in terms of \{\vphantom\}\emph{both}\vphantom\{\}\emph{ approximation ratio and worst-case update time. Specifically, we give a (2-{\textohm}(1))-approximate algorithm with O(m\textsuperscript{\{\vphantom\}}3/8\vphantom\{\})=O(n\textsuperscript{\{\vphantom\}}3/4\vphantom\{\}) worst-case update time in n-node, m-edge graphs. For sufficiently small constant {$\epsilon>$}0, no deterministic (2+{$\epsilon$})-approximate algorithm with worst-case update time O(n\textsuperscript{\{\vphantom\}}0.99\vphantom\{\}) was known. Our second result is the first deterministic (2+{$\epsilon$})-approximate weighted matching algorithm with O\textsubscript{{$\epsilon$}}(1){$\cdot$}O({\textsurd}[4]\{m\})=O\textsubscript{{$\epsilon$}}(1){$\cdot$}O({\textsurd}\{n\}) worst-case update time. Our main technical contributions are threefold: first, we characterize the tight cases for \{kernels\vphantom\}}\vphantom\{\}\emph{, which are the well-studied matching sparsifiers underlying much of the (2+{$\epsilon$})-approximate dynamic matching literature. This characterization, together with multiple ideas -- old and new -- underlies our result for breaking the approximation barrier of 2. Our second technical contribution is the first example of a dynamic matching algorithm whose running time is improved due to improving the \{recourse\vphantom\}}\vphantom\{\}\emph{ of other dynamic matching algorithms. Finally, we show how to use dynamic bipartite matching algorithms as black-box subroutines for dynamic matching in general graphs without incurring the natural \textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{3}$}\vphantom\{\}\{2\} factor in the approximation ratio which such approaches naturally incur.}},
  citationcount = {25},
  venue = {Information Technology Convergence and Services},
  keywords = {dynamic,update,update time}
}

@article{rothOnIrregularitiesOf1954,
  title = {On Irregularities of Distribution},
  author = {Roth, K. F.},
  year = {1954},
  doi = {10.1112/S0025579300000541},
  abstract = {In 1935 van der Corput, in connection with his work on distribution functions, was led to the following conjecture which expresses the fact that no sequence can, in a certain sense, be too evenly distributed.},
  citationcount = {290},
  venue = {No venue available}
}

@article{rothRemarkConcerningInteger1964,
  title = {Remark Concerning Integer Sequences},
  author = {Roth, K. F.},
  year = {1964},
  journal = {Acta Arithmetica},
  volume = {9},
  pages = {257--260},
  doi = {10.4064/aa-9-3-257-260},
  file = {/Users/tulasi/Zotero/storage/2I34TZIF/Roth - 1964 - Remark concerning integer sequences.pdf;/Users/tulasi/Zotero/storage/HBS77Y68/Roth - 1964 - Remark concerning integer sequences.pdf;/Users/tulasi/Zotero/storage/I3HZ6VJC/Roth - 1964 - Remark concerning integer sequences.pdf;/Users/tulasi/Zotero/storage/J4EJTMI8/Roth - 1964 - Remark concerning integer sequences.pdf;/Users/tulasi/Zotero/storage/KUFSXAAZ/Roth - 1964 - Remark concerning integer sequences.pdf;/Users/tulasi/Zotero/storage/TPI32C5R/Roth - 1964 - Remark concerning integer sequences.pdf}
}

@article{rothvossDiscrepancyTheory2020,
  title = {Discrepancy Theory},
  author = {Rothvoss, Thomas},
  year = {2020},
  doi = {10.1515/9783110652581},
  abstract = {Discrepancy theory is a subfield of combinatorics in which one asks the following question: given a finite set system S 1 ,..., S m {$\subseteq$} \{1,..., n \}; color the elements \{1,..., n \} with two colors, say red and blue . What is the difference between red and blue elements in the most unbalanced set for the best coloring? The two main results are the Beck-Fiala Theorem and Spencer's Theorem, which both have very elegant proofs as we will see in this lecture.},
  citationcount = {102},
  venue = {No venue available}
}

@article{roughgardenCommunicationComplexityFor2015,
  title = {Communication Complexity (for Algorithm Designers)},
  author = {Roughgarden, Tim},
  year = {2015},
  doi = {10.1561/0400000076},
  abstract = {This document collects the lecture notes from my course "Communication Complexity (for Algorithm Designers),'' taught at Stanford in the winter quarter of 2015. The two primary goals of the course are: 1. Learn several canonical problems that have proved the most useful for proving lower bounds (Disjointness, Index, Gap-Hamming, etc.). 2. Learn how to reduce lower bounds for fundamental algorithmic problems to communication complexity lower bounds. Along the way, we'll also: 3. Get exposure to lots of cool computational models and some famous results about them --- data streams and linear sketches, compressive sensing, space-query time trade-offs in data structures, sublinear-time algorithms, and the extension complexity of linear programs. 4. Scratch the surface of techniques for proving communication complexity lower bounds (fooling sets, corruption bounds, etc.).},
  citationcount = {52},
  venue = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
  keywords = {communication,communication complexity,data structure,lower bound,query,query time}
}

@article{rouraANewMethod2001,
  title = {A New Method for Balancing Binary Search Trees},
  author = {Roura, Salvador},
  year = {2001},
  doi = {10.1007/3-540-48224-5_39},
  abstract = {No abstract available},
  citationcount = {14},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{royAnEfficientAsymmetric2007,
  title = {An Efficient Asymmetric Route to Tertiary Carbinols: {{Synthesis}} of ({{R}})-Mevalonolactone},
  author = {Roy, Siddhartha and Sharma, Anubha and Dhotare, B. and Vichare, Prasad and Chattopadhyay, A. and Chattopadhyay, S.},
  year = {2007},
  doi = {10.1055/S-2007-965956},
  abstract = {An efficient strategy for the asymmetric construction of tertiary carbinols has been devised using cyclohexylideneglyceraldehyde as a chiral template. This involves (a) addition of a Grignard reagent (R 1 MgX) to cyclohexylideneglyceraldehyde, followed by oxidation with pyridinium chlorochromate to give the alkylated ketone, and (b) addition of a second Grignard reagent (R 2 MgX) to the previously formed ketone. For alkyl chain lengths up to that of N-decane, in the second Grignard reagent (R 2 MgX) or the ketone formed in the first reaction, the reaction proceeded with complete diastereoselectivity; with longer chains, the selectivity dropped. The presence of a C=C bond in the second Grignard reagent (R 2 MgX) or the ketone also reduced the diastereoselectivity of the reaction. When, with regard to chain lengths of the alkyl groups in the Grignard reactions, R 2 {\textquestiondown} R 1 , the reaction proceeded with SYN selectivity, and vice versa. In general, the C-3 epimers of the target tertiary carbinols could be prepared easily by altering the sequence of the addition of the Grignard reagents (R 1 MgX and R 2 MgX) to cyclo-hexylideneglyceraldehyde. This strategy, using inexpensive chemicals, was applied in the simple enantiomeric synthesis of ( R)-mevalonolactone.},
  citationcount = {3},
  venue = {No venue available}
}

@article{royschwartzOnlineOfflineAlgorithms2019,
  title = {Online and {{Offline Algorithms}} for {{Circuit Switch Scheduling}}},
  author = {Roy Schwartz and Mohit Singh and Sina Yazdanbod},
  year = {2019},
  journal = {Foundations of Software Technology and Theoretical Computer Science},
  doi = {10.4230/LIPIcs.FSTTCS.2019.27},
  abstract = {Motivated by the use of high speed circuit switches in large scale data centers, we consider the problem of circuit switch scheduling. In this problem we are given demands between pairs of servers and the goal is to schedule at every time step a matching between the servers while maximizing the total satisfied demand over time. The crux of this scheduling problem is that once one shifts from one matching to a different one a fixed delay {$\delta$} is incurred during which no data can be transmitted. For the offline version of the problem we present a (1 - 1/e - ) approximation ratio (for any constant {$>$} 0). Since the natural linear programming relaxation for the problem has an unbounded integrality gap, we adopt a hybrid approach that combines the combinatorial greedy with randomized rounding of a different suitable linear program. For the online version of the problem we present a (bi-criteria) ((e- 1)/(2e- 1)- )-competitive ratio (for any constant {$>$} 0 ) that exceeds time by an additive factor of O({$\delta$}/ ). We note that no uni-criteria online algorithm is possible. Surprisingly, we obtain the result by reducing the online version to the offline one. 2012 ACM Subject Classification Theory of computation {$\rightarrow$} Online algorithms; Theory of computation {$\rightarrow$} Scheduling algorithms},
  annotation = {Citation Count: 0}
}

@article{rubinsteinHardnessOfApproximate2018,
  title = {Hardness of Approximate Nearest Neighbor Search},
  author = {Rubinstein, A.},
  year = {2018},
  doi = {10.1145/3188745.3188916},
  abstract = {We prove conditional near-quadratic running time lower bounds for approximate Bichromatic Closest Pair with Euclidean, Manhattan, Hamming, or edit distance. Specifically, unless the Strong Exponential Time Hypothesis (SETH) is false, for every {$\delta$}{\textquestiondown}0 there exists a constant {$\varepsilon$}{\textquestiondown}0 such that computing a (1+{$\varepsilon$})-approximation to the Bichromatic Closest Pair requires {\textohm}(n2-{$\delta$}) time. In particular, this implies a near-linear query time for Approximate Nearest Neighbor search with polynomial preprocessing time. Our reduction uses the recently introduced Distributed PCP framework, but obtains improved efficiency using Algebraic Geometry (AG) codes. Efficient PCPs from AG codes have been constructed in other settings before, but our construction is the first to yield new hardness results.},
  citationcount = {100},
  venue = {Symposium on the Theory of Computing},
  keywords = {lower bound,query,query time,reduction}
}

@article{rubinsteinSethVsApproximation2019,
  title = {{{SETH}} vs Approximation},
  author = {Rubinstein, A. and Williams, V. V.},
  year = {2019},
  doi = {10.1145/3374857.3374870},
  abstract = {Our story is about hardness of problems in P, but its roots begin with two algorithmic approaches that have been developed to cope with NP-hard problems: approximation algorithms and fasterthan- brute-force algorithms. Approximation algorithms were proposed as a response for NP-hardness almost immediately (in historical perspective of almost half a century), and have been one of the most celebrated success stories of our eld. An outstanding complexity result in this area, which has since turned into a sub- eld of its own, is the Probabilistically Checkable Proof (PCP) Theorem. For many problems like Max-3-SAT we now have nearly tight hardness-of-approximation results.},
  citationcount = {22},
  venue = {SIGA}
}

@article{russoRangeMinimumQueries2021,
  title = {Range Minimum Queries in Minimal Space},
  author = {Russo, L.},
  year = {2021},
  doi = {10.1016/j.tcs.2022.01.025},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Theoretical Computer Science},
  keywords = {query}
}

@article{ruzicMakingDeterministicSignatures2007,
  title = {Making Deterministic Signatures Quickly},
  author = {Ruzic, M.},
  year = {2007},
  doi = {10.1145/1541885.1541887},
  abstract = {We present a new technique of universe reduction. Primary applications are the dictionary problem and the predecessor problem. We give several new results on static dictionaries in different computational models: the word RAM, the Practical RAM, and for strings - the cache-oblivious model. All algorithms and data structures are deterministic and use linear space. Representative results are: a dictionary with a lookup time of O(log log n) and construction time of O(n log log n) on a word RAM, and a static predecessor structure for variable-length binary strings with a query performance of O({\textbar}s{\textbar}/B + log {\textbar}s{\textbar} + log log n) I/Os, for query argument s, in the cache-oblivious model.},
  citationcount = {16},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,query,reduction,static}
}

@article{ruzicUniformDeterministicDictionaries2008,
  title = {Uniform Deterministic Dictionaries},
  author = {Ruzic, M.},
  year = {2008},
  doi = {10.1145/1328911.1328912},
  abstract = {We present a new analysis of the well-known family of multiplicative hash functions, and improved deterministic algorithms for selecting ``good'' hash functions. The main motivation is realization of deterministic dictionaries with fast lookups and reasonably fast updates. The model of computation is the Word RAM, and it is assumed that the machine word-size matches the size of keys in bits. Many of the modern solutions to the dictionary problem are weakly nonuniform, that is, they require a number of constants to be computed at ``compile time'' for the stated time bounds to hold. The currently fastest deterministic dictionary uses constants not known to be computable in polynomial time. In contrast, our dictionaries do not require any special constants or instructions, and running times are independent of word (and key) length. Our family of dynamic dictionaries achieves a performance of the following type: lookups in time O(t) and updates in amortized time O(n1/t), for an appropriate parameter function t. Update procedures require division, whereas searching uses multiplication only.},
  citationcount = {129},
  venue = {TALG},
  keywords = {dynamic,update}
}

@article{s.cookParallelPointerMachines2005,
  title = {Parallel Pointer Machines},
  author = {S. Cook and Patrick W. Dymond},
  year = {2005},
  journal = {Computational Complexity},
  doi = {10.1007/BF01200405},
  annotation = {Citation Count: 11}
}

@article{s.dughmiAlgorithmicInformationStructure2017,
  title = {Algorithmic Information Structure Design: A Survey},
  author = {S. Dughmi},
  year = {2017},
  journal = {SeCO Workshops},
  doi = {10.1145/3055589.3055591},
  abstract = {Information structure design, also sometimes known as signaling or persuasion, is concerned with understanding the effects of information on the outcomes of strategic interactions (the descriptive question), and in characterizing and computing the information sharing strategies which optimize some design objective (the prescriptive question). Both questions are illuminated through the lens of algorithms and complexity, as evidenced by recent work on the topic in the algorithmic game theory community. This monograph is a biased survey of this work, and paints a picture of the current state of progress and challenges ahead.  We divide information structure design into single agent and multiple agent models, and further subdivide the multiple agent case into the public channel and private channel modes of information revelation. In each of these three cases, we describe the most prominent models and applications, survey the associated algorithms and complexity results and their structural implications, and outline directions for future work.},
  keywords = {information},
  annotation = {Citation Count: 90}
}

@article{s.rahulAlmostOptimalSolution2020,
  title = {An ({{Almost}}) {{Optimal Solution}} for {{Orthogonal Point Enclosure Query}} in {{$\mathbb{R}$3}}},
  author = {S. Rahul},
  year = {2020},
  journal = {Mathematics of Operations Research},
  doi = {10.1287/moor.2019.0994},
  abstract = {The orthogonal point enclosure query (OPEQ) problem is a fundamental problem in the context of data management for modeling user preferences. Formally, preprocess a set S of n axis-aligned boxes (p...},
  keywords = {query},
  annotation = {Citation Count: 1}
}

@article{s.sahniDataStructUres2009,
  title = {Data {{Struct}} Ures},
  author = {S. Sahni and {Susan Anderson-Frees}},
  year = {2009},
  doi = {10.1007/978-3-540-89185-7_3},
  annotation = {Citation Count: 0}
}

@article{s.senDeletionRebalancingBinary2016,
  title = {Deletion {{Without Rebalancing}} in {{Binary Search Trees}}},
  author = {S. Sen and R. Tarjan and David H. K. Kim},
  year = {2016},
  journal = {ACM Trans. Algorithms},
  doi = {10.1145/2903142},
  abstract = {We address the vexing issue of deletions in balanced trees. Rebalancing after a deletion is generally more complicated than rebalancing after an insertion. Textbooks neglect deletion rebalancing, and many B-tree--based database systems do not do it. We describe a relaxation of AVL trees in which rebalancing is done after insertions but not after deletions, yet worst-case access time remains logarithmic in the number of insertions. For any application of balanced trees in which the number of updates is polynomial in the tree size, our structure offers performance competitive with that of classical balanced trees. With the addition of periodic rebuilding, the performance of our structure is theoretically superior to that of many, if not all, classic balanced tree structures. Our structure needs lg lg m+ 1 bits of balance information per node, where m is the number of insertions and lg is the base-two logarithm, or lg lg n+ O(1) with periodic rebuilding, where n is the number of nodes. An insertion takes up to two rotations and O(1) amortized time, not counting the time to find the insertion position. This is the same as in standard AVL trees. Using an analysis that relies on an exponential potential function, we show that rebalancing steps occur with a frequency that is exponentially small in the height of the affected node. Our techniques apply to other types of balanced trees, notably B-trees, as we show in a companion article, and particularly red-black trees, which can be viewed as a special case of B-trees.},
  keywords = {information,update},
  annotation = {Citation Count: 7}
}

@article{s.skienaAlgorithmDesignManual2020,
  title = {The {{Algorithm Design Manual}}},
  author = {S. Skiena},
  year = {2020},
  journal = {Texts in Computer Science},
  doi = {10.1007/978-3-030-54256-6},
  annotation = {Citation Count: 531}
}

@article{sadakaneCompressedSuffixTrees2007,
  title = {Compressed Suffix Trees with Full Functionality},
  author = {Sadakane, K.},
  year = {2007},
  doi = {10.1007/s00224-006-1198-x},
  abstract = {No abstract available},
  citationcount = {312},
  venue = {Theory of Computing Systems}
}

@article{sadakaneFullyFunctionalStatic2009,
  title = {Fully Functional Static and Dynamic Succinct Trees},
  author = {Sadakane, K. and Navarro, G.},
  year = {2009},
  doi = {10.1145/2601073},
  abstract = {We propose new succinct representations of ordinal trees and match various space/time lower bounds. It is known that any {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}-node static tree can be represented in 2{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {\textexclamdown}i{\textquestiondown}o{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) bits so that a number of operations on the tree can be supported in constant time under the word-RAM model. However, the data structures are complicated and difficult to dynamize. We propose a simple and flexible data structure, called the {\textexclamdown}i{\textquestiondown}range min-max tree{\textexclamdown}/i{\textquestiondown}, that reduces the large number of relevant tree operations considered in the literature to a few primitives that are carried out in constant time on polylog-sized trees. The result is extended to trees of arbitrary size, retaining constant time and reaching 2{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}/polylog({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown})) bits of space. This space is optimal for a core subset of the operations supported and significantly lower than in any previous proposal. For the dynamic case, where insertion/deletion (indels) of nodes is allowed, the existing data structures support a very limited set of operations. Our data structure builds on the range min-max tree to achieve 2{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}/log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) bits of space and {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) time for all operations supported in the static scenario, plus indels. We also propose an improved data structure using 2{\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}log log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}/log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) bits and improving the time to the optimal {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}/log log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) for most operations. We extend our support to forests, where whole subtrees can be attached to or detached from others, in time {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log{\textexclamdown}sup{\textquestiondown}1+{$\varepsilon$}{\textexclamdown}/sup{\textquestiondown} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) for any {$\varepsilon$} {\textquestiondown} 0. Such operations had not been considered before. Our techniques are of independent interest. An immediate derivation yields an improved solution to range minimum/maximum queries where consecutive elements differ by {\textpm} 1, achieving {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}/polylog({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown})) bits of space. A second one stores an array of numbers supporting operations {\textexclamdown}i{\textquestiondown}sum{\textexclamdown}/i{\textquestiondown} and {\textexclamdown}i{\textquestiondown}search{\textexclamdown}/i{\textquestiondown} and limited updates, in optimal time {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}/log log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}). A third one allows representing dynamic bitmaps and sequences over alphabets of size {$\sigma$}, supporting rank/select and indels, within zero-order entropy bounds and time {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} log {$\sigma$}/(log log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}){\textexclamdown}sup{\textquestiondown}2{\textexclamdown}/sup{\textquestiondown}) for all operations. This time is the optimal {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}/log log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) on bitmaps and polylog-sized alphabets. This improves upon the best existing bounds for entropy-bounded storage of dynamic sequences, compressed full-text self-indexes, and compressed-space construction of the Burrows-Wheeler transform.},
  citationcount = {204},
  venue = {TALG},
  keywords = {data structure,dynamic,lower bound,query,static,update}
}

@article{sadakaneFullyFunctionalSuccinct2010,
  title = {Fully-Functional Succinct Trees},
  author = {Sadakane, K. and Navarro, G.},
  year = {2010},
  doi = {10.1137/1.9781611973075.13},
  abstract = {We propose new succinct representations of ordinal trees, which have been studied extensively. It is known that any n-node static tree can be represented in 2n + o(n) bits and a large number of operations on the tree can be supported in constant time under the word-RAM model. However existing data structures are not satisfactory in both theory and practice because (1) the lower-order term is {\textohm}(n log log n/ log n), which cannot be neglected in practice, (2) the hidden constant is also large, (3) the data structures are complicated and difficult to implement, and (4) the techniques do not extend to dynamic trees supporting insertions and deletions of nodes. We propose a simple and flexible data structure, called the range min-max tree, that reduces the large number of relevant tree operations considered in the literature to a few primitives, which are carried out in constant time on sufficiently small trees. The result is then extended to trees of arbitrary size, achieving 2n + O(n/polylog(n)) bits of space. The redundancy is significantly lower than in any previous proposal, and the data structure is easily implemented. Furthermore, using the same framework, we derive the first fully-functional dynamic succinct trees.},
  citationcount = {136},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,dynamic,static}
}

@article{sadakaneSqueezingSuccinctData2006,
  title = {Squeezing Succinct Data Structures into Entropy Bounds},
  author = {Sadakane, K. and Grossi, R.},
  year = {2006},
  doi = {10.1145/1109557.1109693},
  abstract = {Consider a sequence {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown} of {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} symbols drawn from an alphabet {\textexclamdown}i{\textquestiondown}A{\textexclamdown}/i{\textquestiondown} = \{1, 2,. . .,{$\sigma$}\}, stored as a binary string of {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}log {$\sigma$} bits. A {\textexclamdown}i{\textquestiondown}succinct{\textexclamdown}/i{\textquestiondown} data structure on {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown} supports a given set of primitive operations on {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown} using just {\textexclamdown}i{\textquestiondown}f (n){\textexclamdown}/i{\textquestiondown} = {\textexclamdown}i{\textquestiondown}o{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} log {$\sigma$}) extra bits. We present a technique for transforming succinct data structures (which do not change the binary content of {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown}) into {\textexclamdown}i{\textquestiondown}compressed{\textexclamdown}/i{\textquestiondown} data structures using {\textexclamdown}i{\textquestiondown}nH{\textexclamdown}/i{\textquestiondown}{\textexclamdown}inf{\textquestiondown}{\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/inf{\textquestiondown} + {\textexclamdown}i{\textquestiondown}f(n){\textexclamdown}/i{\textquestiondown} + {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} log {$\sigma$} + log log{\textexclamdown}inf{\textquestiondown}{$\sigma$}{\textexclamdown}/inf{\textquestiondown} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} + {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown})/ log{\textexclamdown}inf{\textquestiondown}{$\sigma$}{\textexclamdown}/inf{\textquestiondown} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) bits of space, where {\textexclamdown}i{\textquestiondown}H{\textexclamdown}/i{\textquestiondown}{\textexclamdown}inf{\textquestiondown}{\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/inf{\textquestiondown} {$\leq$} log {$\sigma$} is the {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}th-order empirical entropy of {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown}. When {\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown} + log {$\sigma$} = o(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}), we improve the space complexity of the succinct data structure from {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} log {$\sigma$} + {\textexclamdown}i{\textquestiondown}o{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} log {$\sigma$}) to {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown} {\textexclamdown}i{\textquestiondown}H{\textexclamdown}/i{\textquestiondown}{\textexclamdown}inf{\textquestiondown}{\textexclamdown}i{\textquestiondown}k{\textexclamdown}/i{\textquestiondown}{\textexclamdown}/inf{\textquestiondown} + {\textexclamdown}i{\textquestiondown}o{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}log {$\sigma$}) bits by keeping {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown} in compressed format, so that any substring of {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log {$\sigma$} {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) symbols in {\textexclamdown}i{\textquestiondown}S{\textexclamdown}/i{\textquestiondown} (i.e. {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}(log {\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) bits) can be decoded on the fly in constant time. Thus, the time complexity of the supported operations does not change asymptotically. Namely, if an operation takes {\textexclamdown}i{\textquestiondown}t{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}) time in the succinct data structure, it requires {\textexclamdown}i{\textquestiondown}O{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}t{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown})) time in the resulting compressed data structure. Using this simple approach we improve the space complexity of some of the best known results on succinct data structures We extend our results to handle another definition of entropy.},
  citationcount = {134},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure}
}

@article{sadakaneSuccinctDataStructures2007,
  title = {Succinct Data Structures for Flexible Text Retrieval Systems},
  author = {Sadakane, K.},
  year = {2007},
  doi = {10.1016/j.jda.2006.03.011},
  abstract = {No abstract available},
  citationcount = {164},
  venue = {J. Discrete Algorithms}
}

@article{saglamOnTheCommunication2013,
  title = {On the Communication Complexity of Sparse Set Disjointness and Exists-Equal Problems},
  author = {Saglam, Mert and Tardos, G.},
  year = {2013},
  doi = {10.1109/FOCS.2013.78},
  abstract = {In this paper we study the two player randomized communication complexity of the sparse set disjointness and the exists-equal problems and give matching lower and upper bounds (up to constant factors) for any number of rounds for both of these problems. In the sparse set disjointness problem, each player receives a k-subset of [m] and the goal is to determine whether the sets intersect. For this problem, we give a protocol that communicates a total of O(k log(r) k) bits over r rounds and errs with very small probability. Here we can take r = log* k to obtain a O(k) total communication log* k-round protocol with exponentially small error probability, improving on the O(k)-bits O(log k)-round constant error probability protocol of Hastad and Wigderson from 1997. In the exists-equal problem, the players receive vectors x, y {$\in$} [t]n and the goal is to determine whether there exists a coordinate i such that xi = yi. Namely, the exists-equal problem is the OR of n equality problems. Observe that exists-equal is an instance of sparse set disjointness with k = n, hence the protocol above applies here as well, giving an O(n log(r) n) upper bound. Our main technical contribution in this paper is a matching lower bound: we show that when t = {\textohm}(n), any r-round randomized protocol for the exists-equal problem with error probability at most 1/3 should have a message of size {\textohm}(n log(r) n). Our lower bound holds even for super-constant r {$\leq$} log* n, showing that any O(n) bits exists-equal protocol should have log* n - O(1) rounds. Note that the protocol we give errs only with less than polynomially small probability and provides guarantees on the total communication for the harder set disjointness problem, whereas our lower bound holds even for constant error probability protocols and for the easier exists-equal problem with guarantees on the max-communication. Hence our upper and lower bounds match in a strong sense. Our lower bound on the constant round protocols for exist-sequal shows that solving the OR of n instances of the equality problems requires strictly more than n times the cost of a single instance. To our knowledge this is the first example of such a super-linear increase in complexity.},
  citationcount = {33},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {communication,communication complexity,lower bound}
}

@article{sahinalpDistanceBasedIndexing2003,
  title = {Distance Based Indexing for String Proximity Search},
  author = {Sahinalp, S. C. and Tasan, M. and Macker, J. and {\"O}zsoyoglu, Z. M.},
  year = {2003},
  doi = {10.1109/ICDE.2003.1260787},
  abstract = {In many database applications involving string data, it is common to have near neighbor queries (asking for strings that are similar to a query string) or nearest neighbor queries (asking for strings that are most similar to a query string). The similarity between strings is defined in terms of a distance function determined by the application domain. The most popular string distance measures are based on (a weighted) count of (i) character edit or (ii) block edit operations to transform one string into the other. Examples include the Levenshtein edit distance and the recently introduced compression distance. The main goal is to develop efficient near(est) neighbor search tools that work for both character and block edit distances. Our premise is that distance-based indexing methods, which are originally designed for metric distances can be modified for string distance measures, provided that they form almost metrics. We show that several distance measures, such as the compression distance and weighted character edit distance are almost metrics. In order to analyze the performance of distance based indexing methods (in particular VP trees) for strings, we then develop a model based on distribution of pairwise distances. Based on this model we show how to modify VP trees to improve their performance on string data, providing tradeoffs between search time and space. We test our theoretical results on synthetic data sets and protein strings.},
  citationcount = {67},
  venue = {Proceedings / International Conference on Data Engineering},
  keywords = {query}
}

@article{sahinalpHardnessOfString2004,
  title = {Hardness of String Similarity Search and Other Indexing Problems},
  author = {Sahinalp, S. C. and Utis, Andrey},
  year = {2004},
  doi = {10.1007/978-3-540-27836-8_90},
  abstract = {No abstract available},
  citationcount = {13},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{sairamAComplexityTheoretic1991,
  title = {A Complexity Theoretic Approach to Incremental Computation},
  author = {Sairam, S. and Vitter, J. and Tamassia, R.},
  year = {1991},
  doi = {10.1007/3-540-56503-5_63},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Symposium on Theoretical Aspects of Computer Science}
}

@article{sakaiMaximalCommonSubsequence2019,
  title = {Maximal Common Subsequence Algorithms},
  author = {Sakai, Y.},
  year = {2019},
  doi = {10.4230/LIPIcs.CPM.2018.1},
  abstract = {A common subsequence of two strings is maximal, if inserting any character into the subsequence can no longer yield a common subsequence of the two strings. The present article proposes a (sub)linearithmic-time, linear-space algorithm for finding a maximal common subsequence of two strings and also proposes a linear-time algorithm for determining if a common subsequence of two strings is maximal.},
  citationcount = {9},
  venue = {Annual Symposium on Combinatorial Pattern Matching}
}

@article{salamovPredictionOfProtein1995,
  title = {Prediction of Protein Secondary Structure by Combining Nearest-Neighbor Algorithms and Multiple Sequence Alignments.},
  author = {Salamov, A. and Solovyev, V.},
  year = {1995},
  doi = {10.1006/JMBI.1994.0116},
  abstract = {Recently Yi \& Lander used a neural network and nearest-neighbor method with a scoring system that combined a sequence-similarity matrix with the local structural environment scoring scheme described by Bowie and co-workers for predicting protein secondary structure. We have improved their scoring system by taking into consideration N and C-terminal positions of alpha-helices and beta-strands and also beta-turns as distinctive types of secondary structure. Another improvement, which also decreases the time of computation, is performed by restricting a data base with a smaller subset of proteins that are similar with a query sequence. Using multiple sequence alignments rather than single sequences and a simple jury decision procedure our method reaches a sustained overall three-state accuracy of 72.2},
  citationcount = {321},
  venue = {Journal of Molecular Biology}
}

@article{salehDynamicSubsetSum2022,
  title = {Dynamic Subset Sum with Truly Sublinear Processing Time},
  author = {Saleh, Hamed and Seddighin, Saeed},
  year = {2022},
  doi = {10.48550/arXiv.2209.04936},
  abstract = {Subset sum is a very old and fundamental problem in theoretical computer science. In this problem, n items with weights w{$_1$},w{$_2$},w{$_3$},{\dots},w\textsubscript{n} are given as input and the goal is to find out if there is a subset of them whose weights sum up to a given value t. While the problem is NP-hard in general, when the values are non-negative integer, subset sum can be solved in pseudo-polynomial time {\textasciitilde}(n+t). In this work, we consider the dynamic variant of subset sum. In this setting, an upper bound  is provided in advance to the algorithm and in each operation, either a new item is added to the problem or for a given integer value t{$\leq$}, the algorithm is required to output whether there is a subset of items whose sum of weights is equal to t. Unfortunately, none of the existing subset sum algorithms is able to process these operations in truly sublinear time\{Truly sublinear means n\vphantom\}\textsuperscript{\{\vphantom\}}1-{\textohm}(1)\vphantom\{\}.\vphantom\{\} in terms of . Our main contribution is an algorithm whose amortized processing time\{Since the runtimes are amortized, we do not use separate terms update time and query time for different operations and use processing time for all types of operations.\} for each operation is truly sublinear in  when the number of operations is at least \textsuperscript{\{\vphantom\}}2/3+{\textohm}(1)\vphantom\{\}. We also show that when both element addition and element removal are allowed, there is no algorithm that can process each operation in time \textsuperscript{\{\vphantom\}}1-{\textohm}(1)\vphantom\{\} on average unless \{SETH\}\{The \{strong exponential time hypothesis\} states that no algorithm can solve the satisfiability problem in time 2\vphantom\}\textsuperscript{\{\vphantom\}}n(1-{\textohm}(1))\vphantom\{\}.\vphantom\{\} fails.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {dynamic,query,query time,update,update time}
}

@article{salvadorDesignAndEvaluation2012,
  title = {Design and Evaluation of a Publish/Subscribe Framework for Ubiquitous Systems},
  author = {Salvador, Zigor and Lafuente, A. and Larrea, M.},
  year = {2012},
  doi = {10.1007/978-3-642-40238-8_5},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {International Conference on Mobile and Ubiquitous Systems: Networking and Services}
}

@article{salzbergComparisonOfAccess1999,
  title = {Comparison of Access Methods for Time-Evolving Data},
  author = {Salzberg, B. and Tsotras, V.},
  year = {1999},
  doi = {10.1145/319806.319816},
  abstract = {This paper compares different indexing techniques proposed for supporting efficient access to temporal data. The comparison is based on a collection of important performance criteria, including the space consumed, update processing, and query time for representative queries. The comparison is based on worst-case analysis, hence no assumptions on data distribution or query frequencies are made. When a number of methods have the same asymptotic worst-case behavior, features in the methods that affect average case behavior are discussed. Additional criteria examined are the pagination of an index, the ability to cluster related data together, and the ability to efficiently separate old from current data (so that larger archival storage media such as write-once optical disks can be used). The purpose of the paper is to identify the difficult problems in accessing temporal data and describe how the different methods aim to solve them. A general lower bound for answering basic temporal queries is also introduced.},
  citationcount = {383},
  venue = {CSUR}
}

@article{sandlundFasterDynamicRange2020,
  title = {Faster Dynamic Range Mode},
  author = {Sandlund, Bryce and Xu, Yinzhan},
  year = {2020},
  doi = {10.4230/LIPIcs.ICALP.2020.94},
  abstract = {In the dynamic range mode problem, we are given a sequence a of length bounded by N and asked to support element insertion, deletion, and queries for the most frequent element of a contiguous subsequence of a. In this work, we devise a deterministic data structure that handles each operation in worst-case O\vphantom\{\}(N\textsuperscript{\{\vphantom\}}0.655994\vphantom\{\}) time, thus breaking the O(N\textsuperscript{\{\vphantom\}}2/3\vphantom\{\}) per-operation time barrier for this problem. The data structure is achieved by combining the ideas in Williams and Xu (SODA 2020) for batch range mode with a novel data structure variant of the Min-Plus product.},
  citationcount = {6},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {data structure,dynamic,query}
}

@article{sankowskiDynamicTransitiveClosure2004,
  title = {Dynamic Transitive Closure via Dynamic Matrix Inverse: Extended Abstract},
  author = {Sankowski, P.},
  year = {2004},
  doi = {10.1109/FOCS.2004.25},
  abstract = {We consider dynamic evaluation of algebraic functions such as computing determinant, matrix adjoint, matrix inverse and solving linear system of equations. We show that in the dynamic setup the above problems can be solved faster than evaluating everything from scratch. In the case when rows and columns of the matrix can change we show an algorithm that achieves O(n/sup 2/) arithmetic operations per update and O(1) arithmetic operations per query. Next, we describe two algorithms, with different tradeoffs, for updating the inverse and determinant when single entries of the matrix are changed. The fastest update for the first tradeoff is O(n/sup 1.575/) arithmetic operations per update and O(n/sup 0.575/) arithmetic operations per query. The second tradeoff gives O(n/sup 1.495/) arithmetic operations per update and O(n/sup 1.495/) arithmetic operations per query. We also consider the case when some number of columns or rows can change. We use dynamic determinant computations to solve the following problems in the dynamic setup: computing the number of spanning trees in a graph and testing if an edge in a graph is contained in some perfect matching. These are the first dynamic algorithms for these problems. Next, with the use of dynamic matrix inverse, we solve fully dynamic transitive closure in general directed graphs. The bounds on arithmetic operations for dynamic matrix inverse translate directly to time bounds for dynamic transitive closure. Thus we obtain the first known algorithm with O(n/sup 2/) worst-case update time and constant query time and two algorithms for transitive closure in general digraphs with subquadratic update and query times. Our algorithms for transitive closure are randomized with one-sided error. We also consider for the first time the case when the edges incident with a part of vertices of the graph can be changed.},
  citationcount = {89},
  venue = {45th Annual IEEE Symposium on Foundations of Computer Science}
}

@article{sankowskiShortestPathsIn2005,
  title = {Shortest Paths in Matrix Multiplication Time},
  author = {Sankowski, P.},
  year = {2005},
  doi = {10.1007/11561071_68},
  abstract = {No abstract available},
  citationcount = {37},
  venue = {Embedded Systems and Applications}
}

@article{sankowskiSubquadraticAlgorithmFor2005,
  title = {Subquadratic Algorithm for Dynamic Shortest Distances},
  author = {Sankowski, P.},
  year = {2005},
  doi = {10.1007/11533719_47},
  abstract = {No abstract available},
  citationcount = {41},
  venue = {International Computing and Combinatorics Conference}
}

@article{santalIntegralGeometryAnd1976,
  title = {Integral Geometry and Geometric Probability},
  author = {Santal{\'o}, Llu{\'i}s},
  year = {1976},
  doi = {10.1017/CBO9780511617331.004},
  abstract = {Part I. Integral Geometry in the Plane: 1. Convex sets in the plane 2. Sets of points and Poisson processes in the plane 3. Sets of lines in the plane 4. Pairs of points and pairs of lines 5. Sets of strips in the plane 6. The group of motions in the plane: kinematic density 7. Fundamental formulas of Poincare and Blaschke 8. Lattices of figures Part II. General Integral Geometry: 9. Differential forms and Lie groups 10. Density and measure in homogenous spaces 11. The affine groups 12. The group of motions in En Part III. Integral Geometry in En: 13. Convex sets in En 14. Linear subspaces, convex sets and compact manifolds 15. The kinematic density in En 16. Geometric and statistical applications: stereology Part IV. Integral Geometry in Spaces of Constant Curvature: 17. Noneuclidean integral geometry 18. Crofton's formulas and the kinematic fundamental formula in noneuclidean spaces 19. Integral geometry and foliated spaces: trends in integral geometry.},
  citationcount = {1877},
  venue = {No venue available}
}

@article{santoroAnImprovedAlgorithm1986,
  title = {An Improved Algorithm for {{Boolean}} Matrix Multiplication},
  author = {Santoro, N. and Urrutia, J.},
  year = {1986},
  doi = {10.1007/BF02240211},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {Computing}
}

@article{sarielhar-peledSpaceExplorationProximity2014,
  title = {Space {{Exploration}} via {{Proximity Search}}},
  author = {{Sariel Har-Peled} and Nirman Kumar and D. Mount and Benjamin Raichel},
  year = {2014},
  journal = {Discrete \& Computational Geometry},
  doi = {10.1007/s00454-016-9801-7},
  annotation = {Citation Count: 8}
}

@article{savageAnAlgorithmFor1974,
  title = {An Algorithm for the Computation of Linear Forms},
  author = {Savage, J.},
  year = {1974},
  doi = {10.1137/0203011},
  abstract = {Many problems, including matrix-vector multiplication and polynomial evaluation, involve the computation of linear forms. An algorithm is presented here which offers a substantial improvement on the conventional algorithm for this problem when the coefficient set is small. In particular, this implies that every polynomial of degree n with at most s distinct coefficients can be realized with O(n/\textsubscript{s}n) operations. It is demonstrated that the algorithm is sharp for some problems.},
  citationcount = {46},
  venue = {SIAM journal on computing (Print)}
}

@article{sawlaniNearOptimalFully2019,
  title = {Near-Optimal Fully Dynamic Densest Subgraph},
  author = {Sawlani, Saurabh and Wang, Junxing},
  year = {2019},
  doi = {10.1145/3357713.3384327},
  abstract = {We give the first fully dynamic algorithm which maintains a (1-{\cyrchar\cyrie})-approximate densest subgraph in worst-case time poly(logn, {\cyrchar\cyrie}-1) per update. Dense subgraph discovery is an important primitive for many real-world applications such as community detection, link spam detection, distance query indexing, and computational biology. We approach the densest subgraph problem by framing its dual as a graph orientation problem, which we solve using an augmenting path-like adjustment technique. Our result improves upon the previous best approximation factor of (1/4 - {\cyrchar\cyrie}) for fully dynamic densest subgraph [Bhattacharya et. al., STOC `15]. We also extend our techniques to solving the problem on vertex-weighted graphs with similar runtimes. Additionally, we reduce the (1-{\cyrchar\cyrie})-approximate densest subgraph problem on directed graphs to O(logn/{\cyrchar\cyrie}) instances of (1-{\cyrchar\cyrie})-approximate densest subgraph on vertex-weighted graphs. This reduction, together with our algorithm for vertex-weighted graphs, gives the first fully-dynamic algorithm for directed densest subgraph in worst-case time poly(logn, {\cyrchar\cyrie}-1) per update. Moreover, combined with a near-linear time algorithm for densest subgraph [Bahmani et. al., WAW `14], this gives the first near-linear time algorithm for directed densest subgraph.},
  citationcount = {49},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,query,reduction,update}
}

@article{saxenaDominanceMadeSimple2009,
  title = {Dominance Made Simple},
  author = {Saxena, Sanjeev},
  year = {2009},
  doi = {10.1016/j.ipl.2008.12.006},
  abstract = {No abstract available},
  citationcount = {19},
  venue = {Information Processing Letters}
}

@article{schiffWileyInterscienceSeries2011,
  title = {Wiley-interscience Series in Discrete Mathematics and Optimization},
  author = {Schiff, J.},
  year = {2011},
  doi = {10.1002/9781118032381.SCARD},
  abstract = {No abstract available},
  citationcount = {400},
  venue = {No venue available}
}

@article{schmidtChernoffHoeffdingBounds1995,
  title = {Chernoff-{{Hoeffding}} Bounds for Applications with Limited Independence},
  author = {Schmidt, Jeanette P. and Siegel, A. and Srinivasan, A.},
  year = {1995},
  doi = {10.1137/S089548019223872X},
  abstract = {Chernoff-Hoeffding (CH) bounds are fundamental tools used in bounding the tail probabilities of the sums of bounded and independent random variables (r.v.'s). We present a simple technique that gives slightly better bounds than these and that more importantly requires only limited independence among the random variables, thereby importing a variety of standard results to the case of limited independence for free. Additional methods are also presented, and the aggregate results are sharp and provide a better understanding of the proof techniques behind these bounds. These results also yield improved bounds for various tail probability distributions and enable improved approximation algorithms for jobshop scheduling. The limited independence result implies that a reduced amount and weaker sources of randomness are sufficient for randomized algorithms whose analyses use the CH bounds, e.g., the analysis of randomized algorithms for random sampling and oblivious packet routing.},
  citationcount = {421},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{schmidtIrregularitiesOfDistribution1968,
  title = {Irregularities of Distribution},
  author = {Schmidt, W.},
  year = {1968},
  doi = {10.1093/QMATH/19.1.181},
  abstract = {No abstract available},
  citationcount = {79},
  venue = {No venue available}
}

@article{schmidtIrregularitiesOfDistribution1969,
  title = {Irregularities of Distribution. {{IV}}},
  author = {Schmidt, W.},
  year = {1969},
  doi = {10.1007/BF01418774},
  abstract = {No abstract available},
  citationcount = {33},
  venue = {No venue available}
}

@article{schmidtIrregularitiesOfDistribution1988,
  title = {{{IRREGULARITIES}} of {{DISTRIBUTION}} (Cambridge Tracts in Mathematics 89)},
  author = {Schmidt, W.},
  year = {1988},
  doi = {10.1112/BLMS/20.6.622},
  abstract = {No abstract available},
  citationcount = {73},
  venue = {No venue available}
}

@article{schmidtOnAspectsOf1989,
  title = {On Aspects of University and Performance for Closed Hashing},
  author = {Schmidt, Jeanette P. and Siegel, A.},
  year = {1989},
  doi = {10.1145/73007.73041},
  abstract = {We consider two hashing models for storing a set {\textexclamdown}italic{\textquestiondown}S{\textexclamdown}/italic{\textquestiondown} {$\subset$} \{0, 1, 2, {\dots}, {\textexclamdown}italic{\textquestiondown}m{\textexclamdown}/italic{\textquestiondown} - 1\} in a table {\textexclamdown}italic{\textquestiondown}T{\textexclamdown}/italic{\textquestiondown} of size {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}. The first model uses universal hashing for a partially loaded table. A set of hash functions is universal if, for any the input set, a randomly selected function has an efficient expected performance. Universal hash functions originate in [CW79], where they were used for open hashing using chaining. [CW79] poses as an open question whether comparable results can be achieved for any closed hashing schemes. The second model is perfect hashing for a full table. In preprocessing the input set is used to determine a hash function that achieves some desired performance criteria. This model was used among others in [ME82] and [FKS84]. In both models a key problem is to construct a ``small'' set of functions, which will permit a short description (program) for each function in the set. {\textexclamdown}list{\textquestiondown}{\textexclamdown}item{\textquestiondown}We show, for the first time, that universal hashing can be successfully used for closed hashing and in particular for double hashing. Specifically, the set of congruential polynomials of {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}(log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) degree is universal for double hashing if the table load is below .75; the program size (or number of random bits generated by the algorithm) is {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}(log log {\textexclamdown}italic{\textquestiondown}m{\textexclamdown}/italic{\textquestiondown} + log{\textexclamdown}supscrpt{\textquestiondown}2{\textexclamdown}/supscrpt{\textquestiondown} {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}). {\textexclamdown}/item{\textquestiondown}{\textexclamdown}/list{\textquestiondown}For perfect hashing, we obtain nearly tight results on the size of oblivious {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}(1)-probe hash functions:{\textexclamdown}list{\textquestiondown}{\textexclamdown}item{\textquestiondown}Oblivious {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}-probe hash functions require \&OHgr;({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}/{\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}2{\textexclamdown}/supscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}e{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}-{\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown} + log log {\textexclamdown}italic{\textquestiondown}m{\textexclamdown}/italic{\textquestiondown}) bits of description. {\textexclamdown}/item{\textquestiondown}{\textexclamdown}item{\textquestiondown}A probabilistic construction is presented, which shows that oblivious {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}-probe hash functions, can be specified in {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}ne{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}-{\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown} + log log {\textexclamdown}italic{\textquestiondown}m{\textexclamdown}/italic{\textquestiondown}) bits, which nearly matches the above lower bound. {\textexclamdown}/item{\textquestiondown}{\textexclamdown}item{\textquestiondown}We give a variation of an {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}(1) time 1-probe (perfect) hash function that can be specified in {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}({\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} + log log {\textexclamdown}italic{\textquestiondown}m{\textexclamdown}/italic{\textquestiondown}) bits, which is tight to within a constant factor of the lower bound. {\textexclamdown}/item{\textquestiondown}{\textexclamdown}/list{\textquestiondown} In view of the adaptive schemes presented in [FNSS88], these bounds establish a significant gap between oblivious and non-oblivious {\textexclamdown}italic{\textquestiondown}\&Ogr;{\textexclamdown}/italic{\textquestiondown}(1)-probe search.},
  citationcount = {14},
  venue = {Symposium on the Theory of Computing},
  keywords = {adaptive,lower bound}
}

@article{schmidtOnIrregularitiesOf1972,
  title = {On Irregularities of Distribution Vii},
  author = {Schmidt, W.},
  year = {1972},
  doi = {10.4064/AA-21-1-45-50},
  abstract = {No abstract available},
  citationcount = {263},
  venue = {No venue available}
}

@article{schoenbergOnCertainMetric1937,
  title = {On Certain Metric Spaces Arising from Euclidean Spaces by a Change of Metric and Their Imbedding in Hilbert Space},
  author = {Schoenberg, I. J.},
  year = {1937},
  doi = {10.2307/1968835},
  abstract = {1. W. A. Wilson ([9])2 has recently investigated those metric spaces which arise from a metric space by taking as its new metric a suitable (one variable) function of the old one. He considered in particular the euclidean straight line R, whose metric 6 = PP' is changed to A = d(P, P') = PP'S and showed that this new metric space can be imbedded3 in Hilbert space A). Here the old metric 6 and the new metric A are connected by the relation A2 = 6. In an article soon to appear ([5]), John von Neumann and the author have determined all the functions f(b) such that if R, is provided with the new metric A, defined by A = f(b), 6 = PP', the new metric space thus arising shall be imbeddable in A. They are of the form},
  citationcount = {227},
  venue = {No venue available}
}

@article{schnhageOnThePower1979,
  title = {On the Power of Random Access Machines},
  author = {Sch{\"o}nhage, A.},
  year = {1979},
  doi = {10.1007/3-540-09510-1_42},
  abstract = {No abstract available},
  citationcount = {122},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{senLowerBoundsPredecessor2003,
  title = {Lower Bounds for Predecessor Searching in the Cell Probe Model},
  author = {Sen, P. and Srinivasan, Venkatesh},
  year = {2003},
  doi = {10.1109/CCC.2003.1214411},
  abstract = {We consider a fundamental problem in data structures, static predecessor searching: Given a subset S of size n from the universe [m], store S so that queries of the form "What is the predecessor of x in S?" can be answered efficiently. We study this problem in the cell probe model introduced by Yao [1981]. Recently, Beame and Fich [2002] obtained optimal bounds on the number of probes needed by any deterministic query scheme if the associated storage scheme uses only n/sup O(1)/ cells of word size (log m)/sup O(1)/ bits. We give a new lower bound proof for this problem that matches the bounds of Beame and Fich. Our lower bound proof has the following advantages: it works for randomised query schemes too, while Beame and Fich's proof works for deterministic query schemes only. In addition, it is simpler than Beame and Fich's proof. We prove our lower bound using the round elimination approach of Miltersen, Nisan, Safra and Wigderson [1998]. Using tools from information theory, we prove a strong round elimination lemma for communication complexity that enables us to obtain a tight lower bound for the predecessor problem. We also use our round elimination lemma to obtain a rounds versus communication tradeoff for the 'greater-than' problem, improving on the tradeoff in [1998]. We believe that our round elimination lemma is of independent interest and should have other applications.},
  citationcount = {85},
  venue = {18th IEEE Annual Conference on Computational Complexity, 2003. Proceedings.},
  keywords = {cell probe,communication,communication complexity,data structure,lower bound,query,static}
}

@article{senLowerBoundsPredecessor2008,
  title = {Lower Bounds for Predecessor Searching in the Cell Probe Model},
  author = {Sen, Pranab and Venkatesh, S.},
  year = {2008},
  month = may,
  journal = {J. Comput. Syst. Sci.},
  volume = {74},
  number = {3},
  pages = {364--385},
  issn = {0022-0000},
  doi = {10.1016/j.jcss.2007.06.016},
  url = {https://doi.org/10.1016/j.jcss.2007.06.016},
  urldate = {2024-11-19},
  abstract = {We consider a fundamental problem in data structures, static predecessor searching: Given a subset S of size n from the universe [m], store S so that queries of the form ''What is the predecessor of x in S?'' can be answered efficiently. We study this problem in the cell probe model introduced by Yao [A.C.-C. Yao, Should tables be sorted, J. Assoc. Comput. Mach. 28 (3) (1981) 615-628]. Recently, Beame and Fich [P. Beame, F. Fich, Optimal bounds for the predecessor problem and related problems, J. Comput. System Sci. 65 (1) (2002) 38-72] obtained optimal bounds as functions of either m or n only on the number of probes needed by any deterministic query scheme if the associated storage scheme uses only n{\textasciicircum}O{\textasciicircum}({\textasciicircum}1{\textasciicircum}) cells of word size (logm){\textasciicircum}O{\textasciicircum}({\textasciicircum}1{\textasciicircum}) bits. We give a new lower bound proof for this problem that matches the bounds of Beame and Fich. Our lower bound proof has the following advantages: it works for randomised query schemes too, while Beame and Fich's proof works for deterministic query schemes only. In addition, it is simpler than Beame and Fich's proof. In fact, our lower bound for predecessor searching extends to the 'quantum address-only' query schemes that we define in this paper. In these query schemes, quantum parallelism is allowed only over the 'address lines' of the queries. These query schemes subsume classical randomised query schemes, and include many quantum query algorithms like Grover's algorithm [L. Grover, A fast quantum mechanical algorithm for database search, in: Proceedings of the 28th Annual ACM Symposium on Theory of Computing, 1996, pp. 212-219]. We prove our lower bound using the round elimination approach of Miltersen, Nisan, Safra and Wigderson [P. Bro Miltersen, Noam Nisan, S. Safra, A. Wigderson, On data structures and asymmetric communication complexity, J. Comput. System Sci. 57 (1) (1998) 37-49]. Using tools from information theory, we prove a strong round elimination lemma for communication complexity that enables us to obtain a tight lower bound for the predecessor problem. Our strong round elimination lemma also extends to quantum communication complexity. We also use our round elimination lemma to obtain a rounds versus communication tradeoff for the 'greater-than' problem, improving on the tradeoff in [P. Bro Miltersen, Noam Nisan, S. Safra, A. Wigderson, On data structures and asymmetric communication complexity, J. Comput. System Sci. 57 (1) (1998) 37-49]. We believe that our round elimination lemma is of independent interest and should have other applications.},
  keywords = {cell probe,communication,communication complexity,data structure,lower bound,query,static},
  file = {/Users/tulasi/Zotero/storage/YHY2CU47/Sen and Venkatesh - 2008 - Lower bounds for predecessor searching in the cell probe model.pdf}
}

@article{seoConstantRoundMulti2012,
  title = {Constant-Round Multi-Party Private Set Union Using Reversed Laurent Series},
  author = {Seo, Jae Hong and Cheon, J. and Katz, Jonathan},
  year = {2012},
  doi = {10.1007/978-3-642-30057-8_24},
  abstract = {No abstract available},
  citationcount = {30},
  venue = {International Conference on Theory and Practice of Public Key Cryptography}
}

@article{seroussiVectorSetsFor1988,
  title = {Vector Sets for Exhaustive Testing of Logic Circuits},
  author = {Seroussi, G. and Bshouty, N.},
  year = {1988},
  doi = {10.1109/18.6031},
  abstract = {(L, d)-universal sets are useful for exhaustively testing logic circuits with a large number of functional components, designed so that every functional component depends on at most d inputs. Randomized and deterministic constructions of (L, d)-universal test sets are presented, and lower and upper bounds on the optimal sizes of such sets are proven. It is also proven that the design of an optimal exhaustive test set for an arbitrary logic circuit is an NP-complete problem. {\textquestiondown}},
  citationcount = {186},
  venue = {IEEE Transactions on Information Theory}
}

@article{sethpettieApplicationsForbidden012010,
  title = {Applications of Forbidden 0-1 Matrices to Search Tree and Path Compression-Based Data Structures},
  author = {Seth Pettie},
  year = {2010},
  journal = {ACM-SIAM Symposium on Discrete Algorithms},
  doi = {10.1137/1.9781611973075.118},
  abstract = {In this paper we improve, reprove, and simplify several theorems on the performance of data structures based on path compression and search trees. We apply a technique very familiar to computational geometers but still foreign to many researchers in (non-geometric) algorithms and data structures, namely, to bound the complexity of an object via its forbidden substructures.  To analyze an algorithm or data structure in the forbidden substructure framework one proceeds in three discrete steps. First, one transcribes the behavior of the algorithm as some combinatorial object M; for example, M may be a graph, sequence, permutation, matrix, set system, or tree. (The size of M should ideally be linear in the running time.) Second, one shows that M excludes some forbidden substructure P, and third, one bounds the size of any object avoiding this substructure. The power of this framework derives from the fact that M lies in a more pristine environment and that upper bounds on the size of a P-free object M may be reused in different contexts.  Among our results, we present the first asymptotically sharp bound on the length of arbitrary path compressions on arbitrary trees, improving analyses of Tarjan [35] and Seidel and Sharir [31]. We reprove the linear bound on postordered path compressions, due to Lucas [23] and Loebel and Ne{\v s}et{\v r}il [22], the linear bound on deque-ordered path compressions, due to Buchsbaum, Sundar, and Tarjan [5], and the sequential access theorem for splay trees, originally due to Tarjan [38]. We disprove a conjecture of Aronov et al. [3] related to the efficiency of their data structure for half-plane proximity queries and provide a significantly cleaner analysis of their structure. With the exception of the sequential access theorem, all our proofs are exceptionally simple. Notably absent are calculations of any kind.},
  keywords = {data structure,query},
  annotation = {Citation Count: 30}
}

@article{sethpettieFasterAllPairsShortest2002,
  title = {A {{Faster All-Pairs Shortest Path Algorithm}} for {{Real-Weighted Sparse Graphs}}},
  author = {Seth Pettie},
  year = {2002},
  journal = {International Colloquium on Automata, Languages and Programming},
  doi = {10.1007/3-540-45465-9_9},
  annotation = {Citation Count: 31}
}

@article{sethpettieInverseAckermannStyleLower2002,
  title = {An Inverse-{{Ackermann}} Style Lower Bound for the Online Minimum Spanning Tree Verification Problem},
  author = {Seth Pettie},
  year = {2002},
  journal = {The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.},
  doi = {10.1109/SFCS.2002.1181892},
  abstract = {We consider the problem of preprocessing an edge-weighted tree T in order to quickly answer queries of the following type: does a given edge e belong in the minimum spanning tree of T /spl cup/ \{e\}? Whereas the offline minimum spanning tree verification problem admits a lovely linear time solution, we demonstrate an inherent inverse-Ackermann type tradeoff in the online MST verification problem. In particular, any scheme that answers queries in t comparisons must invest /spl Omega/(n log /spl lambda//sub t/ (n)) time preprocessing the tree, where /spl lambda//sub t/ is the inverse of the t/sup th/ row of Ackermann's function. This implies a query lower bound of /spl Omega/(/spl alpha/(n)) for the case of linear preprocessing time. We also show that our lower bound is tight to within a factor of 2 in the t parameter.},
  keywords = {lower bound,query},
  annotation = {Citation Count: 6}
}

@article{sethpettieInverseAckermannTypeLower2006,
  title = {An {{Inverse-Ackermann Type Lower Bound For Online Minimum Spanning Tree Verification}}*},
  author = {Seth Pettie},
  year = {2006},
  journal = {Comb.},
  doi = {10.1007/s00493-006-0014-1},
  keywords = {lower bound},
  annotation = {Citation Count: 22}
}

@article{sethpettieNewApproachAllpairs2004,
  title = {A New Approach to All-Pairs Shortest Paths on Real-Weighted Graphs},
  author = {Seth Pettie},
  year = {2004},
  journal = {Theoretical Computer Science},
  doi = {10.1016/S0304-3975(03)00402-X},
  annotation = {Citation Count: 185}
}

@article{sethpettieOptimalMinimumSpanning2000,
  title = {An Optimal Minimum Spanning Tree Algorithm},
  author = {Seth Pettie and V. Ramachandran},
  year = {2000},
  journal = {JACM},
  doi = {10.1145/505241.505243},
  abstract = {We establish that the algorithmic complexity of the minimumspanning tree problem is equal to its decision-tree complexity.Specifically, we present a deterministic algorithm to find aminimum spanning tree of a graph with \emph{n} vertices and\emph{m} edges that runs in time\emph{O}(\emph{T}\textsuperscript{*}(\emph{m,n})) where\emph{T}\textsuperscript{*} is the minimum number of edge-weightcomparisons needed to determine the solution. The algorithm isquite simple and can be implemented on a pointer machine.Althoughour time bound is optimal, the exact function describing it is notknown at present. The current best bounds known for\emph{T}\textsuperscript{*} are \emph{T}\textsuperscript{*}(\emph{m,n}) ={\textohm}(\emph{m}) and \emph{T}\textsuperscript{*}(\emph{m,n}) =\emph{O}(\emph{m} {$\bullet$} {$\alpha$}(\emph{m,n})), where {$\alpha$} is acertain natural inverse of Ackermann's function.Even under theassumption that \emph{T}\textsuperscript{*} is superlinear, we show thatif the input graph is selected from \emph{G}\textsubscript{\emph{n,m}},our algorithm runs in linear time with high probability, regardlessof \emph{n}, \emph{m}, or the permutation of edge weights. Theanalysis uses a new martingale for \emph{G}\textsubscript{\emph{n,m}}similar to the edge-exposure martingale for\emph{G}\textsubscript{\emph{n,p}}.},
  annotation = {Citation Count: 333}
}

@article{sethpettieSensitivityAnalysisMinimum2005,
  title = {Sensitivity {{Analysis}} of {{Minimum Spanning Trees}} in {{Sub-Inverse-Ackermann Time}}},
  author = {Seth Pettie},
  year = {2005},
  journal = {J. Graph Algorithms Appl.},
  doi = {10.7155/jgaa.00365},
  abstract = {We present a deterministic algorithm for computing the sensitivity of a minimum spanning tree or shortest path tree in O(m log {$\alpha$}(m,n)) time, where {$\alpha$} is the inverse-Ackermann function. This improves upon a long standing bound of O(m{$\alpha$}(m,n)) established by Tarjan. Our algorithms are based on an efficient split-findmin data structure, which maintains a collection of sequences of weighted elements that may be split into smaller subsequences. As far as we are aware, our split-findmin algorithm is the first with superlinear but sub-inverse-Ackermann complexity.},
  keywords = {data structure},
  annotation = {Citation Count: 36}
}

@article{sethpettieShortestPathAlgorithm2005,
  title = {A {{Shortest Path Algorithm}} for {{Real-Weighted Undirected Graphs}}},
  author = {Seth Pettie and V. Ramachandran},
  year = {2005},
  journal = {SIAM journal on computing (Print)},
  doi = {10.1137/S0097539702419650},
  abstract = {We present a new scheme for computing shortest paths on real-weighted undirected graphs in the fundamental comparison-addition model. In an efficient preprocessing phase our algorithm creates a linear-size structure that facilitates single-source shortest path computations in O(m log \${\textbackslash}alpha\$) time, where \${\textbackslash}alpha\$ = \${\textbackslash}alpha\$(m,n) is the very slowly growing inverse-Ackermann function, m the number of edges, and n the number of vertices. As special cases our algorithm implies new bounds on both the all-pairs and single-source shortest paths problems. We solve the all-pairs problem in O(mn log \${\textbackslash}alpha\$(m,n)) time and, if the ratio between the maximum and minimum edge lengths is bounded by n(log n)O(1), we can solve the single-source problem in O(m + n log log n) time. Both these results are theoretical improvements over Dijkstra's algorithm, which was the previous best for real weighted undirected graphs. Our algorithm takes the hierarchy-based approach invented by Thorup.},
  annotation = {Citation Count: 85}
}

@article{shahAmdbAVisual1999,
  title = {Amdb: A Visual Access Method Development Tool},
  author = {Shah, Mehul A. and Kornacker, Marcel and Hellerstein, J.},
  year = {1999},
  doi = {10.1109/UIDIS.1999.791469},
  abstract = {The development process for access methods (AMs) in database systems is complex and tedious. Amdb is a graphical tool that facilitates the design and tuning process for height-balanced tree-structured AMs. Central to amdb's user interface is a suite of graphical views that visualize the entire search tree, paths and subtrees within the tree, and data contained in the tree. These views animate search tree operations in order to visualize the behavior of an access method. Amdb provides metrics that characterize the performance of queries, the tree structure, and the structure-shaping aspects of an AM implementation. The visualizations can be used to browse the performance metrics in the context of the tree structure. The combination of these features allows a designer to locate the sources of performance loss reported by the metrics and investigate causes for those deficiencies.},
  citationcount = {18},
  venue = {Proceedings User Interfaces to Data Intensive Systems}
}

@article{shaileshhbojjavenkatakrishnanCostlyCircuitsSubmodular2016,
  title = {Costly Circuits, Submodular Schedules and Approximate {{Carath{\'e}odory Theorems}}},
  author = {Shaileshh Bojja Venkatakrishnan and Mohammad Alizadeh and P. Viswanath},
  year = {2016},
  journal = {Queueing systems},
  doi = {10.1007/s11134-017-9546-x},
  annotation = {Citation Count: 83}
}

@article{shakhnarovichFastPoseEstimation2003,
  title = {Fast Pose Estimation with Parameter-Sensitive Hashing},
  author = {Shakhnarovich, Gregory and Viola, Paul A. and Darrell, Trevor},
  year = {2003},
  doi = {10.1109/ICCV.2003.1238424},
  abstract = {Example-based methods are effective for parameter estimation problems when the underlying system is simple or the dimensionality of the input is low. For complex and high-dimensional problems such as pose estimation, the number of required examples and the computational complexity rapidly become prohibitively high. We introduce a new algorithm that learns a set of hashing functions that efficiently index examples relevant to a particular estimation task. Our algorithm extends locality-sensitive hashing, a recently developed method to find approximate neighbors in time sublinear in the number of examples. This method depends critically on the choice of hash functions that are optimally relevant to a particular estimation problem. Experiments demonstrate that the resulting algorithm, which we call parameter-sensitive hashing, can rapidly and accurately estimate the articulated pose of human figures from a large database of example images.},
  citationcount = {933},
  venue = {Proceedings Ninth IEEE International Conference on Computer Vision}
}

@article{shaltielTowardsProvingStrong2001,
  title = {Towards Proving Strong Direct Product Theorems},
  author = {Shaltiel, Ronen},
  year = {2001},
  doi = {10.1007/s00037-003-0175-x},
  abstract = {No abstract available},
  citationcount = {113},
  venue = {Proceedings 16th Annual IEEE Conference on Computational Complexity}
}

@article{shamosGeometricComplexity1975,
  title = {Geometric Complexity},
  author = {Shamos, M.},
  year = {1975},
  doi = {10.1145/800116.803772},
  abstract = {The complexity of a number of fundamental problems in computational geometry is examined and a number of new fast algorithms are presented and analyzed. General methods for obtaining results in geometric complexity are given and upper and lower bounds are obtained for problems involving sets of points, lines, and polygons in the plane. An effort is made to recast classical theorems into a useful computational form and analogies are developed between constructibility questions in Euclidean geometry and computability questions in modern computational complexity.},
  citationcount = {289},
  venue = {Symposium on the Theory of Computing}
}

@article{shantboodaghiansSmoothedComplexity2player2020,
  title = {Smoothed {{Complexity}} of 2-Player {{Nash Equilibria}}},
  author = {Shant Boodaghians and Joshua Brakensiek and Samuel B. Hopkins and A. Rubinstein},
  year = {2020},
  journal = {IEEE Annual Symposium on Foundations of Computer Science},
  doi = {10.1109/FOCS46700.2020.00034},
  abstract = {We prove that computing a Nash equilibrium of a two-player (\$n{\textbackslash}times n\$) game with payoffs in [-1, 1] is PPAD-hard (under randomized reductions) even in the smoothed analysis setting, smoothing with noise of constant magnitude. This gives a strong negative answer to conjectures of Spielman and Teng [ST06] and Cheng, Deng, and Teng [CDT09]. In contrast to prior work proving PPAD-hardness after smoothing by noise of magnitude \$1/{\textbackslash}text\{poly\}(n)\$ [CDT09], our smoothed complexity result is not proved via hardness of approximation for Nash equilibria. This is by necessity, since Nash equilibria can be approximated to constant error in quasi-polynomial time [LMM03]. Our results therefore separate smoothed complexity and hardness of approximation for Nash equilibria in two-player games. The key ingredient in our reduction is the use of a random zero-sum game as a gadget to produce two-player games which remain hard even after smoothing. Our analysis crucially shows that all Nash equilibria of random zero-sum games are far from pure (with high probability), and that this remains true even after smoothing.},
  keywords = {reduction},
  annotation = {Citation Count: 14}
}

@article{sherstovCommunicationComplexityUnder2008,
  title = {Communication Complexity under Product and Nonproduct Distributions},
  author = {Sherstov, Alexander A.},
  year = {2008},
  doi = {10.1007/s00037-009-0285-1},
  abstract = {No abstract available},
  citationcount = {29},
  venue = {2008 23rd Annual IEEE Conference on Computational Complexity}
}

@article{sherstovSeparatingAc0From2007,
  title = {Separating {{AC0}} from Depth-2 Majority Circuits},
  author = {Sherstov, Alexander A.},
  year = {2007},
  doi = {10.1145/1250790.1250834},
  abstract = {We prove that AC{\textexclamdown}sup{\textquestiondown}0{\textexclamdown}/sup{\textquestiondown} cannot be efficiently simulated by MAJMAJ circuits. Namely, we construct an AC{\textexclamdown}sup{\textquestiondown}0{\textexclamdown}/sup{\textquestiondown} circuit of depth 3 that requires MAJMAJ circuits of size 2{\textexclamdown}sup{\textquestiondown}{\textohm}(n{\textexclamdown}sup{\textquestiondown}1/5{\textexclamdown}/sup{\textquestiondown}){\textexclamdown}/sup{\textquestiondown}. This matches Allender's classic result that AC{\textexclamdown}sup{\textquestiondown}0{\textexclamdown}/sup{\textquestiondown} can be simulated by MAJMAJMAJ circuits of quasipolynomial size. Our proof is based on communication complexity. To obtain the above result, we develop a novel technique for communication lower bounds, the Degree/Discrepancy Theorem. This technique is a separate contribution of our paper. It translates lower bounds on the threshold degree of a Boolean function into upper bounds on the discrepancy of a related function. Upper bounds on the discrepancy, in turn, immediately imply communication lower bounds as well as lower bounds against threshold circuits. As part of our proof, we use the Degree/Discrepancy Theorem to obtain an explicit AC{\textexclamdown}sup{\textquestiondown}0{\textexclamdown}/sup{\textquestiondown} circuit of depth 3 that has discrepancy 2{\textexclamdown}sup{\textquestiondown}-{\textohm}(n{\textexclamdown}sup{\textquestiondown}1/5{\textexclamdown}/sup{\textquestiondown}){\textexclamdown}/sup{\textquestiondown}, under an explicit distribution. This yields the first known AC{\textexclamdown}sup{\textquestiondown}0{\textexclamdown}/sup{\textquestiondown} function with exponentially small discrepancy. Finally, we apply our work to learning theory, showing that polynomial-size DNF and CNF formulas have margin complexity 2{\textexclamdown}sup{\textquestiondown}{\textohm}(n{\textexclamdown}sup{\textquestiondown}1/5{\textexclamdown}/sup{\textquestiondown}){\textexclamdown}/sup{\textquestiondown}.},
  citationcount = {92},
  venue = {Symposium on the Theory of Computing}
}

@article{sherstovSeparatingac0FromDepth2009,
  title = {{{SeparatingAC0}} from Depth-2 Majority Circuits},
  author = {Sherstov, Alexander A.},
  year = {2009},
  doi = {10.1137/08071421X},
  abstract = {We construct a function in \{AC\}{$^0$} that cannot be computed by a depth-2 majority circuit of size less than ({$\Theta$}(n\textsuperscript{\{\vphantom\}}1/5\vphantom\{\})). This solves an open problem due to Krause and Pudlak [Theoret. Comput. Sci., 174 (1997), pp. 137-156] and matches Allender's classic result [A note on the power of threshold circuits, in Proceedings of the 30th Annual IEEE Symposium on Foundations of Computer Science (FOCS), Research Triangle Park, NC, 1989, pp. 580-584] that \{AC\}{$^0$} can be efficiently simulated by depth-3 majority circuits. To obtain our result, we develop a novel technique for proving lower bounds on communication complexity. This technique, the Degree/Discrepancy Theorem, is of independent interest. It translates lower bounds on the threshold degree of any Boolean function into upper bounds on the discrepancy of a related function. Upper bounds on the discrepancy, in turn, immediately imply lower bounds on communication and circuit size. In particular, we exhibit the first known function in \{AC\}{$^0$} with exponentially small discrepancy, (-{\textohm}(n\textsuperscript{\{\vphantom\}}1/5\vphantom\{\})), thereby establishing the separations {$\Sigma_{2}$}\textsuperscript{\{\vphantom\}}cc}{$\subseteq$}\{PP\}\textsuperscript{\{\vphantom\}}cc\vphantom\{\} and {$\Pi_{2}$}\textsuperscript{\{\vphantom\}}cc}{$\subseteq$}\{PP\}\textsuperscript{\{\vphantom\}}cc\vphantom\{\} in communication complexity.},
  citationcount = {51},
  venue = {SIAM journal on computing (Print)}
}

@article{sherstovTheCommunicationComplexity2012,
  title = {The Communication Complexity of Gap Hamming Distance},
  author = {Sherstov, Alexander A.},
  year = {2012},
  doi = {10.4086/toc.2012.v008a008},
  abstract = {In the gap Hamming distance problem, two parties must determine whether their respective strings x; y2f0; 1g n are at Hamming distance less than n=2 p n or greater than n=2+ p n: In a recent tour de force, Chakrabarti and Regev (2010) proved the long- conjectured W(n) lower bound on the randomized communication complexity of this problem. In follow-up work, Vidick (2010) discovered a simpler proof. We contribute a new proof, which is simpler yet and a page-and-a-half long.},
  citationcount = {78},
  venue = {Theory of Computing}
}

@article{shibuyaMatchChainingAlgorithms2003,
  title = {Match Chaining Algorithms for {{cDNA}} Mapping},
  author = {Shibuya, T. and Kurochkin, I.},
  year = {2003},
  doi = {10.1007/978-3-540-39763-2_33},
  abstract = {No abstract available},
  citationcount = {32},
  venue = {Workshop on Algorithms in Bioinformatics}
}

@article{shindeSimilaritySearchAnd2010,
  title = {Similarity Search and Locality Sensitive Hashing Using Ternary Content Addressable Memories},
  author = {Shinde, Rajendra and Goel, Ashish and Gupta, Pankaj and Dutta, Debojyoti},
  year = {2010},
  doi = {10.1145/1807167.1807209},
  abstract = {Similarity search methods are widely used as kernels in various data mining and machine learning applications including those in computational biology, web search/clustering. Nearest neighbor search (NNS) algorithms are often used to retrieve similar entries, given a query. While there exist efficient techniques for exact query lookup using hashing, similarity search using exact nearest neighbors suffers from a "curse of dimensionality", i.e. for high dimensional spaces, best known solutions offer little improvement over brute force search and thus are unsuitable for large scale streaming applications. Fast solutions to the approximate NNS problem include Locality Sensitive Hashing (LSH) based techniques, which need storage polynomial in n with exponent greater than 1, and query time sublinear, but still polynomial in n, where n is the size of the database. In this work we present a new technique of solving the approximate NNS problem in Euclidean space using a Ternary Content Addressable Memory (TCAM), which needs near linear space and has O(1) query time. In fact, this method also works around the best known lower bounds in the cell probe model for the query time using a data structure near linear in the size of the data base. TCAMs are high performance associative memories widely used in networking applications such as address lookups and access control lists. A TCAM can query for a bit vector within a database of ternary vectors, where every bit position represents 0, 1 or *. The * is a wild card representing either a 0 or a 1. We leverage TCAMs to design a variant of LSH, called Ternary Locality Sensitive Hashing (TLSH) wherein we hash database entries represented by vectors in the Euclidean space into \{0,1,*\}. By using the added functionality of a TLSH scheme with respect to the * character, we solve an instance of the approximate nearest neighbor problem with 1 TCAM access and storage nearly linear in the size of the database. We validate our claims with extensive simulations using both real world (Wikipedia) as well as synthetic (but illustrative) datasets. We observe that using a TCAM of width 288 bits, it is possible to solve the approximate NNS problem on a database of size 1 million points with high accuracy. Finally, we design an experiment with TCAMs within an enterprise ethernet switch (Cisco Catalyst 4500) to validate that TLSH can be used to perform 1.5 million queries per second per 1Gb/s port. We believe that this work can open new avenues in very high speed data mining.},
  citationcount = {42},
  venue = {SIGMOD Conference},
  keywords = {cell probe,data structure,lower bound,query,query time}
}

@article{shinNeighborhoodPropertybasedPattern2007,
  title = {Neighborhood {{PropertyBased}} Pattern Selection for Support Vector Machines},
  author = {Shin, Hyunjung and Cho, Sungzoon},
  year = {2007},
  doi = {10.1162/neco.2007.19.3.816},
  abstract = {The support vector machine (SVM) has been spotlighted in the machine learning community because of its theoretical soundness and practical performance. When applied to a large data set, however, it requires a large memory and a long time for training. To cope with the practical difficulty, we propose a pattern selection algorithm based on neighborhood properties. The idea is to select only the patterns that are likely to be located near the decision boundary. Those patterns are expected to be more informative than the randomly selected patterns. The experimental results provide promising evidence that it is possible to successfully employ the proposed algorithm ahead of SVM training.},
  citationcount = {59},
  venue = {Neural Computation}
}

@article{shiOptimalAndNear2005,
  title = {Optimal and Near-Optimal Algorithms for Generalized Intersection Reporting on Pointer Machines},
  author = {Shi, Qingmin and J{\'a}J{\'a}, J.},
  year = {2005},
  doi = {10.1016/j.ipl.2005.04.008},
  abstract = {No abstract available},
  citationcount = {26},
  venue = {Information Processing Letters}
}

@article{shoupAComputationalIntroduction2008,
  title = {A Computational Introduction to Number Theory and Algebra: {{Quadratic}} Reciprocity and Computing Modular Square Roots},
  author = {Shoup, V.},
  year = {2008},
  doi = {10.1017/CBO9780511814549.014},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {No venue available}
}

@article{shoupEfficientComputationOf1999,
  title = {Efficient Computation of Minimal Polynomials in Algebraic Extensions of Finite Fields},
  author = {Shoup, V.},
  year = {1999},
  doi = {10.1145/309831.309859},
  abstract = {New algorithnls iire prcsmtcd for coniput,iug the minimit polyn0min.l over a finit,e field li of a given element in xi : lgel{\textquestiondown}r;tii: cxt.ension of Ii of t.he form Ji[o] or Ii[tr][/j]. The Iicw illg0rithIus arc explicit and can be iniplenieiit ctl rather easily iu tcrnis of pOlyIlOIllii l multiplicat,ioli, and are much iiiorc cfficieut than ot her algorithuis in the literature.},
  citationcount = {72},
  venue = {International Symposium on Symbolic and Algebraic Computation}
}

@article{shoupFastConstructionOf1994,
  title = {Fast Construction of Irreducible Polynomials over Finite Fields},
  author = {Shoup, V.},
  year = {1994},
  doi = {10.1006/jsco.1994.1025},
  abstract = {The main result of this paper a new algorithm for constructing an irreducible polynomial of specified degree n over a finite field Fq . The algorithm is probabilistic, and is asymptotically faster than previously known algorithms for this problem. It uses an expected number of O-(n2 + n log q) operations in Fq, where the "soft-O" O- indicates an implicit factor of (log n )O(1). In addition, two new polynomial irreducibility tests are described.},
  citationcount = {161},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{shoupLowerBoundsFor1991,
  title = {Lower Bounds for Polynomial Evaluation and Interpolation Problems},
  author = {Shoup, V. and Smolensky, R.},
  year = {1991},
  doi = {10.1007/BF01270384},
  abstract = {No abstract available},
  citationcount = {44},
  venue = {[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science}
}

@article{shpilkaArithmeticCircuitsSurvey2010,
  title = {Arithmetic {{Circuits}}: {{A}} Survey of Recent Results and Open Questions},
  author = {Shpilka, Amir and Yehudayoff, A.},
  year = {2010},
  doi = {10.1561/0400000039},
  abstract = {A large class of problems in symbolic computation can be expressed as the task of computing some polynomials; and arithmetic circuits form the most standard model for studying the complexity of such computations. This algebraic model of computation attracted a large amount of research in the last five decades, partially due to its simplicity and elegance. Being a more structured model than Boolean circuits, one could hope that the fundamental problems of theoretical computer science, such as separating P from NP, will be easier to solve for arithmetic circuits. However, in spite of the appearing simplicity and the vast amount of mathematical tools available, no major breakthrough has been seen. In fact, all the fundamental questions are still open for this model as well. Nevertheless, there has been a lot of progress in the area and beautiful results have been found, some in the last few years. As examples we mention the connection between polynomial identity testing and lower bounds of Kabanets and Impagliazzo, the lower bounds of Raz for multilinear formulas, and two new approaches for proving lower bounds: Geometric Complexity Theory and Elusive Functions. The goal of this monograph is to survey the field of arithmetic circuit complexity, focusing mainly on what we find to be the most interesting and accessible research directions. We aim to cover the main results and techniques, with an emphasis on works from the last two decades. In particular, we discuss the recent lower bounds for multilinear circuits and formulas, the advances in the question of deterministically checking polynomial identities, and the results regarding reconstruction of arithmetic circuits. We do, however, also cover part of the classical works on arithmetic circuits. In order to keep this monograph at a reasonable length, we do not give full proofs of most theorems, but rather try to convey the main ideas behind each proof and demonstrate it, where possible, by proving some special cases.},
  citationcount = {445},
  venue = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
  keywords = {lower bound}
}

@article{siddharthbarmanEdgeworthConjectureSmall2019,
  title = {The {{Edgeworth Conjecture}} with {{Small Coalitions}} and {{Approximate Equilibria}} in {{Large Economies}}},
  author = {Siddharth Barman and F. Echenique},
  year = {2019},
  journal = {ACM Conference on Economics and Computation},
  doi = {10.1145/3391403.3399481},
  abstract = {We revisit the connection between bargaining and equilibrium in exchange economies, and study its algorithmic implications. We consider bargaining outcomes to be allocations that cannot be blocked (i.e., profitably re-traded) by coalitions of small size and show that these allocations must be approximate Walrasian equilibria. Our results imply that deciding whether an allocation is approximately Walrasian can be done in polynomial time, even in economies for which finding an equilibrium is known to be computationally hard.},
  annotation = {Citation Count: 2}
}

@article{siegelOnUniversalClasses1989,
  title = {On Universal Classes of Fast High Performance Hash Functions, Their Time-Space Tradeoff, and Their Applications},
  author = {Siegel, A.},
  year = {1989},
  doi = {10.1109/SFCS.1989.63450},
  abstract = {A mechanism is provided for constructing log-n-wise-independent hash functions that can be evaluated in O(1) time. A probabilistic argument shows that for fixed epsilon {\textexclamdown}1, a table of n/sup epsilon / random words can be accessed by a small O(1)-time program to compute one important family of hash functions. An explicit algorithm for such a family, which achieves comparable performance for all practical purposes, is also given. A lower bound shows that such a program must take Omega (k/ epsilon ) time, and a probabilistic arguments shows that programs can run in O(k/sup 2// epsilon /sup 2/) time. An immediate consequence of these constructions is that double hashing using these universal functions has (constant factor) optimal performance in time, for suitably moderate loads. Another consequence is that a T-time PRAM algorithm for n log n processors (and n/sup k/ memory) can be emulated on an n-processor machine interconnected by an n*log n Omega network with a multiplicative penalty for total work that, with high probability, is only O(1).<<ETX>>},
  citationcount = {119},
  venue = {30th Annual Symposium on Foundations of Computer Science},
  keywords = {lower bound,time-space}
}

@article{siegelOnUniversalClasses1995,
  title = {On Universal Classes of Extremely Random Constant-Time Hash Functions},
  author = {Siegel, A.},
  year = {1995},
  doi = {10.1137/S0097539701386216},
  abstract = {A family of functions F that map [0,n]- 1, that can be evaluated in constant time for the standard random access model of computation. Simple extensions give comparable behavior for larger domains. As a consequence, many probabilistic algorithms can for the first time be shown to achieve their expected asymptotic performance for a feasible model of computation. This paper also establishes a tight tradeoff in the number of random seeds that must be precomputed for a random function that runs in time T and is h-wise independent.},
  citationcount = {121},
  venue = {SIAM journal on computing (Print)}
}

@article{silvaCyclicSpacesFor1994,
  title = {Cyclic Spaces for Grassmann Derivatives and Additive Theory},
  author = {{da Silva}, J. D. and Hamidoune, Y. O.},
  year = {1994},
  doi = {10.1112/BLMS/26.2.140},
  abstract = {No abstract available},
  citationcount = {240},
  venue = {No venue available}
}

@article{simoviciPerceptrons2021,
  title = {Perceptrons},
  author = {Simovici, Dan A.},
  year = {2021},
  doi = {10.1007/BFb0027023},
  abstract = {No abstract available},
  citationcount = {742},
  venue = {Artificial Neural Networks}
}

@article{sinhaLocalDecodabilityOf2018,
  title = {Local Decodability of the {{Burrows-Wheeler}} Transform},
  author = {Sinha, S. and Weinstein, Omri},
  year = {2018},
  doi = {10.1145/3313276.3316317},
  abstract = {The Burrows-Wheeler Transform (BWT) is among the most influential discoveries in text compression and DNA storage. It is a reversible preprocessing step that rearranges an n-letter string into runs of identical characters (by exploiting context regularities), resulting in highly compressible strings, and is the basis of the bzip compression program. Alas, the decoding process of BWT is inherently sequential and requires {\textohm}(n) time even to retrieve a single character. We study the succinct data structure problem of locally decoding short substrings of a given text under its compressed BWT, i.e., with small additive redundancy r over the Move-To-Front (bzip) compression. The celebrated BWT-based FM-index (FOCS '00), as well as other related literature, yield a trade-off of r={\~O}(n/{\textsurd}t) bits, when a single character is to be decoded in O(t) time. We give a near-quadratic improvement r={\~O}(nlg(t)/t). As a by-product, we obtain an exponential (in t) improvement on the redundancy of the FM-index for counting pattern-matches on compressed text. In the interesting regime where the text compresses to o(n) (say, n/polylg(n)) bits, these results provide an exp(t) overall space reduction. For the local decoding problem of BWT, we also prove an {\textohm}(n/t2) cell-probe lower bound for ``symmetric'' data structures. We achieve our main result by designing a compressed partial-sums (Rank) data structure over BWT. The key component is a locally-decodable Move-to-Front (MTF) code: with only O(1) extra bits per block of length n{\textohm}(1), the decoding time of a single character can be decreased from {\textohm}(n) to O(lgn). This result is of independent interest in algorithmic information theory.},
  citationcount = {2},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {cell probe,data structure,lower bound,reduction}
}

@article{sinhaLowerBoundsFor2017,
  title = {Lower Bounds for Approximating the Matching Polytope},
  author = {Sinha, Makrand},
  year = {2017},
  doi = {10.1137/1.9781611975031.104},
  abstract = {We prove that any extended formulation that approximates the matching polytope on n-vertex graphs up to a factor of (1+{$\varepsilon$}) for any {$^2$}{\textfractionsolidus}\textsubscript{n}{$\leq\varepsilon\leq$}1 must have at least \{n\}\{\{{$\alpha$}\}/\{{$\varepsilon$}\}\} defining inequalities where 0{$<\alpha<$}1 is an absolute constant. This is tight as exhibited by the (1+{$\varepsilon$}) approximating linear program obtained by dropping the odd set constraints of size larger than (\{1+{$\varepsilon$}\})/\{{$\varepsilon$}\} from the description of the matching polytope. Previously, a tight lower bound of 2\textsuperscript{\{\vphantom\}}{\textohm}(n)\vphantom\{\} was only known for {$\varepsilon$}=O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_{1}$}\vphantom\{\}\{n\}) [Rothvoss, STOC '14; Braun and Pokutta, IEEE Trans. Information Theory '15] whereas for {$^2$}{\textfractionsolidus}\textsubscript{n}{$\leq\varepsilon\leq$}1, the best lower bound was 2\textsuperscript{\{\vphantom\}}{\textohm}(\{1\}/\{{$\varepsilon$}\})\vphantom\{\} [Rothvoss, STOC '14]. The key new ingredient in our proof is a close connection to the non-negative rank of a lopsided version of the unique disjointness matrix.},
  citationcount = {9},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {lower bound}
}

@article{sleatorADataStructure1981,
  title = {A Data Structure for Dynamic Trees},
  author = {Sleator, D. and Tarjan, R.},
  year = {1981},
  doi = {10.1145/800076.802464},
  abstract = {We propose a data structure to maintain a collection of vertex-disjoint trees under a sequence of two kinds of operations: a link operation that combines two trees into one by adding an edge, and a cut operation that divides one tree into two by deleting an edge. Our data structure requires O(log n) time per operation when the time is amortized over a sequence of operations. Using our data structure, we obtain new fast algorithms for the following problems: (1) Computing deepest common ancestors. (2) Solving various network flow problems including finding maximum flows, blocking flows, and acyclic flows. (3) Computing certain kinds of constrained minimum spanning trees. (4) Implementing the network simplex algorithm for the transshipment problem. Our most significant application is (2); we obtain an O(mn log n)-time algorithm to find a maximum flow in a network of n vertices and m edges, beating by a factor of log n the fastest algorithm previously known for sparse graphs.},
  citationcount = {1244},
  venue = {Symposium on the Theory of Computing}
}

@article{smeuldersImageDatabasesAnd1998,
  title = {Image Databases and Multi-Media Search},
  author = {Smeulders, A. and Jain, R.},
  year = {1998},
  doi = {10.1142/3656},
  abstract = {From the Publisher: Where Lycos and AltaVista are already accepted tools for textual information, image databases and multi-media search engines are the natural answers in the quest for pictorial information. This book provides a state-of-the-art description of that field. It contains the proceedings of a valuable workshop in Amsterdam, where people gathered to discuss the progress in the field. The topics cover computational methods of searching for pictures, the powerful pictorial clues in the recognition of objects, storage and indexing of objects in a database, and, ways to access the requested pictorial information.},
  citationcount = {112},
  venue = {Image Databases and Multi-Media Search}
}

@article{smidMaintainingRangeTrees1990,
  title = {Maintaining Range Trees in Secondary Memory},
  author = {Smid, M. and Overmars, M.},
  year = {1990},
  doi = {10.1007/BF00289019},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {Acta Informatica}
}

@article{smolenskyAlgebraicMethodsIn1987,
  title = {Algebraic Methods in the Theory of Lower Bounds for {{Boolean}} Circuit Complexity},
  author = {Smolensky, R.},
  year = {1987},
  doi = {10.1145/28395.28404},
  abstract = {We use algebraic methods to get lower bounds for complexity of different functions based on constant depth unbounded fan-in circuits with the given set of basic operations. In particular, we prove that depth k circuits with gates NOT, OR and MODp where p is a prime require Exp(\&Ogr;(n1/2k)) gates to calculate MODr functions for any r {$\neq$} pm. This statement contains as special cases Yao's PARITY result [ Ya 85 ] and Razborov's new MAJORITY result [Ra 86] (MODm gate is an oracle which outputs zero, if the number of ones is divisible by m).},
  citationcount = {906},
  venue = {Symposium on the Theory of Computing}
}

@article{smythCombinatorialAlgorithms2012,
  title = {Combinatorial Algorithms},
  author = {Smyth, S.},
  year = {2012},
  doi = {10.1007/978-3-642-35926-2},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {Lecture Notes in Computer Science}
}

@article{socolinskyIlluminationInvariantFace2001,
  title = {Illumination Invariant Face Recognition Using Thermal Infrared Imagery},
  author = {Socolinsky, Diego A. and Wolff, L. B. and Neuheisel, Joshua D. and Eveland, Christopher K.},
  year = {2001},
  doi = {10.1109/CVPR.2001.990519},
  abstract = {A key problem for face recognition has been accurate identification under variable illumination conditions. Conventional video cameras sense reflected light so that image grayvalues are a product of both intrinsic skin reflectivity and external incident illumination, thus obfuscating the intrinsic reflectivity of skin. Thermal emission from skin, on the other hand, is an intrinsic measurement that can be isolated from external illumination. We examine the invariance of Long-Wave InfraRed (LWIR) imagery with respect to different illumination conditions from the viewpoint of performance comparisons of two well-known face recognition algorithms applied to LWIR and visible imagery. We develop rigourous data collection protocols that formalize face recognition analysis for computer vision in the thermal IR.},
  citationcount = {191},
  venue = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001}
}

@article{sokolovDagLikeCommunication2017,
  title = {Dag-like Communication and Its Applications},
  author = {Sokolov, Dmitry},
  year = {2017},
  doi = {10.1007/978-3-319-58747-9_26},
  abstract = {No abstract available},
  citationcount = {33},
  venue = {Computer Science Symposium in Russia}
}

@article{sommerDistanceOraclesSparse2009,
  title = {Distance Oracles for Sparse Graphs},
  author = {Sommer, Christian and Verbin, Elad and Yu, Wei},
  year = {2009},
  doi = {10.1109/FOCS.2009.27},
  abstract = {Thorup and Zwick, in their seminal work, introduced the approximate distance oracle, which is a data structure that answers distance queries in a graph. For any integer k, they showed an efficient algorithm to construct an approximate distance oracle using space O(kn{\textasciicircum}\{1+1/k\}) that can answer queries in time O(k) with a distance estimate that is at most 2k-1 times larger than the actual shortest distance (this ratio is called the stretch).They proved that, under a combinatorial conjecture, their data structure is optimal in terms of space: if a stretch of at most 2k-1 is desired, then the space complexity is at least n{\textasciicircum}\{1+1/k\}. Their proof holds even if infinite query time is allowed: it is essentially an "incompressibility" result. Also, the proof only holds for dense graphs, and the best bound it can prove only implies that the size of the data structure is lower bounded by the number of edges of the graph. Naturally, the following question arises: what happens for sparse graphs? In this paper we give a new lower bound for approximate distance oracles in the cell-probe model. This lower bound holds even for sparse (polylog(n)-degree) graphs, and it is not an "incompressibility" bound: we prove a three-way tradeoff between space, stretch, and query time. We show that when the query time is t and the stretch is a, then the space S must be at least n{\textasciicircum}\{1+1/Omega(a*t)\} / lg n. This lower bound follows by a reduction from lopsided set disjointness to distance oracles, based on and motivated by recent work of Patrascu. Our results in fact show that for any high-girth regular graph, an approximate distance oracle that supports efficient queries for all subgraphs of G must obey this tradeoff. We also prove some lemmas that count sets of paths in high-girth regular graphs and high-girth regular expanders, which might be of independent interest.},
  citationcount = {66},
  venue = {2009 50th Annual IEEE Symposium on Foundations of Computer Science},
  keywords = {cell probe,data structure,lower bound,query,query time,reduction}
}

@article{sommerShortestPathQueries2014,
  title = {Shortest-Path Queries in Static Networks},
  author = {Sommer, Christian},
  year = {2014},
  doi = {10.1145/2530531},
  abstract = {We consider the point-to-point (approximate) shortest-path query problem, which is the following generalization of the classical single-source (SSSP) and all-pairs shortest-path (APSP) problems: we are first presented with a network (graph). A so-called preprocessing algorithm may compute certain information (a data structure or index) to prepare for the next phase. After this preprocessing step, applications may ask shortest-path or distance queries, which should be answered as fast as possible. Due to its many applications in areas such as transportation, networking, and social science, this problem has been considered by researchers from various communities (sometimes under different names): algorithm engineers construct fast route planning methods; database and information systems researchers investigate materialization tradeoffs, query processing on spatial networks, and reachability queries; and theoretical computer scientists analyze distance oracles and sparse spanners. Related problems are considered for compact routing and distance labeling schemes in networking and distributed computing and for metric embeddings in geometry as well. In this survey, we review selected approaches, algorithms, and results on shortest-path queries from these fields, with the main focus lying on the tradeoff between the index size and the query time. We survey methods for general graphs as well as specialized methods for restricted graph classes, in particular for those classes with arguable practical significance such as planar graphs and complex networks.},
  citationcount = {241},
  venue = {ACM Computing Surveys},
  keywords = {data structure,query,query time,static}
}

@article{soMomentInequalitiesFor2011,
  title = {Moment Inequalities for Sums of Random Matrices and Their Applications in Optimization},
  author = {So, A. M.},
  year = {2011},
  doi = {10.1007/s10107-009-0330-5},
  abstract = {No abstract available},
  citationcount = {80},
  venue = {Mathematical programming}
}

@article{songSketchingMeetsDifferential2022,
  title = {Sketching Meets Differential Privacy: {{Fast}} Algorithm for Dynamic Kronecker Projection Maintenance},
  author = {Song, Zhao and Yang, Xin and Yang, Yuanyuan and Zhang, Licheng},
  year = {2022},
  doi = {10.48550/arXiv.2210.11542},
  abstract = {Projection maintenance is one of the core data structure tasks. Efficient data structures for projection maintenance have led to recent breakthroughs in many convex programming algorithms. In this work, we further extend this framework to the Kronecker product structure. Given a constraint matrix \{A\} and a positive semi-definite matrix W{$\in$}\{R\}\textsuperscript{\{\vphantom\}}n{\texttimes}n\vphantom\{\} with a sparse eigenbasis, we consider the task of maintaining the projection in the form of \{B\}\textsuperscript{{$\top$}}(\{B\}\{B\}\textsuperscript{{$\top$}})\textsuperscript{\{\vphantom\}}-1\vphantom\{\}\{B\}, where \{B\}=\{A\}(W{$\otimes$}I) or \{B\}=\{A\}(W\textsuperscript{\{\vphantom\}}1/2\vphantom\{\}{$\otimes$}W\textsuperscript{\{\vphantom\}}1/2\vphantom\{\}). At each iteration, the weight matrix W receives a low rank change and we receive a new vector h. The goal is to maintain the projection matrix and answer the query \{B\}\textsuperscript{{$\top$}}(\{B\}\{B\}\textsuperscript{{$\top$}})\textsuperscript{\{\vphantom\}}-1\vphantom\{\}\{B\}h with good approximation guarantees. We design a fast dynamic data structure for this task and it is robust against an adaptive adversary. Following the beautiful and pioneering work of [Beimel, Kaplan, Mansour, Nissim, Saranurak and Stemmer, STOC'22], we use tools from differential privacy to reduce the randomness required by the data structure and further improve the running time.},
  citationcount = {22},
  venue = {International Conference on Machine Learning},
  keywords = {adaptive,data structure,dynamic,query}
}

@article{sossPreprocessingChainsFor2002,
  title = {Preprocessing Chains for Fast Dihedral Rotations Is Hard or Even Impossible},
  author = {Soss, Michael A. and Erickson, Jeff and Overmars, M.},
  year = {2002},
  doi = {10.1016/S0925-7721(02)00156-6},
  abstract = {No abstract available},
  citationcount = {37},
  venue = {Computational geometry}
}

@article{spalding-jamiesonScalableKMeans2025,
  title = {Scalable K-{{Means}} Clustering for Large k via Seeded Approximate Nearest-Neighbor Search},
  author = {{Spalding-Jamieson}, Jack and Robson, E. W. and Zheng, Da Wei},
  year = {2025},
  doi = {10.48550/arXiv.2502.06163},
  abstract = {For very large values of k, we consider methods for fast k-means clustering of massive datasets with 10{$^{7}\sim$}10{$^9$} points in high-dimensions (d{$\geq$}100). All current practical methods for this problem have runtimes at least {\textohm}(k{$^2$}). We find that initialization routines are not a bottleneck for this case. Instead, it is critical to improve the speed of Lloyd's local-search algorithm, particularly the step that reassigns points to their closest center. Attempting to improve this step naturally leads us to leverage approximate nearest-neighbor search methods, although this alone is not enough to be practical. Instead, we propose a family of problems we call"Seeded Approximate Nearest-Neighbor Search", for which we propose"Seeded Search-Graph"methods as a solution.},
  citationcount = {Unknown},
  venue = {arXiv.org}
}

@article{spencerProbabilisticMethodsIn1974,
  title = {Probabilistic Methods in Combinatorics},
  author = {Spencer, J.},
  year = {1974},
  doi = {10.1007/978-3-0348-9078-6_132},
  abstract = {No abstract available},
  citationcount = {490},
  venue = {No venue available}
}

@article{spencerSixStandardDeviations1985,
  title = {Six Standard Deviations Suffice},
  author = {Spencer, J.},
  year = {1985},
  doi = {10.1090/S0002-9947-1985-0784009-0},
  abstract = {Given n sets on n elements it is shown that there exists a two-coloring such that all sets have discrepancy at most Knl/2, K an absolute constant. This improves the basic probabilistic method with which K = c(ln n)l/2. The result is extended to n finite sets of arbitrary size. Probabilistic techniques are melded with the pigeonhole principle. An alternate proof of the existence of Rudin-Shapiro functions is given, showing that they are exponential in number. Given n linear forms in n variables with all coefficients in [-1, + 1] it is shown that initial values Pl. .P,, E (0,1\vphantom\{\} may be approximated by el,...,{\pounds},, E \{0,1\} so that the forms have small error. 1. We state our main result first in the language of linear forms. THEOREM 1. Let (1.1) Li(xl,...,x,l) = ailxl + *Xe + ai,txn, 1 {\textexclamdown} i {\textexclamdown} n, be n linear forms in n variables with all laijl {\textexclamdown} 1. Then there exist e1, . . ., Esl E \{ -1 + 1 \} such that (1.2) {\textbar}Li(El,. ,E")l {\textexclamdown} KS for all i, 1 {\textexclamdown} i {\textexclamdown} n. Here K is an absolute constant. When all aij E \{0,1\} we may consider e1 = (aij) as the incidence matrix for a family of n sets on n elements. That is, we may set Ai= \{ j: aij= 1\}. Given a two-coloring, say Red and Blue, of \{1,...,n\}, the discrepancy disc(X) of a set X c \{1, . . . ,n \} is defined as the number of Red points in X minus the number of Blue points in X. If we interpret si = + 1 as meaning i is to be colored Red and ej = -1 as Blue then we obtain the following result. COROLLARY 2. Let Al,...,An c \{1,...,n\}. Then there exists a two-coloring of \{1,...,n\} so that (1.3) {\textbar}disc( Aj){\textbar} {\textexclamdown} KW foralli, 1 {\textexclamdown} i {\textexclamdown} n. Received by the editors July 1, 1984. 1980 Muthemcltics Subject Clclssific(ltion. Primary 05B20; Secondary 41A28, 42A61. ' This work was initiated while the author was an IREX exchange fellow at the Mathematical Institute, Budapest and completed with support of the National Science Foundation. For the many kindnesses and for the creative research environment provided by Institute staff and colleagues-koszonom szepen! t1985 American Mathematical Society 0002-9947/85 1.00+.25 per page},
  citationcount = {311},
  venue = {No venue available}
}

@article{spielmanLinearTimeEncodable1995,
  title = {Linear-Time Encodable and Decodable Error-Correcting Codes},
  author = {Spielman, D.},
  year = {1995},
  doi = {10.1145/225058.225165},
  abstract = {Ab,stract-We present a new class of asymptotically good, linear error-correcting codes. These codes can be both encoded and decoded in linear time. They can also be encoded by logarithmicdepth circuits of linear size and decoded by logarithmic depth circuits of size 0 (n log n) . We present both randomized and explicit constructions of these codes. Zndex Terms- Asymptotically good error-correcting code, linear-time, expander graph, superconcentrators.},
  citationcount = {441},
  venue = {Symposium on the Theory of Computing}
}

@article{stanleyMiscellaneousGemsOf2013,
  title = {Miscellaneous Gems of Algebraic Combinatorics},
  author = {Stanley, R.},
  year = {2013},
  doi = {10.1007/978-1-4614-6998-8_12},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {No venue available}
}

@article{stephenalstrupDominatorsLinearTime1999,
  title = {Dominators in {{Linear Time}}},
  author = {Stephen Alstrup and D. Harel and Peter W. Lauridsen and M. Thorup},
  year = {1999},
  journal = {SIAM journal on computing (Print)},
  doi = {10.1137/S0097539797317263},
  abstract = {A linear time algorithm is presented for (cid:12)nding dominators in control (cid:13)ow graphs.},
  annotation = {Citation Count: 149}
}

@article{stephenalstrupImprovedAlgorithmsFinding2000,
  title = {Improved {{Algorithms}} for {{Finding Level Ancestors}} in {{Dynamic Trees}}},
  author = {Stephen Alstrup and J. Holm},
  year = {2000},
  journal = {International Colloquium on Automata, Languages and Programming},
  doi = {10.1007/3-540-45022-X_8},
  keywords = {dynamic},
  annotation = {Citation Count: 74}
}

@article{stephenalstrupMaintainingCenterMedian2000,
  title = {Maintaining {{Center}} and {{Median}} in {{Dynamic Trees}}},
  author = {Stephen Alstrup and J. Holm and M. Thorup},
  year = {2000},
  journal = {Scandinavian Workshop on Algorithm Theory},
  doi = {10.1007/3-540-44985-X_6},
  keywords = {dynamic},
  annotation = {Citation Count: 33}
}

@article{stephenalstrupNewDataStructures2000,
  title = {New Data Structures for Orthogonal Range Searching},
  author = {Stephen Alstrup and G. Brodal and Theis Rauhe},
  year = {2000},
  journal = {Proceedings 41st Annual Symposium on Foundations of Computer Science},
  doi = {10.1109/SFCS.2000.892088},
  abstract = {We present new general techniques for static orthogonal range searching problems in two and higher dimensions. For the general range reporting problem in R/sup 3/, we achieve query time O(log n+k) using space O(n log/sup 1+/spl epsiv// n), where n denotes the number of stored points and k the number of points to be reported. For the range reporting problem on an n/spl times/n grid, we achieve query time O(log log n+k) using space O(n log/sup /spl epsiv// n). For the two-dimensional semi-group range sum problem we achieve query time O(log n) using space O(n log n).},
  annotation = {Citation Count: 175}
}

@article{stephenalstrupOptimalPointerAlgorithms1996,
  title = {Optimal {{Pointer Algorithms}} for {{Finding Nearest Common Ancestors}} in {{Dynamic Trees}}},
  author = {Stephen Alstrup and M. Thorup},
  year = {1996},
  journal = {Scandinavian Workshop on Algorithm Theory},
  doi = {10.1007/3-540-61422-2_133},
  keywords = {dynamic},
  annotation = {Citation Count: 20}
}

@article{stephenalstrupWorstcaseAmortisedOptimality1999,
  title = {Worst-Case and Amortised Optimality in Union-Find (Extended Abstract)},
  author = {Stephen Alstrup and {Amir M. Ben-Amram} and Theis Rauhe},
  year = {1999},
  journal = {Symposium on the Theory of Computing},
  doi = {10.1145/301250.301383},
  abstract = {We study the interplay between worst-case and amortised time bounds for the classic Disjoint Set Union problem (Union-Find). We ask whether it is possible to achieve optimal worst-case and amortised bounds simultaneously. Furthermore we would like to allow a tradeoff between the worst-case time for a query and for an update. We answer this question by first providing lower bounds for the possible worst-case time tradeoffs, as well as lower bounds which show where in this tradeoff range optimal amortised time is achievable. We then give an algorithm which tightly matches both lower bounds simultaneously. The lower bounds are provided in the cell-probe model as well as in the algebraic real-number RAM, and the upper bounds hold for a RAM with logarithmic word size and a modest instruction set. Our lower bounds show that for worst-case query and update time t, and t, respectively, one must have t, = a(log n/ log t,), and only for t, 2 cx(m, n) can this tradeoff be achieved simultaneously with the optimal amortised time of o(c{\textasciitilde}(m,n)). Our `DIK'', Camp. sci. Dept., ````ivorsity of Copenhagen, Denmark. E-mail: slephe''mdik''.dk. Part of thin work W\&S done while ``isiting BRIGS and Lund University. Q,pyngh, ACM ,999 1.58113.067.8199105...{\textasciitilde}5.00 algorithm can match this tradeoff for any desired choice of t,. For faster worst-case query time, the amortised cost becomes larger but is also matched by our lower bounds,},
  keywords = {cell probe,lower bound,query,query time,update,update time},
  annotation = {Citation Count: 21}
}

@article{stolarskyDiscrepancyAndSums1975,
  title = {Discrepancy and Sums of Distances between Points of a Metric Space},
  author = {Stolarsky, K.},
  year = {1975},
  doi = {10.1007/BFB0081130},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {No venue available}
}

@article{stolarskySphericalDistributionsOf1975,
  title = {Spherical Distributions of {{N}} Points with Maximal Distance Sums Are Well Spaced},
  author = {Stolarsky, K.},
  year = {1975},
  doi = {10.1090/S0002-9939-1975-0365363-X},
  abstract = {It is shown that if N points are placed on the unit sphere in Euclidean 3-space so that the sum of the distances which they determine is maximal, then the distance between any two points is at least 2/3N. Results for sums of Ath powers of distances are also given. The following problem is open: How should one place N points p1, * PN on the surface of the unit sphere U in E3 (i.e. on x + y + z ) so that the sum of the distances which they determine is maximal? Many closely related problems have been studied; see for example [1]-[10]. Here we show that the points must be "well spaced" in the sense that no two can be closer than a distance of 2/3N. In fact, we will show a bit more. Let lpi Pjl denote the Euclidean distance between pi and pi. Theorem. Let 0 [4/A(2 +X)] 1/(2-A)N1/(2-X). We can improve the constant here, but it is more important to improve the exponent of N. Since min 1Pi pjl {\textquestiondown}N-"2 for 0{\textexclamdown}A{\textexclamdown} 2. It is interesting to observe that spherical distributions of points with maximal spherical distance sums are not necessarily well spaced [6], [8]. For the proof of the Theorem let PI, I * * X PN maximize the sum Received by the editors November 20, 1973 and, in revised form, February 12, 1974. AMS (MOS) subject classifications (1970). Primary 52A25, 52A40. Copyright ( 1975, American Mathematical Society},
  citationcount = {21},
  venue = {No venue available}
}

@article{stolarskySumsOfDistances1972,
  title = {Sums of Distances between Points on a Sphere. {{II}}},
  author = {Stolarsky, K.},
  year = {1972},
  doi = {10.1090/S0002-9939-1972-0303418-3},
  abstract = {ABSTRACr. Given N points on a unit sphere in Euclidean m space, m \_ 2, we show that the sum of all distances which they determine plus their discrepancy is a constant. As applications we obtain (i) an upper bound for the sum of the distances which for m\_5 is smaller than any previously known and (ii) the existence of N point distributions with small discrepancy. We make use of W. M. Schmidt's work on the discrepancy of spherical caps.},
  citationcount = {114},
  venue = {No venue available}
}

@article{strassenAlgebraicComplexityTheory1991,
  title = {Algebraic Complexity Theory},
  author = {Strassen, V.},
  year = {1991},
  doi = {10.1016/B978-0-444-88071-0.50016-3},
  abstract = {No abstract available},
  citationcount = {919},
  venue = {Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity}
}

@article{strassenDieBerechnungskomplexittVon1973,
  title = {Die {{Berechnungskomplexit{\"a}t}} von Elementarsymmetrischen {{Funktionen}} Und von {{Interpolationskoeffizienten}}},
  author = {Strassen, Volker},
  year = {1973},
  doi = {10.1007/BF01436566},
  abstract = {No abstract available},
  citationcount = {150},
  venue = {No venue available}
}

@article{strassenGaussianEliminationIs1969,
  title = {Gaussian Elimination Is Not Optimal},
  author = {Strassen, V.},
  year = {1969},
  doi = {10.1007/BF02165411},
  abstract = {No abstract available},
  citationcount = {2638},
  venue = {No venue available}
}

@article{sumigawaEnumeratingRangeModes2019,
  title = {Enumerating Range Modes},
  author = {Sumigawa, Kentaro and Chakraborty, Sankardeep and Sadakane, K. and Satti, S. R.},
  year = {2019},
  doi = {10.4230/LIPIcs.ISAAC.2020.29},
  abstract = {We consider the range mode problem where given a sequence and a query range in it, we want to find items with maximum frequency in the range. We give time- and space- efficient algorithms for this problem. Our algorithms are efficient for small maximum frequency cases. We also consider a natural generalization of the problem: the range mode enumeration problem, for which there has been no known efficient algorithms. Our algorithms have query time complexities which is linear to the output size plus small terms.},
  citationcount = {1},
  venue = {International Symposium on Algorithms and Computation},
  keywords = {query,query time}
}

@article{sumiyoshiFasterAndSimpler2024,
  title = {Faster and Simpler Online/Sliding Rightmost {{Lempel-Ziv}} Factorizations},
  author = {Sumiyoshi, Wataru and Mieno, Takuya and Inenaga, Shunsuke},
  year = {2024},
  doi = {10.48550/arXiv.2408.03008},
  abstract = {We tackle the problems of computing the rightmost variant of the Lempel-Ziv factorizations in the online/sliding model. Previous best bounds for this problem are O(n log n) time with O(n) space, due to Amir et al. [IPL 2002] for the online model, and due to Larsson [CPM 2014] for the sliding model. In this paper, we present faster O(n log n/log log n)-time solutions to both of the online/sliding models. Our algorithms are built on a simple data structure named BP-linked trees, and on a slightly improved version of the range minimum/maximum query (RmQ/RMQ) data structure on a dynamic list of integers. We also present other applications of our algorithms.},
  citationcount = {2},
  venue = {SPIRE},
  keywords = {data structure,dynamic,query}
}

@article{sundarALowerBound1991,
  title = {A Lower Bound for the Dictionary Problem under a Hashing Model},
  author = {Sundar, R.},
  year = {1991},
  doi = {10.1109/SFCS.1991.185427},
  abstract = {A fundamental open question in data structures concerns the existence of a dictionary data structure that processes the operations in constant amortized time and uses space polynomial in the dictionary size. The complexity of the dictionary problem is studied under a multilevel hashing model that is based on A.C. Yao's (1981) cell probe model, and it is proved that dictionary operations require log-algorithmic amortized time jn this model. The model encompasses many known solutions to the dictionary problem, and the result is the first nontrivial lower bound for the problem in a reasonably general model that takes into account the limited wordsize of memory locations and realistically measures the cost of update operations. This lower bound separates the deterministic and randomized complexities of the problem under this model.<<ETX>>},
  citationcount = {12},
  venue = {[1991] Proceedings 32nd Annual Symposium of Foundations of Computer Science},
  keywords = {cell probe,data structure,lower bound,update}
}

@article{sunSrsSolvingC2014,
  title = {{{SRS}}: {{Solving}} c-{{Approximate}} Nearest Neighbor Queries in High Dimensional Euclidean Space with a Tiny Index},
  author = {Sun, Yifang and Wang, Wei and Qin, Jianbin and Zhang, Ying and Lin, Xuemin},
  year = {2014},
  doi = {10.14778/2735461.2735462},
  abstract = {Nearest neighbor searches in high-dimensional space have many important applications in domains such as data mining, and multimedia databases. The problem is challenging due to the phenomenon called "curse of dimensionality". An alternative solution is to consider algorithms that returns a c-approximate nearest neighbor (c-ANN) with guaranteed probabilities. Locality Sensitive Hashing (LSH) is among the most widely adopted method, and it achieves high efficiency both in theory and practice. However, it is known to require an extremely high amount of space for indexing, hence limiting its scalability. In this paper, we propose several surprisingly simple methods to answer c-ANN queries with theoretical guarantees requiring only a single tiny index. Our methods are highly flexible and support a variety of functionalities, such as finding the exact nearest neighbor with any given probability. In the experiment, our methods demonstrate superior performance against the state-of-the-art LSH-based methods, and scale up well to 1 billion high-dimensional points on a single commodity PC.},
  citationcount = {140},
  venue = {Proceedings of the VLDB Endowment},
  keywords = {query}
}

@article{sutherlandOnTheEvaluation2012,
  title = {On the Evaluation of Modular Polynomials},
  author = {Sutherland, Andrew V.},
  year = {2012},
  doi = {10.2140/obs.2013.1.531},
  abstract = {We present two algorithms that, given a prime ell and an elliptic curve E/Fq, directly compute the polynomial Phi\_ell(j(E),Y) in Fq[Y] whose roots are the j-invariants of the elliptic curves that are ell-isogenous to E. We do not assume that the modular polynomial Phi\_ell(X,Y) is given. The algorithms may be adapted to handle other types of modular polynomials, and we consider applications to point counting and the computation of endomorphism rings. We demonstrate the practical efficiency of the algorithms by setting a new point-counting record, modulo a prime q with more than 5,000 decimal digits, and by evaluating a modular polynomial of level ell = 100,019.},
  citationcount = {39},
  venue = {IACR Cryptology ePrint Archive}
}

@article{syeds.aliBacktracking2001,
  title = {Backtracking},
  author = {Syed S. Ali},
  year = {2001},
  journal = {Science},
  doi = {10.1145/378116.378128},
  abstract = {We describe algorithmic techniques and data structures that have been proposed to solve variants of the set union problem in which it is possible to backtrack through one or more of the most recent unions. Their discovery required a new set of algorithmic tools that have proved useful in other areas as well. An attempt is made to provide a unifying theoretical framework for this growing body of algorithms.},
  keywords = {data structure},
  annotation = {Citation Count: 9}
}

@article{t.cormenIntroductionAlgorithms1991,
  title = {Introduction to {{Algorithms}}},
  author = {T. Cormen and C. E. Leiserson and R. L. Rivest},
  year = {1991},
  journal = {Introduction to Computational Thinking},
  doi = {10.1016/B978-0-12-416743-8.00001-4},
  annotation = {Citation Count: 6785}
}

@article{t.kavithaMaxcoloringPathsTight2009,
  title = {Max-Coloring Paths: Tight Bounds and Extensions},
  author = {T. Kavitha and Juli{\'a}n Mestre},
  year = {2009},
  journal = {International Symposium on Algorithms and Computation},
  doi = {10.1007/978-3-642-10631-6_11},
  annotation = {Citation Count: 15}
}

@article{t.kavithaMaxcoloringPathsTight2010,
  title = {Max-Coloring Paths: Tight Bounds and Extensions},
  author = {T. Kavitha and Juli{\'a}n Mestre},
  year = {2010},
  journal = {Journal of combinatorial optimization},
  doi = {10.1007/s10878-010-9290-1},
  annotation = {Citation Count: 1}
}

@article{t.lamPowerParallelPointer1989,
  title = {The Power of Parallel Pointer Manipulation},
  author = {T. Lam and W. L. Ruzzo},
  year = {1989},
  journal = {ACM Symposium on Parallelism in Algorithms and Architectures},
  doi = {10.1145/72935.72946},
  abstract = {An HMM is a very simple parallel machine consisting of finite s tate devices that can manipulate pointers to each other. The more commonly studied P R A M is a much richer parallel machine with a shared global memory and ar i thmetic capabilities. We define a natural restriction of the P R A M that is shown to be equivalent to the HMM to within a constant factor in bo th t ime and hardware simultaneously. Basically, the restricted PRAM is a Concurrent Read, Owner Write (CROW) PRAM, stripped of ar i thmetic capabilities, except for successor (+1) a doubling (x2) operations. 1 I n t r o d u c t i o n and S u m m a r y of R e s u l t s Many sequential Mgorithms spend the bulk of their t ime doing pointer manipula t ion, as opposed to, say, ar i thmetic operations. The Storage Modification Machine (SMM) or Pointer Machine is a formal model that captures the notion of computat ion by pointer manipulat ion. Considerable in*Part of this research performed while the second author visited the EECS Department, UC San Diego, whose hospitality is gratefully acknowledged, as is the financiM support provided to the first author by an IBM Graduate Fellowship, and to both authors by NSF grants ECS-8306622, and CCR-8703196. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of {\textasciitilde}he Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. {\copyright} 1989 ACM 0-89791-323-X/89 /0006 /0092 \$1.50 tui t ion for the power of such machines is provided by Schhnhage's demonst ra t ion of the equivalence of SMM's and unit-cost successor RAM's, i.e. ordinary unit cost RAM's str ipped of all arithmetic capabilities except for the successor, or +1 operation [16]. (See also [17, 9, 8, 12, 1].) Various kinds of parallel random access machines or PRAM's are popular and convenient as models of parallel computers. PRAM's consist of a col lection of ordinary sequential RAM's with shared access to a common global memory. Like their sequential counterparts , many PRAM algorithms spend a considerable proport ion of their t ime manipulat ing pointers in global memory, indeed{\textasciitilde} since interprocessor communicat ion is so fundamental to most parallel algorithms, pointer manipulation in PRAM's may be even more pervasive than in RAM's. The notion of parMlel computat ion by pointer manipula t ion is formally captured by the Hardware Modification Machine (HMM), introduced by Cook and studied by Dymond and Cook [2, 3, 6]. In this paper we prove tile equivalence of HMM's and a restricted version of the PRAM, which is s tr ipped of ar i thmetic capabilities except for the successor (+1) and doubling (x2) operations, and which observes a certMn very naturM restriction on writes to global memory, the turnerwrite restriction, deta{\textasciitilde}iled below. Our simulations show that both t ime and hardware resources of the two models, simultaneously, are the same to within a constant factor. This is an unusually tight correspondence between two parallel models. Indeed, we are aware of no other similarly t ight correspondence between parallel models that are more than},
  annotation = {Citation Count: 22}
}

@article{takagiPackedCompactTries2016,
  title = {Packed Compact Tries: A Fast and Efficient Data Structure for Online String Processing},
  author = {Takagi, Takuya and Inenaga, Shunsuke and Sadakane, K. and Arimura, Hiroki},
  year = {2016},
  doi = {10.1587/transfun.E100.A.1785},
  abstract = {We present a new data structure called the packed compact trie (packed c-trie) which stores a set S of k strings of total length n in n{$\sigma$}+O(kn) bits of space and supports fast pattern matching queries and updates, where {$\sigma$} is the alphabet size. Assume that {$\alpha$}=\textsubscript{{$\sigma$}}n letters are packed in a single machine word on the standard word RAM model, and let f(k, n) denote the query and update times of the dynamic predecessor/successor data structure of our choice which stores k integers from universe [1, n] in O(kn) bits of space. Then, given a string of length m, our packed c-tries support pattern matching queries and insert/delete operations in O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{m}\vphantom\{\}\{{$\alpha$}\}f(k,n)) worst-case time and in O(\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{m}\vphantom\{\}\{{$\alpha$}\}+f(k,n)) expected time. Our experiments show that our packed c-tries are faster than the standard compact tries (a.k.a. Patricia trees) on real data sets. We also discuss applications of our packed c-tries.},
  citationcount = {21},
  venue = {International Workshop on Combinatorial Algorithms},
  keywords = {data structure,dynamic,query,update,update time}
}

@article{talagrandTheSmallBall1994,
  title = {The Small Ball Problem for the Brownian Sheet},
  author = {Talagrand, M.},
  year = {1994},
  doi = {10.1214/AOP/1176988605},
  abstract = {We show that the logarithm of the probability that the Brownian sheet has a supremum at most e over [0, 1] 2 is of order e -2 (log(1/e)) 3},
  citationcount = {97},
  venue = {No venue available}
}

@article{tamassiaDynamicMaintenanceOf1990,
  title = {Dynamic Maintenance of Planar Digraphs, with Applications},
  author = {Tamassia, R. and Preparata, F.},
  year = {1990},
  doi = {10.1007/BF01840401},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {Algorithmica},
  keywords = {dynamic}
}

@article{tamassiaOnLinePlanar1996,
  title = {On-Line Planar Graph Embedding},
  author = {Tamassia, R.},
  year = {1996},
  doi = {10.1006/jagm.1996.0044},
  abstract = {We present a dynamic data structure for the incremental construction of a planar embedding of a planar graph. The data structure supports the following operations: (i) testing if a new edge can be added to the embedding without introducing crossing; and (ii) adding vertices and edges. The time complexity of each operation isO(logn) (amortized for edge insertion), and the memory space and preprocessing time areO(n), wherenis the current number of vertices of the graph.},
  citationcount = {26},
  venue = {J. Algorithms},
  keywords = {data structure,dynamic}
}

@article{tamonLearningWithThe2008,
  title = {Learning with the Aid of an Oracle},
  author = {Tamon, C.},
  year = {2008},
  doi = {10.1007/978-0-387-30162-4_193},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Encyclopedia of Algorithms}
}

@article{taoADynamicI2012,
  title = {A Dynamic {{I}}/{{O-efficient}} Structure for One-Dimensional Top-k Range Reporting},
  author = {Tao, Yufei},
  year = {2012},
  doi = {10.1145/2594538.2594543},
  abstract = {We present a structure in external memory for top-k range reporting, which uses linear space, answers a query in O(lgB n + k/B) I/Os, and supports an update in O(lgB n) amortized I/Os, where n is the input size, and B is the block size. This improves the state of the art which incurs O(lg2B n) amortized I/Os per update.},
  citationcount = {8},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {dynamic,query,update}
}

@article{taoIntersectionJoinsUnder2022,
  title = {Intersection Joins under Updates},
  author = {Tao, Yufei and Yi, K.},
  year = {2022},
  doi = {10.1016/j.jcss.2021.09.004},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Journal of computer and system sciences (Print)},
  keywords = {update}
}

@article{tarjanApplicationsOfPath1979,
  title = {Applications of Path Compression on Balanced Trees},
  author = {Tarjan, R.},
  year = {1979},
  doi = {10.1145/322154.322161},
  abstract = {We devise a method for computing functions defined on paths in trees. The method is based on tree manipulation techniques first used for efficiently representing equivalence relations. It has an almost-linear running time. We apply the method to give O(m {$\alpha$}(m,n)) algorithms for two problems. A. Verifying a minimum spanning tree in an undirected graph (best previous bound: O(m log log n) ). B. Finding dominators in a directed graph (best previous bound: O(n log n + m) ). Here n is the number of vertices and m the number of edges in the problem graph, and {$\alpha$}(m,n) is a very slowly growing function which is related to a functional inverse of Ackermann''s function. The method is also useful for solving, in O(m {$\alpha$}(m,n)) time, certain kinds of pathfinding problems on reducible graphs. Such problems occur in global flow analysis of computer programs and in other contexts. A companion paper will discuss this application.},
  citationcount = {317},
  venue = {JACM}
}

@article{tarjanClassAlgorithmsWhich1979,
  title = {A Class of Algorithms Which Require Nonlinear Time to Maintain Disjoint Sets},
  author = {Tarjan, Robert Endre},
  year = {1979},
  month = apr,
  journal = {Journal of Computer and System Sciences},
  volume = {18},
  number = {2},
  pages = {110--127},
  issn = {0022-0000},
  doi = {10.1016/0022-0000(79)90042-4},
  url = {https://www.sciencedirect.com/science/article/pii/0022000079900424},
  urldate = {2024-09-20},
  abstract = {This paper describes a machine model intended to be useful in deriving realistic complexity bounds for tasks requiring list processing. As an example of the use of the model, the paper defines a class of algorithms which compute unions of disjoint sets on-line, and proves that any such algorithm requires nonlinear time in the worst case. All set union algorithms known to the author are instances of the model and are thus subject to the derived bound. One of the known algorithms achieves the bound to within a constant  factor.},
  annotation = {Reference DOIs: 10.1007/978-1-4684-2001-2\_14, 10.1007/978-3-7091-9459-1\_5, 10.1016/0020-0190(76)90061-2, 10.1016/0304-3975(76)90053-0, 10.1016/0304-3975(78)90009-9, 10.1109/SWAT.1972.29, 10.1137/0202024, 10.1145/321879.321884, 10.1145/322154.322161, 10.1145/361227.361231, 10.1145/364099.364331},
  file = {/Users/tulasi/Zotero/storage/N8YNZH9R/Tarjan - 1979 - A class of algorithms which require nonlinear time to maintain disjoint sets.pdf}
}

@article{tarjanDataStructuresAnd1983,
  title = {Data Structures and Network Algorithms},
  author = {Tarjan, R.},
  year = {1983},
  doi = {10.1137/1.9781611970265},
  abstract = {Foundations Disjoint Sets Heaps Search Trees Linking and Cutting Trees Minimum Spanning Trees Shortest Paths Network Flows Matchings.},
  citationcount = {2188},
  venue = {CBMS-NSF Regional Conference Series in Applied Mathematics},
  keywords = {data structure}
}

@article{tarjanEfficiencyOfA1972,
  title = {Efficiency of a Good but Not Linear Set Union Algorithm},
  author = {Tarjan, R.},
  year = {1972},
  doi = {10.1145/321879.321884},
  abstract = {TWO types of instructmns for mampulating a family of disjoint sets which partitmn a umverse of n elements are considered FIND(x) computes the name of the (unique) set containing element x UNION(A, B, C) combines sets A and B into a new set named C. A known algorithm for implementing sequences of these mstructmns is examined It is shown that, if t(m, n) as the maximum time reqmred by a sequence of m {\textquestiondown} n FINDs and n -- 1 intermixed UNIONs, then kima(m, n) \_  t(m, n) {\textexclamdown} k:ma(m, n) for some positive constants ki and k2, where a(m, n) is related to a functional inverse of Ackermann's functmn and as very slow-growing.},
  citationcount = {1453},
  venue = {JACM}
}

@article{tarjanFindingStrongComponents2022,
  title = {Finding Strong Components Using Depth-First Search},
  author = {Tarjan, R. and Zwick, Uri},
  year = {2022},
  doi = {10.1016/j.ejc.2023.103815},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {European journal of combinatorics (Print)}
}

@article{tarjanOptimalResizableArrays2022,
  title = {Optimal Resizable Arrays},
  author = {Tarjan, R. and Zwick, Uri},
  year = {2022},
  doi = {10.48550/arXiv.2211.11009},
  abstract = {A \{resizable array\} is an array that can \{grow\} and \{shrink\} by the addition or removal of items from its end, or both its ends, while still supporting constant-time \{access\} to each item stored in the array given its \{index\}. Since the size of an array, i.e., the number of items in it, varies over time, space-efficient maintenance of a resizable array requires dynamic memory management. A standard doubling technique allows the maintenance of an array of size N using only O(N) space, with O(1) amortized time, or even O(1) worst-case time, per operation. Sitarski and Brodnik et al.describe much better solutions that maintain a resizable array of size N using only N+O({\textsurd}\{N\}) space, still with O(1) time per operation. Brodnik et al.give a simple proof that this is best possible. We distinguish between the space needed for \{storing\} a resizable array, and accessing its items, and the \{temporary\} space that may be needed while growing or shrinking the array. For every integer r{$\geq$}2, we show that N+O(N\textsuperscript{\{\vphantom\}}1/r\vphantom\{\}) space is sufficient for storing and accessing an array of size N, if N+O(N\textsuperscript{\{\vphantom\}}1-1/r\vphantom\{\}) space can be used briefly during grow and shrink operations. Accessing an item by index takes O(1) worst-case time while grow and shrink operations take O(r) amortized time. Using an exact analysis of a \{growth game\}, we show that for any data structure from a wide class of data structures that uses only N+O(N\textsuperscript{\{\vphantom\}}1/r\vphantom\{\}) space to store the array, the amortized cost of grow is {\textohm}(r), even if only grow and access operations are allowed. The time for grow and shrink operations cannot be made worst-case, unless r=2.},
  citationcount = {1},
  venue = {SIAM Symposium on Simplicity in Algorithms},
  keywords = {data structure,dynamic}
}

@article{tarjanReferenceMachinesRequire1977,
  title = {Reference Machines Require Non-Linear Time to Maintain Disjoint Sets},
  author = {Tarjan, R.},
  year = {1977},
  doi = {10.1145/800105.803392},
  abstract = {This paper describes a machine model intended to be useful in deriving realistic complexity bounds for tasks requiring list processing. As an example of the use of the model, the paper shows that any such machine requires non-linear time in the worst case to compute unions of disjoint sets on-line. All set union algorithms known to the author are instances of the model and are thus subject to the derived bound. One of the known algorithms achieves the bound to within a constant factor.},
  citationcount = {35},
  venue = {Symposium on the Theory of Computing}
}

@article{temlyakovAnInequalityFor1995,
  title = {An Inequality for Trigonometric Polynomials and Its Application for Estimating the Entropy Numbers},
  author = {Temlyakov, V.},
  year = {1995},
  doi = {10.1006/jcom.1995.1012},
  abstract = {Abstract We prove in the two-dimensional case an inequality for trigonometric polynomials with frequencies between two hyperbolic crosses. This inequality is an analog of Talagrand{$\prime$}s inequality for the Haar polynomials. We use this inequality to prove some new estimates of the entropy numbers of classes of functions with bounded mixed difference or derivative in the most difficult case of the uniform norm.},
  citationcount = {51},
  venue = {Journal of Complexity}
}

@article{themistoklismelissourgosAlgorithmsComplexityProblems2019,
  title = {Algorithms and Complexity of Problems Arising from Strategic Settings},
  author = {Themistoklis Melissourgos},
  year = {2019},
  doi = {10.17638/03061794},
  abstract = {Different strategic environments enforce different rules and incentives to the interacting entities, and as a result, they need to be analysed through environment-specific models. For each environment, suitable concepts of stability that capture the required properties have been defined over the years. For example, an appropriate solution concept in an evolutionary environment is the Evolutionarily Stable Strategy (ESS); in an environment modelled as a normal-form game is the Nash Equilibrium (NE); and for some environments where we require fair division of an object is a Consensus Halving. In this thesis we study such solution concepts from a computational point of view. Given a pair of environment and its solution concept, we are interested in answering if every instance of this environment admits a solution. If the answer is no, we investigate the computational complexity of deciding the existence of a solution for a given instance. If the answer is yes, we try to determine the complexity of computing a solution. Most of the problems we study are intractable. In this case, either we further explore the space of the problem's instances to identify in which of them lies the computational hardness, or we consider a meaningful relaxation of the problem for which we seek an efficient algorithm. This thesis extends the current state of the art on both computational complexity, and approximation algorithms of various problems in strategic settings. First, we deal with an evolutionary setting where we show that for a wide range of symmetric bimatrix games, deciding ESS existence is intractable. Then, we consider a setting where numerous entities compete repeatedly over a common resource. We present NEs and further categorize them in terms of desirable efficiency qualities. Next, we study a network security game. We characterize the NEs, study their complexity, and measure how effective they are in securing the network using the Price of Defense notion, analogous to the Price of Anarchy. After that, we consider an important fair division problem, namely the Consensus Halving problem. We bound the complexity of computing an exact solution and en route define a new complexity class which has interesting relations with already existing ones. Finally, we present a general framework for constructing approximation schemes for problems that can be written as an Existential Theory of the Reals formula with variables constrained in a bounded convex set. Using this framework, we provide new quasi-polynomial and polynomial time approximation schemes for optimisation problems, variations of problems on normal form games, Consensus Halving, and computational geometry.},
  annotation = {Citation Count: 0}
}

@article{thorupDecrementalDynamicConnectivity1997,
  title = {Decremental Dynamic Connectivity},
  author = {Thorup, M.},
  year = {1997},
  doi = {10.1006/jagm.1999.1033},
  abstract = {Abstract We consider Las Vegas randomized dynamic algorithms for on-line connectivity problems with deletions only. In particular, we show that starting from a graph with m edges and n nodes, we can maintain a spanning forest during m deletions in O ( m  log( n 2 / m ) + n(log  n ) 3 (log log  n ) 2 ) expected time, which is O ( m ) if m  = {$\Theta$}( n 2 ) and O ( m  log  n ) if m  = {\textohm}( n (log  n  log log  n ) 2 ). The deletions may be interspersed with connectivity queries, each of which is answered in constant time. The previous best bound was O ( m  log 2 n ) by Henzinger and Thorup which covered both insertions and deletions. The result is based on a general randomized reduction for edge connectivity problems of many deletions-only queries to a few deletions and insertions queries. For 2-edge connectivity, the complexity is improved from O ( m (log  n ) 5 ) to O ( m  log( n 2 / m ) +  n (log  n ) 6 (log log  n ) 2 ). For the general decremental k -edge-connectivity problem, we get a total running time of O ( k 2 n 2  polylog  n ). Here the previous best bound was O ( kmn  polylog  n ). Improved running times are also achieved for the static consensus tree problem, with applications to computational biology and relational data bases.},
  citationcount = {55},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {dynamic,query,reduction,static}
}

@article{thorupDynamicGraphAlgorithms2000,
  title = {Dynamic Graph Algorithms with Applications},
  author = {Thorup, M. and Karger, David R},
  year = {2000},
  doi = {10.1007/3-540-44985-X_1},
  abstract = {No abstract available},
  citationcount = {33},
  venue = {Scandinavian Workshop on Algorithm Theory}
}

@article{thorupDynamicOrderedSets2019,
  title = {Dynamic Ordered Sets with Approximate Queries, Approximate Heaps and Soft Heaps},
  author = {Thorup, M. and Zamir, Or and Zwick, Uri},
  year = {2019},
  doi = {10.4230/LIPIcs.ICALP.2019.95},
  abstract = {We consider word RAM data structures for maintaining ordered sets of integers whose select and rank operations are allowed to return approximate results, i.e., ranks, or items whose rank, differ by less than Delta from the exact answer, where Delta=Delta(n) is an error parameter. Related to approximate select and rank is approximate (one-dimensional) nearest-neighbor. A special case of approximate select queries are approximate min queries. Data structures that support approximate min operations are known as approximate heaps (priority queues). Related to approximate heaps are soft heaps, which are approximate heaps with a different notion of approximation. We prove the optimality of all the data structures presented, either through matching cell-probe lower bounds, or through equivalences to well studied static problems. For approximate select, rank, and nearest-neighbor operations we get matching cell-probe lower bounds. We prove an equivalence between approximate min operations, i.e., approximate heaps, and the static partitioning problem. Finally, we prove an equivalence between soft heaps and the classical sorting problem, on a smaller number of items. Our results have many interesting and unexpected consequences. It turns out that approximation greatly speeds up some of these operations, while others are almost unaffected. In particular, while select and rank have identical operation times, both in comparison-based and word RAM implementations, an interesting separation emerges between the approximate versions of these operations in the word RAM model. Approximate select is much faster than approximate rank. It also turns out that approximate min is exponentially faster than the more general approximate select. Next, we show that implementing soft heaps is harder than implementing approximate heaps. The relation between them corresponds to the relation between sorting and partitioning. Finally, as an interesting byproduct, we observe that a combination of known techniques yields a deterministic word RAM algorithm for (exactly) sorting n items in O(n log log\_w n) time, where w is the word length. Even for the easier problem of finding duplicates, the best previous deterministic bound was O(min\{n log log n,n log\_w n\}). Our new unifying bound is an improvement when w is sufficiently large compared with n.},
  citationcount = {1},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {cell probe,data structure,dynamic,lower bound,query,static}
}

@article{thorupEfficientAlgorithmsAnd2012,
  title = {Efficient Algorithms and Data Structures},
  author = {Thorup, M.},
  year = {2012},
  doi = {10.1007/0-306-48725-x_5},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {No venue available},
  keywords = {data structure}
}

@article{thorupEquivalenceBetweenPriority2002,
  title = {Equivalence between Priority Queues and Sorting},
  author = {Thorup, M.},
  year = {2002},
  doi = {10.1109/SFCS.2002.1181889},
  abstract = {We present a general deterministic linear space reduction from priority queues to sorting implying that if we can sort up to n keys in S(n) time per key, then there is a priority queue supporting delete and insert in S(n)+O(1) time and find-min in constant time. Conversely, a priority queue can trivially be used for sorting: first insert all keys to be sorted, then extract them in sorted order by repeatedly deleting the minimum. Hence, asymptotically this settles the complexity of priority queues in terms of that of sorting. Besides nailing down the complexity of priority queues to that of sorting, and vice versa, we translate known sorting results into new results on priority queues for integers and strings in different computational models.},
  citationcount = {71},
  venue = {The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.},
  keywords = {reduction}
}

@article{thorupFullyDynamicMin2001,
  title = {Fully-Dynamic Min-Cut},
  author = {Thorup, M.},
  year = {2001},
  doi = {10.1145/380752.380804},
  abstract = {We show that we can maintain up to polylogarithmic edge connectivity for a fully-dynamic graph in {\textexclamdown}italic{\textquestiondown}{\~O}({\textsurd}\{n\}){\textexclamdown}/italic{\textquestiondown} time per edge insertion or deletion. Within logarithmic factors, this matches the best time bound for 1-edge connectivity. Previously, no {\textexclamdown}italic{\textquestiondown}o(n){\textexclamdown}/italic{\textquestiondown} bound was known for edge connectivity above {\textexclamdown}italic{\textquestiondown}3{\textexclamdown}/italic{\textquestiondown}, and even for {\textexclamdown}italic{\textquestiondown}3{\textexclamdown}/italic{\textquestiondown}-edge connectivity, the best update time was {\textexclamdown}italic{\textquestiondown}O(n{\textasciicircum}\{2/3\}){\textexclamdown}/italic{\textquestiondown}, dating back to FOCS'92. Our algorithm maintains a concrete min-cut in terms of a pointer to a tree spanning one side of the cut plus ability to list the cut edges in {\textexclamdown}italic{\textquestiondown}O(n){\textexclamdown}/italic{\textquestiondown} time per edge. By dealing with polylogarithmic edge connectivity, we immediately get a sampling based expected factor {\textexclamdown}italic{\textquestiondown}(1+o(1)){\textexclamdown}/italic{\textquestiondown} approximation to general edge connectivity in {\textexclamdown}italic{\textquestiondown}{\~O}({\textsurd}\{n\}){\textexclamdown}/italic{\textquestiondown} time per edge insertion or deletion. This algorithm also maintains a pointer to one side of a min-cut, but if we want to list the cut edges in {\textexclamdown}italic{\textquestiondown}O(n){\textexclamdown}/italic{\textquestiondown} time per edge, the update time increases to {\textexclamdown}italic{\textquestiondown}{\~O}({\textsurd}\{m\}){\textexclamdown}/italic{\textquestiondown}.},
  citationcount = {64},
  venue = {Symposium on the Theory of Computing}
}

@article{thorupMihaiPatrascuObituary2013,
  title = {Mihai {{P{\v a}tra{\c s}cu}}: Obituary and Open Problems},
  author = {Thorup, M.},
  year = {2013},
  doi = {10.1145/2447712.2447737},
  abstract = {Mihai P{\v a}tra{\c s}cu, aged 29, passed away on Tuesday June 5, 2012, after a 1.5 year battle with brain cancer. Mihai's academic career was short but explosive, full of rich and beautiful ideas as witnessed, e.g., in his 20 STOC/FOCS papers. His many interesting papers are available online at: http://people.csail.mit.edu/mip/papers/index.html. Mihai's talent showed early. In high school he received numerous medals at national (Romanian) and international olympiads including prizes in informatics, physics and applied math. He received gold medals at the International Olympiad in Informatics (IOI) in both 2000 and 2001. He remained involved with olympiads and was elected member of the International Scientific Committee for the International Olympiad of Informatics since 2010. Mihai's main research area was data structure lower bounds. In data structures we try to understand how we can efficiently represent, access, and update information. Mihai revolutionized and revitalized the lower bound side, in many cases matching known upper bounds. The lower bounds were proved in the powerful cell-probe model that only charges for memory access, hence which captures both RAM and external memory. Already in 2004 [17], as a second year undergraduate student, with his supervisor Erik Demaine as non-alphabetic second author, he broke the {\textohm}(log n/ log log n) lower bound barrier that had impeded dynamic lower bounds since 1989 [6], and showed the first logarithmic lower bound by an elegant short proof, a true combinatorial gem. The important conclusion was that binary search trees are optimal algorithms for the textbook problem of maintaining prefix sums in a dynamic array. They also proved an {\textohm}(log n) lower bound for dynamic trees, matching Sleator and Tarjan's upper bound from 1983 [20]. In 2005 he received from the Computing Research Association (CRA) the Outstanding Undergraduate Award for best undergraduate research in the US and Canada. I was myself lucky enough to meet Mihai in 2004, starting one of most intense collaborations I have experienced in my career. It took us almost two years to find the first separation between near-linear and polynomial space in data structures [19]. What kept us going on this hard problem was that we always had lots of fun on the side: playing squash, going on long hikes, and having beers celebrating every potentially useful idea we found on the way. A strong friendship was formed. Mihai published more than 10 papers while pursuing his undergraduate studies at MIT from 2002 to 2006. Nevertheless he finished with a perfect 5.0/5.0 GPA. Over the next 2 years, he did his PhD at MIT. His thesis ``Lower Bound Techniques for Data Structures'' [11] is a must-read for researchers who want to get into data structure lower bounds. During Mihai's PhD, I got to be his mentor at AT\&T, and in 2009, after a year as Raviv Postdoctoral Fellow at IBM Almaden, he joined me at AT\&T. We continued our work on lower bounds, but I also managed to get him interested in hashing which is of immense importance to real computing. We sought schemes that were both truly practical and theoretically powerful [15].},
  citationcount = {9},
  venue = {SIGA},
  keywords = {cell probe,data structure,dynamic,lower bound,update}
}

@article{thorupNearOptimalFully2000,
  title = {Near-Optimal Fully-Dynamic Graph Connectivity},
  author = {Thorup, M.},
  year = {2000},
  doi = {10.1145/335305.335345},
  abstract = {In this paper we present near-optimal bounds for fullydynamic graph connectivity which is the most basic nontrivial fully-dynamic graph problem. Connectivity queries are supported in O(log n/log log log n) time while the updates are supported in O(log n(log log n) 3) expected amortized time. The previous best update time was O((log n)2). Our new bound is only doubly-logarithmic factors from a general cell probe lower bound of f2(log n  log log n). Our algorithm runs on a pointer machine, and uses only standard AC {$^\circ$} instructions. In our developments we make some comparatively trivial observations improving some deterministic bounds. The space bound of the previous O((log n)  ) connectivity algorithm is improved from O(m + n log n) to O(m). The previous time complexity of fully-dynamic 2-edge and biconnectivity is improved from O((log n) 4) to O((log n) 3 log log n).},
  citationcount = {201},
  venue = {Symposium on the Theory of Computing}
}

@article{thorupOnRamPriority1996,
  title = {On {{RAM}} Priority Queues},
  author = {Thorup, M.},
  year = {1996},
  doi = {10.1137/S0097539795288246},
  abstract = {Priority queues are some of the most fundamental data structures. For example, they are used directly for task scheduling in operating systems. Moreover, they are essential to greedy algorithms. We study the complexity of integer priority queue operations on a RAM with arbitrary word size, modeling the possibilities in standard imperative programming languages such as C. We present exponential improvements over previous bounds, and we show tight relations to sorting. Our first result is a RAM priority queue supporting find-min in constant time and insert and delete-min in time O(log log n), where n is the current number of keys in the queue. This is an exponential improvement over the O({\textsurd}\{n\}) bound of Fredman and Willard [ Proceedings of the 22nd ACM Symposium on the Theory of Computing, Baltimore, MD, pp. 1--7]. Plugging this priority queue into Dijkstra's algorithm gives an O(mlog log m) algorithm for the single source shortest path problem on a graph with m edges, as compared with the previous O(m{\textsurd}\{m\}) bound based on Fredman and Willard's priority queue. The above bounds assume O(n2\textsuperscript{\{\vphantom\}}\{{$\varepsilon$}\}w\vphantom\{\}) space, where w is the word length and \{{$\varepsilon$}\}{$>$}0. They can, however, be achieved in linear space using randomized hashing. Our second result is a general equivalence between sorting and priority queues. A priority queue is monotone if the minimum is nondecreasing over time, as in many greedy algorithms. We show that on a RAM, the amortized operation cost of a monotone priority queue is equivalent to the per-key cost of sorting. For example, the equivalence implies that the single source shortest paths problem on a graph with m edges is no harder than that of sorting m keys. With the current RAM sorting, this gives an O(m log log m) time bound, as above, but the relation holds regardless of the future developments in RAM sorting. From the equivalence result, for any fixed \{{$\varepsilon$}\}{$>$}0, we derive a randomized monotone O({\textsurd}\{n\}\textsuperscript{\{\vphantom\}}1+\{{$\varepsilon$}\}\vphantom\{\}) priority queue with expected constant time decrease-key. Plugging this into Dijkstra's algorithm gives an O(n{\textsurd}\{n\}\textsuperscript{\{\vphantom\}}1+\{{$\varepsilon$}\}\vphantom\{\}+m) algorithm for the single source shortest path problem on a graph with n nodes and m edges, complementing the above O(mlog log m) algorithm if m{$\gg$}n. This improves the O(nlog n/log log n + m) bound by Fredman and Willard [Proceedings of the 31st IEEE Symposium on the Foundations of Computer Science, St. Louis, MO, 1990, pp. 719--725], based on their O(log n/log log n) priority queue with constant decrease-key.},
  citationcount = {182},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{thorupSpaceEfficientDynamic2003,
  title = {Space Efficient Dynamic Stabbing with Fast Queries},
  author = {Thorup, M.},
  year = {2003},
  doi = {10.1145/780542.780636},
  abstract = {In dynamic stabbing, we operate on a dynamic set of intervals. A stabbing query asks for an interval containing a given point. This basic problem encodes problems such as method look-up in object oriented programming languages and classification in IP firewalls. For such application, very fast, say constant, query time is extremely important, small space is very important, and fast updates are good but the least important. Previous solutions traded space and update time for fast queries. We show here that space needs not be sacrificed. We get the same trade-off between update time and query time but using only the space necessary for locating a query point among the interval end-points. All our bounds are optimal or near-optimal.},
  citationcount = {27},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,query,query time,update,update time}
}

@article{thorupWorstcaseUpdateTimes2005,
  title = {Worst-Case Update Times for Fully-Dynamic All-Pairs Shortest Paths},
  author = {Thorup, M.},
  year = {2005},
  doi = {10.1145/1060590.1060607},
  abstract = {We present here the first solution to the fully-dynamic all pairs shortest path problem where every update is faster than a recomputation from scratch in {\textohm}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}3{\textexclamdown}/sup{\textquestiondown}log {\textfractionsolidus}n) time. This is for a directed graph with arbitrary non-negative edge weights. An update inserts or deletes a vertex with all incident edges. After each such vertex update, we update a complete distance matrix in {\textexclamdown}i{\textquestiondown}{\~O}{\textexclamdown}/i{\textquestiondown}({\textexclamdown}i{\textquestiondown}n{\textexclamdown}/i{\textquestiondown}{\textexclamdown}sup{\textquestiondown}2.75{\textexclamdown}/sup{\textquestiondown}) time.},
  citationcount = {101},
  venue = {Symposium on the Theory of Computing},
  keywords = {dynamic,update,update time}
}

@article{timohintschExactSolutionSoftclustered2020,
  title = {Exact Solution of the Soft-Clustered Vehicle-Routing Problem},
  author = {Timo Hintsch and Stefan Irnich},
  year = {2020},
  journal = {European Journal of Operational Research},
  doi = {10.1016/J.EJOR.2019.07.019},
  annotation = {Citation Count: 30}
}

@article{tongyangliSublinearClassicalQuantum2020,
  title = {Sublinear Classical and Quantum Algorithms for General Matrix Games},
  author = {Tongyang Li and Chunhao Wang and Shouvanik Chakrabarti and Xiaodi Wu},
  year = {2020},
  journal = {AAAI Conference on Artificial Intelligence},
  doi = {10.1609/aaai.v35i10.17028},
  abstract = {We investigate sublinear classical and quantum algorithms for matrix games, a fundamental problem in optimization and machine learning, with provable guarantees. Given a matrix, sublinear algorithms for the matrix game were previously known only for two special cases: (1) the maximizing vectors live in the L1-norm unit ball, and (2) the minimizing vectors live in either the L1- or the L2-norm unit ball. We give a sublinear classical algorithm that can interpolate smoothly between these two cases: for any fixed q between 1 and 2, we solve, within some additive error, matrix games where the minimizing vectors are in an Lq-norm unit ball. We also provide a corresponding sublinear quantum algorithm that solves the same task with a quadratic improvement in dimensions of the maximizing and minimizing vectors. Both our classical and quantum algorithms are optimal in the dimension parameters up to poly-logarithmic factors. Finally, we propose sublinear classical and quantum algorithms for the approximate Carath{\'e}odory problem and the Lq-margin support vector machines as applications.},
  annotation = {Citation Count: 17}
}

@article{toniLowerBoundsOn2004,
  title = {Lower Bounds on Zero--One Matrices},
  author = {Toni, A.},
  year = {2004},
  doi = {10.1016/J.LAA.2003.06.018},
  abstract = {No abstract available},
  citationcount = {5},
  venue = {No venue available},
  keywords = {lower bound}
}

@article{tonyw.laiAdaptiveHeuristicsBinary1991,
  title = {Adaptive Heuristics for Binary Search Trees and Constant Linkage Cost},
  author = {Tony W. Lai and D. Wood},
  year = {1991},
  journal = {ACM-SIAM Symposium on Discrete Algorithms},
  doi = {10.1137/S0097539793250329},
  abstract = {We present lower and upper bounds on adaptive heuristics for maintaining binary search trees using a constant number of link or pointer changes for each operation (constant linkage cost (CLC)). We show that no adaptive heuristic with an amortized linkage cost of o(logn) can be competitive. In particular, we show that any heuristic that performs f(n )= o(logn) promotions (rotations) amortized over each access has a competitive ratio of at least {\guilsinglright}(log n=f(n)) against an oblivious adversary, and any heuristic that performs f(n )= o(logn) pointer changes amortized over each access has a competitive ratio of at least {\guilsinglright}( log n f(n) log(log n=f(n)) ) against an adaptive online adversary. In our investigation of upper bounds we present four adaptive heuristics: a randomized, worst-case-CLC heuristic randomized two-promotion (R2P) whose expected search time is within a constant factor of the search time using an optimal tree; that is, it is statically competitive against an oblivious adversary; a randomized, expected-CLC heuristic (locally optimized randomized partial splay (LORPS)) that has O(logn) expected-amortized update time and is statically competitive against an oblivious adversary; a deterministic, amortized-CLC heuristic (locally optimized partial splay (LOPS)) that has O(logn) amortized update time and is statically competitive against an adaptive adversary; a practical, randomized heuristic (randomized partial splay (RPS)) that is not CLC but has performance bounds comparable with those of the splay heuristic of Sleator and Tarjan; it is statically competitive against an adaptive adversary. The randomized heuristics use only constant extra space, whereas the deterministic heuristic uses O(n) extra space.},
  keywords = {adaptive,static,update,update time},
  annotation = {Citation Count: 14}
}

@article{tsaknakisAnOptimizationApproach2007,
  title = {An Optimization Approach for Approximate Nash Equilibria},
  author = {Tsaknakis, H. and Spirakis, P.},
  year = {2007},
  doi = {10.1080/15427951.2008.10129172},
  abstract = {In this paper we propose a new methodology for determining approximate Nash equilibria of noncooperative bimatrix games, and based on that, we provide an efficient algorithm that computes 0.3393-approximate equilibria, the best approximation to date. The methodology is based on the formulation of an appropriate function of pairs of mixed strategies reflecting the maximum deviation of the players' payoffs from the best payoff each player could achieve given the strategy chosen by the other. We then seek to minimize such a function using descent procedures. Because it is unlikely to be able to find global minima in polynomial time, given the recently proven intractability of the problem, we concentrate on the computation of stationary points and prove that they can be approximated arbitrarily closely in polynomial time and that they have the above-mentioned approximation property. Our result provides the best {$\varepsilon$} to date for polynomially computable {$\varepsilon$}-approximate Nash equilibria of bimatrix games. Furthermore, our methodology for computing approximate Nash equilibria has not been used by others.},
  citationcount = {172},
  venue = {Workshop on Internet and Network Economics}
}

@article{tsaknakisPracticalAndEfficient2010,
  title = {Practical and Efficient Approximations of Nash Equilibria for Win-Lose Games Based on Graph Spectra},
  author = {Tsaknakis, H. and Spirakis, P.},
  year = {2010},
  doi = {10.1007/978-3-642-17572-5_31},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {Workshop on Internet and Network Economics}
}

@article{tsengParallelBatchDynamic2022,
  title = {Parallel Batch-Dynamic Minimum Spanning Forest and the Efficiency of Dynamic Agglomerative Graph Clustering},
  author = {Tseng, Tom and Dhulipala, Laxman and Shun, Julian},
  year = {2022},
  doi = {10.1145/3490148.3538584},
  abstract = {Hierarchical agglomerative clustering (HAC) is a popular algorithm for clustering data, but despite its importance, no dynamic algorithms for HAC with good theoretical guarantees exist. In this paper, we study dynamic HAC on edge-weighted graphs. As single-linkage HAC reduces to computing a minimum spanning forest (MSF), our first result is a parallel batch-dynamic algorithm for maintaining MSFs. On a batch of k edge insertions or deletions, our batch-dynamic MSF algorithm runs in O(k log6 n) expected amortized work and O(log4 n) span with high probability. It is the first fully dynamic MSF algorithm handling batches of edge updates with polylogarithmic work per update and polylogarithmic span. Using our MSF algorithm, we obtain a parallel batch-dynamic algorithm that can answer queries about single-linkage graph HAC clusters. Our second result is that dynamic graph HAC is significantly harder for other common linkage functions. For example, assuming the strong exponential time hypothesis, dynamic graph HAC requires {\textohm}(n1-o(1)) work per update or query on a graph with n vertices for complete linkage, weighted average linkage, and average linkage. For complete linkage and weighted average linkage, the bound still holds even for incremental or decremental algorithms and even if we allow poly(n)-approximation. For average linkage, the bound weakens to {\textohm}(n1/2-o(1)) for incremental and decremental algorithms, and the bounds still hold when allowing no(1) -approximation.},
  citationcount = {10},
  venue = {ACM Symposium on Parallelism in Algorithms and Architectures},
  keywords = {dynamic,query,update}
}

@article{tsotrasEfficientAlgorithmsFor1990,
  title = {Efficient Algorithms for Managing the History of Evolving Databases},
  author = {Tsotras, V. and Gopinath, B.},
  year = {1990},
  doi = {10.1007/3-540-53507-1_75},
  abstract = {No abstract available},
  citationcount = {12},
  venue = {International Conference on Database Theory}
}

@article{tsotrasEfficientManagementOf1995,
  title = {Efficient Management of Time-Evolving Databases},
  author = {Tsotras, V. and Gopinath, B. and Hart, G.},
  year = {1995},
  doi = {10.1109/69.404032},
  abstract = {Efficiently managing the history of a time-evolving system is one of the central problems in many database environments, like database systems that incorporate versioning, or object-oriented databases that implicitly or explicitly maintain the history of persistent objects. In this paper we propose algorithms that reconstruct past states of an evolving system for two general cases, i.e., when the system's state is represented by a set or by a hierarchy (a forest of trees). Sets are widely used as a canonical form of representing information in databases or program states. For more complex applications, like schema evolution in object-oriented databases, it becomes necessary to manage the history of data structures that have the form of forests or even graphs. The proposed algorithms use minimal space (proportional to the number of changes occurring in the evolution) and have the advantage of being on-line (in the amortized sense). Any past system state s(t) is reconstructed in time O. {\textquestiondown}},
  citationcount = {36},
  venue = {IEEE Transactions on Knowledge and Data Engineering},
  keywords = {data structure}
}

@article{tsotrasOptimallyManagingThe1990,
  title = {Optimally Managing the History of an Evolving Forest},
  author = {Tsotras, V. and Gopinath, B. and Hart, G.},
  year = {1990},
  doi = {10.1007/3-540-52921-7_96},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {SIGAL International Symposium on Algorithms}
}

@article{tsotrasOptimalVersioningOf1992,
  title = {Optimal Versioning of Objects},
  author = {Tsotras, V. and Gopinath, B.},
  year = {1992},
  doi = {10.1109/ICDE.1992.213174},
  abstract = {The purpose of versioning is to reconstruct any past state of an object class. The authors show that access to any past version is possible in almost constant time, while the space used is only linear to the number of changes occurring in the class evolution. As a result, versioning with fast reconstruction can be supported in an object-oriented environment without using excessive space requirements. It is also proved that the solution is optimal among all approaches that use the same space limitations. A crucial characteristic of the results is that they can be easily implemented on a storage facility that uses a magnetic disk and an optical disk.<<ETX>>},
  citationcount = {15},
  venue = {[1992] Eighth International Conference on Data Engineering}
}

@article{tsotrasUsingUniverseKnowledge1990,
  title = {Using Universe Knowledge and Arithmetic to Get Faster Parallel Algorithms},
  author = {Tsotras, V. and Gopinath, B. and Hart, G.},
  year = {1990},
  doi = {10.1109/SPDP.1990.143618},
  abstract = {The authors provide algorithms that achieve better upper bounds for three classical parallel problems, i.e., searching, merging and maximum finding. The algorithms assume that the p parallel processors can use arithmetic and that the universe U of the input elements is bounded and known. They show that parallel searching is done in O(loglogU/logp), while parallel merging and maximum finding in O(logloglogU). The results hold for the CRCW and CREW PRAM models.<<ETX>>},
  citationcount = {Unknown},
  venue = {Proceedings of the Second IEEE Symposium on Parallel and Distributed Processing 1990}
}

@article{tsurutaCTrieA2019,
  title = {C-{{Trie}}++: A Dynamic Trie Tailored for Fast Prefix Searches},
  author = {Tsuruta, Kazuya and K{\"o}ppl, Dominik and Kanda, Shunsuke and Nakashima, Yuto and Inenaga, Shunsuke and Bannai, H. and Takeda, Masayuki},
  year = {2019},
  doi = {10.1109/DCC47342.2020.00032},
  abstract = {Given a dynamic set K of k strings of total length n whose characters are drawn from an alphabet of size {$\sigma$}, a keyword dictionary is a data structure built on K that provides locate, prefix search, and update operations on K. Under the assumption that {$\alpha$} = w / lg {$\sigma$} characters fit into a single machine word w, we propose a keyword dictionary that represents K in n lg {$\sigma$} + {$\Theta$}(k lg n) bits of space, supporting all operations in {$\Theta$}(m / {$\alpha$} + lg {$\alpha$}) expected time on an input string of length m in the word RAM model. This data structure is underlined with an exhaustive practical evaluation, highlighting the practical usefulness of the proposed data structure, especially for prefix searches - one of the most elementary keyword dictionary operations.},
  citationcount = {14},
  venue = {Data Compression Conference},
  keywords = {data structure,dynamic,update}
}

@article{tziavelisEfficientComputationOf2023,
  title = {Efficient Computation of Quantiles over Joins},
  author = {Tziavelis, Nikolaos and Carmeli, Nofar and Gatterbauer, Wolfgang and Kimelfeld, B. and Riedewald, Mirek},
  year = {2023},
  doi = {10.1145/3584372.3588670},
  abstract = {We present efficient algorithms for Quantile Join Queries, abbreviated as},
  citationcount = {4},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {query}
}

@article{ulrichbauerRipserEfficientComputation2019,
  title = {Ripser: Efficient Computation of {{Vietoris}}--{{Rips}} Persistence Barcodes},
  author = {Ulrich Bauer},
  year = {2019},
  journal = {Journal of Applied and Computational Topology},
  doi = {10.1007/s41468-021-00071-5},
  annotation = {Citation Count: 334}
}

@article{umansFastPolynomialFactorization2008,
  title = {Fast Polynomial Factorization and Modular Composition in Small Characteristic},
  author = {Umans, C.},
  year = {2008},
  doi = {10.1145/1374376.1374445},
  abstract = {We obtain randomized algorithms for factoring degree n univariate polynomials over F\_q that use O(n1.5 + o(1) + n1 + o(1)log q) field operations, when the characteristic is at most no(1). When log q {\textexclamdown} n, this is asymptotically faster than the best previous algorithms (von zur Gathen \& Shoup (1992) and Kaltofen \& Shoup (1998)); for log q {$\geq$} n, it matches the asymptotic running time of the best known algorithms. The improvements come from a new algorithm for modular composition of degree n univariate polynomials, which is the asymptotic bottleneck in fast algorithms for factoring polynomials over finite fields. The best previous algorithms for modular composition use O(n(omega + 1)/2) field operations, where omega is the exponent of matrix multiplication (Brent \& Kung (1978)), with a slight improvement in the exponent achieved by employing fast rectangular matrix multiplication (Huang \& Pan (1997)). We show that modular composition and multipoint evaluation of multivariate polynomials are essentially equivalent in the sense that an algorithm for one achieving exponent {$\alpha$} implies an algorithm for the other with exponent {$\alpha$} + o(1), and vice versa. We then give a new algorithm that requires O(n1 + o(1)) field operations when the characteristic is at most no(1), which is optimal up to lower order terms. Our algorithms do not rely on fast matrix multiplication, in contrast to all previous subquadratic algorithms for these problems. The main operations are fast univariate polynomial arithmetic, multipoint evaluation, and interpolation, and consequently the algorithms could be feasible in practice.},
  citationcount = {35},
  venue = {Symposium on the Theory of Computing}
}

@article{Vad12,
  title = {Pseudorandomness},
  author = {Vadhan, Salil P.},
  year = {2012},
  journal = {Foundations and Trends in Theoretical Computer Science},
  volume = {7},
  number = {13},
  pages = {1--336},
  doi = {10.1561/0400000010},
  file = {/Users/tulasi/Zotero/storage/Y5N47IDZ/Vadhan - 2012 - Pseudorandomness.pdf}
}

@article{vaidyaSpaceTimeTrade1989,
  title = {Space-Time Trade-Offs for Orthogonal Range Queries},
  author = {Vaidya, P. M.},
  year = {1989},
  doi = {10.1137/0218051},
  abstract = {This paper investigates the question of (storage) space-(retrieval) time trade-off for orthogonal range queries on a static data base. Each record in the data base consists of a key that is a d-tuple of integers, and a data value that is an element in a commutative semigroup G. An orthogonal range query is specified by a d-dimensional parallelepiped (box). Two types of response to such a query are considered: one where the output is the semigroup sum of the data values whose keys are located in the query parallelepiped, and the other where the output is a list of all the records whose keys lie in the query parallelepiped. This paper studies two models, the arithmetic model and the tree model and obtains lower bounds on the product of retrieval time and storage space in both models.},
  citationcount = {7},
  venue = {SIAM journal on computing (Print)}
}

@article{vaidyaSpaceTimeTradeoffs1985,
  title = {Space-Time Tradeoffs for Orthogonal Range Queries},
  author = {Vaidya, P. M.},
  year = {1985},
  doi = {10.1145/22145.22164},
  abstract = {We investigate the question of (storage) space - (retrieval) time tradeoff for orthogonal range queries on a static database. Lower bounds on the product of retrieval time and storage space are obtained in the arithmetic and tree models.},
  citationcount = {24},
  venue = {Symposium on the Theory of Computing}
}

@article{valiantGraphTheoreticArguments1977,
  title = {Graph-Theoretic Arguments in Low-Level Complexity},
  author = {Valiant, L.},
  year = {1977},
  doi = {10.1007/3-540-08353-7_135},
  abstract = {No abstract available},
  citationcount = {379},
  venue = {International Symposium on Mathematical Foundations of Computer Science}
}

@article{valiantOnNonLinear1975,
  title = {On Non-Linear Lower Bounds in Computational Complexity},
  author = {Valiant, L.},
  year = {1975},
  doi = {10.1145/800116.803752},
  abstract = {The purpose of this paper is to explore the possibility that purely graph-theoretic reasons may account for the superlinear complexity of wide classes of computational problems. The results are therefore of two kinds: reductions to graph theoretic conjectures on the one hand, and graph theoretic results on the other. We show that the graph of any algorithm for any one of a number of arithmetic problems (e.g. polynomial multiplication, discrete Fourier transforms, matrix multiplication) must have properties closely related to concentration networks.},
  citationcount = {92},
  venue = {Symposium on the Theory of Computing}
}

@article{valiantTheComplexityOf1979,
  title = {The Complexity of Computing the Permanent},
  author = {Valiant, L.},
  year = {1979},
  doi = {10.1016/0304-3975(79)90044-6},
  abstract = {No abstract available},
  citationcount = {2836},
  venue = {Theoretical Computer Science}
}

@article{vlimkiSpaceEfficientAlgorithms2007,
  title = {Space-Efficient Algorithms for Document Retrieval},
  author = {V{\"a}lim{\"a}ki, Niko and M{\"a}kinen, V.},
  year = {2007},
  doi = {10.1007/978-3-540-73437-6_22},
  abstract = {No abstract available},
  citationcount = {83},
  venue = {Annual Symposium on Combinatorial Pattern Matching}
}

@article{vapnikChervonenkisOnThe1971,
  title = {Chervonenkis: {{On}} the Uniform Convergence of Relative Frequencies of Events to Their Probabilities},
  author = {Vapnik, V.},
  year = {1971},
  doi = {10.1007/978-3-319-21852-6_3},
  abstract = {No abstract available},
  citationcount = {4225},
  venue = {No venue available}
}

@article{varmanMergingMultipleLists1991,
  title = {Merging Multiple Lists on Hierarchical-Memory Multiprocessors},
  author = {Varman, P. and Scheufler, Scott D. and Iyer, B. and Ricard, Gary R.},
  year = {1991},
  doi = {10.1016/0743-7315(91)90022-2},
  abstract = {No abstract available},
  citationcount = {38},
  venue = {J. Parallel Distributed Comput.}
}

@article{vegterInHandbookOf1997,
  title = {In Handbook of Discrete and Computational Geometry},
  author = {Vegter, Gert},
  year = {1997},
  doi = {10.5860/choice.35-5707b},
  abstract = {Handbook of discrete and computational geometry, 2d.ed, Handbook of differential geometry. QA 372 P725 2003, Handbook of geometric topology. Jacob E. Goodman, co-founder and editor of Discrete \& Computational Geometry, the preeminent journal on this area in the international mathematics. Section 2 is devoted to geometric transversal theory. booktitle = (Handbook of Discrete and Computational Geometry, chapter 4), year = (1997),},
  citationcount = {1485},
  venue = {No venue available}
}

@article{vempalaTheCommunicationComplexity2019,
  title = {The Communication Complexity of Optimization},
  author = {Vempala, S. and Wang, Ruosong and Woodruff, David P.},
  year = {2019},
  doi = {10.1137/1.9781611975994.106},
  abstract = {We consider the communication complexity of a number of distributed optimization problems. We start with the problem of solving a linear system. Suppose there is a coordinator together with s servers P{$_1$},{\dots},P\textsubscript{s}, the i-th of which holds a subset A\textsuperscript{\{\vphantom\}}(i)\vphantom\{\}x=b\textsuperscript{\{\vphantom\}}(i)\vphantom\{\} of n\textsubscript{i} constraints of a linear system in d variables, and the coordinator would like to output x{$\in$}\{R\}\textsuperscript{d} for which A\textsuperscript{\{\vphantom\}}(i)\vphantom\{\}x=b\textsuperscript{\{\vphantom\}}(i)\vphantom\{\} for i=1,{\dots},s. We assume each coefficient of each constraint is specified using L bits. We first resolve the randomized and deterministic communication complexity in the point-to-point model of communication, showing it is {$\Theta$}\vphantom\{\}(d{$^2$}L+sd) and {$\Theta$}\vphantom\{\}(sd{$^2$}L), respectively. We obtain similar results for the blackboard model. When there is no solution to the linear system, a natural alternative is to find the solution minimizing the {$\ell_p$} loss. While this problem has been studied, we give improved upper or lower bounds for every value of p{$\geq$}1. One takeaway message is that sampling and sketching techniques, which are commonly used in earlier work on distributed optimization, are neither optimal in the dependence on d nor on the dependence on the approximation {$\epsilon$}, thus motivating new techniques from optimization to solve these problems. Towards this end, we consider the communication complexity of optimization tasks which generalize linear systems. For linear programming, we first resolve the communication complexity when d is constant, showing it is {$\Theta$}\vphantom\{\}(sL) in the point-to-point model. For general d and in the point-to-point model, we show an O\vphantom\{\}(sd{$^3$}L) upper bound and an {\textohm}\vphantom\{\}(d{$^2$}L+sd) lower bound. We also show if one perturbs the coefficients randomly by numbers as small as 2\textsuperscript{\{\vphantom\}}-{$\Theta$}(L)\vphantom\{\}, then the upper bound is O\vphantom\{\}(sd{$^2$}L)+\{poly\}(dL).},
  citationcount = {32},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {communication,communication complexity,lower bound}
}

@article{vengroffEfficient3D1996,
  title = {Efficient 3-{{D}} Range Searching in External Memory},
  author = {Vengroff, Darren Erik and Vitter, J.},
  year = {1996},
  doi = {10.1145/237814.237864}
}

@article{verbinDataStructureLower2012,
  title = {Data Structure Lower Bounds on Random Access to Grammar-Compressed Strings},
  author = {Verbin, Elad and Yu, Wei},
  year = {2012},
  doi = {10.1007/978-3-642-38905-4_24},
  abstract = {No abstract available},
  citationcount = {48},
  venue = {Annual Symposium on Combinatorial Pattern Matching},
  keywords = {data structure,lower bound}
}

@article{verbinLimitsBufferingTight2010,
  title = {The Limits of Buffering: A Tight Lower Bound for Dynamic Membership in the External Memory Model},
  author = {Verbin, Elad and Zhang, Qin},
  year = {2010},
  doi = {10.1145/1806689.1806752},
  abstract = {We study the dynamic membership (or dynamic dictionary) problem, which is one of the most fundamental problems in data structures. We study the problem in the external memory model with cell size b bits and cache size m bits. We prove that if the amortized cost of updates is at most 0.999 (or any other constant {\textexclamdown} 1), then the query cost must be (logb log n(n/m)), where n is the number of elements in the dictionary. In contrast, when the update time is allowed to be 1 + o(1), then a bit vector or hash table give query time O(1). Thus, this is a threshold phenomenon for data structures. This lower bound answers a folklore conjecture of the external memory community. Since almost any data structure task can solve membership, our lower bound implies a dichotomy between two alternatives: (i) make the amortized update time at least 1 (so the data structure does not buffer, and we lose one of the main potential advantages of the cache), or (ii) make the query time at least roughly logarithmic in n. Our result holds even when the updates and queries are chosen uniformly at random and there are no deletions; it holds for randomized data structures, holds when the universe size is O(n), and does not make any restrictive assumptions such as indivisibility. All of the lower bounds we prove hold regardless of the space consumption of the data structure, while the upper bounds only need linear space. The lower bound has some striking implications for external memory data structures. It shows that the query complexities of many problems such as 1D-range counting, predecessor, rank-select, and many others, are all the same in the regime where the amortized update time is less than 1, as long as the cell size is large enough (b = polylog(n) suffices). The proof of our lower bound is based on a new combinatorial lemma called the Lemma of Surprising Intersections (LOSI) which allows us to use a proof methodology where we first analyze the intersection structure of the positive queries by using encoding arguments, and then use statistical arguments to deduce properties of the intersection structure of all queries, even the negative ones. In most other data structure arguments that we know, it is difficult to argue anything about the negative queries. Therefore we believe that the LOSI and this proof methodology might find future uses for other problems.},
  citationcount = {27},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,dynamic,lower bound,query,query time,update,update time}
}

@article{victorreisApproximateCaratheodoryBounds2022,
  title = {Approximate {{Carath{\'e}odory}} Bounds via {{Discrepancy Theory}}},
  author = {Victor Reis and T. Rothvoss},
  year = {2022},
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2207.03614},
  abstract = {The approximate Carath{\textbackslash}'eodory problem in general form is as follows: Given two symmetric convex bodies \$P,Q {\textbackslash}subseteq {\textbackslash}mathbb\{R\}{\textasciicircum}m\$, a parameter \$k {\textbackslash}in {\textbackslash}mathbb\{N\}\$ and \${\textbackslash}mathbf\{z\} {\textbackslash}in {\textbackslash}textrm\{conv\}(X)\$ with \$X {\textbackslash}subseteq P\$, find \${\textbackslash}mathbf\{v\}\_1,{\textbackslash}ldots,{\textbackslash}mathbf\{v\}\_k {\textbackslash}in X\$ so that \${\textbackslash}{\textbar}{\textbackslash}mathbf\{z\} - {\textbackslash}frac\{1\}\{k\}{\textbackslash}sum\_\{i=1\}{\textasciicircum}k {\textbackslash}mathbf\{v\}\_i{\textbackslash}{\textbar}\_Q\$ is minimized. Maurey showed that if both \$P\$ and \$Q\$ coincide with the \${\textbackslash}{\textbar} {\textbackslash}cdot {\textbackslash}{\textbar}\_p\$-ball, then an error of \$O({\textbackslash}sqrt\{p/k\})\$ is possible. We prove a reduction to the vector balancing constant from discrepancy theory which for most cases can provide tight bounds for general \$P\$ and \$Q\$. For the case where \$P\$ and \$Q\$ are both \${\textbackslash}{\textbar} {\textbackslash}cdot {\textbackslash}{\textbar}\_p\$-balls we prove an upper bound of \${\textbackslash}sqrt\{ {\textbackslash}frac\{{\textbackslash}min{\textbackslash}\{ p, {\textbackslash}log ({\textbackslash}frac\{2m\}\{k\}) {\textbackslash}\}\}\{k\}\}\$. Interestingly, this bound cannot be obtained taking independent random samples; instead we use the Lovett-Meka random walk. We also prove an extension to the more general case where \$P\$ and \$Q\$ are \${\textbackslash}{\textbar}{\textbackslash}cdot {\textbackslash}{\textbar}\_p\$ and \${\textbackslash}{\textbar} {\textbackslash}cdot {\textbackslash}{\textbar}\_q\$-balls with \$2 {\textbackslash}leq p {\textbackslash}leq q {\textbackslash}leq {\textbackslash}infty\$.},
  keywords = {reduction},
  annotation = {Citation Count: 0}
}

@article{vignaQuasiSuccinctIndices2012,
  title = {Quasi-Succinct Indices},
  author = {Vigna, S.},
  year = {2012},
  doi = {10.1145/2433396.2433409},
  abstract = {Compressed inverted indices in use today are based on the idea of gap compression: documents pointers are stored in increasing order, and the gaps between successive document pointers are stored using suitable codes which represent smaller gaps using less bits. Additional data such as counts and positions is stored using similar techniques. A large body of research has been built in the last 30 years around gap compression, including theoretical modeling of the gap distribution, specialized instantaneous codes suitable for gap encoding, and ad hoc document reorderings which increase the efficiency of instantaneous codes. This paper proposes to represent an index using a different architecture based on quasi-succinct representation of monotone sequences. We show that, besides being theoretically elegant and simple, the new index provides expected constant-time operations, space savings, and, in practice, significant performance improvements on conjunctive, phrasal and proximity queries.},
  citationcount = {93},
  venue = {Web Search and Data Mining},
  keywords = {query}
}

@article{vilfanLowerBoundsFor1976,
  title = {Lower Bounds for the Size of Expressions for Certain Functions in d -Ary Logic},
  author = {Vilfan, B.},
  year = {1976},
  doi = {10.1016/0304-3975(76)90035-9},
  abstract = {No abstract available},
  citationcount = {7},
  venue = {Theoretical Computer Science}
}

@article{vinyalsSimplifiedAndImproved2020,
  title = {Simplified and Improved Separations between Regular and General Resolution by Lifting},
  author = {Vinyals, Marc and Elffers, J. and Johannsen, Jan and Nordstr{\"o}m, Jakob},
  year = {2020},
  doi = {10.1007/978-3-030-51825-7_14},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {International Conference on Theory and Applications of Satisfiability Testing}
}

@article{violaHowToStore2019,
  title = {How to Store a Random Walk},
  author = {Viola, Emanuele and Weinstein, Omri and Yu, Huacheng},
  year = {2019},
  doi = {10.1137/1.9781611975994.26},
  abstract = {Motivated by storage applications, we study the following data structure problem: An encoder wishes to store a collection of jointly-distributed files \{X\}:=(X{$_1$},X{$_2$},{\dots},X\textsubscript{n}){$\sim\mu$} which are \{correlated\} (H\textsubscript{{$\mu$}}(\{X\}){$\ll\sum$}\textsubscript{i}H\textsubscript{{$\mu$}}(X\textsubscript{i})), using as little (expected) memory as possible, such that each individual file X\textsubscript{i} can be recovered quickly with few (ideally constant) memory accesses. In the case of independent random files, a dramatic result by (FOCS'08) and subsequently by Dodis, and Thorup (STOC'10) shows that it is possible to store \{X\} using just a \{constant\} number of extra bits beyond the information-theoretic minimum space, while at the same time decoding each X\textsubscript{i} in constant time. However, in the (realistic) case where the files are correlated, much weaker results are known, requiring at least {\textohm}(n/polyn) extra bits for constant decoding time, even for "simple" joint distributions {$\mu$}. We focus on the natural case of compressing\{Markov chains\}, i.e., storing a length-n random walk on any (possibly directed) graph G. Denoting by {$\kappa$}(G,n) the number of length-n walks on G, we show that there is a succinct data structure storing a random walk using {$_{2}\kappa$}(G,n)+O(n) bits of space, such that any vertex along the walk can be decoded in O(1) time on a word-RAM. For the harder task of matching the \{point-wise\} optimal space of the walk, i.e., the empirical entropy {$\sum$}\textsubscript{\{\vphantom\}}i=1\vphantom\{\}\textsuperscript{\{\vphantom\}}n-1\vphantom\{\}(deg(v\textsubscript{i})), we present a data structure with O(1) extra bits at the price of O(n) decoding time, and show that any improvement on this would lead to an improved solution on the long-standing Dictionary problem. All of our data structures support the \{online\} version of the problem with constant update and query time.},
  citationcount = {6},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,information theoretic,query,query time,update}
}

@article{violaLowerBoundsData2019,
  title = {Lower Bounds for Data Structures with Space Close to Maximum Imply Circuit Lower Bounds},
  author = {Viola, Emanuele},
  year = {2019},
  doi = {10.4086/toc.2019.v015a018},
  abstract = {: Let f : \{ 0 , 1 \} n {$\rightarrow$} \{ 0 , 1 \} m be a function computable by a circuit with unbounded fan-in, arbitrary gates, w wires and depth d . With a very simple argument we show that the m -query problem corresponding to f has data structures with space s = n + r and time ( w / r ) d , for any r . As a consequence, in the setting where s is close to m a slight improvement on the state of existing data-structure lower bounds would solve long-standing problems in circuit complexity. We also use this connection to obtain a data structure for error-correcting codes which nearly matches the 2007 lower bound by G{\'a}l and Miltersen. This data structure can also be made dynamic. Finally we give a problem that requires at least 3 bit probes for m = n O ( 1 ) and even s = m / 2 - 1. Independent work by Dvir, Golovnev, and Weinstein (2018) and by Corrigan-Gibbs and Kogan (2018) give incomparable connections between data-structure and other types of lower bounds.},
  citationcount = {18},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {data structure,dynamic,lower bound,query}
}

@article{violaNewSamplingLower2023,
  title = {New Sampling Lower Bounds via the Separator},
  author = {Viola, Emanuele},
  year = {2023},
  doi = {10.4230/LIPIcs.CCC.2023.26},
  abstract = {Suppose that a target distribution can be approximately sampled by a low-depth decision tree, or more generally by an efficient cell-probe algorithm. It is shown to be possible to restrict the input to the sampler so that its output distribution is still not too far from the target distribution},
  citationcount = {3},
  venue = {Cybersecurity and Cyberforensics Conference},
  keywords = {cell probe,lower bound}
}

@article{violaSamplingLowerBounds2020,
  title = {Sampling Lower Bounds: Boolean Average-Case and Permutations},
  author = {Viola, Emanuele},
  year = {2020},
  doi = {10.1137/18m1198405},
  abstract = {We show that for every small AC\textsuperscript{\{\vphantom\}}0\vphantom\{\} circuit C: 0,1 \textsuperscript{\{\vphantom\}}{$\ell$}\vphantom\{\}{$\rightarrow$} 0,1 \textsuperscript{\{\vphantom\}}m\vphantom\{\} there exists a multiset S of 2\textsuperscript{\{\vphantom\}}m-m\textsuperscript{\{\vphantom\}}{\textohm}(1)\vphantom\{\}\vphantom\{\} restrictions that preserve the output distribution of C and, mo...},
  citationcount = {15},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {lower bound}
}

@article{violaTheComplexityOf2005,
  title = {The Complexity of Constructing Pseudorandom Generators from Hard Functions},
  author = {Viola, Emanuele},
  year = {2005},
  doi = {10.1007/s00037-004-0187-1},
  abstract = {No abstract available},
  citationcount = {94},
  venue = {Computational Complexity}
}

@article{vishwasbhargavaDeterministicFactorizationSparse2018,
  title = {Deterministic {{Factorization}} of {{Sparse Polynomials}} with {{Bounded Individual Degree}}},
  author = {Vishwas Bhargava and Shubhangi Saraf and Ilya Volkovich},
  year = {2018},
  journal = {IEEE Annual Symposium on Foundations of Computer Science},
  doi = {10.1145/3365667},
  abstract = {In this paper we study the problem of deterministic factorization of sparse polynomials. We show that if f is an n-variate polynomial with s monomials, with individual degrees of its variables bounded by d, then f can be deterministically factored in time s{\textasciicircum}poly(d) log n. Prior to our work, the only efficient factoring algorithms known for this class of polynomials were randomized, and other than for the cases of d=1 and d=2, only exponential time deterministic factoring algorithms were known. A crucial ingredient in our proof is a quasi-polynomial sparsity bound for factors of sparse polynomials of bounded individual degree. In particular we show if f is an s-sparse polynomial in n variables, with individual degrees of its variables bounded by d, then the sparsity of each factor of f is bounded by s{\textasciicircum}O(d{\textasciicircum}2 log n). This is the first nontrivial bound on factor sparsity for d{$>$}2. Our sparsity bound uses techniques from convex geometry, such as the theory of Newton polytopes and an approximate version of the classical Carath{\'e}odory's Theorem. Our work addresses and partially answers a question of von zur Gathen and Kaltofen (JCSS 1985) who asked whether a quasi-polynomial bound holds for the sparsity of factors of sparse polynomials.},
  annotation = {Citation Count: 17}
}

@article{vitterAlgorithmsAndData2008,
  title = {Algorithms and Data Structures for External Memory},
  author = {Vitter, J.},
  year = {2008},
  doi = {10.1561/0400000014},
  abstract = {Data sets in large applications are often too massive to fit completely inside the computer's internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this manuscript, we survey the state of the art in the design and analysis of algorithms and data structures for external memory (or EM for short), where the goal is to exploit locality and parallelism in order to reduce the I/O costs. We consider a variety of EM paradigms for solving batched and online problems efficiently in external memory. For the batched problem of sorting and related problems like permuting and fast Fourier transform, the key paradigms include distribution and merging. The paradigm of disk striping offers an elegant way to use multiple disks in parallel. For sorting, however, disk striping can be nonoptimal with respect to I/O, so to gain further improvements we discuss distribution and merging techniques for using the disks independently. We also consider useful techniques for batched EM problems involving matrices, geometric data, and graphs. In the online domain, canonical EM applications include dictionary lookup and range searching. The two important classes of indexed data structures are based upon extendible hashing and B-trees. The paradigms of filtering and bootstrapping provide convenient means in online data structures to make effective use of the data accessed from disk. We also re-examine some of the above EM problems in slightly different settings, such as when the data items are moving, when the data items are variable-length such as character strings, when the data structure is compressed to save space, or when the allocated amount of internal memory can change dynamically. Programming tools and environments are available for simplifying the EM programming task. We report on some experiments in the domain of spatial databases using the TPIE system (Transparent Parallel I/O programming Environment). The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than other methods used in practice.},
  citationcount = {227},
  venue = {Foundations and Trends{\textregistered} in Theoretical Computer Science}
}

@article{vladimirkolovskiOptimizingEnterpriseScaleOWL2010,
  title = {Optimizing {{Enterprise-Scale OWL}} 2 {{RL Reasoning}} in a {{Relational Database System}}},
  author = {Vladimir Kolovski and Zhe Wu and G. Eadon},
  year = {2010},
  journal = {International Workshop on the Semantic Web},
  doi = {10.1007/978-3-642-17746-0_28},
  annotation = {Citation Count: 58}
}

@article{vuSomeNewResults2007,
  title = {Some New Results on Subset Sums},
  author = {Vu, V.},
  year = {2007},
  doi = {10.1016/J.JNT.2006.08.012},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {No venue available}
}

@article{w.watsonFinancialTableExtraction2020,
  title = {Financial Table Extraction in Image Documents},
  author = {W. Watson and Bo Liu},
  year = {2020},
  journal = {International Conference on AI in Finance},
  doi = {10.1145/3383455.3422520},
  abstract = {Table extraction has long been a pervasive problem in financial services. This is more challenging in the image domain, where content is locked behind cumbersome pixel format. Luckily, advances in deep learning for image segmentation, OCR, and sequence modeling provides the necessary heavy lifting to achieve impressive results. This paper presents an end-to-end pipeline for identifying, extracting and transcribing tabular content in image documents, while retaining the original spatial relations with high fidelity.},
  annotation = {Citation Count: 4}
}

@article{wadhwaDistributedLocalitySensitivity2010,
  title = {Distributed Locality Sensitivity Hashing},
  author = {Wadhwa, Smita and Gupta, Pawan},
  year = {2010},
  doi = {10.1109/CCNC.2010.5421655},
  abstract = {In this paper, we present DLSH Distributed Locality Sensitive Hashing, a similar-data search technology. The huge growth in the size of video content has broken the traditional multi-media index hosting and look-up solutions, these are not able to scale to the size of the current and projected index requirements. Distributed LSH (D-LSH) addresses this need of a highly scalable multi-media index. DLSH performs better for finding approximate near neighbors on extremely large scales, as DLSH distributes close points on single boxes, and far points on different boxes based on projections.},
  citationcount = {6},
  venue = {2010 7th IEEE Consumer Communications and Networking Conference}
}

@article{wajcMatchingTheoryUnder2021,
  title = {Matching Theory under Uncertainty},
  author = {Wajc, David},
  year = {2021},
  doi = {10.1184/r1/14428181},
  abstract = {Traditionally, optimization in computer science has been studied in the full information setting: data is collected, a program is run, and then the output is used. However, the increasing pervasiveness of user-facing applications is increasingly shifting the focus to computation under incomplete information: data is generated continuously by users, who expect their new data to quickly affect the externalized solution. This modern computational paradigm motivates a renewed interest in computation under uncertainty (about the input), including online, dynamic and streaming algorithms. Many problems providing the renewed impetus for studying algorithms under uncertainty come from the field of matching theory---the study of pairing agents/items. Examples abound, arising in disparate applications, from ride-sharing apps, to Internet advertising, to online gaming. This motivates the study of matching theory under uncertainty. Moreover, the study of matching theory has historically played a key role in the development of immensely influential techniques for computation more broadly. An additional motivation for studying matching theory under uncertainty, then, is its potential to provide similar fundamental insights for computation under uncertainty more broadly. In this thesis we answer several longstanding open problems in the area of matching theory under uncertainty, and hint at some methods with potential broader applicability to computation under uncertainty. This again illustrates the pivotal role of matching theory, this time in modern settings. In Part I, we study online algorithms; here, the input is revealed in parts, in some adversarial order, in the form of requests, to which we must respond immediately and irrevocably. In Part II, we study online algorithms under structural and stochastic assumptions which are motivated by information about practical inputs of interest, allowing for better guarantees than possible for worst-case inputs. Finally, in Part III, we study dynamic algorithms, where the input is constantly changing, and the algorithm's choices, while not irrevocable, must be quick; in the same part, we study streaming algorithms, motivated by big-data applications, where choices are not irrevocable, but are restricted to only using a limited amount of memory compared to the (massive) input size.},
  citationcount = {3},
  venue = {No venue available},
  keywords = {dynamic}
}

@article{wajcRoundingDynamicMatchings2019,
  title = {Rounding Dynamic Matchings against an Adaptive Adversary},
  author = {Wajc, David},
  year = {2019},
  doi = {10.1145/3357713.3384258},
  abstract = {We present a new dynamic matching sparsification scheme. From this scheme we derive a framework for dynamically rounding fractional matchings against adaptive adversaries. Plugging in known dynamic fractional matching algorithms into our framework, we obtain numerous randomized dynamic matching algorithms which work against adaptive adversaries. In contrast, all previous randomized algorithms for this problem assumed a weaker, oblivious, adversary. Our dynamic algorithms against adaptive adversaries include, for any constant {\cyrchar\cyrie} {\textquestiondown}0, a (2+{\cyrchar\cyrie})-approximate algorithm with constant update time or polylog worst-case update time, as well as (2-{$\delta$})-approximate algorithms in bipartite graphs with arbitrarily-small polynomial update time. All these results achieve polynomially better update time to approximation trade-offs than previously known to be achievable against adaptive adversaries.},
  citationcount = {54},
  venue = {Symposium on the Theory of Computing},
  keywords = {adaptive,dynamic,update,update time}
}

@article{walkerNewFastMethod1974,
  title = {New Fast Method for Generating Discrete Random Numbers with Arbitrary Frequency Distributions},
  author = {Walker, A. J.},
  year = {1974},
  doi = {10.1049/EL:19740097},
  abstract = {A new method for generating discrete random numbers with arbitrary amplitude/frequency distributions is presented. It consists essentially of amplitude manipulation of uniformly distributed, statistically independent sequential numbers. A digital-hardware implementation of the system features fast single-clock operation, with good statistical properties, and is well suited to software implementation.},
  citationcount = {227},
  venue = {No venue available}
}

@article{wangAnIndexFor2024,
  title = {An Index for Set Intersection with Post-Filtering},
  author = {Wang, Ru and Lu, Shangqi and Tao, Yufei},
  year = {2024},
  doi = {10.1109/TKDE.2023.3329145},
  abstract = {This paper studies how to design an index structure on a collection of sets {\textexclamdown}inline-formula{\textquestiondown}{\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}S\textsubscript{\{\vphantom\}}1\vphantom\{\},S\textsubscript{\{\vphantom\}}2\vphantom\{\},\{{\dots}\},S\textsubscript{\{\vphantom\}}n\vphantom\{\}{\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}alternatives{\textquestiondown}{\textexclamdown}mml:math{\textquestiondown}{\textexclamdown}mml:mrow{\textquestiondown}{\textexclamdown}mml:msub{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}S{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mn{\textquestiondown}1{\textexclamdown}/mml:mn{\textquestiondown}{\textexclamdown}/mml:msub{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown},{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:msub{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}S{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mn{\textquestiondown}2{\textexclamdown}/mml:mn{\textquestiondown}{\textexclamdown}/mml:msub{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown},{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}...{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown},{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:msub{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}S{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}n{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}/mml:msub{\textquestiondown}{\textexclamdown}/mml:mrow{\textquestiondown}{\textexclamdown}/mml:math{\textquestiondown}{\textexclamdown}inline-graphic xlink:href="tao-ieq1-3329145.gif"/{\textquestiondown}{\textexclamdown}/alternatives{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} to answer the following queries: given distinct set ids {\textexclamdown}inline-formula{\textquestiondown}{\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}a,b{$\in$}[1,n]{\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}alternatives{\textquestiondown}{\textexclamdown}mml:math{\textquestiondown}{\textexclamdown}mml:mrow{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}a{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown},{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}b{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}{$\in$}{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}[{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:mn{\textquestiondown}1{\textexclamdown}/mml:mn{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown},{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}n{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}]{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}/mml:mrow{\textquestiondown}{\textexclamdown}/mml:math{\textquestiondown}{\textexclamdown}inline-graphic xlink:href="tao-ieq2-3329145.gif"/{\textquestiondown}{\textexclamdown}/alternatives{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}, report {\textexclamdown}inline-formula{\textquestiondown}{\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}F(S\textsubscript{\{\vphantom\}}a\vphantom\{\}{$\cap$}S\textsubscript{\{\vphantom\}}b\vphantom\{\}){\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}alternatives{\textquestiondown}{\textexclamdown}mml:math{\textquestiondown}{\textexclamdown}mml:mrow{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}F{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}({\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:msub{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}S{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}a{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}/mml:msub{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}{$\cap$}{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:msub{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}S{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}b{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}/mml:msub{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}){\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}/mml:mrow{\textquestiondown}{\textexclamdown}/mml:math{\textquestiondown}{\textexclamdown}inline-graphic xlink:href="tao-ieq3-3329145.gif"/{\textquestiondown}{\textexclamdown}/alternatives{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} where {\textexclamdown}inline-formula{\textquestiondown}{\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}F(.){\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}alternatives{\textquestiondown}{\textexclamdown}mml:math{\textquestiondown}{\textexclamdown}mml:mrow{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}F{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}({\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}.{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}){\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}/mml:mrow{\textquestiondown}{\textexclamdown}/mml:math{\textquestiondown}{\textexclamdown}inline-graphic xlink:href="tao-ieq4-3329145.gif"/{\textquestiondown}{\textexclamdown}/alternatives{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown} is a filtering function. We present a solution that can support a great variety of filtering functions --- range research, skyline, convex hull, nearest neighbor search, quantile (to name just a few) --- with attractive performance guarantees. The guarantees are sensitive to the set collection's {\textexclamdown}italic{\textquestiondown}pseudoarboricity{\textexclamdown}/italic{\textquestiondown}, a new notion for quantifying the density of {\textexclamdown}inline-formula{\textquestiondown}{\textexclamdown}tex-math notation="LaTeX"{\textquestiondown}\{S\vphantom\}\textsubscript{\{\vphantom\}}1\vphantom\{\},S\textsubscript{\{\vphantom\}}2\vphantom\{\},\{{\dots}\},S\textsubscript{\{\vphantom\}}n\vphantom\{\}\vphantom\{\}{\textexclamdown}/tex-math{\textquestiondown}{\textexclamdown}alternatives{\textquestiondown}{\textexclamdown}mml:math{\textquestiondown}{\textexclamdown}mml:mrow{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}\{{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:msub{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}S{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mn{\textquestiondown}1{\textexclamdown}/mml:mn{\textquestiondown}{\textexclamdown}/mml:msub{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown},{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:msub{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}S{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mn{\textquestiondown}2{\textexclamdown}/mml:mn{\textquestiondown}{\textexclamdown}/mml:msub{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown},{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}...{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown},{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}mml:msub{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}S{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}mml:mi{\textquestiondown}n{\textexclamdown}/mml:mi{\textquestiondown}{\textexclamdown}/mml:msub{\textquestiondown}{\textexclamdown}mml:mo{\textquestiondown}\}{\textexclamdown}/mml:mo{\textquestiondown}{\textexclamdown}/mml:mrow{\textquestiondown}{\textexclamdown}/mml:math{\textquestiondown}{\textexclamdown}inline-graphic xlink:href="tao-ieq5-3329145.gif"/{\textquestiondown}{\textexclamdown}/alternatives{\textquestiondown}{\textexclamdown}/inline-formula{\textquestiondown}. Our index structures are simple to understand and implement.},
  citationcount = {Unknown},
  venue = {IEEE Transactions on Knowledge and Data Engineering},
  keywords = {query}
}

@inproceedings{wangCertificatesDataStructures2014,
  title = {Certificates in {{Data Structures}}},
  booktitle = {Automata, {{Languages}}, and {{Programming}}},
  author = {Wang, Yaoyu and Yin, Yitong},
  editor = {Esparza, Javier and Fraigniaud, Pierre and Husfeldt, Thore and Koutsoupias, Elias},
  year = {2014},
  pages = {1039--1050},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-43948-7_86},
  abstract = {We study certificates in static data structures. In the cell-probe model, certificates are the cell probes which can uniquely identify the answer to the query. As a natural notion of nondeterministic cell probes, lower bounds for certificates in data structures immediately imply deterministic cell-probe lower bounds. In spite of this extra power brought by nondeterminism, we prove that two widely used tools for cell-probe lower bounds: richness lemma of Miltersen et al. [9] and direct-sum richness lemma of P{\v a}tra{\c s}cu and Thorup [15], both hold for certificates in data structures with even better parameters. Applying these lemmas and adopting existing reductions, we obtain certificate lower bounds for a variety of static data structure problems. These certificate lower bounds are at least as good as the highest known cell-probe lower bounds for the respective problems. In particular, for approximate near neighbor (ANN) problem in Hamming distance, our lower bound improves the state of the art. When the space is strictly linear, our lower bound for ANN in d-dimensional Hamming space becomes t\,=\,{\textohm}(d), which along with the recent breakthrough for polynomial evaluation of Larsen [7], are the only two t\,=\,{\textohm}(d) lower bounds ever proved for any problems in the cell-probe model.},
  isbn = {978-3-662-43948-7},
  langid = {english},
  keywords = {cell probe,data structure,lower bound},
  file = {/Users/tulasi/Zotero/storage/UXALMUKS/Wang and Yin - 2014 - Certificates in Data Structures.pdf}
}

@article{wangChangePropagationWithout2023,
  title = {Change Propagation without Joins},
  author = {Wang, Qichen and Hu, Xiao and Dai, Binyang and Yi, K.},
  year = {2023},
  doi = {10.14778/3579075.3579080},
  abstract = {We revisit the classical change propagation framework for query evaluation under updates. The standard framework takes a query plan and materializes the intermediate views, which incurs high polynomial costs in both space and time, with the join operator being the culprit. In this paper, we propose a new change propagation framework without joins, thus naturally avoiding this polynomial blowup. Meanwhile, we show that the new framework still supports constant-delay enumeration of both the deltas and the full query results, the same as in the standard framework. Furthermore, we provide a quantitative analysis of its update cost, which not only recovers many recent theoretical results on the problem, but also yields an effective approach to optimizing the query plan. The new framework is also easy to be integrated into an existing streaming database system. Experimental results show that our system prototype, implemented using Flink DataStream API, significantly outperforms other systems in terms of space, time, and latency.},
  citationcount = {9},
  venue = {Proceedings of the VLDB Endowment},
  keywords = {query,update}
}

@article{wangConjunctiveQueriesWith2022,
  title = {Conjunctive Queries with Comparisons},
  author = {Wang, Qichen and Yi, Ke},
  year = {2022},
  doi = {10.1145/3514221.3517830},
  abstract = {Conjunctive queries with predicates in the form of comparisons that span multiple relations have regained interest recently, due to their relevance in OLAP queries, spatiotemporal databases, and machine learning over relational data. The standard technique, predicate pushdown, has limited efficacy on such comparisons. A technique by Willard can be used to process short comparisons that are adjacent in the join tree in time linear in the input size plus output size. In this paper, we describe a new algorithm for evaluating conjunctive queries with both short and long comparisons, and identify an acyclic condition under which linear time can be achieved. We have also implemented the new algorithm on top of Spark, and our experimental results demonstrate order-of-magnitude speedups over SparkSQL on a variety of graph pattern and analytical queries.},
  citationcount = {8},
  venue = {SIGMOD Conference},
  keywords = {query}
}

@article{wangConjunctiveQueriesWith2023,
  title = {Conjunctive Queries with Comparisons},
  author = {Wang, Qichen and Yi, K.},
  year = {2023},
  doi = {10.1145/3604437.3604450},
  abstract = {Conjunctive queries with predicates in the form of comparisons that span multiple relations have regained interest recently, due to their relevance in OLAP queries, spatiotemporal databases, and machine learning over relational data. The standard technique, predicate pushdown, has limited efficacy on such comparisons. A technique by Willard can be used to process short comparisons that are adjacent in the join tree in time linear in the input size plus output size. In this paper, we describe a new algorithm for evaluating conjunctive queries with both short and long comparisons, and identify an acyclic condition under which linear time can be achieved. We have also implemented the new algorithm on top of Spark, and our experimental results demonstrate order-of-magnitude speedups over SparkSQL on a variety of graph patterns and analytical queries.},
  citationcount = {9},
  venue = {SIGMOD record},
  keywords = {query}
}

@article{wangDoubleEndedPalindromic2022,
  title = {Double-Ended Palindromic Trees: A Linear-Time Data Structure and Its Applications},
  author = {Wang, Qisheng and Yang, Ming and Zhu, Xinrui},
  year = {2022},
  doi = {10.48550/arXiv.2210.02292},
  abstract = {The palindromic tree (a.k.a. eertree) is a linear-size data structure that provides access to all palindromic substrings of a string. In this paper, we propose a generalized version of eertree, called double-ended eertree, which supports linear-time online double-ended queue operations on the stored string. At the heart of our construction, is a class of substrings, called surfaces, of independent interest. Namely, surfaces are neither prefixes nor suffixes of any other palindromic substrings and characterize the link structure of all palindromic substrings in the eertree. As an application, we develop a framework for range queries involving palindromes on strings, including counting distinct palindromic substrings, and finding the longest palindromic substring, shortest unique palindromic substring and shortest absent palindrome of any substring. In particular, offline queries only use linear space. Apart from range queries, we enumerate palindromic rich strings with a given word in linear time on the length of the given word.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {data structure,query}
}

@article{wangEfficientDataStructures2013,
  title = {Efficient Data Structures for Range Selections Problem},
  author = {Wang, Xiao and Tian, Jun},
  year = {2013},
  doi = {10.4028/www.scientific.net/AMR.756-759.1387},
  abstract = {Building an efficient data structure for range selection problems is considered. While there are several theoretical solutions to the problem, only a few have been tried out, and there is little idea on how the others would perform. The computation model used in this paper is the RAM model with word-size . Our data structure is a practical linear space data structure that supports range selection queries in time with preprocessing time.},
  citationcount = {Unknown},
  venue = {No venue available},
  keywords = {data structure,query}
}

@article{wangHighDimensionalNearest2002,
  title = {High-Dimensional Nearest Neighbor Search with Remote Data Centers},
  author = {Wang, Changzhou and Wang, X.},
  year = {2002},
  doi = {10.1007/s101150200015},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {Knowledge and Information Systems}
}

@article{wangOLogLog2008,
  title = {O(Log Log n)-Competitive Binary Search Tree},
  author = {Wang, C.},
  year = {2008},
  doi = {10.1007/978-0-387-30162-4_263},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {Encyclopedia of Algorithms}
}

@article{wangSharpSquareFunction1991,
  title = {Sharp Square-Function Inequalities for Conditionally Symmetric Martingales},
  author = {Wang, Gang},
  year = {1991},
  doi = {10.1090/S0002-9947-1991-1018577-3},
  abstract = {Let t be a conditionally symmetric martingale taking values in a Hilbert space 1Ei and let S(i) be its square function. If vp is the smallest positive zero of the confluent hypergeometric function and Ap is the largest positive zero of the parabolic cylinder function of parameter p, then the following inequalities are sharp: 3, vp IIS({\textbar})llp 2. Moreover, the constants vp and Ap for the cases mentioned above are also best possible for the Marcinkiewicz-Paley inequalities for Haar functions.},
  citationcount = {51},
  venue = {No venue available}
}

@article{wangSpaceEfficientRandomized2014,
  title = {Space-Efficient Randomized Algorithms for k-{{SUM}}},
  author = {Wang, Joshua R.},
  year = {2014},
  doi = {10.1007/978-3-662-44777-2_67},
  abstract = {No abstract available},
  citationcount = {17},
  venue = {Embedded Systems and Applications}
}

@article{wangSupportingSubseriesNearest2000,
  title = {Supporting Subseries Nearest Neighbor Search via Approximation},
  author = {Wang, Changzhou and Wang, X.},
  year = {2000},
  doi = {10.1145/354756.354834},
  abstract = {Searc hingfor nearest neigh b orsin a large set of time series is an importan tdata mining task. This paper studies the following type of time series nearest neighbor queries: Given a query series and a starting time, among all the subseries (of a collection of data series) that have the same length as the query series and start at the given time, nd the K subseries that are closest to the query series. T o support such queries, the paper develops a tec hnique that uses a xed number of values to approximate each whole data series, and obtains the appro ximationof an y required subseries at the query time. The paper then proposes three subseries search algorithms and compares them with the naive method that sequen tially scans the whole data set, as well as a method adapted from a state-of-art subseries search algorithm. Experiments are conducted on both a real-life data set and a synthetic one. Results show that the proposed methods access only a small portion of the precise data and outperform the others in run time.},
  citationcount = {4},
  venue = {International Conference on Information and Knowledge Management},
  keywords = {query,query time}
}

@article{warshauerTheLockerPuzzle2006,
  title = {The Locker Puzzle},
  author = {Warshauer, Max L. and Curtin, Eucene},
  year = {2006},
  doi = {10.1007/BF02986999},
  abstract = {No abstract available},
  citationcount = {17},
  venue = {No venue available}
}

@article{wasaPolynomialDelayAnd2013,
  title = {Polynomial Delay and Space Discovery of Connected and Acyclic Sub-Hypergraphs in a Hypergraph},
  author = {Wasa, Kunihiro and Uno, T. and Hirata, Kouichi and Arimura, Hiroki},
  year = {2013},
  doi = {10.1007/978-3-642-40897-7_21},
  abstract = {No abstract available},
  citationcount = {2},
  venue = {IFIP Working Conference on Database Semantics}
}

@article{watanabeFastAlgorithmsFor2019,
  title = {Fast Algorithms for the Shortest Unique Palindromic Substring Problem on Run-Length Encoded Strings},
  author = {Watanabe, Kiichi and Nakashima, Yuto and Inenaga, Shunsuke and Bannai, H. and Takeda, Masayuki},
  year = {2019},
  doi = {10.1007/s00224-020-09980-x},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {Theory of Computing Systems}
}

@article{watanabeShortestUniquePalindromic2019,
  title = {Shortest Unique Palindromic Substring Queries on Run-Length Encoded Strings},
  author = {Watanabe, Kiichi and Nakashima, Yuto and Inenaga, Shunsuke and Bannai, H. and Takeda, Masayuki},
  year = {2019},
  doi = {10.1007/978-3-030-25005-8_35},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {International Workshop on Combinatorial Algorithms},
  keywords = {query}
}

@article{watsonALowerBound2020,
  title = {A Lower Bound for Sampling Disjoint Sets},
  author = {Watson, Thomas},
  year = {2020},
  doi = {10.1145/3404858},
  abstract = {Suppose Alice and Bob each start with private randomness and no other input, and they wish to engage in a protocol in which Alice ends up with a set x{$\subseteq$} [n] and Bob ends up with a set y{$\subseteq$} [n], such that (x,y) is uniformly distributed over all pairs of disjoint sets. We prove that for some constant {$\beta$} {\textexclamdown} 1, this requires {\textohm} (n) communication even to get within statistical distance 1- {$\beta$}n of the target distribution. Previously, Ambainis, Schulman, Ta-Shma, Vazirani, and Wigderson (FOCS 1998) proved that {\textohm} ({\textsurd}n) communication is required to get within some constant statistical distance {$\varepsilon$} {\textquestiondown} 0 of the uniform distribution over all pairs of disjoint sets of size {\textsurd}n.},
  citationcount = {7},
  venue = {Electron. Colloquium Comput. Complex.},
  keywords = {communication,lower bound}
}

@article{watsonAZppnp12020,
  title = {A {{ZPPNP}}[1] Lifting Theorem},
  author = {Watson, Thomas},
  year = {2020},
  doi = {10.1145/3428673},
  abstract = {The complexity class ZPPNP[1] (corresponding to zero-error randomized algorithms with access to one NP oracle query) is known to have a number of curious properties. We further explore this class in the settings of time complexity, query complexity, and communication complexity. {$\bullet$} For starters, we provide a new characterization: ZPPNP[1] equals the restriction of BPPNP[1] where the algorithm is only allowed to err when it forgoes the opportunity to make an NP oracle query. {$\bullet$} Using the above characterization, we prove a query-to-communication lifting theorem, which translates any ZPPNP[1] decision tree lower bound for a function f into a ZPPNP[1] communication lower bound for a two-party version of f. {$\bullet$} As an application, we use the above lifting theorem to prove that the ZPPNP[1] communication lower bound technique introduced by G{\"o}{\"o}s, Pitassi, and Watson (ICALP 2016) is not tight. We also provide a ``primal'' characterization of this lower bound technique as a complexity class.},
  citationcount = {2},
  venue = {Symposium on Theoretical Aspects of Computer Science},
  keywords = {communication,communication complexity,lower bound,query,query complexity}
}

@article{weberSimilaritySearchIn2001,
  title = {Similarity Search in High-Dimensional Vector Spaces},
  author = {Weber, R.},
  year = {2001},
  doi = {10.3929/ETHZ-A-004126124},
  abstract = {This dissertation addresses the problem of identifying the most similar objects in a database given a set of reference objects and a set of features. It investigates the so-called "Curse of Dimensionality", and presents an organization for NN-Search ("Nearest Neighbour Search") optimized for high-dimensional spaces - the so-called "Vector Approximation File" (VA-File). The text shows the superiority of the VA-File theoretically and through experiments. The VA-File is also discussed with reference to approximate search and parallel search in a cluster of workstations. This dissertaion also provides an indexing technique that allows for interactive-time similarity search even in huge databases.},
  citationcount = {7},
  venue = {DISDBIS}
}

@article{weiEquivalenceBetweenPriority2012,
  title = {Equivalence between Priority Queues and Sorting in External Memory},
  author = {Wei, Zhewei and Yi, K.},
  year = {2012},
  doi = {10.1007/978-3-662-44777-2_68},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {Embedded Systems and Applications}
}

@article{weiMonotoneDescentPath2014,
  title = {Monotone Descent Path Queries on Dynamic Terrains},
  author = {Wei, Xiangzhi and Joneja, Ajay and Tian, Yao-bin and Yao, Yan-an},
  year = {2014},
  doi = {10.1115/1.4025780},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {Journal of Computing and Information Science in Engineering},
  keywords = {dynamic,query}
}

@article{weinsteinAmortizedDynamicCellprobe2016,
  title = {Amortized Dynamic Cell-Probe Lower Bounds from Four-Party Communication},
  author = {Weinstein, Omri and Yu, Huacheng},
  year = {2016},
  doi = {10.1109/FOCS.2016.41},
  abstract = {This paper develops a new technique for proving amortized, randomized cell-probe lower bounds on dynamic data structure problems. We introduce a new randomized nondeterministic four-party communication model that enables "accelerated", error-preserving simulations of dynamic data structures. We use this technique to prove an {\textohm}(n(log n/log log n)2) cell-probe lower bound for the dynamic 2D weighted orthogonal range counting problem (2D-ORC) with n/poly log n updates and n queries, that holds even for data structures with exp(-{\~{\textohm}}(n)) success probability. This result not only proves the highest amortized lower bound to date, but is also tight in the strongest possible sense, as a matching upper bound can be obtained by a deterministic data structure with worst-case operational time. This is the first demonstration of a "sharp threshold" phenomenon for dynamic data structures. Our broader motivation is that cell-probe lower bounds for exponentially small success facilitate reductions from dynamic to static data structures. As a proof-of-concept, we show that a slightly strengthened version of our lower bound would imply an {\textohm}((log n/log log n)2) lower bound for the static 3D-ORC problem with O(n logO(1) n) space. Such result would give a near quadratic improvement over the highest known static cell-probe lower bound, and break the long standing {\textohm}(log n) barrier for static data structures.},
  citationcount = {14},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {cell probe,communication,data structure,dynamic,lower bound,query,reduction,static,update}
}

@article{weissDataStructures2016,
  title = {Data Structures},
  author = {Weiss, Mark Allen},
  year = {2016},
  doi = {10.1007/0-387-22081-x_2},
  abstract = {No abstract available},
  citationcount = {114},
  venue = {Computing Handbook, 3rd ed.},
  keywords = {data structure}
}

@article{weissFastMedianAnd2006,
  title = {Fast Median and Bilateral Filtering},
  author = {Weiss, Ben},
  year = {2006},
  doi = {10.1145/1179352.1141918},
  abstract = {Median filtering is a cornerstone of modern image processing and is used extensively in smoothing and de-noising applications. The fastest commercial implementations (e.g. in Adobe{\textregistered} Photoshop{\textregistered} CS2) exhibit O(r) runtime in the radius of the filter, which limits their usefulness in realtime or resolution-independent contexts. We introduce a CPU-based, vectorizable O(log r) algorithm for median filtering, to our knowledge the most efficient yet developed. Our algorithm extends to images of any bit-depth, and can also be adapted to perform bilateral filtering. On 8-bit data our median filter outperforms Photoshop's implementation by up to a factor of fifty.},
  citationcount = {405},
  venue = {ACM Transactions on Graphics}
}

@article{weissIsThereAn2018,
  title = {Is There an Oblivious {{RAM}} Lower Bound for Online Reads?},
  author = {Weiss, Mor and Wichs, Daniel},
  year = {2018},
  doi = {10.1007/s00145-021-09392-1},
  abstract = {No abstract available},
  citationcount = {21},
  venue = {Journal of Cryptology},
  keywords = {lower bound}
}

@article{weiSubspaceCollisionAn2025,
  title = {Subspace Collision: {{An}} Efficient and Accurate Framework for High-Dimensional Approximate Nearest Neighbor Search},
  author = {Wei, Jiuqi and Lee, Xiaodong and Liao, Zhenyu and Palpanas, Themis and Peng, Botao},
  year = {2025},
  doi = {10.1145/3709729},
  abstract = {Approximate Nearest Neighbor (ANN) search in high-dimensional Euclidean spaces is a fundamental problem with a wide range of applications. However, there is currently no ANN method that performs well in both indexing and query answering performance, while providing rigorous theoretical guarantees for the quality of the answers. In this paper, we first design SC-score, a metric that we show follows the Pareto principle and can act as a proxy for the Euclidean distance between data points. Inspired by this, we propose a novel ANN search framework called Subspace Collision (SC), which can provide theoretical guarantees on the quality of its results. We further propose SuCo, which achieves efficient and accurate ANN search by designing a clustering-based lightweight index and query strategies for our proposed subspace collision framework. Extensive experiments on real-world datasets demonstrate that both the indexing and query answering performance of SuCo outperform state-of-the-art ANN methods that can provide theoretical guarantees, performing 1-2 orders of magnitude faster query answering with only up to one-tenth of the index memory footprint. Moreover, SuCo achieves top performance (best for hard datasets) even when compared to methods that do not provide theoretical guarantees.},
  citationcount = {Unknown},
  venue = {Proceedings of the ACM on Management of Data},
  keywords = {query}
}

@article{welzlPartitionTreesFor1988,
  title = {Partition Trees for Triangle Counting and Other Range Searching Problems},
  author = {Welzl, E.},
  year = {1988},
  doi = {10.1145/73393.73397},
  abstract = {The range searching problems which allow partition trees where every query enters only a sublinear number of nodes are characterized as those with finite Vapnik - Chervonenk is dimension. The concrete combinatorial bounds obtained imply---among others --- that every set of n points in the plane{\textexclamdown}list{\textquestiondown}{\textexclamdown}item{\textquestiondown}can be stored in an \&Ogr;(n) space data structure which allows triangle counting queries in \&Ogr;({\textsurd}n{$\cdot$} {\textexclamdown}italic{\textquestiondown}log{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}3{\textexclamdown}/supscrpt{\textquestiondown}n) time, and {\textexclamdown}/item{\textquestiondown}{\textexclamdown}item{\textquestiondown}can be stored in an \&Ogr;(n {$\cdot$} {\textexclamdown}italic{\textquestiondown}log{\textexclamdown}/italic{\textquestiondown} n) space data structure which allows disk counting queries in \&Ogr;({\textsurd}n{$\cdot$} {\textexclamdown}italic{\textquestiondown}log{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}3{\textexclamdown}/supscrpt{\textquestiondown}n) time; {\textexclamdown}/item{\textquestiondown}{\textexclamdown}/list{\textquestiondown} the preprocessing time for the data structures is polynomial. Recent results by Chazelle entail that these bounds for space and query time are optimal up to polylog --- factors.},
  citationcount = {117},
  venue = {SCG '88}
}

@article{wenmianyangTimeSyncVideoTag2019,
  title = {Time-{{Sync Video Tag Extraction Using Semantic Association Graph}}},
  author = {Wenmian Yang and Kun Wang and Na Ruan and Wenyuan Gao and Weijia Jia and Wei Zhao and Nan Liu and Yunyong Zhang},
  year = {2019},
  journal = {ACM Transactions on Knowledge Discovery from Data},
  doi = {10.1145/3332932},
  abstract = {Time-sync comments (TSCs) reveal a new way of extracting the online video tags. However, such TSCs have lots of noises due to users' diverse comments, introducing great challenges for accurate and fast video tag extractions. In this article, we propose an unsupervised video tag extraction algorithm named Semantic Weight-Inverse Document Frequency (SW-IDF). Specifically, we first generate corresponding semantic association graph (SAG) using semantic similarities and timestamps of the TSCs. Second, we propose two graph cluster algorithms, i.e., dialogue-based algorithm and topic center-based algorithm, to deal with the videos with different density of comments. Third, we design a graph iteration algorithm to assign the weight to each comment based on the degrees of the clustered subgraphs, which can differentiate the meaningful comments from the noises. Finally, we gain the weight of each word by combining Semantic Weight (SW) and Inverse Document Frequency (IDF). In this way, the video tags are extracted automatically in an unsupervised way. Extensive experiments have shown that SW-IDF (dialogue-based algorithm) achieves 0.4210 F1-score and 0.4932 MAP (Mean Average Precision) in high-density comments, 0.4267 F1-score and 0.3623 MAP in low-density comments; while SW-IDF (topic center-based algorithm) achieves 0.4444 F1-score and 0.5122 MAP in high-density comments, 0.4207 F1-score and 0.3522 MAP in low-density comments. It has a better performance than the state-of-the-art unsupervised algorithms in both F1-score and MAP.},
  annotation = {Citation Count: 13}
}

@article{wentongliLabelefficientSegmentationAffinity2023,
  title = {Label-Efficient {{Segmentation}} via {{Affinity Propagation}}},
  author = {Wentong Li and Yuqian Yuan and Song Wang and Wenyu Liu and Dongqi Tang and Jian Liu and Jianke Zhu and Lei Zhang},
  year = {2023},
  journal = {Neural Information Processing Systems},
  doi = {10.48550/arXiv.2310.10533},
  abstract = {Weakly-supervised segmentation with label-efficient sparse annotations has attracted increasing research attention to reduce the cost of laborious pixel-wise labeling process, while the pairwise affinity modeling techniques play an essential role in this task. Most of the existing approaches focus on using the local appearance kernel to model the neighboring pairwise potentials. However, such a local operation fails to capture the long-range dependencies and ignores the topology of objects. In this work, we formulate the affinity modeling as an affinity propagation process, and propose a local and a global pairwise affinity terms to generate accurate soft pseudo labels. An efficient algorithm is also developed to reduce significantly the computational cost. The proposed approach can be conveniently plugged into existing segmentation networks. Experiments on three typical label-efficient segmentation tasks, i.e. box-supervised instance segmentation, point/scribble-supervised semantic segmentation and CLIP-guided semantic segmentation, demonstrate the superior performance of the proposed approach.},
  annotation = {Citation Count: 5}
}

@article{werneckDynamicTrees2008,
  title = {Dynamic Trees},
  author = {Werneck, Renato F.},
  year = {2008},
  doi = {10.1007/978-1-4939-2864-4_121},
  abstract = {No abstract available},
  citationcount = {8},
  venue = {Encyclopedia of Algorithms},
  keywords = {dynamic}
}

@article{westbrookAmortizedAnalysisOf1988,
  title = {Amortized Analysis of Algorithms for Set Union with Backtracking},
  author = {Westbrook, J. and Tarjan, R.},
  year = {1988},
  doi = {10.1137/0218001},
  abstract = {Abstract : Mannila and Ukkonen have studied a variant of the classical disjoint set union (equivalence) problem in which an extra operation, called deunion, can undo the most recently performed union operation not yet undone. They proposed a way to modify standard set union algorithms to handle deunion operations. This document analyzes several algorithms based on their approach. The most efficient such algorithms have an amortized running time of O(log n/log log n) per operation, where n is the total number of elements in all the sets. These algorithms use O(n log n) space, but the space usage can be reduced to O(n) by a simple change. It is proven that any separable pointer-based algorithm for the problem required omega(log n/log log n) time per operation, thus showing that our upper bound an amortized time is tight. (KR)},
  citationcount = {41},
  venue = {SIAM journal on computing (Print)}
}

@article{westbrookFastIncrementalPlanarity1992,
  title = {Fast Incremental Planarity Testing},
  author = {Westbrook, J.},
  year = {1992},
  doi = {10.1007/3-540-55719-9_86},
  abstract = {No abstract available},
  citationcount = {71},
  venue = {International Colloquium on Automata, Languages and Programming}
}

@article{weylberDieGleichverteilung1916,
  title = {{\"U}ber Die Gleichverteilung von Zahlen Mod. {{Eins}}},
  author = {Weyl, H.},
  year = {1916},
  doi = {10.1007/BF01475864},
  abstract = {No abstract available},
  citationcount = {1404},
  venue = {No venue available}
}

@article{wilberLowerBoundsFor1986,
  title = {Lower Bounds for Accessing Binary Search Trees with Rotations},
  author = {Wilber, Robert E.},
  year = {1986},
  doi = {10.1137/0218004},
  abstract = {We describe two methods for obtaining lower bounds on the cost of accessing a sequence of nodes of a symmetrically ordered binary search tree, where rotations can be done on the tree. The bounds apply to offline as well as online algorithms.},
  citationcount = {115},
  venue = {27th Annual Symposium on Foundations of Computer Science (sfcs 1986)}
}

@article{wilkesTheArtOf1974,
  title = {The Art of Computer Programming, Volume 3, Sorting and Searching},
  author = {Wilkes, M.},
  year = {1974},
  doi = {10.1093/COMJNL/17.4.324},
  abstract = {No abstract available},
  citationcount = {1288},
  venue = {No venue available}
}

@article{willardAddingRangeRestriction1985,
  title = {Adding Range Restriction Capability to Dynamic Data Structures},
  author = {Willard, D. and Lueker, G. S.},
  year = {1985},
  doi = {10.1145/3828.3839},
  abstract = {A database is said to allow range restrictions if one may request that only records with some specified field in a specified range be considered when answering a given query. A transformation is presented that enables range restrictions to be added to an arbitrary dynamic data structure on {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} elements, provided that the problem satisfies a certain decomposability condition and that one is willing to allow increases by a factor of {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(log {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown}) in the worst-case time for an operation and in the space used. This is a generalization of a known transformation that works for static structures. This transformation is then used to produce a data structure for range queries in {\textexclamdown}italic{\textquestiondown}k{\textexclamdown}/italic{\textquestiondown} dimensions with worst-case times of {\textexclamdown}italic{\textquestiondown}O{\textexclamdown}/italic{\textquestiondown}(log{\textexclamdown}italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}k{\textexclamdown}/supscrpt{\textquestiondown} n{\textexclamdown}/italic{\textquestiondown}) for each insertion, deletion, or query operation.},
  citationcount = {181},
  venue = {JACM},
  keywords = {data structure,dynamic,query,static}
}

@article{willardADensityControl1992,
  title = {A Density Control Algorithm for Doing Insertions and Deletions in a Sequentially Ordered File in Good Worst-Case Time},
  author = {Willard, D.},
  year = {1992},
  doi = {10.1016/0890-5401(92)90034-D},
  abstract = {No abstract available},
  citationcount = {70},
  venue = {Information and Computation}
}

@article{willardLogLogarithmicWorst1983,
  title = {Log-Logarithmic Worst-Case Range Queries Are Possible in Space Theta({{N}})},
  author = {Willard, D.},
  year = {1983},
  doi = {10.1016/0020-0190(83)90075-3},
  abstract = {No abstract available},
  citationcount = {185},
  venue = {Information Processing Letters}
}

@article{willardLogLogarithmicWorst1983,
  title = {Log-Logarithmic Worst-Case Range Queries Are Possible in Space {$\oplus$}({{N}})},
  author = {Willard, D.},
  year = {1983},
  doi = {10.1016/0167-7136(83)90289-5},
  abstract = {No abstract available},
  citationcount = {254},
  venue = {No venue available}
}

@article{willardPolygonRetrieval1982,
  title = {Polygon Retrieval},
  author = {Willard, D.},
  year = {1982},
  doi = {10.1137/0211012},
  abstract = {No abstract available},
  citationcount = {110},
  venue = {SIAM journal on computing (Print)}
}

@article{willardSearchingUnindexedAnd1985,
  title = {Searching Unindexed and Nonuniformly Generated Files in Log Log {{N}} Time},
  author = {Willard, D.},
  year = {1985},
  doi = {10.1137/0214071},
  abstract = {The first algorithm that searches unindexed and nonuniformly distributed ordered files in N expected time is presented in this paper. Our analysis rests on a synthesis of concepts from the literature on interpolation search and on the method of regula falsi in numerical analysis.},
  citationcount = {41},
  venue = {SIAM journal on computing (Print)}
}

@article{williams3sumAndRelated2021,
  title = {{{3SUM}} and Related Problems in Fine-Grained Complexity (Invited Talk)},
  author = {Williams, V. V.},
  year = {2021},
  doi = {10.4230/LIPIcs.SoCG.2021.2},
  abstract = {3SUM is a simple to state problem: given a set S of n numbers, determine whether S contains three a, b, c so that a + b + c = 0. The fastest algorithms for the problem run in n 2 poly(log log n ) / (log n ) 2 time both when the input numbers are integers [1] (in the word RAM model with O (log n ) bit words) and when they are real numbers [2] (in the real RAM model). A hypothesis that is now central in Fine-Grained Complexity (FGC) states that 3SUM requires n 2 - o (1) time (on the real RAM for real inputs and on the word RAM with O (log n ) bit numbers for integer inputs). This hypothesis was first used in Computational Geometry by Gajentaan and Overmars [4] 1 who built a web of reductions showing that many geometric problems are hard, assuming that 3SUM is hard. The web of reductions within computational geometry has grown considerably since then (see some citations in [11]). A seminal paper by P{\textasciicaron}atra{\c s}cu [7] showed that the integer version of the 3SUM hypothesis can be used to prove polynomial conditional lower bounds for several problems in data structures and graph algorithms as well, extending the implications of the hypothesis to outside computational geometry. P{\textasciicaron}atra{\c s}cu proved an important tight equivalence between (integer) 3SUM and a problem called 3SUM-Convolution (see also [3]) that is easier to use in reductions: given an integer array a of length n , do there exist i, j {$\in$} [ n ] so that a [ i ] + a [ j ] = a [ i + j ]. From 3SUM-Convolution, many 3SUM-based hardness results have been proven: e.g. to listing graphs in triangles, dynamically maintaining shortest paths or bipartite matching, subset intersection and many more. It is interesting to consider more runtime-equivalent formulations of 3SUM, with the goal of uncovering more relationships to different problems. The talk will outline some such equivalences. For instance, 3SUM (over the reals or the integers) is equivalent to All-Numbers-3SUM:},
  citationcount = {1},
  venue = {International Symposium on Computational Geometry},
  keywords = {data structure,dynamic,lower bound,reduction}
}

@article{williamsANewAlgorithm2005,
  title = {A New Algorithm for Optimal 2-Constraint Satisfaction and Its Implications},
  author = {Williams, Ryan},
  year = {2005},
  doi = {10.1007/978-3-540-27836-8_101},
  abstract = {No abstract available},
  citationcount = {488},
  venue = {Theoretical Computer Science}
}

@article{williamsFasterAllPairs2013,
  title = {Faster All-Pairs Shortest Paths via Circuit Complexity},
  author = {Williams, Ryan},
  year = {2013},
  doi = {10.1145/2591796.2591811},
  abstract = {We present a new randomized method for computing the min-plus product (a.k.a., tropical product) of two n {\texttimes} n matrices, yielding a faster algorithm for solving the all-pairs shortest path problem (APSP) in dense n-node directed graphs with arbitrary edge weights. On the real RAM, where additions and comparisons of reals are unit cost (but all other operations have typical logarithmic cost), the algorithm runs in time [EQUATION] and is correct with high probability. On the word RAM, the algorithm runs in [EQUATION] + n2+o(1) logM time for edge weights in ([0,M]{$\cap$}Z){$\cup$}\{{$\infty$}\}. Prior algorithms took either O(n3/logc n) time for various c {$\leq$} 2, or O(M{$\alpha$}n{$\beta$}) time for various {$\alpha$} {\textquestiondown} 0 and {$\beta$} {\textquestiondown} 2. The new algorithm applies a tool from circuit complexity, namely the Razborov-Smolensky polynomials for approximately representing AC0[p] circuits, to efficiently reduce a matrix product over the (min, +) algebra to a relatively small number of rectangular matrix products over F2, each of which are computable using a particularly efficient method due to Coppersmith. We also give a deterministic version of the algorithm running in n[EQUATION] time for some {$\delta$} {\textquestiondown} 0, which utilizes the Yao-Beigel-Tarui translation of AC0[m] circuits into "nice" depth-two circuits.},
  citationcount = {267},
  venue = {Symposium on the Theory of Computing}
}

@article{williamsFindingMinimizingCounting2009,
  title = {Finding, Minimizing, and Counting Weighted Subgraphs},
  author = {Williams, V. V. and Williams, Ryan},
  year = {2009},
  doi = {10.1145/1536414.1536477},
  abstract = {For a pattern graph H on k nodes, we consider the problems of finding and counting the number of (not necessarily induced) copies of H in a given large graph G on n nodes, as well as finding minimum weight copies in both node-weighted and edge-weighted graphs. Our results include: The number of copies of an H with an independent set of size s can be computed exactly in O*(2s nk-s+3) time. A minimum weight copy of such an H (with arbitrary real weights on nodes and edges) can be found in O(4s+o(s) nk-s+3) time. (The O* notation omits (k) factors.) These algorithms rely on fast algorithms for computing the permanent of a k x n matrix, over rings and semirings. The number of copies of any H having minimum (or maximum) node-weight (with arbitrary real weights on nodes) can be found in O(n{$\omega$} k/3 + n2k/3+o(1)) time, where {$\omega$} {\textexclamdown} 2.4 is the matrix multiplication exponent and k is divisible by 3. Similar results hold for other values of k. Also, the number of copies having exactly a prescribed weight can be found within this time. These algorithms extend the technique of Czumaj and Lingas (SODA 2007) and give a new (algorithmic) application of multiparty communication complexity. Finding an edge-weighted triangle of weight exactly 0 in general graphs requires {\textohm}(n2.5-{$\varepsilon$}) time for all {$\varepsilon$} {\textquestiondown} 0, unless the 3SUM problem on N numbers can be solved in O(N2 - {$\varepsilon$}) time. This suggests that the edge-weighted problem is much harder than its node-weighted version.},
  citationcount = {201},
  venue = {Symposium on the Theory of Computing},
  keywords = {communication,communication complexity}
}

@article{williamsFindingOrthogonalVectors2014,
  title = {Finding Orthogonal Vectors in Discrete Structures},
  author = {Williams, Ryan and Yu, Huacheng},
  year = {2014},
  doi = {10.1137/1.9781611973402.135},
  abstract = {Hopcroft's problem in d dimensions asks: given n points and n hyperplanes in Rd, does any point lie on any hyperplane? Equivalently, if we are given two sets of n vectors each in Rd+1, is there a pair of vectors (one from each set) that are orthogonal? This problem has a long history and a multitude of applications. It is widely believed that for large d, the problem is subject to the curse of dimensionality: all known algorithms need at least f(d) {$\cdot$} n2-1/O(d) time for fast-growing functions f, and at the present time there is little hope that a n2-e {$\cdot$} poly(d) time algorithm will be found. We consider Hopcroft's problem over finite fields and integers modulo composites, leading to both surprising algorithms and hardness reductions. The algorithms arise from studying the communication problem of determining whether two lists of vectors (one list held by Alice, one by Bob) contain an orthogonal pair of vectors over a discrete structure (one from each list). We show the randomized communication complexity of the problem is closely related to the sizes of matching vector families, which have been studied in the design of locally decodable codes. Letting HOPCROFTR denote Hopcroft's problem over a ring R, we give randomized algorithms and almost matching lower bounds (modulo a breakthrough in SAT algorithms) for HOPCROFTR, when R is the ring of integers modulo m or a finite field. Building on the ideas developed here, we give a very simple and efficient output-sensitive algorithm for matrix multiplication that works over any field.},
  citationcount = {53},
  venue = {ACM-SIAM Symposium on Discrete Algorithms}
}

@article{williamsHardnessOfEasy2015,
  title = {Hardness of Easy Problems: {{Basing}} Hardness on Popular Conjectures Such as the Strong Exponential Time Hypothesis (Invited Talk)},
  author = {Williams, V. V.},
  year = {2015},
  doi = {10.4230/LIPIcs.IPEC.2015.17},
  abstract = {Algorithmic research strives to develop fast algorithms for fundamental problems. Despite its many successes, however, many problems still do not have very efficient algorithms. For years researchers have explained the hardness for key problems by proving NP-hardness, utilizing polynomial time reductions to base the hardness of key problems on the famous conjecture P != NP. For problems that already have polynomial time algorithms, however, it does not seem that one can show any sort of hardness based on P != NP. Nevertheless, we would like to provide evidence that a problem A with a running time O(n{\textasciicircum}k) that has not been improved in decades, also requires n{\textasciicircum}\{k-o(1)\} time, thus explaining the lack of progress on the problem. Such unconditional time lower bounds seem very difficult to obtain, unfortunately. Recent work has concentrated on an approach mimicking NP-hardness: (1) select a few key problems that are conjectured to require T(n) time to solve, (2) use special, fine-grained reductions to prove time lower bounds for many diverse problems in P based on the conjectured hardness of the key problems. In this abstract we outline the approach, give some examples of hardness results based on the Strong Exponential Time Hypothesis, and present an overview of some of the recent work on the topic.},
  citationcount = {144},
  venue = {International Symposium on Parameterized and Exact Computation},
  keywords = {lower bound,reduction}
}

@article{williamsMonochromaticTrianglesTriangle2020,
  title = {Monochromatic Triangles, Triangle Listing and {{APSP}}},
  author = {Williams, V. V. and Xu, Yinzhan},
  year = {2020},
  doi = {10.1109/FOCS46700.2020.00078},
  abstract = {All-Pairs Shortest Paths (APSP) is one of the most basic problems in graph algorithms. Given an n-node directed or undirected graph with integer weights in  -n{\textasciicircum}\{c\},{\dots},n{\textasciicircum}\{c\}  and no negative cycles, APSP asks to compute the shortest paths distance between every pair of vertices. The fastest known algorithm for APSP runs in n\textsuperscript{\{\vphantom\}}3\vphantom\{\}/2\textsuperscript{\{\vphantom\}}{$\Theta$}({\textsurd}\{n\})\vphantom\{\} time [Williams'14], and no truly subcubic time algorithms are known. One of the main hypotheses in fine-grained complexity is that APSP requires n\textsuperscript{\{\vphantom\}}3-o(1)\vphantom\{\} time. Another famous hypothesis in fine-grained complexity is that the 3SUM problem for n integers (which can be solved in O(n\textsuperscript{\{\vphantom\}}2\vphantom\{\}) time) requires n\textsuperscript{\{\vphantom\}}2-o(1)\vphantom\{\} time. Although there are no direct reductions between 3SUM and APSP, it is known that they are related: the (min, +)-convolution problem reduces in a fine-grained way to both, and both fine-grained reduce to the Exact Triangle problem. In this paper we find more relationships between these two problems and other basic problems. P{\u a}tra{\c s}cu had shown that under the 3SUM hypothesis the All-Edges Sparse Triangle problem in m-edge graphs requires m\textsuperscript{\{\vphantom\}}4/3-o(1)\vphantom\{\} time. The latter problem asks to determine for every edge e, whether e is in a triangle. It is equivalent to the problem of listing m triangles in an m-edge graph where m=-O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}1.5\vphantom\{\}), and can be solved in O(m\textsuperscript{\{\vphantom\}}1.41\vphantom\{\}) time [Alon et al.'97] with the current matrix multiplication bounds, and in O\vphantom\{\}(m\textsuperscript{\{\vphantom\}}4/3\vphantom\{\}) time if {$\omega$}=2. We show that one can reduce Exact Triangle to All-Edges Sparse Triangle, showing that All-Edges Sparse Triangle (and hence Triangle Listing) requires m\textsuperscript{\{\vphantom\}}4/3-o(1)\vphantom\{\} time also assuming the APSP hypothesis. This allows us to provide APSP-hardness for many dynamic problems that were previously known to be hard under the 3SUM hypothesis. We also consider the All-Edges Monochromatic Triangle problem. Via work of [Lincoln et al.'20], our result on All-Edges Sparse Triangle implies that if the All-Edges Monochromatic Triangle problem has an O(n\textsuperscript{\{\vphantom\}}2.5-{$\varepsilon$}\vphantom\{\}) time algorithm for {$\epsilon>$}0, then both the APSP and 3SUM hypotheses are false. The fastest algorithm for All-Edges Monochromatic Triangle runs in O\vphantom\{\}(n\textsuperscript{\{\vphantom\}}(3+{$\omega$})/2\vphantom\{\}) time [Vassilevska et al.'06], and our new reduction shows that if {$\omega$}=2, this algorithm is best possible, unless 3SUM or APSP can be solved faster. Besides 3SUM, previously the only problems known to be fine-grained reducible to All-Edges Monochromatic Triangle were the seemingly easier problems directed unweighted APSP and Min-Witness Product [Lincoln et al.'20]. Our reduction shows that this problem is much harder. We also connect the problem to other ``intermediate'' problems, whose runtimes are between O(n\textsuperscript{\{\vphantom\}}{$\omega$}\vphantom\{\}) and O(n\textsuperscript{\{\vphantom\}}3\vphantom\{\}), such as the Max-Min product problem.},
  citationcount = {35},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {dynamic,reduction}
}

@article{williamsMultiplyingMatricesFaster2012,
  title = {Multiplying Matrices Faster than Coppersmith-Winograd},
  author = {Williams, V. V.},
  year = {2012},
  doi = {10.1145/2213977.2214056},
  abstract = {We develop an automated approach for designing matrix multiplication algorithms based on constructions similar to the Coppersmith-Winograd construction. Using this approach we obtain a new improved bound on the matrix multiplication exponent {$\omega$}{\textexclamdown}2.3727.},
  citationcount = {1007},
  venue = {Symposium on the Theory of Computing}
}

@article{williamsOnSomeFine2019,
  title = {On Some Fine-Grained Questions in Algorithms and Complexity},
  author = {Williams, V. V.},
  year = {2019},
  doi = {10.1142/9789813272880_0188},
  abstract = {In recent years, a new ``fine-grained'' theory of computational hardness has been developed, based on ``fine-grained reductions'' that focus on exact running times for problems. Mimicking NP-hardness, the approach is to (1) select a key problem X that for some function t , is conjectured to not be solvable by any O(t(n)1 ") time algorithm for " {\textquestiondown} 0, and (2) reduce X in a fine-grained way to many important problems, thus giving tight conditional time lower bounds for them. This approach has led to the discovery of many meaningful relationships between problems, and to equivalence},
  citationcount = {209},
  venue = {International Congress of Mathematicans},
  keywords = {lower bound,reduction}
}

@article{williamsSubcubicEquivalencesPath2010,
  title = {Subcubic Equivalences between Path, Matrix and Triangle Problems},
  author = {Williams, V. V. and Williams, Ryan},
  year = {2010},
  doi = {10.1145/3186893},
  abstract = {We say an algorithm on n by n matrices with entries in [-M, M] (or n-node graphs with edge weights from [-M, M]) is truly sub cubic if it runs in O(n{\textasciicircum}\{3-{$\delta$}\} (M)) time for some {$\delta$}{\textquestiondown} 0. We define a notion of sub cubic reducibility, and show that many important problems on graphs and matrices solvable in O(n{\textasciicircum}3) time are equivalent under sub cubic reductions. Namely, the following weighted problems either all have truly sub cubic algorithms, or none of them do: - The all-pairs shortest paths problem (APSP). - Detecting if a weighted graph has a triangle of negative total edge weight. - Listing up to n{\textasciicircum}\{2.99\} negative triangles in an edge-weighted graph. - Finding a minimum weight cycle in a graph of non-negative edge weights. - The replacement paths problem in an edge-weighted digraph. - Finding the second shortest simple path between two nodes in an edge-weighted digraph. - Checking whether a given matrix defines a metric. - Verifying the correctness of a matrix product over the (, +)-semiring. Therefore, if APSP cannot be solved in n{\textasciicircum}\{3-\} time for any {\textquestiondown} 0, then many other problems also need essentially cubic time. In fact we show generic equivalences between matrix products over a large class of algebraic structures used in optimization, verifying a matrix product over the same structure, and corresponding triangle detection problems over the structure. These equivalences simplify prior work on sub cubic algorithms for all-pairs path problems, since it now suffices to give appropriate sub cubic triangle detection algorithms. Other consequences of our work are new combinatorial approaches to Boolean matrix multiplication over the (OR, AND)-semiring (abbreviated as BMM). We show that practical advances in triangle detection would imply practical BMM algorithms, among other results. Building on our techniques, we give two new BMM algorithms: a derandomization of the recent combinatorial BMM algorithm of Bansal and Williams (FOCS'09), and an improved quantum algorithm for BMM.},
  citationcount = {434},
  venue = {IEEE Annual Symposium on Foundations of Computer Science},
  keywords = {reduction}
}

@article{wirthTheProgrammingLanguage1973,
  title = {The Programming Language {{Pascal}} ({{Revised Report}})},
  author = {Wirth, N.},
  year = {1973},
  doi = {10.3929/ETHZ-A-000814158},
  abstract = {No abstract available},
  citationcount = {54},
  venue = {No venue available}
}

@article{wittenManagingGigabytesCompressing1999,
  title = {Managing Gigabytes: {{Compressing}} and Indexing Documents and Images},
  author = {Witten, I. and Moffat, Alistair and Bell, T.},
  year = {1999},
  doi = {10.1109/tit.1995.476344},
  abstract = {PREFACE 1. OVERVIEW 2. TEXT COMPRESSION 3. INDEXING 4. QUERYING 5. INDEX CONSTRUCTION 6. IMAGE COMPRESSION 7. TEXTUAL IMAGES 8. MIXED TEXT AND IMAGES 9. IMPLEMENTATION 10. THE INFORMATION EXPLOSION A. GUIDE TO THE MG SYSTEM B. GUIDE TO THE NZDL REFERENCES INDEX},
  citationcount = {2259},
  venue = {No venue available}
}

@article{wolfgangmulzerComputationalAspectsColorful2014,
  title = {Computational {{Aspects}} of the {{Colorful Carath{\'e}odory Theorem}}},
  author = {Wolfgang Mulzer and Yannik Stein},
  year = {2014},
  journal = {Discrete \& Computational Geometry},
  doi = {10.1007/s00454-018-9979-y},
  annotation = {Citation Count: 10}
}

@article{wolfgangmulzerNotePredecessorSearching2009,
  title = {A Note on Predecessor Searching in the Pointer Machine Model},
  author = {Wolfgang Mulzer},
  year = {2009},
  journal = {Information Processing Letters},
  doi = {10.1016/j.ipl.2009.03.003},
  annotation = {Citation Count: 4}
}

@article{woodruffSketchingAsA2014,
  title = {Sketching as a Tool for Numerical Linear Algebra},
  author = {Woodruff, David P.},
  year = {2014},
  doi = {10.1561/0400000060},
  abstract = {This survey highlights the recent advances in algorithms for numericallinear algebra that have come from the technique of linear sketching,whereby given a matrix, one first compresses it to a much smaller matrixby multiplying it by a (usually) random matrix with certain properties.Much of the expensive computation can then be performed onthe smaller matrix, thereby accelerating the solution for the originalproblem. In this survey we consider least squares as well as robust regressionproblems, low rank approximation, and graph sparsification.We also discuss a number of variants of these problems. Finally, wediscuss the limitations of sketching methods.},
  citationcount = {1170},
  venue = {Foundations and Trends{\textregistered} in Theoretical Computer Science}
}

@article{worltonTheArtOf1968,
  title = {The Art of Computer Programming},
  author = {Worlton, W.},
  year = {1968},
  doi = {10.13182/NSE70-A19705},
  abstract = {Knuth began the project, originally conceived as a single book with twelve chapters, in 1962. The first three volumes of what was then expected to be a seven-volume set were published in 1968, 1969, and 1973. The first installment of Volume 4 (a paperback fascicle) was published in 2005. The hardback Volume 4A, combining Volume 4, Fascicles 0--4, was published in 2011. Additional fascicle installments are planned for release approximately biannually; Volume 4, Fascicle 6 (``Satisfiability'') was released in December 2015.},
  citationcount = {10396},
  venue = {No venue available}
}

@article{wulff-nilsenApproximateDistanceOracles2016,
  title = {Approximate Distance Oracles for Planar Graphs with Improved Query Time-Space Tradeoff},
  author = {{Wulff-Nilsen}, Christian},
  year = {2016},
  doi = {10.1137/1.9781611974331.CH26},
  abstract = {We consider approximate distance oracles for edge-weighted n-vertex undirected planar graphs. Given fixed e {\textquestiondown} 0. we present a (1 + e)-approximate distance oracle with O(n(log log n)2) space and O((log log n)3) query time. This improves the previous best product of query time and space of the oracles of Thorup (FOCS 2001, J. ACM 2004) and Klein (SODA 2002) from O(n log n) to O(n(log log n)5).},
  citationcount = {22},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {query,query time,time-space}
}

@article{wulff-nilsenFasterDeterministicFully2012,
  title = {Faster Deterministic Fully-Dynamic Graph Connectivity},
  author = {{Wulff-Nilsen}, Christian},
  year = {2012},
  doi = {10.1137/1.9781611973105.126},
  abstract = {We give new deterministic bounds for fully-dynamic graph connectivity. Our data structure supports updates (edge insertions/deletions) in O(log2 n/log log n) amortized time and connectivity queries in O(log n/log log n) worst-case time, where n is the number of vertices of the graph. This improves the deterministic data structures of Holm, de Lichtenberg, and Thorup (STOC 1998, J. ACM 2001) and Thorup (STOC 2000) which both have O(log2 n) amortized update time and O(log n/log log n) worst-case query time. Our model of computation is the same as that of Thorup, i.e., a pointer machine with standard AC0 instructions.},
  citationcount = {80},
  venue = {Encyclopedia of Algorithms},
  keywords = {data structure,dynamic,query,query time,update,update time}
}

@article{wulff-nilsenFullyDynamicMinimum2016,
  title = {Fully-Dynamic Minimum Spanning Forest with Improved Worst-Case Update Time},
  author = {{Wulff-Nilsen}, Christian},
  year = {2016},
  doi = {10.1145/3055399.3055415},
  abstract = {We give a Las Vegas data structure which maintains a minimum spanning forest in an n-vertex edge-weighted undirected dynamic graph undergoing updates consisting of any mixture of edge insertions and deletions. Each update is supported in O(n1/2 - c) worst-case time w.h.p. where c {\textquestiondown} 0 is some constant, and this bound also holds in expectation. This is the first data structure achieving an improvement over the O({\textsurd}n) deterministic worst-case update time of Eppstein et al., a bound that has been standing for 25 years. In fact, it was previously not even known how to maintain a spanning forest of an unweighted graph in worst-case time polynomially faster than {$\Theta$}({\textsurd}n). Our result is achieved by first giving a reduction from fully-dynamic to decremental minimum spanning forest preserving worst-case update time up to logarithmic factors. Then decremental minimum spanning forest is solved using several novel techniques, one of which involves keeping track of low-conductance cuts in a dynamic graph. An immediate corollary of our result is the first Las Vegas data structure for fully-dynamic connectivity where each update is handled in worst-case time polynomially faster than {$\Theta$}({\textsurd}n) w.h.p.; this data structure has O(1) worst-case query time.},
  citationcount = {97},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,dynamic,query,query time,reduction,update,update time}
}

@article{wuTheUniversalityOf1981,
  title = {The Universality of the Shuffle-Exchange Network},
  author = {Wu, Chuan-lin and Feng, T.},
  year = {1981},
  doi = {10.1109/TC.1981.1675790},
  abstract = {This paper has focused on the realization of every arbitrary permutation with the shuffle-exchange network. Permutation properties of shuffle-exchange networks are studied and are used to demonstrate several universal networks. It is concluded that 3(log2 N) --1 passes through a single-stage regular shuffle exchange network are sufficient to realize every arbitrary permutation where N is network size. A routing algorithm is also developed to calculate control settings of the shuffle-exchange switches for the permutation realization. Three optimal universal networks, namely, expanded direct- connection shuffle-exchange network, multiple-pass omega network, and modified shuffle-exchange network are then exploited for better interconnection purposes. In addition, this work specifies the inherent relationship between the shuffle-exchange network and the Benes binary network so that designers can have a broad prospect.},
  citationcount = {174},
  venue = {IEEE transactions on computers}
}

@inproceedings{WY16,
  title = {Amortized Dynamic Cell-Probe Lower Bounds from Four-Party Communication},
  booktitle = {Proc. 57th {{IEEE}} Symposium on Foundations of Computer Science},
  author = {Weinstein, Omri and Yu, Huacheng},
  year = {2016},
  pages = {305--314},
  doi = {10.1109/focs.2016.41},
  abstract = {This paper develops a new technique for proving amortized, randomized cell-probe lower bounds on dynamic data structure problems. We introduce a new randomized nondeterministic four-party communication model that enables "accelerated", error-preserving simulations of dynamic data structures. We use this technique to prove an {\textohm}(n(log n/log log n)2) cell-probe lower bound for the dynamic 2D weighted orthogonal range counting problem (2D-ORC) with n/poly log n updates and n queries, that holds even for data structures with exp(-{\~{\textohm}}(n)) success probability. This result not only proves the highest amortized lower bound to date, but is also tight in the strongest possible sense, as a matching upper bound can be obtained by a deterministic data structure with worst-case operational time. This is the first demonstration of a "sharp threshold" phenomenon for dynamic data structures. Our broader motivation is that cell-probe lower bounds for exponentially small success facilitate reductions from dynamic to static data structures. As a proof-of-concept, we show that a slightly strengthened version of our lower bound would imply an {\textohm}((log n/log log n)2) lower bound for the static 3D-ORC problem with O(n logO(1) n) space. Such result would give a near quadratic improvement over the highest known static cell-probe lower bound, and break the long standing {\textohm}(log n) barrier for static data structures.},
  keywords = {cell probe,communication,Complexity theory,Computational modeling,Computer science,Data models,Data structures,dynamic,Integrated circuit modeling,lower bound,Probes},
  file = {/Users/tulasi/Zotero/storage/NEAZTM2X/Weinstein and Yu - 2016 - Amortized Dynamic Cell-Probe Lower Bounds from Four-Party Communication.pdf}
}

@article{x.huangFastDensityBasedClustering2023,
  title = {Fast {{Density-Based Clustering}}: {{Geometric Approach}}},
  author = {X. Huang and Tiefeng Ma},
  year = {2023},
  journal = {Proc. ACM Manag. Data},
  doi = {10.1145/3588912},
  abstract = {DBSCAN is a fundamental density-based clustering algorithm with extensive applications. However, a bottleneck of DBSCAN is its O(n2) worst-case time complexity. In this paper, we propose an algorithm called GAP-DBC, which exploits the geometric relationships between points to solve this problem. GAP-DBC introduces an efficient partitioning algorithm to partition the data set with a limited number of range queries and then establishes an initial cluster structure based on the partition. GAP-DBC proceeds to iteratively refine the cluster structure by additional range queries. Finally, the cluster structure is accomplished using an iterative algorithm that utilizes the spatial relationships among points to reduce unnecessary distance calculations. We further demonstrate theoretically that GAP-DBC has an excellent guarantee in terms of computational efficiency. We conducted experiments on both synthetic and real-world data sets to evaluate the performance of GAP-DBC. The results show that our algorithm is competitive with other state-of-the-art algorithms.},
  keywords = {query},
  annotation = {Citation Count: 3}
}

@article{x.huangGriTDBSCANSpatialClustering2022,
  title = {{{GriT-DBSCAN}}: {{A Spatial Clustering Algorithm}} for {{Very Large Databases}}},
  author = {X. Huang and T. Ma and Conan Liu and Shuangzhe Liu},
  year = {2022},
  journal = {Pattern Recognition},
  doi = {10.48550/arXiv.2210.07580},
  abstract = {DBSCAN is a fundamental spatial clustering algorithm with numerous practical applications. However, a bottleneck of the algorithm is in the worst case, the run time complexity is \$O(n{\textasciicircum}2)\$. To address this limitation, we propose a new grid-based algorithm for exact DBSCAN in Euclidean space called GriT-DBSCAN, which is based on the following two techniques. First, we introduce a grid tree to organize the non-empty grids for the purpose of efficient non-empty neighboring grids queries. Second, by utilising the spatial relationships among points, we propose a technique that iteratively prunes unnecessary distance calculations when determining whether the minimum distance between two sets is less than or equal to a certain threshold. We theoretically prove that the complexity of GriT-DBSCAN is linear to the data set size. In addition, we obtain two variants of GriT-DBSCAN by incorporating heuristics, or by combining the second technique with an existing algorithm. Experiments are conducted on both synthetic and real-world data sets to evaluate the efficiency of GriT-DBSCAN and its variants. The results of our analyses show that our algorithms outperform existing algorithms.},
  keywords = {query},
  annotation = {Citation Count: 18}
}

@article{xiaojuntanNclusterCorrelationsFour2020,
  title = {N-Cluster Correlations in Four- and Five-Dimensional Percolation},
  author = {Xiaojun Tan and Youjin Deng and J. Jacobsen},
  year = {2020},
  journal = {Frontiers of Physics},
  doi = {10.1007/s11467-020-0972-6},
  annotation = {Citation Count: 7}
}

@article{xichenWellSupportedVsApproximate2015,
  title = {Well-{{Supported}} vs. {{Approximate Nash Equilibria}}: {{Query Complexity}} of {{Large Games}}},
  author = {Xi Chen and Yu Cheng and Bo Tang},
  year = {2015},
  journal = {Information Technology Convergence and Services},
  doi = {10.4230/LIPIcs.ITCS.2017.57},
  abstract = {We study the randomized query complexity of approximate Nash equilibria (ANE) in large games. We prove that, for some constant \${\textbackslash}epsilon{$>$}0\$, any randomized oracle algorithm that computes an \${\textbackslash}epsilon\$-ANE in a binary-action, \$n\$-player game must make \$2{\textasciicircum}\{{\textbackslash}Omega(n/{\textbackslash}log n)\}\$ payoff queries. For the stronger solution concept of well-supported Nash equilibria (WSNE), Babichenko previously gave an exponential \$2{\textasciicircum}\{{\textbackslash}Omega(n)\}\$ lower bound for the randomized query complexity of \${\textbackslash}epsilon\$-WSNE, for some constant \${\textbackslash}epsilon{$>$}0\$; the same lower bound was shown to hold for \${\textbackslash}epsilon\$-ANE, but only when \${\textbackslash}epsilon=O(1/n)\$.  Our result answers an open problem posed by Hart and Nisan and Babichenko and is very close to the trivial upper bound of \$2{\textasciicircum}n\$. Our proof relies on a generic reduction from the problem of finding an \${\textbackslash}epsilon\$-WSNE to the problem of finding an \${\textbackslash}epsilon/(4{\textbackslash}alpha)\$-ANE, in large games with \${\textbackslash}alpha\$ actions, which might be of independent interest.},
  keywords = {lower bound,query,query complexity,reduction},
  annotation = {Citation Count: 23}
}

@article{xieSpatialIndependentRange2021,
  title = {Spatial Independent Range Sampling},
  author = {Xie, Dong and Phillips, J. M. and Matheny, Michael and Li, Feifei},
  year = {2021},
  doi = {10.1145/3448016.3452806},
  abstract = {Thanks to the wide adoption of GPS-equipped devices, the volume of collected spatial data is exploding. To achieve interactive exploration and analysis over big spatial data, people are willing to trade off accuracy for performance through approximation. As a foundation in many approximate algorithms, data sampling now requires more flexibility and better performance. In this paper, we study the spatial independent range sampling (SIRS) problem aiming at retrieving random samples with independence over points residing in a query region. Specifically, we have designed concise index structures with careful data layout based on various space decomposition strategies. Moreover, we propose novel algorithms for both uniform and weighted SIRS queries with low theoretical cost and complexity as well as excellent practical performance. Last but not least, we demonstrate how to support data updates and trade-offs between different sampling methods in practice. According to comprehensive evaluations conducted on real-world datasets, our methods achieve orders of magnitude performance improvement against baselines derived by existing works.},
  citationcount = {8},
  venue = {SIGMOD Conference},
  keywords = {query,update}
}

@article{xinyuluoDimensionalityReductionGeneral2023,
  title = {Dimensionality {{Reduction}} for {{General KDE Mode Finding}}},
  author = {Xinyu Luo and Christopher Musco and C. Widdershoven},
  year = {2023},
  journal = {International Conference on Machine Learning},
  doi = {10.48550/arXiv.2305.18755},
  abstract = {Finding the mode of a high dimensional probability distribution \$D\$ is a fundamental algorithmic problem in statistics and data analysis. There has been particular interest in efficient methods for solving the problem when \$D\$ is represented as a mixture model or kernel density estimate, although few algorithmic results with worst-case approximation and runtime guarantees are known. In this work, we significantly generalize a result of (LeeLiMusco:2021) on mode approximation for Gaussian mixture models. We develop randomized dimensionality reduction methods for mixtures involving a broader class of kernels, including the popular logistic, sigmoid, and generalized Gaussian kernels. As in Lee et al.'s work, our dimensionality reduction results yield quasi-polynomial algorithms for mode finding with multiplicative accuracy \$(1-{\textbackslash}epsilon)\$ for any \${\textbackslash}epsilon{$>$}0\$. Moreover, when combined with gradient descent, they yield efficient practical heuristics for the problem. In addition to our positive results, we prove a hardness result for box kernels, showing that there is no polynomial time algorithm for finding the mode of a kernel density estimate, unless \${\textbackslash}mathit\{P\} = {\textbackslash}mathit\{NP\}\$. Obtaining similar hardness results for kernels used in practice (like Gaussian or logistic kernels) is an interesting future direction.},
  keywords = {reduction},
  annotation = {Citation Count: 2}
}

@article{y.kodaGrayCodeIdeals1993,
  title = {A {{Gray Code}} for the {{Ideals}} of a {{Forest Poset}}},
  author = {Y. Koda and F. Ruskey},
  year = {1993},
  journal = {J. Algorithms},
  doi = {10.1006/jagm.1993.1044},
  abstract = {Abstract We present two algorithms for listing all the ideals of a forest poset. These algorithms generate ideals in a gray code manner; that is, consecutive ideals differ by exactly one element. Both algorithms use storage O(n), where n is the number of elements in the poset. On each iteration, the first algorithm does a partial traversal of the current ideal being listed and runs in time O(nN), where N is the number of ideals of the poset. The second algorithm mimics the first, but it eliminates the traversal and runs in time O(N). This algorithm has the property that the amount of computation between successive ideals is O(1); such algorithms are said to be loopless.},
  annotation = {Citation Count: 41}
}

@article{y.levyUniformlySupportedApproximate2021,
  title = {Uniformly Supported Approximate Equilibria in Families of Games},
  author = {Y. Levy},
  year = {2021},
  journal = {Journal of Mathematical Economics},
  doi = {10.1016/j.jmateco.2021.102571},
  annotation = {Citation Count: 0}
}

@article{yakovnekrich4DRangeReporting2022,
  title = {{{4D Range Reporting}} in the {{Pointer Machine Model}} in {{Almost-Optimal Time}}},
  author = {Yakov Nekrich and S. Rahul},
  year = {2022},
  journal = {ACM-SIAM Symposium on Discrete Algorithms},
  doi = {10.48550/arXiv.2211.03161},
  abstract = {In the orthogonal range reporting problem we must pre-process a set \$P\$ of multi-dimensional points, so that for any axis-parallel query rectangle \$q\$ all points from \$q{\textbackslash}cap P\$ can be reported efficiently. In this paper we study the query complexity of multi-dimensional orthogonal range reporting in the pointer machine model. We present a data structure that answers four-dimensional orthogonal range reporting queries in almost-optimal time \$O({\textbackslash}log n{\textbackslash}log{\textbackslash}log n + k)\$ and uses \$O(n{\textbackslash}log{\textasciicircum}4 n)\$ space, where \$n\$ is the number of points in \$P\$ and \$k\$ is the number of points in \$q{\textbackslash}cap P\$ . This is the first data structure with nearly-linear space usage that achieves almost-optimal query time in 4d. This result can be immediately generalized to \$d{\textbackslash}ge 4\$ dimensions: we show that there is a data structure supporting \$d\$-dimensional range reporting queries in time \$O({\textbackslash}log{\textasciicircum}\{d-3\} n{\textbackslash}log{\textbackslash}log n+k)\$ for any constant \$d{\textbackslash}ge 4\$.},
  keywords = {data structure,query,query complexity,query time},
  annotation = {Citation Count: 1}
}

@article{yakovnekrichDynamicStabbingMaxData2011,
  title = {A {{Dynamic Stabbing-Max Data Structure}} with {{Sub-Logarithmic Query Time}}},
  author = {Yakov Nekrich},
  year = {2011},
  journal = {International Symposium on Algorithms and Computation},
  doi = {10.1007/978-3-642-25591-5_19},
  keywords = {data structure,dynamic,query,query time},
  annotation = {Citation Count: 12}
}

@article{yakovnekrichFastAlgorithmThreeDimensional2010,
  title = {A {{Fast Algorithm}} for {{Three-Dimensional Layers}} of {{Maxima Problem}}},
  author = {Yakov Nekrich},
  year = {2010},
  journal = {Workshop on Algorithms and Data Structures},
  doi = {10.1007/978-3-642-22300-6_51},
  annotation = {Citation Count: 14}
}

@article{yakovnekrichLinearSpaceData2009,
  title = {A {{Linear Space Data Structure}} for {{Orthogonal Range Reporting}} and {{Emptiness Queries}}},
  author = {Yakov Nekrich},
  year = {2009},
  journal = {International journal of computational geometry and applications},
  doi = {10.1142/S0218195909002800},
  abstract = {In this paper we present a linear space dynamic data structure for two-dimensional orthogonal range reporting and emptiness queries. This data structure answers range reporting queries in time for any e {$>$} 0 and k the size of the answer. Our data structure also supports emptiness and one-reporting queries in time O(log n log log n). The model of computation used in this paper is a unit-cost RAM model.},
  keywords = {data structure,dynamic,query},
  annotation = {Citation Count: 10}
}

@article{yakovnekrichOrthogonalRangeSearching2007,
  title = {Orthogonal {{Range Searching}} in {{Linear}} and {{Almost-Linear Space}}},
  author = {Yakov Nekrich},
  year = {2007},
  journal = {Workshop on Algorithms and Data Structures},
  doi = {10.1007/978-3-540-73951-7_3},
  annotation = {Citation Count: 59}
}

@article{yakovnekrichSpaceEfficientDynamic2005,
  title = {Space {{Efficient Dynamic Orthogonal Range Reporting}}},
  author = {Yakov Nekrich},
  year = {2005},
  journal = {Algorithmica},
  doi = {10.1007/s00453-007-9030-9},
  annotation = {Citation Count: 18}
}

@article{yamamotoFasterCompactOn2013,
  title = {Faster Compact On-Line Lempel-Ziv Factorization},
  author = {Yamamoto, Jun-ichi and Tomohiro, I. and Bannai, H. and Inenaga, Shunsuke and Takeda, Masayuki},
  year = {2013},
  doi = {10.4230/LIPICS.STACS.2014.675},
  abstract = {We present a new on-line algorithm for computing the Lempel-Ziv factorization of a string that runs in O(NN) time and uses only O(N{$\sigma$}) bits of working space, where N is the length of the string and {$\sigma$} is the size of the alphabet. This is a notable improvement compared to the performance of previous on-line algorithms using the same order of working space but running in either O(N{$^3$}N) time (Okanohara \& Sadakane 2009) or O(N{$^2$}N) time (Starikovskaya 2012). The key to our new algorithm is in the utilization of an elegant but less popular index structure called Directed Acyclic Word Graphs, or DAWGs (Blumer et al. 1985). We also present an opportunistic variant of our algorithm, which, given the run length encoding of size m of a string of length N, computes the Lempel-Ziv factorization on-line, in O(m{$\cdot$} \textsuperscript{\{\vphantom\}}{\textfractionsolidus}{$_($}m)(N)\vphantom\{\}\{N\},{\textsurd}\{\vphantom\}\textsuperscript{\{\vphantom\}}{\textfractionsolidus}m\vphantom\{\}\{m\}\vphantom\{\} ) time and O(mN) bits of space, which is faster and more space efficient when the string is run-length compressible.},
  citationcount = {22},
  venue = {Symposium on Theoretical Aspects of Computer Science}
}

@article{yangEvolutionForestIndex2024,
  title = {Evolution Forest Index: {{Towards}} Optimal Temporal {{k}}-Core Component Search via Time-Topology Isomorphic Computation},
  author = {Yang, Jun-cheng and Zhong, Ming and Zhu, Yuanyuan and Qian, Tieyun and Liu, Mengchi and Yu, Jeffrey Xu},
  year = {2024},
  doi = {10.14778/3681954.3681967},
  abstract = {For a temporal graph like transaction network, finding a densely connected subgraph that contains a vertex like a suspicious account during a period is valuable. Thus, we study the Temporal k -Core Component Search (TCCS) problem, which aims to find a connected component of temporal k -core for any given vertex and time interval. Towards this goal, we propose a novel Evolution Forest Index (EF-Index) that can address TCCS in optimal time. Essentially, EF-Index leverages the evolutionary order on temporal k -cores to both compress the connectivity between vertices in temporal k -cores of all time intervals into a minimum set of compactest Minimum Temporal Spanning Forests (MTSFs) and retrieve MTSF for a given time interval rapidly. Here, a crucial innovation is that, we extend the temporal k -core evolution theory by introducing a pair of time-topology isomorphic relations, on top of which the evolutionary order in topology domain can be simply computed by a "kernel function" in time domain. Moreover, we design an efficient mechanism to update EF-Index incrementally for dynamic edge streams. The experimental results on a variety of real-world temporal graphs demonstrate that, EF-Index outperforms the state-of-the-art approach by 1--3 orders of magnitude on processing TCCS, and its space overhead is reduced by 4--5 orders of magnitude compared with preserving connectivity uncompressedly.},
  citationcount = {Unknown},
  venue = {Proceedings of the VLDB Endowment},
  keywords = {dynamic,update}
}

@article{yangLiftingTheoremsMeet2023,
  title = {Lifting Theorems Meet Information Complexity: {{Known}} and New Lower Bounds of Set-Disjointness},
  author = {Yang, Guangxu and Zhang, Jiapeng},
  year = {2023},
  doi = {10.48550/arXiv.2309.13517},
  abstract = {Set-disjointness problems are one of the most fundamental problems in communication complexity and have been extensively studied in past decades. Given its importance, many lower bound techniques were introduced to prove communication lower bounds of set-disjointness. Combining ideas from information complexity and query-to-communication lifting theorems, we introduce a density increment argument to prove communication lower bounds for set-disjointness: We give a simple proof showing that a large rectangle cannot be 0-monochromatic for multi-party unique-disjointness. We interpret the direct-sum argument as a density increment process and give an alternative proof of randomized communication lower bounds for multi-party unique-disjointness. Avoiding full simulations in lifting theorems, we simplify and improve communication lower bounds for sparse unique-disjointness. Potential applications to be unified and improved by our density increment argument are also discussed.},
  citationcount = {2},
  venue = {arXiv.org},
  keywords = {communication,communication complexity,lower bound,query}
}

@article{yankaichenAugmentedIndexbasedEfficient2023,
  title = {An {{Augmented Index-based Efficient Community Search}} for {{Large Directed Graphs}}},
  author = {Yankai Chen and Jie Zhang and Yixiang Fang and Xin Cao and Irwin King},
  year = {2023},
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2311.06487},
  abstract = {Given a graph G and a query vertex q, the topic of community search (CS), aiming to retrieve a dense subgraph of G containing q, has gained much attention. Most existing works focus on undirected graphs which overlooks the rich information carried by the edge directions. Recently, the problem of community search over directed graphs (or CSD problem) has been studied; it finds a connected subgraph containing q, where the in-degree and out-degree of each vertex within the subgraph are at least k and l, respectively. However, existing solutions are inefficient, especially on large graphs. To tackle this issue, in this paper, we propose a novel index called D-Forest, which allows a CSD query to be completed within the optimal time cost. We further propose efficient index construction methods. Extensive experiments on six real large graphs show that our index-based query algorithm is up to two orders of magnitude faster than existing solutions.},
  keywords = {information,query},
  annotation = {Citation Count: 0}
}

@article{yankaichenEfficientCommunitySearch2020,
  title = {Efficient {{Community Search}} over {{Large Directed Graph}}: {{An Augmented Index-based Approach}}},
  author = {Yankai Chen and J. Zhang and Yixiang Fang and Xin Cao and Irwin King},
  year = {2020},
  journal = {International Joint Conference on Artificial Intelligence},
  doi = {10.24963/ijcai.2020/490},
  abstract = {Given a graph G and a query vertex q, the topic of community search (CS), aiming to retrieve a dense subgraph of G containing q, has gained much attention. Most existing works focus on undirected graphs which overlooks the rich information carried by the edge directions. Recently, the problem of community search over directed graphs (or CSD problem) has been studied [Fang et al., 2019b]; it finds a connected subgraph containing q, where the in-degree and out-degree of each vertex within the subgraph are at least k and l, respectively. However, existing solutions are inefficient, especially on large graphs. To tackle this issue, in this paper we propose a novel index called D-Forest, which allows a CSD query to be completed within the optimal time cost. We further propose efficient index construction methods. Extensive experiments on six real large graphs show that our index-based query algorithm is up to two orders of magnitude faster than existing solutions.},
  keywords = {information,query},
  annotation = {Citation Count: 28}
}

@article{yanniksteinColorfulCaratheodoryProblem2016,
  title = {The {{Colorful Carath{\'e}odory Problem}} and Its {{Descendants}} ({{Das Colorful Carath{\'e}odory Problem}} Und Seine {{Anwendungen}})},
  author = {Yannik Stein},
  year = {2016},
  doi = {10.17169/REFUBIUM-16228},
  abstract = {The colorful Carath{\'e}odory theorem is an existence theorem that implies several statements on convex intersection patterns such as Tverberg's theorem, the centerpoint theorem, the first selection lemma, and the colorful Kirchberger theorem. Interestingly, these proofs can be interpreted as polynomial-time reductions to COLORFULCARATH{\'E}ODORY, the computational search problem that corresponds to the colorful Carath{\'e}odory theorem. We exploit this existing web of reductions by developing approximation algorithms and complexity bounds on COLORFULCARATH{\'E}ODORY that also apply to its polynomial-time descendants. Let C1, . . . ,Cd+1 {$\subset$} Rd be finite point sets such that 0 {$\in$} conv(Ci ) for i {$\in$} [d +1]. Then, the colorful Carath{\'e}odory theorem asserts that we can choose one point from each set Ci such that the chosen points C contain the origin in their convex hull. COLORFULCARATH{\'E}ODORY is then the computational problem of finding C . Since a solution always exists and since it can be verified in polynomial time, COLORFULCARATH{\'E}ODORY is contained in total function NP (TFNP), the class of NP search problems that always admit a solution. We show that COLORFULCARATH{\'E}ODORY belongs to the intersection of two important subclasses of TFNP: the complexity classes polynomial-time parity argument on directed graphs (PPAD) and polynomial-time local search (PLS). The formulation of COLORFULCARATH{\'E}ODORY as a PPADproblem is based on a new constructive proof of the colorful Carath{\'e}odory theorem that uses Sperner's lemma. Moreover, we show that already a slight change in the definition of COLORFULCARATH{\'E}ODORY results in a PLS-complete problem. In the second part, we present several constructive results. First, we consider an approximation version of COLORFULCARATH{\'E}ODORY in which we are allowed to take more than one point from each set Ci . This notion of approximation has not been studied before and it is compatible with the polynomial-time reductions to COLORFULCARATH{\'E}ODORY. For any fixed {$\varepsilon>$} 0, we can compute a set C with 0 {$\in$} conv(C ) and at most d{$\varepsilon$}de points from each Ci in dO({$\varepsilon$} -1 log{$\varepsilon-$}1) time by repeatedly combining recursively computed approximations for lowerdimensional problem instances. Additionally, we consider a further notion of approximation in which we are given only k {$<$} d +1 sets Ci with 0 {$\in$} conv(Ci ), and we want to find a set C with at most d(d +1)/ke points from each set Ci . The existence of C is a direct implication of the colorful Carath{\'e}odory theorem. Using linear programming techniques, we can solve the case k = 2 in weakly polynomial time. Moreover, we show that COLORFULCARATH{\'E}ODORY can be solved exactly in quasi-polynomial time when given poly(d) sets Ci that contain the origin in their convex hulls instead of only d +1. Finally, we consider the problem of computing the simplicial depth. The simplicial depth {$\sigma$}P (q) of a point q {$\in$}Rd w.r.t. a set P is the number of distinct d-simplices with vertices in P that contain q . If the dimension is constant, we show that {$\sigma$}P (q) can be (1+{$\varepsilon$})-approximated w.h.p. in time {\~O} ( nd/2+1 ) , where {$\varepsilon>$} 0 is an arbitrary constant. Furthermore, we show that the problem becomes \#P-complete and W[1]-hard if the dimension is part of the input.},
  keywords = {reduction},
  annotation = {Citation Count: 4}
}

@article{yaoA3Space1983,
  title = {A 3-Space Partition and Its Applications},
  author = {Yao, F. Frances},
  year = {1983},
  doi = {10.1145/800061.808755},
  abstract = {Let S be a set of n points in three-dimensional space. It is shown that one can always find three planes that divide S into eight open regions, of which no seven together contain more than \&agr; n points where \&agr; is a constant {\textexclamdown} 1. This result gives rise to a data structure, what we call an octant-tree, for representing any point set in 3-space. Efficient solutions to various data retrieval problems are readily available with this structure. For example, using octant-trees, one can answer in sublinear time T (n) @@@@O(n0.98) 1) half-space queries: find all points of S that lie to one side of a plane P; 2) polytope queries: find all points that lie inside (outside) a polytope; and 3) circular queries in E2: given a planar set S, find all points that lie within (without) a circle of radius r and center c for any r and c. An octant-tree for n points occupies O(n) space and can be constructed with O(n4) preprocessing time.},
  citationcount = {61},
  venue = {Symposium on the Theory of Computing}
}

@article{yaoAGeneralApproach1985,
  title = {A General Approach to D-Dimensional Geometric Queries},
  author = {Yao, A. and Yao, F.},
  year = {1985},
  doi = {10.1145/22145.22163},
  abstract = {It is shown that any bounded region in {\textexclamdown}italic{\textquestiondown}E{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown} can be divided into 2{\textexclamdown}supscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown} subregions of equal volume in such a way that no hyperplane in {\textexclamdown}italic{\textquestiondown}E{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}supscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown} can intersect all 2{\textexclamdown}supscrpt{\textquestiondown}{\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown}{\textexclamdown}/supscrpt{\textquestiondown} of the subregions. This theorem provides the basis of a data structure scheme for organizing {\textexclamdown}italic{\textquestiondown}n{\textexclamdown}/italic{\textquestiondown} points in {\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown} dimensions. Under this scheme, a broad class of geometric queries in {\textexclamdown}italic{\textquestiondown}d{\textexclamdown}/italic{\textquestiondown} dimensions, including many common problems in range search and optimization, can be solved in linear storage space and sublinear time.},
  citationcount = {170},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,query}
}

@article{yaoContextFreeGrammars1985,
  title = {Context-Free Grammars and Random Number Generation},
  author = {Yao, A.},
  year = {1985},
  doi = {10.1007/978-3-642-82456-2_25},
  abstract = {No abstract available},
  citationcount = {5},
  venue = {No venue available}
}

@article{yaoLowerBoundsBy1983,
  title = {Lower Bounds by Probabilistic Arguments},
  author = {Yao, A.},
  year = {1983},
  doi = {10.1109/SFCS.1983.30},
  abstract = {The purpose of this paper is to resolve several open problems in the current literature on Boolean circuits, communication complexity, and hashing functions. These lower bound results share the common feature that their proofs utilize probabilistic arguments in an essential way. Specifically, we prove that, to compute the majority function of n Boolean variables, the size of any depth-3 monotone circuit must be greater than 2n{$\varepsilon$}, and the size of any width-2 branching program must have super-polynomial growth. We also show that, for the problem of deciding whether i {$\leq$} j for two n-bit integers i and j, the probabilistic {$\varepsilon$}-error one-way communication complexity is of order {\texttheta}(n), while the two-way {$\varepsilon$}-error complexity is O((log n)2). We will also prove that, to compute i {\textquestiondown} j mod p for an n-bit prime p, the probabilistic {$\varepsilon$}-error two-way communication complexity is of order {\texttheta}(n). Finally, we prove a conjecture of Ullman that uniform hashing is asymptotically optimal in its expected retrieval cost among open address hashing schemes.},
  citationcount = {213},
  venue = {24th Annual Symposium on Foundations of Computer Science (sfcs 1983)}
}

@article{yaoOnAccAnd1990,
  title = {{{ON ACC}} and Threshold Circuits},
  author = {Yao, A.},
  year = {1990},
  doi = {10.1109/FSCS.1990.89583},
  abstract = {It is proved that any language in ACC can be approximately computed by two-level circuits of size 2 raised to the (log n)/sup k/ power, with a symmetric-function gate at the top and only AND gates on the first level. This implies that any language in ACC can be recognized by depth-3 threshold circuits of that size. This result gives the first nontrivial upper bound on the computing power of ACC circuits.<<ETX>>},
  citationcount = {226},
  venue = {Proceedings [1990] 31st Annual Symposium on Foundations of Computer Science}
}

@article{yaoOnTheComplexity1985,
  title = {On the Complexity of Maintaining Partial Sums},
  author = {Yao, A.},
  year = {1985},
  doi = {10.1137/0214022},
  abstract = {Let F= (\{r\}\_i,s\_i){\textbar}0{$\leqq$}ii are d-dimensional vectors and s\textsubscript{i} are elements of a commutative semigroup S. We are interested in the partial sum problem, in which queries of the form ``{$\sum$}\textsubscript{\{\vphantom\}}r\textsubscript{i}{$\leqq$}\{a\}\vphantom\{\}\{s\vphantom\}\textsubscript{i}=?\vphantom\{\}'' are to be answered. A space-time tradeoff t={\textohm}(n/(mn/n) is established for storing a static two-dimensional file. It will also be shown that, for the one-dimensional problem, any dynamic algorithm must have a worst-case time {\textohm}(nn/n) in processing a sequence O(n) INSERT and QUERY instructions.},
  citationcount = {96},
  venue = {SIAM journal on computing (Print)},
  keywords = {dynamic,query,static}
}

@article{yaoProbabilisticComputationsToward1977,
  title = {Probabilistic Computations: {{Toward}} a Unified Measure of Complexity},
  author = {Yao, A.},
  year = {1977},
  doi = {10.1109/SFCS.1977.24},
  abstract = {1. Introduction The study of expected running time of algoritruns is an interesting subject from both a theoretical and a practical point of view. Basically there exist two approaches to this study. In the first approach (we shall call it the distributional approach), some "natural" distribution is assumed for the input of a problem, and one looks for fast algorithms under this assumption (see Knuth [8J). For example, in sorting n numbers, it is usually assumed that all n! initial orderings of the numbers are equally likely. A common criticism of this approach is that distributions vary a great deal in real life situations; fu.rthermore, very often the true distribution of the input is simply not known. An alternative approach which attempts to overcome this shortcoming by allowing stochastic moves in the computation has recently been proposed. This is the randomized approach made popular by Habin [lOJ(also see Gill[3J, Solovay and Strassen [13J), although the concept was familiar to statisticians (for exa'1lple, see Luce and Raiffa [9J). Note that by allowing stochastic moves in an algorithm, the input is effectively being randomized. We shall refer to such an algoritlvn as a randomized algorithm. These two approaches lead naturally to two different definitions of intrinsic complexity of a problem, which we term the distributional complexity and the randomized complexity, respectively. (Precise definitions and examples will be given in Sections 2 and 3.) To solidify the ideas, we look at familiar combinatorial problems that can be modeled by decision trees. In particular, we consider (a) the testing of an arbitrary graph property from an adjacency matrix (Section 2), and (b) partial order problems on n We will show that for these two classes of problems, the two complexity measures always agree by virtue of a famous theorem, the Minimax Theorem of Von Neumann [14J. The connection between the two approaches lends itself to applications. With two different views (and in a sense complementary to each other) on the complexity of a problem, it is frequently easier to derive upper and lower bounds. For example, using adjacency matrix representation for a graph, it can be shown that no randomized algorithm can determine 2 the existence of a perfect matching in less than O(n) probes. Such lower bounds to the randomized approach were lacking previously. As another example of application , we can prove that for the partial order problems in (b), assuming uniform {\dots}},
  citationcount = {1129},
  venue = {18th Annual Symposium on Foundations of Computer Science (sfcs 1977)}
}

@inproceedings{yaoShouldTablesBe1978a,
  title = {Should Tables Be Sorted?},
  booktitle = {19th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} (Sfcs 1978)},
  author = {Yao, Andrew G.},
  year = {1978},
  month = oct,
  pages = {22--27},
  issn = {0272-5428},
  doi = {10.1109/SFCS.1978.33},
  url = {https://ieeexplore.ieee.org/document/4567958},
  urldate = {2024-09-01},
  keywords = {Artificial intelligence,Computer science,Costs,Data structures,Information retrieval,Probes},
  file = {/Users/tulasi/Zotero/storage/H66AAUSE/Yao - 1978 - Should tables be sorted.pdf}
}

@article{yaoSomeComplexityQuestions1979,
  title = {Some Complexity Questions Related to Distributive Computing({{Preliminary Report}})},
  author = {Yao, A.},
  year = {1979},
  doi = {10.1145/800135.804414},
  abstract = {Let M = \{0, 1, 2, ..., m---1\} , N = \{0, 1, 2,..., n---1\} , and f:M {\texttimes} N {$\rightarrow$} \{0, 1\} a Boolean-valued function. We will be interested in the following problem and its related questions. Let i \&egr; M, j \&egr; N be integers known only to two persons P1 and P2, respectively. For P1 and P2 to determine cooperatively the value f(i, j), they send information to each other alternately, one bit at a time, according to some algorithm. The quantity of interest, which measures the information exchange necessary for computing f, is the minimum number of bits exchanged in any algorithm. For example, if f(i, j) = (i + j) mod 2. then 1 bit of information (conveying whether i is odd) sent from P1 to P2 will enable P2 to determine f(i, j), and this is clearly the best possible. The above problem is a variation of a model of Abelson [1] concerning information transfer in distributive computions.},
  citationcount = {1285},
  venue = {Symposium on the Theory of Computing}
}

@article{yaoSpaceTimeTradeoff1982,
  title = {Space-Time Tradeoff for Answering Range Queries ({{Extended Abstract}})},
  author = {Yao, A.},
  year = {1982},
  doi = {10.1145/800070.802185},
  abstract = {In this paper, we raise and investigate the question of (storage) space- (retrieval) time tradeoff for a static database, in the general framework of Fredman's. As will be seen, such tradeoff results also lead to lower bounds on the complexity of processing a sequence of m INSERT and QUERY instructions. The latter results are incomparable to Fredman's, since the presence of DELETE instructions was crucial for his proof technique. We will present our results in detail in the next few sections. Here we will only mention three main conclusions. Firstly, circular query is shown to be intrinsically hard in the sense that, for some static database with n records, there is a space-time tradeoff TS -\&-gt; n1 + -\&-egr; where -\&-egr;-\&-gt;0; in contrast, orthogonal query can always be implemented with space S-\&-equil;0(n(log n)k) and time T-\&-equil;0((log n)k) for fixed k. Furthermore, any algorithm for processing 0(n) INSERT and QUERY instructions must use time -\&-Ohgr;(n1+-\&-egr;) in the worst case. Secondly, for the -\&-ldquo;interval-\&-rdquo; query, we have determined the space-time tradeoff quite precisely to be T @@@@ -\&-agr;(S,n), where -\&-agr; is the inverse to an Ackermann's function first defined by Tarjan [9]. This is a rare case where the function -\&-agr; arises outside the context of path compression, and is obtained through a totally independent derivation. Thirdly, we prove that, for the interval query, any algorithm to process a sequence of 0(n) INSERT and QUERY must take time -\&-Ohgr;((n log n)/(log log n)) in the worst case. This means that one cannot hope to maintain the most efficient static data structure (with retrieval time -\&-agr;(S,n)) in the dynamic case.},
  citationcount = {123},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,dynamic,lower bound,query,static}
}

@article{yaowuComplexityDependentAndParallelism2003,
  title = {On the {{Complexity}} of {{Dependent And-Parallelism}} in {{Logic Programming}}},
  author = {Yao Wu and Enrico Pontelli and D. Ranjan},
  year = {2003},
  journal = {International Conference on Logic Programming},
  doi = {10.1007/978-3-540-24599-5_25},
  annotation = {Citation Count: 4}
}

@article{yaowuComputationalIssuesExploiting2005,
  title = {Computational {{Issues}} in {{Exploiting Dependent And-Parallelism}} in {{Logic Programming}}: {{Leftness Detection}} in {{Dynamic Search Trees}}},
  author = {Yao Wu and Enrico Pontelli and D. Ranjan},
  year = {2005},
  journal = {Logic Programming and Automated Reasoning},
  doi = {10.1007/11591191_7},
  keywords = {dynamic},
  annotation = {Citation Count: 1}
}

@article{yarovoyPrivacyPreservingPublishing2009,
  title = {Privacy-Preserving Publishing of Moving Objects Databases},
  author = {Yarovoy, Roman},
  year = {2009},
  doi = {10.14288/1.0051481},
  abstract = {Moving Objects Databases (MOD) have gained popularity as a subject for research due to the latest developments in the positioning technologies and mobile networking. Analysis of mobility data can be used to discover and deliver knowledge that can enhance public welfare. For instance, a study of traffic patterns and congestion trends can reveal some information that can be used to improve routing and scheduling of public transit vehicles. To enable analysis of mobility data, a MOD must be published. However, publication of MOD can pose a threat to location privacy of users, whose movement is recorded in the database. A user's location at one or more time points can be publicly available prior to the publication of MOD. Based on this public knowledge, an attacker can potentially find the user's entire trajectory and learn his/her positions at other time points, which constitutes privacy breach. This public knowledge is a user's quasi-identifier (QID), i.e. a set of attributes that can uniquely identify the user's trajectory in the published database. We argue that unlike in relational microdata, where all tuples have the same set of quasi-identifiers, in mobility data, the concept of quasi-identifier must be modeled subjectively on an individual basis. In this work, we study the problem of privacy preserving publication of MOD. We conjecture that each Moving Object (MOB) may have a distinct QID. We develop a possible attack model on the published MOD given public knowledge of some or all MOBs. We develop k-anonymity model (based on classical k-anonymity), which ensures that every object is indistinguishable (with respect to its QID) from at least k - 1 other objects, and show that this model is impervious to the proposed attack model. We employ space generalization to achieve MOB anonymity. We propose three anonymization algorithms that generate a MOD that satisfies the k-anonymity model, while minimizing the information loss. We conduct several sets of experiments on synthetic and real-world data sets of vehicular traffic to analyze and evaluate our proposed algorithms.},
  citationcount = {Unknown},
  venue = {No venue available}
}

@article{yeoLowerBoundsFor2022,
  title = {Lower Bounds for (Batch) {{PIR}} with Private Preprocessing},
  author = {Yeo, Kevin},
  year = {2022},
  doi = {10.1007/978-3-031-30545-0_18},
  abstract = {No abstract available},
  citationcount = {15},
  venue = {IACR Cryptology ePrint Archive},
  keywords = {lower bound}
}

@article{yianilosLocallyLiftingThe2000,
  title = {Locally Lifting the Curse of Dimensionality for Nearest Neighbor Search (Extended Abstract)},
  author = {Yianilos, P.},
  year = {2000},
  doi = {10.1090/dimacs/059/08},
  abstract = {A b s t r a c t We consider the problem of nearest neighbor search in the Euclidean hypercube [ -1 ,+1 ] d with uniform distributions, and the additional natural assumption that the nearest neighbor is located within a constant fraction R of the maximum interpoint distance in this space, i.e. within distance 2 R v   of the query. We introduce the idea of aggressive pruning and give a family of practical algorithms, an idealized analysis, and describe experiments. Our main result is that search complexity measured in terms of d-dimensional inner product operations, is i) strongly sublinear with respect to the data set size n for moderate R, ii) asymptotically, and as a practical matter, independent of dimension. Given a random data set, a random query within distance 2Rv/d of some database element, and a randomly constructed data structure, the search succeeds with a specified probability, which is a parameter of the search algorithm. On average a search performs  -. n p distance computations where n is the number of points in the database, and p {\textexclamdown} 1 is calculated in our analysis. Linear and near-linear space structures are described, and our algorithms and analysis are free of large hidden constants, i.e. the algorithms perform far less work than exhaustive search both in theory and practice. This paper focuses on uniform distributions and Euclidean space, but we believe our ideas may contribute to improved general purpose algorithms for high dimensions.},
  citationcount = {54},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {data structure,query}
}

@article{yiCellProbeComplexity2010,
  title = {On the Cell Probe Complexity of Dynamic Membership},
  author = {Yi, K. and Zhang, Qin},
  year = {2010},
  doi = {10.1137/1.9781611973075.12},
  abstract = {We study the dynamic membership problem, one of the most fundamental data structure problems, in the cell probe model with an arbitrary cell size. We consider a cell probe model equipped with a cache that consists of at least a constant number of cells; reading or writing the cache is free of charge. For nearly all common data structures, it is known that with sufficiently large cells together with the cache, we can significantly lower the amortized update cost to o(1). In this paper, we show that this is not the case for the dynamic membership problem. Specifically, for any deterministic membership data structure under a random input sequence, if the expected average query cost is no more than 1+{$\Delta$} for some small constant {$\Delta$}, we prove that the expected amortized update cost must be at least {\textohm}(1), namely, it does not benefit from large block writes (and a cache). The space the structure uses is irrelevant to this lower bound. We also extend this lower bound to randomized membership structures, by using a variant of Yao's minimax principle. Finally, we show that the structure cannot do better even if it is allowed to answer a query mistakenly with a small constant probability.},
  citationcount = {12},
  venue = {ACM-SIAM Symposium on Discrete Algorithms},
  keywords = {cell probe,data structure,dynamic,lower bound,query,update}
}

@article{yiDynamicIndexabilityAnd2009,
  title = {Dynamic Indexability and Lower Bounds for Dynamic One-Dimensional Range Query Indexes},
  author = {Yi, K.},
  year = {2009},
  doi = {10.1145/1559795.1559825},
  abstract = {The B-tree is a fundamental external index structure that is widely used for answering one-dimensional range reporting queries. Given a set of N keys, a range query can be answered in O(logB NoverM + KoverB) I/Os, where B is the disk block size, K the output size, and M the size of the main memory buffer. When keys are inserted or deleted, the B-tree is updated in O(logB N) I/Os, if we require the resulting changes to be committed to disk right away. Otherwise, the memory buffer can be used to buffer the recent updates, and changes can be written to disk in batches, which significantly lowers the amortized update cost. A systematic way of batching up updates is to use the logarithmic method, combined with fractional cascading, resulting in a dynamic B-tree that supports insertions in O(1overB log NoverM) I/Os and queries in O(log NoverM + KoverB) I/Os. Such bounds have also been matched by several known dynamic B-tree variants in the database literature. Note that, however, the query cost of these dynamic B-trees is substantially worse than the O(logB NoverM + KoverB) bound of the static B-tree by a factor of ?(log B). In this paper, we prove that for any dynamic one dimensional range query index structure with query cost O(q + KoverB) and amortized insertion cost O(u/B), the tradeoff q {$\cdot$} log(u/q) = {\copyright}(log B) must hold if q = O(log B). For most reasonable values of the parameters, we have NoverM = BO(1), in which case our query-insertion tradeoff implies that the bounds mentioned above are already optimal. We also prove a lower bound of u {$\cdot$} log q = {\copyright}(log B), which is relevant for larger values of q. Our lower bounds hold in a dynamic version of the indexability model, which is of independent interests. Dynamic indexability is a clean yet powerful model for studying dynamic indexing problems, and can potentially lead to more interesting complexity results.},
  citationcount = {7},
  venue = {ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems},
  keywords = {dynamic,lower bound,query,static,update}
}

@article{yiDynamicIndexabilityAnd2012,
  title = {Dynamic Indexability and the Optimality of B-Trees},
  author = {Yi, K.},
  year = {2012},
  doi = {10.1145/2339123.2339129},
  abstract = {One-dimensional range queries, as one of the most basic type of queries in databases, have been studied extensively in the literature. For large databases, the goal is to build an external index that is optimized for disk block accesses (or I/Os). The problem is well understood in the static case. Theoretically, there exists an index of linear size that can answer a range query in O(1 + KB) I/Os, where K is the output size and B is the disk block size, but it is highly impractical. In practice, the standard solution is the B-tree, which answers a query in O(logB NM + KB) I/Os on a data set of size N, where M is the main memory size. For typical values of N, M, and B, logB NM can be considered a constant. However, the problem is still wide open in the dynamic setting, when insertions and deletions of records are to be supported. With smart buffering, it is possible to speed up updates significantly to o(1) I/Os amortized. Indeed, several dynamic B-trees have been proposed, but they all cause certain levels of degradation in the query performance, with the most interesting tradeoff point at O(1B log NM) I/Os for updates and O(log NM + KB) I/Os for queries. In this article, we prove that the query-update tradeoffs of all the known dynamic B-trees are optimal, when logB NM is a constant. This implies that one should not hope for substantially better solutions for all practical values of the parameters. Our lower bounds hold in a dynamic version of the indexability model, which is of independent interests. Dynamic indexability is a clean yet powerful model for studying dynamic indexing problems, and can potentially lead to more interesting lower bound results.},
  citationcount = {9},
  venue = {JACM},
  keywords = {dynamic,lower bound,query,static,update}
}

@article{yinCellProbeProofs2008,
  title = {Cell-Probe Proofs and Nondeterministic Cell-Probe Complexity},
  author = {Yin, Yitong},
  year = {2008},
  abstract = {No abstract available},
  citationcount = {1},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {cell probe}
}

@article{yinCellprobeProofs2010,
  title = {Cell-Probe Proofs},
  author = {Yin, Y.},
  year = {2010},
  month = nov,
  journal = {ACM Transactions on Computation Theory},
  volume = {2},
  pages = {1:1--1:17},
  doi = {10.1145/1867719.1867720},
  keywords = {cell probe},
  annotation = {Husfeldt , T. and Rauhe , T . 1998. Hardness Results for dynamic problems by extensions of fredman and saks' chronogram method . In Proceedings of the 25th International Colloquium on Automata, Languages and Programming. Springer , Berlin, 67--78. Husfeldt, T. and Rauhe, T. 1998. Hardness Results for dynamic problems by extensions of fredman and saks' chronogram method. In Proceedings of the 25th International Colloquium on Automata, Languages and Programming. Springer, Berlin, 67--78.\\
Indyk , P. , Goodman , J. , and O'Rourke , J. 2004 . Handbook of Discrete and Computational Geometry. CRC Press , Ch. 39 , 877 -- 892 . Indyk, P., Goodman, J., and O'Rourke, J. 2004. Handbook of Discrete and Computational Geometry. CRC Press, Ch. 39, 877--892.\\
\\
Kushilevitz E. and Nisan N. 1997. Communication Complexity. Cambridge University Press Cambridge UK. Kushilevitz E. and Nisan N. 1997. Communication Complexity . Cambridge University Press Cambridge UK.\\
\\
Miltersen , P. 1999 . Cell probe complexity-a survey . In Pre-Conference Workshop on Advances in Data Structures at the 19th Conference on Foundations of Software Technology and Theoretical Computer Science. Miltersen, P. 1999. Cell probe complexity-a survey. In Pre-Conference Workshop on Advances in Data Structures at the 19th Conference on Foundations of Software Technology and Theoretical Computer Science.\\
Miltersen P. 2008. Private communication. Miltersen P. 2008. Private communication.\\
\\
P{\v a}tra{\c s}cu , M. 2008 . Unifying the landscape of cell-probe lower bounds . In Proceedings of the IEEE Conference on Foundations of Computer Science. IEEE , Los Alamitos, CA, 434--443. P{\v a}tra{\c s}cu, M. 2008. Unifying the landscape of cell-probe lower bounds. In Proceedings of the IEEE Conference on Foundations of Computer Science. IEEE, Los Alamitos, CA, 434--443.},
  file = {/Users/tulasi/Zotero/storage/H7LXSL9K/Yin - 2010 - Cell-Probe Proofs.pdf}
}

@article{yinSimpleAveragecaseLower2016,
  title = {Simple Average-Case Lower Bounds for Approximate near-Neighbor from Isoperimetric Inequalities},
  author = {Yin, Yitong},
  year = {2016},
  doi = {10.4230/LIPIcs.ICALP.2016.84},
  abstract = {We prove an {\textohm}(d/\textsuperscript{\{\vphantom\}}{\textfractionsolidus}\textsubscript{s}w\vphantom\{\}\{nd\}) lower bound for the average-case cell-probe complexity of deterministic or Las Vegas randomized algorithms solving approximate near-neighbor (ANN) problem in d-dimensional Hamming space in the cell-probe model with w-bit cells, using a table of size s. This lower bound matches the highest known worst-case cell-probe lower bounds for any static data structure problems. This average-case cell-probe lower bound is proved in a general framework which relates the cell-probe complexity of ANN to isoperimetric inequalities in the underlying metric space. A tighter connection between ANN lower bounds and isoperimetric inequalities is established by a stronger richness lemma proved by cell-sampling techniques.},
  citationcount = {8},
  venue = {International Colloquium on Automata, Languages and Programming},
  keywords = {cell probe,cell sampling,data structure,lower bound,static}
}

@article{yiProteinSecondaryStructure1993,
  title = {Protein Secondary Structure Prediction Using Nearest-Neighbor Methods.},
  author = {Yi, Tau-Mu and Lander, Eric S.},
  year = {1993},
  doi = {10.1006/JMBI.1993.1464},
  abstract = {We have studied the use of nearest-neighbor classifiers to predict the secondary structure of proteins. The nearest-neighbor rule states that a test instance is classified according to the classifications of "nearby" training examples from a database of known structures. In the context of secondary structure prediction, the test instances are windows of n consecutive residues, and the label is the secondary structure type (alpha-helix, beta-strand, or coil) of the center position of the window. To define the neighborhood of a test instance, we employed a novel similarity metric based on the local structural environment scoring scheme of Bowie et al. In this manner, we have attempted to exploit the underlying structural similarity between segments of different proteins to aid in the prediction of secondary structure. Furthermore, in addition to using neighborhoods of fixed radius, we explored a modification of the standard nearest-neighbor algorithm that involved defining an "effective radius" for each exemplar by measuring its performance on a training set. Using these ideas, we achieved a peak prediction accuracy of 68},
  citationcount = {162},
  venue = {Journal of Molecular Biology}
}

@article{yuCellprobeLowerBounds2015,
  title = {Cell-Probe Lower Bounds for Dynamic Problems via a New Communication Model},
  author = {Yu, Huacheng},
  year = {2015},
  doi = {10.1145/2897518.2897556},
  abstract = {In this paper, we develop a new communication model to prove a data structure lower bound for the dynamic interval union problem. The problem is to maintain a multiset of intervals I over [0, n] with integer coordinates, supporting the following operations: 1) insert(a, b), add an interval [a, b] to I, provided that a and b are integers in [0, n]; 2) delete(a, b), delete an (existing) interval [a, b] from I; 3) query(), return the total length of the union of all intervals in I. It is related to the two-dimensional case of Klee's measure problem. We prove that there is a distribution over sequences of operations with O(n) insertions and deletions, and O(n0.01) queries, for which any data structure with any constant error probability requires {\textohm}(nlogn) time in expectation. Interestingly, we use the sparse set disjointness protocol of H{\aa}stad and Wigderson to speed up a reduction from a new kind of nondeterministic communication games, for which we prove lower bounds. For applications, we prove lower bounds for several dynamic graph problems by reducing them from dynamic interval union.},
  citationcount = {14},
  venue = {Symposium on the Theory of Computing},
  keywords = {cell probe,communication,data structure,dynamic,lower bound,query,reduction}
}

@article{yuchengMixtureSelectionMechanism2015,
  title = {Mixture {{Selection}}, {{Mechanism Design}}, and {{Signaling}}},
  author = {Yu Cheng and Ho Yee Cheung and S. Dughmi and {E. Emamjomeh-Zadeh} and Lining Han and S. Teng},
  year = {2015},
  journal = {IEEE Annual Symposium on Foundations of Computer Science},
  doi = {10.1109/FOCS.2015.91},
  abstract = {We pose and study a fundamental algorithmic problem which we term mixture selection, arising as a building block in a number of game-theoretic applications: Given a function g from the n-dimensional hypercube to the bounded interval [-1, 1], and an n {\texttimes} rn matrix A with bounded entries, maximize g(Ax) over x in the m-dimensional simplex. This problem arises naturally when one seeks to design a lottery over items for sale in an auction, or craft the posterior beliefs for agents in a Bayesian game through the provision of information (a.k.a. signaling). We present an approximation algorithm for this problem when g simultaneously satisfies two ``smoothness'' properties: Lipschitz continuity with respect to the L{$\infty$} norm, and noise stability. The latter notion, which we define and cater to our setting, controls the degree to which low-probability - and possibly correlated - errors in the inputs of g can impact its output. The approximation guarantee of our algorithm degrades gracefully as a function of the Lipschitz continuity and noise stability of g. In particular, when g is both 0(1)-Lipschitz continuous and 0(1)-stable, we obtain an (additive) polynomial-time approximation scheme (PTAS) for mixture selection. We also show that neither assumption suffices by itself for an additive PTAS, and both assumptions together do not suffice for an additive fully polynomial-time approximation scheme (FPTAS). We apply our algorithm for mixture selection to a number of different game-theoretic applications, focusing on problems from mechanism design and optimal signaling. In particular, we make progress on a number of open problems suggested in prior work by easily reducing them to mixture selection: we resolve an important special case of the small-menu lottery design problem posed by Dughmi, Han, and Nisan [10]; we resolve the problem of revenue-maximizing signaling in Bayesian secondprice auctions posed by Emek et al. [12] and Miltersen and Sheffet [5]; we design a quasipolynomial-time approximation scheme for the optimal signaling problem in normal form games suggested by Dughmi [9]; and we design an approximation algorithm for the optimal signaling problem in the voting model of Alonso and Camara [3].},
  keywords = {information},
  annotation = {Citation Count: 53}
}

@article{yueSpatialPyramidFormulation2013,
  title = {Spatial Pyramid Formulation in Weakly Supervised Manner},
  author = {Yue, Yawei and Yue, ZhongTao and Wang, Xiaolin and Ni, Guangyun},
  year = {2013},
  doi = {10.1007/978-3-642-39068-5_51},
  abstract = {No abstract available},
  citationcount = {Unknown},
  venue = {International Symposium on Neural Networks}
}

@article{yuNearlyOptimalStatic2019,
  title = {Nearly Optimal Static {{Las Vegas}} Succinct Dictionary},
  author = {Yu, Huacheng},
  year = {2019},
  doi = {10.1145/3357713.3384274},
  abstract = {Given a set S of n (distinct) keys from key space [U], each associated with a value from {$\Sigma$}, the static dictionary problem asks to preprocess these (key, value) pairs into a data structure, supporting value-retrieval queries: for any given x{$\in$} [U], valRet(x) must return the value associated with x if x{$\in$} S, or return {$\perp$} if x{$\notin$} S. The special case where {\textbar}{$\Sigma\vert$}=1 is called the membership problem. The ``textbook'' solution is to use a hash table, which occupies linear space and answers each query in constant time. On the other hand, the minimum possible space to encode all (key, value) pairs is only OPT:= {$\lceil$}lg2( U n )+nlg2{\textbar}{$\Sigma\vert\rceil$} bits, which could be much less. In this paper, we design a randomized dictionary data structure using OPT+lgn+O(lglglglglgU) bits of space, and it has expected constant query time, assuming the query algorithm can access an external lookup table of size n 0.001. The lookup table depends only on U, n and {\textbar}{$\Sigma\vert$}, and not the input. Previously, even for membership queries and U{$\leq$} n O(1), the best known data structure with constant query time requires OPT+n/lgn bits of space by Pagh (SIAM J. Comput. 2001) and P{\v a}tra{\c s}cu (FOCS 2008); the best known using OPT+n 0.999 space has query time O(lgn); the only known non-trivial data structure with OPT+n 0.001 space has O(lgn) query time and requires a lookup table of size {$\geq$} n 2.99 (!). Our new data structure answers open questions by P{\v a}tra{\c s}cu and Thorup.},
  citationcount = {6},
  venue = {Symposium on the Theory of Computing},
  keywords = {data structure,query,query time,static}
}

@article{yuOptimalSuccinctRank2018,
  title = {Optimal Succinct Rank Data Structure via Approximate Nonnegative Tensor Decomposition},
  author = {Yu, Huacheng},
  year = {2018},
  doi = {10.1145/3313276.3316352},
  abstract = {Given an n-bit array A, the succinct rank data structure problem asks to construct a data structure using space n+r bits for r{$\ll$} n, supporting rank queries of form rank(u)={$\sum$}i=0u-1 A[i]. In this paper, we design a new succinct rank data structure with r=n/(logn){\textohm}(t)+n1-c and query time O(t) for some constant c{\textquestiondown}0, improving the previous best-known by P{\v a}tra{\c s}cu, which has r=n/(logn/t){\textohm}(t)+{\~O}(n3/4) bits of redundancy. For r{\textquestiondown}n1-c, our space-time tradeoff matches the cell-probe lower bound by P{\v a}tra{\c s}cu and Viola, which asserts that r must be at least n/(logn)O(t). Moreover, one can avoid an n1-c-bit lookup table when the data structure is implemented in the cell-probe model, achieving r={$\lceil$} n/(logn){\textohm}(t){$\rceil$}. It matches the lower bound for the full range of parameters. En route to our new data structure design, we establish an interesting connection between succinct data structures and approximate nonnegative tensor decomposition. Our connection shows that for specific problems, to construct a space-efficient data structure, it suffices to approximate a particular tensor by a sum of (few) nonnegative rank-1 tensors. For the rank problem, we explicitly construct such an approximation, which yields an explicit construction of the data structure.},
  citationcount = {9},
  venue = {Symposium on the Theory of Computing},
  keywords = {cell probe,data structure,lower bound,query,query time}
}

@article{yuRearrangementOnLattices2021,
  title = {Rearrangement on Lattices with Swaps: {{Optimality}} Structures and Efficient Algorithms},
  author = {Yu, Jingjin},
  year = {2021},
  doi = {10.15607/RSS.2021.XVII.014},
  abstract = {---We investigate a class of multi-object rearrangement problems in which a robotic manipulator, capable of carrying an item and making item swaps, is tasked to sort items stored in lattices in a time-optimal manner. We systematically analyze the intrinsic optimality structure, which is fairly rich and intriguing, under different levels of item distinguishability (fully labeled or partially labeled) and different lattice dimensions. Focusing on the most practical setting of one and two dimensions, we develop efficient (low polynomial time) algorithms that optimally perform rearrangements on 1D lattices under both fully and partially labeled settings. On the other hand, we prove that rearrangement on 2D and higher dimensional lattices becomes computationally intractable to optimally solve. Despite their NP-hardness, we are able to again develop efficient algorithms for 2D fully and partially labeled settings that are asymptotically optimal, in expectation, assuming that the initial configuration is randomly selected. Simulation studies confirm the effectiveness of our algorithms in comparison to natural greedy best-first algorithms.},
  citationcount = {4},
  venue = {Robotics: Science and Systems}
}

@article{yuRearrangementOnLattices2021,
  title = {Rearrangement on Lattices with Pick-n-Swaps: {{Optimality}} Structures and Efficient Algorithms},
  author = {Yu, Jingjin},
  year = {2021},
  doi = {10.1177/02783649231153901},
  abstract = {We study a class of rearrangement problems under a novel pick-n-swap prehensile manipulation model, in which a robotic manipulator, capable of carrying an item and making item swaps, is tasked to sort items stored in lattices of variable dimensions in a time-optimal manner. We systematically analyze the intrinsic optimality structure, which is fairly rich and intriguing, under different levels of item distinguishability (fully-labeled, where each item has a unique label, or partially-labeled, where multiple items may be of the same type) and different lattice dimensions. Focusing on the most practical setting of one and two dimensions, we develop low polynomial time cycle-following-based algorithms that optimally perform rearrangements on 1D lattices under both fully- and partially-labeled settings. On the other hand, we show that rearrangement on 2D and higher-dimensional lattices become computationally intractable to optimally solve. Despite their NP-hardness, we prove that efficient cycle-following-based algorithms remain optimal in the asymptotic sense for 2D fully- and partially-labeled settings, in expectation, using the interesting fact that random permutations induce only a small number of cycles. We further improve these algorithms to provide 1.x-optimality when the number of items is small. Simulation studies corroborate the effectiveness of our algorithms. The implementation of the algorithms from the paper can be found at github.com/arc-l/lattice-rearrangement.},
  citationcount = {4},
  venue = {Int. J. Robotics Res.}
}

@article{yuxuanchenLGRASSLinearGraph2022,
  title = {{{LGRASS}}: {{Linear Graph Spectral Sparsification}} for {{Final Task}} of the 3rd {{ACM-China International Parallel Computing Challenge}}},
  author = {Yuxuan Chen and Jiyan Qiu and Zidong Han and Chenhan Bai},
  year = {2022},
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2212.07297},
  abstract = {This paper presents our solution for optimization task of the 3rd ACM-China IPCC. By the complexity analysis, we identified three time-consuming subroutines of original algorithm: marking edges, computing pseudo inverse and sorting edges. These subroutines becomes the main performance bottleneck owing to their super-linear time complexity. To address this, we proposed LGRASS, a linear graph spectral sparsification algorithm to run in strictly linear time. LGRASS takes advantage of spanning tree properties and efficient algorithms to optimize bottleneck subroutines. Furthermore, we crafted a parallel processing scheme for LGRASS to make full use of multi-processor hardware. Experiment shows that our proposed method fulfils the task in dozens of milliseconds on official test cases and keep its linearity as graph size scales up on random test cases.},
  annotation = {Citation Count: 1}
}

@article{z.galilDataStructuresAlgorithms1991,
  title = {Data Structures and Algorithms for Disjoint Set Union Problems},
  author = {Z. Galil and G. Italiano},
  year = {1991},
  journal = {CSUR},
  doi = {10.1145/116873.116878},
  abstract = {This paper surveys algorithmic techniques and data structures that have been proposed tosolve thesetunion problem and its variants, Thediscovery of these data structures required anew set ofalgorithmic tools that have proved useful in other areas. Special attention is devoted to recent extensions of the original set union problem, and an attempt is made to provide a unifying theoretical framework for this growing body of algorithms.},
  keywords = {data structure},
  annotation = {Citation Count: 202}
}

@article{z.galilNoteSetUnion1991,
  title = {A {{Note}} on {{Set Union}} with {{Arbitrary Deunions}}},
  author = {Z. Galil and G. Italiano},
  year = {1991},
  journal = {Information Processing Letters},
  doi = {10.1016/0020-0190(91)90151-7},
  annotation = {Citation Count: 10}
}

@article{zeighamiOnDistributionDependent2023,
  title = {On Distribution Dependent Sub-Logarithmic Query Time of Learned Indexing},
  author = {Zeighami, Sepanta and Shahabi, C.},
  year = {2023},
  doi = {10.48550/arXiv.2306.10651},
  abstract = {A fundamental problem in data management is to find the elements in an array that match a query. Recently, learned indexes are being extensively used to solve this problem, where they learn a model to predict the location of the items in the array. They are empirically shown to outperform non-learned methods (e.g., B-trees or binary search that answer queries in O(logn) time) by orders of magnitude. However, success of learned indexes has not been theoretically justified. Only existing attempt shows the same query time of O(logn), but with a constant factor improvement in space complexity over non-learned methods, under some assumptions on data distribution. In this paper, we significantly strengthen this result, showing that under mild assumptions on data distribution, and the same space complexity as non-learned methods, learned indexes can answer queries in O(loglogn) expected query time. We also show that allowing for slightly larger but still near-linear space overhead, a learned index can achieve O(1) expected query time. Our results theoretically prove learned indexes are orders of magnitude faster than non-learned methods, theoretically grounding their empirical success.},
  citationcount = {9},
  venue = {International Conference on Machine Learning},
  keywords = {query,query time}
}

@article{zeighamiTowardsEstablishingGuaranteed2024,
  title = {Towards Establishing Guaranteed Error for Learned Database Operations},
  author = {Zeighami, Sepanta and Shahabi, Cyrus},
  year = {2024},
  doi = {10.48550/arXiv.2411.06243},
  abstract = {Machine learning models have demonstrated substantial performance enhancements over non-learned alternatives in various fundamental data management operations, including indexing (locating items in an array), cardinality estimation (estimating the number of matching records in a database), and range-sum estimation (estimating aggregate attribute values for query-matched records). However, real-world systems frequently favor less efficient non-learned methods due to their ability to offer (worst-case) error guarantees - an aspect where learned approaches often fall short. The primary objective of these guarantees is to ensure system reliability, ensuring that the chosen approach consistently delivers the desired level of accuracy across all databases. In this paper, we embark on the first theoretical study of such guarantees for learned methods, presenting the necessary conditions for such guarantees to hold when using machine learning to perform indexing, cardinality estimation and range-sum estimation. Specifically, we present the first known lower bounds on the model size required to achieve the desired accuracy for these three key database operations. Our results bound the required model size for given average and worst-case errors in performing database operations, serving as the first theoretical guidelines governing how model size must change based on data size to be able to guarantee an accuracy level. More broadly, our established guarantees pave the way for the broader adoption and integration of learned models into real-world systems.},
  citationcount = {1},
  venue = {International Conference on Learning Representations},
  keywords = {lower bound,query}
}

@article{zeumeOnTheQuantifier2013,
  title = {On the Quantifier-Free Dynamic Complexity of {{Reachability}}},
  author = {Zeume, Thomas and Schwentick, T.},
  year = {2013},
  doi = {10.1016/j.ic.2014.09.011.},
  abstract = {No abstract available},
  citationcount = {18},
  venue = {Information and Computation},
  keywords = {dynamic}
}

@article{zhangIncrementalSlidingWindow2024,
  title = {Incremental Sliding Window Connectivity over Streaming Graphs},
  author = {Zhang, Chao and Bonifati, Angela and Ozsu, M. Tamer},
  year = {2024},
  doi = {10.48550/arXiv.2406.06754},
  abstract = {We study index-based processing for connectivity queries within sliding windows on streaming graphs. These queries, which determine whether two vertices belong to the same connected component, are fundamental operations in real-time graph data processing and demand high throughput and low latency. While indexing methods that leverage data structures for fully dynamic connectivity can facilitate efficient query processing, they encounter significant challenges with deleting expired edges from the window during window updates. We introduce a novel indexing approach that eliminates the need for physically performing edge deletions. This is achieved through a unique bidirectional incremental computation framework, referred to as the BIC model. The BIC model implements two distinct incremental computations to compute connected components within the window, operating along and against the timeline, respectively. These computations are then merged to efficiently compute queries in the window. We propose techniques for optimized index storage, incremental index updates, and efficient query processing to improve BIC effectiveness. Empirically, BIC achieves a 14{\texttimes} increase in throughput and a reduction in P95 latency by up to 3900{\texttimes} when compared to state-of-the-art indexes.},
  citationcount = {3},
  venue = {Proceedings of the VLDB Endowment},
  keywords = {data structure,dynamic,query,reduction,update}
}

@article{zhangLowLatencySliding2024,
  title = {Low-Latency Sliding Window Connectivity},
  author = {Zhang, Chao and Bonifati, Angela and Ozsu, Tamer},
  year = {2024},
  doi = {10.48550/arXiv.2410.00884},
  abstract = {Connectivity queries, which check whether vertices belong to the same connected component, are fundamental in graph computations. Sliding window connectivity processes these queries over sliding windows, facilitating real-time streaming graph analytics. However, existing methods struggle with low-latency processing due to the significant overhead of continuously updating index structures as edges are inserted and deleted. We introduce a novel approach that leverages spanning trees to efficiently process queries. The novelty of this method lies in its ability to maintain spanning trees efficiently as window updates occur. Notably, our approach completely eliminates the need for replacement edge searches, a traditional bottleneck in managing spanning trees during edge deletions. We also present several optimizations to maximize the potential of spanning-tree-based indexes. Our comprehensive experimental evaluation shows that index update latency in spanning trees can be reduced by up to 458{\texttimes} while maintaining query performance, leading to an 8{\texttimes} improvement in throughput. Our approach also significantly outperforms the state-of-the-art in both query processing and index updates. Additionally, our methods use significantly less memory and demonstrate consistent efficiency across various settings.},
  citationcount = {Unknown},
  venue = {arXiv.org},
  keywords = {query,update}
}

@article{zhangTopKHeavy2022,
  title = {Top-k Heavy Weight Triangles Listing on Graph Stream},
  author = {Zhang, Fan and Gou, Xiangyang and Zou, Lei},
  year = {2022},
  doi = {10.1007/s11280-022-01117-z},
  abstract = {No abstract available},
  citationcount = {3},
  venue = {World wide web (Bussum)}
}

@article{zhaoTowardsApproximateEvent2011,
  title = {Towards Approximate Event Processing in a Large-Scale Content-Based Network},
  author = {Zhao, Yaxiong and Wu, Jie},
  year = {2011},
  doi = {10.1109/ICDCS.2011.67},
  abstract = {Event matching is a critical component of large-scale content-based publish/subscribe systems. However, most existing methods suffer from a dramatic performance degradation when the system scales up. In this paper, we present TAMA (Table Match), a highly efficient content-based event matching and forwarding engine. We consider range-based attribute constraints that are widely used in real-world applications. TAMA employs approximate matching to provide fast event matching against an enormous amount of subscriptions. To this end, TAMA uses a hierarchical indexing table to store subscriptions. Event matching in TAMA becomes the query to this table, which is substantially faster than traditional methods. In addition, the false positive rate of matching events in TAMA can be adjusted by tuning the size of the matching table, which makes TAMA favorable in practice. We implement TAMA as a forwarding component in Siena and conduct extensive experiments with realistic settings. The results demonstrate that TAMA has a significantly faster event matching speed compared to existing methods, and only incurs a small fraction of false positives.},
  citationcount = {37},
  venue = {IEEE International Conference on Distributed Computing Systems},
  keywords = {query}
}

@article{zhuAnEfficientSolution2015,
  title = {An Efficient Solution for the Generalized Median Problem},
  author = {Zhu, Daxin and Wang, Xiaodong},
  year = {2015},
  doi = {10.1109/CSCI.2015.8},
  abstract = {A generalized median finding problem is studied in this paper. While there are several theoretical solutions to the problem, only a few have been tried out, and there is little idea on how the others would perform. The computation model used in this paper is the RAM model with word-size T(log n). Our data structure is a practical linear space data structure that supports range selection queries in O(log n) time with O(n log n) preprocessing time.},
  citationcount = {Unknown},
  venue = {2015 International Conference on Computational Science and Computational Intelligence (CSCI)},
  keywords = {data structure,query}
}

@article{zhuangImprovingMonteCarlo2015,
  title = {Improving {{Monte-Carlo}} Tree Search for Dots-and-Boxes with a Novel Board Representation and Artificial Neural Networks},
  author = {Zhuang, Yimeng and Li, Shuqin and Peters, Tom and Zhang, Chenguang},
  year = {2015},
  doi = {10.1109/CIG.2015.7317912},
  abstract = {Dots-and-Boxes is a well-known paper-and-pencil, game for two players. It reaches a high level of complexity, posing an interesting challenge for AI development. Previous, board representation techniques for Dots-and-Boxes rely on data, structures like arrays or linked lists to facilitate operations on the, board. These representation techniques usually lack for the ability, to incrementally update information required for efficient move, generation during search. To address this problem a novel board, representation for Dots-and-Boxes is proposed in this paper. It, utilizes game-specific knowledge to classify distinct conditions on, the board and its implementation is based on disjoint-sets. Besides, the novel board representation this paper treats optimizations for, Monte-Carlo Tree Search (MCTS) focusing on artificial neural, networks. Finally we implemented our proposed approach in a new program called QDab and conducted experiments showing, that the new board representation improves the efficiency of basic, operations on the board by more than 6 times. Further tests, against other implementations show the superior playing strength, of our approach.},
  citationcount = {9},
  venue = {IEEE Conference on Computational Intelligence and Games},
  keywords = {update}
}

@article{zhuEvaluatingTopN2016,
  title = {Evaluating {{Top-N}} Queries in n-Dimensional Normed Spaces},
  author = {Zhu, Liang and Liu, Feifei and Meng, W. and Ma, Qin and Wang, Yu and Yuan, Fang},
  year = {2016},
  doi = {10.1016/j.ins.2016.09.035},
  abstract = {No abstract available},
  citationcount = {4},
  venue = {Information Sciences},
  keywords = {query}
}
